commit cc2a8ff3d308c12373652fe02126799b4a34f008
Author: robcore <robpatershuk@gmail.com>
Date:   Mon May 30 18:16:17 2016 -0600

    a couple little tweaks

commit 7e9764a5da52fdf8714aa6010bb7e0ad5c49c705
Author: robcore <robpatershuk@gmail.com>
Date:   Mon May 30 09:25:55 2016 -0600

    From 3c878529cd99948de0746bdd4fdc488b3dd88d4f Mon Sep 17 00:00:00 2001
    From: Varun Chitre <varun.chitre15@gmail.com>
    Date: Mon, 26 Jan 2015 02:48:17 -0500
    Subject: [PATCH] fixup mdp overlay for taoshan
    
    Absence of ION_IOMMU_UNMAP_DELAYED flag causes iommu page faults when trying to map the ion buffer with new length
    Effects: causes heavy frame drops the first time the buffer is mapped
    This patch fixes the issue
    
    Change-Id: Ic87c781f36c36d6d8d7db17016ed627eaf4fa947
    Signed-off-by: Varun Chitre <varun.chitre15@gmail.com>
    ---
     drivers/video/msm/mdp4_overlay.c | 34 ++++++----------------------------
     1 file changed, 6 insertions(+), 28 deletions(-)
    
    diff --git a/drivers/video/msm/mdp4_overlay.c b/drivers/video/msm/mdp4_overlay.c
    index 77822ce..b869f52 100644
    --- a/drivers/video/msm/mdp4_overlay.c
    +++ b/drivers/video/msm/mdp4_overlay.c
    @@ -319,8 +319,6 @@ int mdp4_overlay_iommu_map_buf(int mem_id,
     	struct ion_handle **srcp_ihdl)
     {
     	struct mdp4_iommu_pipe_info *iom;
    -	unsigned long size = 0, map_size = 0;
    -	int ret;
    
     	if (!display_iclient)
     		return -EINVAL;
    @@ -334,32 +332,12 @@ int mdp4_overlay_iommu_map_buf(int mem_id,
     	pr_debug("mixer %u, pipe %u, plane %u\n", pipe->mixer_num,
     		pipe->pipe_ndx, plane);
    
    -	if(mdp4_overlay_format2type(pipe->src_format) == OVERLAY_TYPE_RGB) {
    -		ret = ion_handle_get_size(display_iclient, *srcp_ihdl, &size);
    -		if (ret)
    -			pr_err("ion_handle_get_size failed with ret %d\n", ret);
    -		map_size = mdp_iommu_max_map_size;
    -		if(map_size < size)
    -			map_size = size;
    -
    -		if (ion_map_iommu(display_iclient, *srcp_ihdl,
    -				DISPLAY_READ_DOMAIN, GEN_POOL, SZ_4K, map_size, start,
    -				len, 0, 0)) {
    -			ion_free(display_iclient, *srcp_ihdl);
    -			pr_err("%s(): ion_map_iommu() failed\n",
    -					__func__);
    -			return -EINVAL;
    -		}
    -	} else {
    -
    -		if (ion_map_iommu(display_iclient, *srcp_ihdl,
    -				DISPLAY_READ_DOMAIN, GEN_POOL, SZ_4K, 0, start,
    -				len, 0, 0)) {
    -			ion_free(display_iclient, *srcp_ihdl);
    -			pr_err("%s(): ion_map_iommu() failed\n",
    -					__func__);
    -			return -EINVAL;
    -		}
    +	if (ion_map_iommu(display_iclient, *srcp_ihdl,
    +		DISPLAY_READ_DOMAIN, GEN_POOL, SZ_4K, 0, start,
    +		len, 0, ION_IOMMU_UNMAP_DELAYED)) {
    +		ion_free(display_iclient, *srcp_ihdl);
    +		pr_err("ion_map_iommu() failed\n");
    +		return -EINVAL;
     	}
     	mutex_lock(&iommu_mutex);
     	iom = &pipe->iommu;

commit 39fafcaf055aebc3af36455bc8da5f3607d38c61
Author: robcore <robpatershuk@gmail.com>
Date:   Mon May 30 08:29:41 2016 -0600

    lcd-notify- correct jf attribute to fix definition mismatch

commit cc6a23a316a936e1c254dfd99829ef5c3b083200
Author: robcore <robpatershuk@gmail.com>
Date:   Mon May 30 07:49:05 2016 -0600

    try out lcd_notify and fix derp

commit d538c4a97e63b371f008722e1982da1b794a267d
Author: robcore <robpatershuk@gmail.com>
Date:   Mon May 30 06:45:20 2016 -0600

    disable triple buffer and bump version

commit a32e060bf5d6c9afbfd4d5c925eeb2a6acc386f4
Author: robcore <robpatershuk@gmail.com>
Date:   Mon May 30 06:39:30 2016 -0600

    ext4-fix-memory-leak-in-xattr

commit de4692f847815d9cad2f225bb9f7a1b97840dc92
Author: robcore <robpatershuk@gmail.com>
Date:   Mon May 30 06:39:28 2016 -0600

    added msm-kcal support, including lcdlut updates

commit 7524c2147900c3852872e0553c0f233d71039be3
Author: robcore <robpatershuk@gmail.com>
Date:   Mon May 30 06:18:37 2016 -0600

    make backlight dimmer adjustable and disabled by default

commit 8ac944748661d3a1d0cd93b6682004f7d57fae38
Author: robcore <robpatershuk@gmail.com>
Date:   Mon May 30 06:16:59 2016 -0600

    backlight_dimmer-a-bit-less-dim-and-disable-by-default

commit 4a8fddfcd599ac6869ca17ad4f215dbd0b43120b
Author: robcore <robpatershuk@gmail.com>
Date:   Mon May 30 06:16:13 2016 -0600

    backlight dimmer option

commit 1a08d44e9c45662fcd8fd712a53e6ca34753da78
Author: robcore <robpatershuk@gmail.com>
Date:   Sun May 29 15:18:25 2016 -0600

    gpu-revert bw AGAIN, gpu hates it

commit e57cd45b399cd1acc88ad7bbbe3be41257898ec3
Author: robcore <robpatershuk@gmail.com>
Date:   Sun May 29 14:03:55 2016 -0600

    revert memory, net and selinux commits for now, also disable arch power by default and bump version

commit 0d9aab43353ebc692115d71f571b7ec4cf44f49a
Author: robcore <robpatershuk@gmail.com>
Date:   Sun May 29 11:45:06 2016 -0600

    remove edf, terrible failure

commit e994752d7c6e078f0dd2c6de9c21b667c1a0ec54
Author: robcore <robpatershuk@gmail.com>
Date:   Sun May 29 11:08:17 2016 -0600

    modded cleaning script

commit 806f1400ed73d287828d0e705930729d1b5fc502
Author: robcore <robpatershuk@gmail.com>
Date:   Sun May 29 11:06:31 2016 -0600

    revert non-samsung firmware update capability

commit 667c1e5ce2cc4bb9bf75948196e20c4b848e0154
Author: robcore <robpatershuk@gmail.com>
Date:   Sun May 29 11:05:55 2016 -0600

    reverting some gpu commits

commit fbf0bb482ec885d60910a2e7cd0a2ac79ae65838
Author: robcore <robpatershuk@gmail.com>
Date:   Sun May 29 10:52:16 2016 -0600

    re-engineered edf to meet updated coding standards

commit 2a831740489181cd7cc82eee7ab6f395cbb7d5a4
Author: robcore <robpatershuk@gmail.com>
Date:   Sun May 29 07:50:13 2016 -0600

    added edf-or earliest deadline first scheduler

commit bb2f283ee277c5c59c64d138f02a1bb57837e647
Author: robcore <robpatershuk@gmail.com>
Date:   Sun May 29 07:41:36 2016 -0600

    update to 2.6, enable gpu-oc, need it just for now

commit ee7d20240329213e6e0b24d024bcd3d9b7a2e0be
Author: robcore <robpatershuk@gmail.com>
Date:   Sun May 29 06:49:03 2016 -0600

    disable gpu oc, too unstable in its current form

commit e35986df5e10904ac06f48cae3dd29cfa19490f2
Author: robcore <robpatershuk@gmail.com>
Date:   Sun May 29 06:36:45 2016 -0600

    seq_file: always update file->f_pos in seq_lseek()

commit f2275f12e405b3567ecbe7f9e9d9d8ed854e5393
Author: robcore <robpatershuk@gmail.com>
Date:   Sun May 29 06:29:36 2016 -0600

    JC-allow-non-samsung-app-to-update-firmware

commit 6644d80b5f9438b0dbc7ba5505ea0fd4322f48ce
Author: robcore <robpatershuk@gmail.com>
Date:   Sun May 29 05:41:14 2016 -0600

    msm_fb-Remove-the-extra-freelist-in-display

commit a059dba4bb52f0413832c12e47c0cfff8df28828
Author: robcore <robpatershuk@gmail.com>
Date:   Sun May 29 05:30:05 2016 -0600

    staging: android: ashmem: Avoid deadlock with mmap/shrink

commit f8832bdf5554f80058ee1e5901712062fd447b0b
Author: robcore <robpatershuk@gmail.com>
Date:   Sun May 29 05:30:04 2016 -0600

    SELinux: Fix possible NULL pointer dereference in selinux_inode_permission

commit 95f5a48a2e797d2e259486fcf30e63cbcced5e1e
Author: robcore <robpatershuk@gmail.com>
Date:   Sun May 29 05:14:58 2016 -0600

    ANDROID: mmc: move to a SCHED_FIFO thread

commit e15280f9b37eafd9e40436dedb5996a2c922b715
Author: robcore <robpatershuk@gmail.com>
Date:   Sun May 29 05:14:56 2016 -0600

    ANDROID: dm-crypt: run in a WQ_HIGHPRI workqueue

commit 2e1b8b672d2ab630a67e23bf2abdbaf186796c97
Author: robcore <robpatershuk@gmail.com>
Date:   Sun May 29 04:05:04 2016 -0600

    charger - increased siop level

commit daf464cdbbb100b696295c6aaeafe6d80819b666
Author: robcore <robpatershuk@gmail.com>
Date:   Sun May 29 03:58:27 2016 -0600

    msm video: check for code code robustness-null pointer and array out of bounds

commit 67d57b1fa134a29786b31ba2612b96eb09b88472
Author: robcore <robpatershuk@gmail.com>
Date:   Sun May 29 03:53:48 2016 -0600

    mm: reorder mlock to fix audit denial

commit 3c0545874634423d35dab560c0a7a91d9eb6381f
Author: robcore <robpatershuk@gmail.com>
Date:   Sun May 29 03:42:34 2016 -0600

    net: add length argument to skb_copy_and_csum_datagram_iovec

commit ab584a8efe8b37aeb87551a93bb3c23b3ecef412
Author: robcore <robpatershuk@gmail.com>
Date:   Sat May 28 16:30:52 2016 -0600

    disable cpu overclock for stability testing

commit b6cdaf5bce09ccce4c155d2d21828204aa231a37
Author: robcore <robpatershuk@gmail.com>
Date:   Sat May 28 16:26:01 2016 -0600

    firmware-touch-try out newer version once again

commit 607787068223c88b149c24c64e2b483e17d95036
Author: robcore <robpatershuk@gmail.com>
Date:   Sat May 28 16:03:47 2016 -0600

    kgsl tweaking-revert hot pink underlay, fix small sammy source error

commit f71ee3b31092decaadc6b521bf630fb6d0f6a0f6
Author: robcore <robpatershuk@gmail.com>
Date:   Sat May 28 14:37:33 2016 -0600

    WHOOOO!

commit cd29e293117accdb62947a6d9a0f0199b2720e2a
Author: robcore <robpatershuk@gmail.com>
Date:   Sat May 28 13:21:36 2016 -0600

    oops

commit 208990d40c2796653ede7941a11d2e3697ae58f1
Author: robcore <robpatershuk@gmail.com>
Date:   Sat May 28 12:52:01 2016 -0600

    minor script changes

commit e6d9c497e4d0d4a61ec60bec9e303a7976bcbc0e
Author: robcore <robpatershuk@gmail.com>
Date:   Sat May 28 12:44:28 2016 -0600

    VITAL build script changes, some more sound tweaking

commit d6fdc0dbbce003e94bad6dc8f7c9a5af7904b967
Author: robcore <robpatershuk@gmail.com>
Date:   Sat May 28 12:29:35 2016 -0600

    a couple of audio patches, amic-LDOH connections and DMIC recording support

commit 4db3e7a4c6257f07eec0fac3fa0106fb99bc869b
Author: robcore <robpatershuk@gmail.com>
Date:   Sat May 28 12:28:29 2016 -0600

    cmon, boot for me baby

commit 5dd7174208a5ca66b67f51a628d863959ffc5442
Author: robcore <robpatershuk@gmail.com>
Date:   Fri May 27 09:52:46 2016 -0600

    disable bricked for now

commit 68ce32f9f4fff4367688cc645bcadb91589b89df
Author: robcore <robpatershuk@gmail.com>
Date:   Fri May 27 08:57:52 2016 -0600

    just a few cpufreq reverts

commit d1fddcc23b1b5225e5ae689f9c5be21bf57c5916
Author: robcore <robpatershuk@gmail.com>
Date:   Fri May 27 08:43:32 2016 -0600

    reverting some of tkkgs newer updates

commit c6132fda243e6ecb33033e59e4d04bca61b10238
Author: robcore <robpatershuk@gmail.com>
Date:   Fri May 27 07:29:38 2016 -0600

    reverted to 109, updates seem unneccessary with tkkgs improvements

commit 9fcbc905442ec1f2b61f4ac975d62b5069502403
Author: robcore <robpatershuk@gmail.com>
Date:   Fri May 27 05:47:27 2016 -0600

    revert 3.4.112, this is not the first android kernel build it has broken

commit 41e830fda59ed42dbaf7ac5d837e43e1ac337ce8
Author: robcore <robpatershuk@gmail.com>
Date:   Thu May 26 18:37:03 2016 -0600

    fixed build script and hopefully Makefiles

commit 492369c3b2055fda3a4662964fca551d023de16c
Author: robcore <robpatershuk@gmail.com>
Date:   Thu May 26 18:08:06 2016 -0600

    wcd9xxx-core fixed undefined ref

commit 03447f6cd86c0200c6ff81fce73599b1d6a17109
Author: robcore <robpatershuk@gmail.com>
Date:   Thu May 26 17:27:48 2016 -0600

    bloody gcc

commit 22997dd301265a024c72a098eeb48a597e495889
Author: robcore <robpatershuk@gmail.com>
Date:   Thu May 26 17:14:35 2016 -0600

    (hopefully) fixed intellithermal build error

commit 6b39a931a0621b201110a63eb20801b47a382d6d
Author: robcore <robpatershuk@gmail.com>
Date:   Thu May 26 16:19:25 2016 -0600

    extra fuck off

commit 2510ffb025d5d0818e5029917748e8c5113d6d74
Author: robcore <robpatershuk@gmail.com>
Date:   Thu May 26 15:40:44 2016 -0600

    sound_control-i will play with you later

commit cf7694ef2b3c6d93c33067f02e3d4258bfa74da3
Author: robcore <robpatershuk@gmail.com>
Date:   Thu May 26 15:34:48 2016 -0600

    that should do it

commit 6cae3dbc45a2265d8256787b2df15d7adf5b4fb8
Author: robcore <robpatershuk@gmail.com>
Date:   Thu May 26 15:32:27 2016 -0600

    hopefully fixed wsizeof-pointer-access error

commit cc4dd55a9e18ce8bf0656b72342e1bb0dcae1fcb
Author: robcore <robpatershuk@gmail.com>
Date:   Thu May 26 14:45:49 2016 -0600

    fix build error, likely to upgrade that was already covered by tkkg enhancements

commit 9a064731daf6dbc951915cde00df6bf3e4599e29
Author: robcore <robpatershuk@gmail.com>
Date:   Thu May 26 14:39:00 2016 -0600

    freqs oops fixed

commit 05741b58209c8fe6cfe4ed39ed84044379e7a71a
Author: robcore <robpatershuk@gmail.com>
Date:   Thu May 26 14:11:57 2016 -0600

    audio-cpufreq-misc-some tweaks, some fixes, etc

commit 666f6df2217f1ce43692b981042511c074f4d21c
Author: robcore <robpatershuk@gmail.com>
Date:   Thu May 26 00:41:57 2016 -0600

    audio tweaks, crc check disabled, general rob-i-fying

commit 76d8125b66c06516193bfb1fbc38067490916b7b
Author: robcore <robpatershuk@gmail.com>
Date:   Wed May 25 21:39:12 2016 -0600

    incremental update to 112

commit 7c6135796558450fd4c9b07a7fb6d25b43d7947d
Author: robcore <robpatershuk@gmail.com>
Date:   Wed May 25 21:11:01 2016 -0600

    109-110

commit 3f239a57471fbff096613c3bd7bac7d1980fb57e
Author: robcore <robpatershuk@gmail.com>
Date:   Wed May 25 21:03:50 2016 -0600

    reverted to v2beta2 for now

commit 4bf5494be0305be23db50017dd15c13e11990772
Author: robcore <robpatershuk@gmail.com>
Date:   Tue May 24 21:29:12 2016 -0600

    merge the few updates from OK2 source drop that were relevant

commit e803d92453c45f08b390591a7d46b64c43f68be8
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Mon Dec 21 18:12:22 2015 +0100

    fix build warnings

commit d3edb92d67f2c6fba32f48d3cd63f0ecead2e34a
Author: Dan Pasanen <dan.pasanen@gmail.com>
Date:   Fri May 9 21:42:39 2014 -0500

    vmscan: fix mismerge
    
    Change-Id: Ic955cbf317860074c0a73f30b82413e45256a57d

commit b03ac70b9b539fd3821a7b0549b60c762d1b3341
Author: Olav Haugan <ohaugan@codeaurora.org>
Date:   Mon Nov 4 10:46:16 2013 -0800

    mm: vmscan: Move pages that fail swapout to LRU active list
    
    Move pages that fail swapout to the LRU active list to reduce
    pressure on swap device when swapping out is already failing.
    This helps when using a pseudo swap device such as zram which
    starts failing when memory is low.
    
    Change-Id: Ib136cd0a744378aa93d837a24b9143ee818c80b3
    Signed-off-by: Olav Haugan <ohaugan@codeaurora.org>

commit d1f66ffdab42f60b61a8d6a1932a9e0a357db00a
Author: Subbaraman Narayanamurthy <subbaram@codeaurora.org>
Date:   Fri Sep 20 15:23:56 2013 -0700

    debug-pagealloc: Panic on pagealloc corruption
    
    Currently, we just print the pagealloc corruption warnings and
    proceed. Sometimes, we are getting multiple errors printed down
    the line. It will be good to get the device state as early as
    possible when we get the first pagealloc error.
    
    Change-Id: I79155ac8a039b30a3a98d5dd1384d3923082712f
    Signed-off-by: Subbaraman Narayanamurthy <subbaram@codeaurora.org>

commit 123bbe01cce38f271e702f2c064a8da09b19df40
Author: Pushkar Joshi <pushkarj@codeaurora.org>
Date:   Wed Sep 11 11:23:26 2013 -0700

    mm: panic on the first bad page table entry access
    
    Sometimes having a number of bad page table entries precipitates in
    a crash much later. Because of this, we do not have any context for
    the point at which the first bad pte entry was encountered. Hence,
    panic on first such instance to help gather context for debug.
    
    Change-Id: Idddf2b977214eb1463d08e16630e98264b9af487
    Signed-off-by: Pushkar Joshi <pushkarj@codeaurora.org>

commit 398a0e50fae6f0917e6c5bf9fe8a127d3cba640a
Author: Christoph Lameter <cl@linux.com>
Date:   Wed Jan 23 21:45:48 2013 +0000

    slub: tid must be retrieved from the percpu area of the current processor
    
    As Steven Rostedt has pointer out: rescheduling could occur on a
    different processor after the determination of the per cpu pointer and
    before the tid is retrieved. This could result in allocation from the
    wrong node in slab_alloc().
    
    The effect is much more severe in slab_free() where we could free to the
    freelist of the wrong page.
    
    The window for something like that occurring is pretty small but it is
    possible.
    
    Change-Id: I9e658e97ed2096f658d6e67739764a86151f13e3
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>
    Git-commit: 7cccd80b4397699902aced1ad3d692d384aaab77
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Matt Wagantall <mattw@codeaurora.org>

commit 55bdc1eb0b12c1594da6b552295842b914068f04
Author: Marek Szyprowski <m.szyprowski@samsung.com>
Date:   Fri Apr 13 12:32:09 2012 +0200

    mm: vmalloc: use const void * for caller argument
    
    'const void *' is a safer type for caller function type. This patch
    updates all references to caller function type.
    
    Change-Id: If950cfcfc63911756ac3709c8bf6da10c8b98f1b
    Signed-off-by: Marek Szyprowski <m.szyprowski@samsung.com>
    Reviewed-by: Kyungmin Park <kyungmin.park@samsung.com>
    Reviewed-by: Minchan Kim <minchan@kernel.org>
    Git-commit: 5e6cafc83e30f0f70c79a2b7aef237dc57e29f02
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit 5e3086be70e8a6c3c00eb216d392b24616e0758f
Author: Marek Szyprowski <m.szyprowski@samsung.com>
Date:   Mon Jul 30 09:11:33 2012 +0200

    ARM: dma-mapping: remove custom consistent dma region
    
    This patch changes dma-mapping subsystem to use generic vmalloc areas
    for all consistent dma allocations. This increases the total size limit
    of the consistent allocations and removes platform hacks and a lot of
    duplicated code.
    
    Atomic allocations are served from special pool preallocated on boot,
    because vmalloc areas cannot be reliably created in atomic context.
    
    Change-Id: Ibb2230e80249598a81122083bf3fa2f050a0a71e
    Signed-off-by: Marek Szyprowski <m.szyprowski@samsung.com>
    Reviewed-by: Kyungmin Park <kyungmin.park@samsung.com>
    Reviewed-by: Minchan Kim <minchan@kernel.org>
    Git-commit: e9da6e9905e639b0f842a244bc770b48ad0523e9
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    [lauraa@codeaurora.org: Context fixups and tweaking of some prototypes]
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit a75295f06217d84bb86455ed1b0709323a76c2bf
Author: Laura Abbott <lauraa@codeaurora.org>
Date:   Thu Aug 22 13:46:07 2013 -0700

    mm: Update is_vmalloc_addr to account for vmalloc savings
    
    is_vmalloc_addr current assumes that all vmalloc addresses
    exist between VMALLOC_START and VMALLOC_END. This may not be
    the case when interleaving vmalloc and lowmem. Update the
    is_vmalloc_addr to properly check for this.
    
    Change-Id: I5def3d6ae1a4de59ea36f095b8c73649a37b1f36
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit 852086277b6cecd7d4cdc727fa8eea1754143e50
Author: Laura Abbott <lauraa@codeaurora.org>
Date:   Fri Jun 28 12:52:17 2013 -0700

    mm: Remove __init annotations from free_bootmem_late
    
    free_bootmem_late is currently set up to only be used in init
    functions. Some clients need to use this function past initcalls.
    The functions themselves have no restrictions on being used later
    minus the __init annotations so remove the annotation.
    
    Change-Id: I7c7e15cf2780a8843ebb4610da5b633c9abb0b3d
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit 6a2bc8108d10684218a893ec2bec4e962fb7baac
Author: Neeti Desai <neetid@codeaurora.org>
Date:   Fri Jun 14 17:39:33 2013 -0700

    msm: Increase the kernel virtual area to include lowmem
    
    Even though lowmem is accounted for in vmalloc space, allocation
    comes only from the region bounded by VMALLOC_START and VMALLOC_END.
    The kernel virtual area can now allocate from any unmapped region
    starting from PAGE_OFFSET.
    
    Change-Id: I291b9eb443d3f7445fd979bd7b09e9241ff22ba3
    Signed-off-by: Neeti Desai <neetid@codeaurora.org>

commit bb063728500e2b174291127729ed56735de62028
Author: Neeti Desai <neetid@codeaurora.org>
Date:   Mon Jun 10 17:14:21 2013 -0700

    msm: Allow lowmem to be non contiguous and mixed.
    
    Any image that is expected to have a lifetime of
    the entire system can give the virtual address
    space back for use in vmalloc.
    
    Change-Id: I81ce848cd37e8573d706fa5d1aa52147b3c8da12
    Signed-off-by: Neeti Desai <neetid@codeaurora.org>

commit 4bc123d9c035c4d5f3288deb805e9de955866547
Author: Minchan Kim <minchan@kernel.org>
Date:   Wed Jul 3 15:01:24 2013 -0700

    mm: remove compressed copy from zram in-memory
    
    Swap subsystem does lazy swap slot free with expecting the page would be
    swapped out again so we can avoid unnecessary write.
    
    But the problem in in-memory swap(ex, zram) is that it consumes memory
    space until vm_swap_full(ie, used half of all of swap device) condition
    meet.  It could be bad if we use multiple swap device, small in-memory
    swap and big storage swap or in-memory swap alone.
    
    This patch makes swap subsystem free swap slot as soon as swap-read is
    completed and make the swapcache page dirty so the page should be
    written out the swap device to reclaim it.  It means we never lose it.
    
    I tested this patch with kernel compile workload.
    
    1. before
    
       compile time : 9882.42
       zram max wasted space by fragmentation: 13471881 byte
       memory space consumed by zram: 174227456 byte
       the number of slot free notify: 206684
    
    2. after
    
       compile time : 9653.90
       zram max wasted space by fragmentation: 11805932 byte
       memory space consumed by zram: 154001408 byte
       the number of slot free notify: 426972
    
    [akpm@linux-foundation.org: tweak comment text]
    [artem.savkov@gmail.com: fix BUG due to non-swapcache pages in end_swap_bio_read()]
    [akpm@linux-foundation.org: invert unlikely() test, augment comment, 80-col cleanup]
    Signed-off-by: Dan Magenheimer <dan.magenheimer@oracle.com>
    Signed-off-by: Minchan Kim <minchan@kernel.org>
    Signed-off-by: Artem Savkov <artem.savkov@gmail.com>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: Seth Jennings <sjenning@linux.vnet.ibm.com>
    Cc: Nitin Gupta <ngupta@vflare.org>
    Cc: Konrad Rzeszutek Wilk <konrad@darnok.org>
    Cc: Shaohua Li <shli@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    
    Conflicts:
    	mm/page_io.c
    
    Change-Id: I8f11fb7f0391954435002e7f290352f311c907d4

commit 482985351c9e53c988d627c3acf378747e8b97d5
Author: Johannes Weiner <hannes@cmpxchg.org>
Date:   Tue Dec 4 11:11:31 2012 -0500

    mm: vmscan: do not keep kswapd looping forever due to individual uncompactable zones
    
    When a zone meets its high watermark and is compactable in case of
    higher order allocations, it contributes to the percentage of the node's
    memory that is considered balanced.
    
    This requirement, that a node be only partially balanced, came about
    when kswapd was desparately trying to balance tiny zones when all bigger
    zones in the node had plenty of free memory.  Arguably, the same should
    apply to compaction: if a significant part of the node is balanced
    enough to run compaction, do not get hung up on that tiny zone that
    might never get in shape.
    
    When the compaction logic in kswapd is reached, we know that at least
    25% of the node's memory is balanced properly for compaction (see
    zone_balanced and pgdat_balanced).  Remove the individual zone checks
    that restart the kswapd cycle.
    
    Otherwise, we may observe more endless looping in kswapd where the
    compaction code loops back to reclaim because of a single zone and
    reclaim does nothing because the node is considered balanced overall.
    
    See for example
    
      https://bugzilla.redhat.com/show_bug.cgi?id=866988
    
    Change-Id: I9c8f180466fea814c3038fa019f317799ddc0bcb
    Signed-off-by: Johannes Weiner <hannes@cmpxchg.org>
    Reported-and-tested-by: Thorsten Leemhuis <fedora@leemhuis.info>
    Reported-by: Jiri Slaby <jslaby@suse.cz>
    Tested-by: John Ellson <john.ellson@comcast.net>
    Tested-by: Zdenek Kabelac <zkabelac@redhat.com>
    Tested-by: Bruno Wolff III <bruno@wolff.to>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Git-commit: c702418f8a2fa6cc92e84a39880d458faf7af9cc
    Git-repo: https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/
    Signed-off-by: Chintan Pandya <cpandya@codeaurora.org>

commit edd11a00fc2964c8d8b8f6e0ac9c8326012812bd
Author: Minchan Kim <minchan@kernel.org>
Date:   Thu Dec 20 15:05:06 2012 -0800

    compaction: fix build error in CMA && !COMPACTION
    
    isolate_freepages_block() and isolate_migratepages_range() are used for
    CMA as well as compaction so it breaks build for CONFIG_CMA &&
    !CONFIG_COMPACTION.
    
    This patch fixes it.
    
    Change-Id: I5163a4f20cc2002e19f87d1652ccabd75694b644
    [akpm@linux-foundation.org: add "do { } while (0)", per Mel]
    Signed-off-by: Minchan Kim <minchan@kernel.org>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Marek Szyprowski <m.szyprowski@samsung.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Git-commit: 010fc29a45a2e8dbc08bf45ef80b8622619aaae0
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit 7378636b3067c29da44232b40c0867d40f154ecb
Author: Mel Gorman <mgorman@suse.de>
Date:   Fri Jan 11 14:32:16 2013 -0800

    mm: compaction: partially revert capture of suitable high-order page
    
    Eric Wong reported on 3.7 and 3.8-rc2 that ppoll() got stuck when
    waiting for POLLIN on a local TCP socket.  It was easier to trigger if
    there was disk IO and dirty pages at the same time and he bisected it to
    commit 1fb3f8ca0e92 ("mm: compaction: capture a suitable high-order page
    immediately when it is made available").
    
    The intention of that patch was to improve high-order allocations under
    memory pressure after changes made to reclaim in 3.6 drastically hurt
    THP allocations but the approach was flawed.  For Eric, the problem was
    that page->pfmemalloc was not being cleared for captured pages leading
    to a poor interaction with swap-over-NFS support causing the packets to
    be dropped.  However, I identified a few more problems with the patch
    including the fact that it can increase contention on zone->lock in some
    cases which could result in async direct compaction being aborted early.
    
    In retrospect the capture patch took the wrong approach.  What it should
    have done is mark the pageblock being migrated as MIGRATE_ISOLATE if it
    was allocating for THP and avoided races that way.  While the patch was
    showing to improve allocation success rates at the time, the benefit is
    marginal given the relative complexity and it should be revisited from
    scratch in the context of the other reclaim-related changes that have
    taken place since the patch was first written and tested.  This patch
    partially reverts commit 1fb3f8ca0e92 ("mm: compaction: capture a
    suitable high-order page immediately when it is made available").
    
    Change-Id: I985725a72aac0fdecbf4310c04d176f39e0386dd
    Reported-and-tested-by: Eric Wong <normalperson@yhbt.net>
    Tested-by: Eric Dumazet <eric.dumazet@gmail.com>
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Cc: David Miller <davem@davemloft.net>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Git-commit: 8fb74b9fb2b182d54beee592350d9ea1f325917a
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit 271252c68a6634a81e17a5b011f0819f7d5169e1
Author: Marek Szyprowski <m.szyprowski@samsung.com>
Date:   Tue Dec 11 16:02:57 2012 -0800

    mm: cma: skip watermarks check for already isolated blocks in split_free_page()
    
    Since commit 2139cbe627b8 ("cma: fix counting of isolated pages") free
    pages in isolated pageblocks are not accounted to NR_FREE_PAGES counters,
    so watermarks check is not required if one operates on a free page in
    isolated pageblock.
    
    Change-Id: Id9b38d2e4e504336e11274a30044e3aacbc37d03
    Signed-off-by: Marek Szyprowski <m.szyprowski@samsung.com>
    Cc: Kyungmin Park <kyungmin.park@samsung.com>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Mel Gorman <mel@csn.ul.ie>
    Acked-by: Michal Nazarewicz <mina86@mina86.com>
    Cc: Minchan Kim <minchan@kernel.org>
    Cc: Bartlomiej Zolnierkiewicz <b.zolnierkie@samsung.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Git-commit: 2e30abd1730751d58463d88bc0844ab8fd7112a9
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    [lauraa@codeaurora.org: Context fixups due to being merged
    out of order. Bring over an added 'feature' where we don't
    obey watermarks for CMA pages]
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit 37177c6a6fc63e6e4aa5ce333b2eec0ce8af714e
Author: Mel Gorman <mgorman@suse.de>
Date:   Fri Oct 19 12:00:10 2012 +0100

    mm: compaction: Add scanned and isolated counters for compaction
    
    Compaction already has tracepoints to count scanned and isolated pages
    but it requires that ftrace be enabled and if that information has to be
    written to disk then it can be disruptive. This patch adds vmstat counters
    for compaction called compact_migrate_scanned, compact_free_scanned and
    compact_isolated.
    
    With these counters, it is possible to define a basic cost model for
    compaction. This approximates of how much work compaction is doing and can
    be compared that with an oprofile showing TLB misses and see if the cost of
    compaction is being offset by THP for example. Minimally a compaction patch
    can be evaluated in terms of whether it increases or decreases cost. The
    basic cost model looks like this
    
    Fundamental unit u:	a word	sizeof(void *)
    
    Ca  = cost of struct page access = sizeof(struct page) / u
    
    Cmc = Cost migrate page copy = (Ca + PAGE_SIZE/u) * 2
    Cmf = Cost migrate failure   = Ca * 2
    Ci  = Cost page isolation    = (Ca + Wi)
    	where Wi is a constant that should reflect the approximate
    	cost of the locking operation.
    
    Csm = Cost migrate scanning = Ca
    Csf = Cost free    scanning = Ca
    
    Overall cost =	(Csm * compact_migrate_scanned) +
    	      	(Csf * compact_free_scanned)    +
    	      	(Ci  * compact_isolated)	+
    		(Cmc * pgmigrate_success)	+
    		(Cmf * pgmigrate_failed)
    
    Where the values are read from /proc/vmstat.
    
    This is very basic and ignores certain costs such as the allocation cost
    to do a migrate page copy but any improvement to the model would still
    use the same vmstat counters.
    
    Change-Id: I9db1a609fc86a95e3fd8d3774de994197ecb9adf
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Reviewed-by: Rik van Riel <riel@redhat.com>
    Git-commit: 397487db696cae0b026a474a5cd66f4e372995e6
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit 6473605aad720c17943b8903bb0bbbeb64e5e7b8
Author: Mel Gorman <mgorman@suse.de>
Date:   Fri Oct 19 10:46:20 2012 +0100

    mm: compaction: Move migration fail/success stats to migrate.c
    
    The compact_pages_moved and compact_pagemigrate_failed events are
    convenient for determining if compaction is active and to what
    degree migration is succeeding but it's at the wrong level. Other
    users of migration may also want to know if migration is working
    properly and this will be particularly true for any automated
    NUMA migration. This patch moves the counters down to migration
    with the new events called pgmigrate_success and pgmigrate_fail.
    The compact_blocks_moved counter is removed because while it was
    useful for debugging initially, it's worthless now as no meaningful
    conclusions can be drawn from its value.
    
    Change-Id: I43d66b61a2a6be571ed025213d7f1b9defb1a18f
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Reviewed-by: Rik van Riel <riel@redhat.com>
    Git-commit: 5647bc293ab15f66a7b1cda850c5e9d162a6c7c2
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit e26286804128586b9b69a7948993612f065e9698
Author: Mel Gorman <mgorman@suse.de>
Date:   Thu Dec 6 19:01:14 2012 +0000

    mm: compaction: validate pfn range passed to isolate_freepages_block
    
    Commit 0bf380bc70ec ("mm: compaction: check pfn_valid when entering a
    new MAX_ORDER_NR_PAGES block during isolation for migration") added a
    check for pfn_valid() when isolating pages for migration as the scanner
    does not necessarily start pageblock-aligned.
    
    Since commit c89511ab2f8f ("mm: compaction: Restart compaction from near
    where it left off"), the free scanner has the same problem.  This patch
    makes sure that the pfn range passed to isolate_freepages_block() is
    within the same block so that pfn_valid() checks are unnecessary.
    
    In answer to Henrik's wondering why others have not reported this:
    reproducing this requires a large enough hole with the right aligment to
    have compaction walk into a PFN range with no memmap.  Size and
    alignment depends in the memory model - 4M for FLATMEM and 128M for
    SPARSEMEM on x86.  It needs a "lucky" machine.
    
    Change-Id: I5de91fc8d9792cb339b87fe9ef058143cde5995a
    Reported-by: Henrik Rydberg <rydberg@euromail.se>
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Git-commit: 60177d31d215bc2b4c5a7aa6f742800e04fa0a92
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit e3f4fe3b05f08caf9d4cf63d0f4af2bcc4d7908a
Author: Mel Gorman <mgorman@suse.de>
Date:   Fri Oct 19 13:56:57 2012 -0700

    mm: compaction: correct the nr_strict va isolated check for CMA
    
    Thierry reported that the "iron out" patch for isolate_freepages_block()
    had problems due to the strict check being too strict with "mm:
    compaction: Iron out isolate_freepages_block() and
    isolate_freepages_range() -fix1".  It's possible that more pages than
    necessary are isolated but the check still fails and I missed that this
    fix was not picked up before RC1.  This same problem has been identified
    in 3.7-RC1 by Tony Prisk and should be addressed by the following patch.
    
    Change-Id: I2aefac60b55ee48cf3f9f31dbd7894c024e64f28
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Tested-by: Tony Prisk <linux@prisktech.co.nz>
    Reported-by: Thierry Reding <thierry.reding@avionic-design.de>
    Acked-by: Rik van Riel <riel@redhat.com>
    Acked-by: Minchan Kim <minchan@kernel.org>
    Cc: Richard Davies <richard@arachsys.com>
    Cc: Shaohua Li <shli@kernel.org>
    Cc: Avi Kivity <avi@redhat.com>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Git-commit: 0db63d7e25f96e2c6da925c002badf6f144ddf30
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit 522479856141b738e993ffcfa41b9648cc73d9f9
Author: Minchan Kim <minchan@kernel.org>
Date:   Mon Oct 8 16:33:48 2012 -0700

    CMA: migrate mlocked pages
    
    Presently CMA cannot migrate mlocked pages so it ends up failing to allocate
    contiguous memory space.
    
    This patch makes mlocked pages be migrated out.  Of course, it can affect
    realtime processes but in CMA usecase, contiguous memory allocation failing
    is far worse than access latency to an mlocked page being variable while
    CMA is running.  If someone wants to make the system realtime, he shouldn't
    enable CMA because stalls can still happen at random times.
    
    Change-Id: I560f43fdeb94f8fd2a4cc9e2ac12a1593ca38ecb
    [akpm@linux-foundation.org: tweak comment text, per Mel]
    Signed-off-by: Minchan Kim <minchan@kernel.org>
    Acked-by: Mel Gorman <mgorman@suse.de>
    Cc: Michal Nazarewicz <mina86@mina86.com>
    Cc: Bartlomiej Zolnierkiewicz <b.zolnierkie@samsung.com>
    Cc: Marek Szyprowski <m.szyprowski@samsung.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Git-commit: e46a28790e594c0876d1a84270926abf75460f61
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    [lauraa@codeaurora.org: Minor context fixups]
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit 36077750a30c2d2c38a57daa01849666a574f1ba
Author: Mel Gorman <mgorman@suse.de>
Date:   Mon Oct 8 16:32:47 2012 -0700

    mm: compaction: clear PG_migrate_skip based on compaction and reclaim activity
    
    Compaction caches if a pageblock was scanned and no pages were isolated so
    that the pageblocks can be skipped in the future to reduce scanning.  This
    information is not cleared by the page allocator based on activity due to
    the impact it would have to the page allocator fast paths.  Hence there is
    a requirement that something clear the cache or pageblocks will be skipped
    forever.  Currently the cache is cleared if there were a number of recent
    allocation failures and it has not been cleared within the last 5 seconds.
    Time-based decisions like this are terrible as they have no relationship
    to VM activity and is basically a big hammer.
    
    Unfortunately, accurate heuristics would add cost to some hot paths so
    this patch implements a rough heuristic.  There are two cases where the
    cache is cleared.
    
    1. If a !kswapd process completes a compaction cycle (migrate and free
       scanner meet), the zone is marked compact_blockskip_flush. When kswapd
       goes to sleep, it will clear the cache. This is expected to be the
       common case where the cache is cleared. It does not really matter if
       kswapd happens to be asleep or going to sleep when the flag is set as
       it will be woken on the next allocation request.
    
    2. If there have been multiple failures recently and compaction just
       finished being deferred then a process will clear the cache and start a
       full scan.  This situation happens if there are multiple high-order
       allocation requests under heavy memory pressure.
    
    The clearing of the PG_migrate_skip bits and other scans is inherently
    racy but the race is harmless.  For allocations that can fail such as THP,
    they will simply fail.  For requests that cannot fail, they will retry the
    allocation.  Tests indicated that scanning rates were roughly similar to
    when the time-based heuristic was used and the allocation success rates
    were similar.
    
    Change-Id: If690ae126badb9f9cc5632e9ffb9d376bf210fb0
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Richard Davies <richard@arachsys.com>
    Cc: Shaohua Li <shli@kernel.org>
    Cc: Avi Kivity <avi@redhat.com>
    Cc: Rafael Aquini <aquini@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Git-commit: 62997027ca5b3d4618198ed8b1aba40b61b1137b
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit be61219e5dcc955131598653832782d81e27bf51
Author: Mel Gorman <mgorman@suse.de>
Date:   Mon Oct 8 16:32:45 2012 -0700

    mm: compaction: Restart compaction from near where it left off
    
    This is almost entirely based on Rik's previous patches and discussions
    with him about how this might be implemented.
    
    Order > 0 compaction stops when enough free pages of the correct page
    order have been coalesced.  When doing subsequent higher order
    allocations, it is possible for compaction to be invoked many times.
    
    However, the compaction code always starts out looking for things to
    compact at the start of the zone, and for free pages to compact things to
    at the end of the zone.
    
    This can cause quadratic behaviour, with isolate_freepages starting at the
    end of the zone each time, even though previous invocations of the
    compaction code already filled up all free memory on that end of the zone.
     This can cause isolate_freepages to take enormous amounts of CPU with
    certain workloads on larger memory systems.
    
    This patch caches where the migration and free scanner should start from
    on subsequent compaction invocations using the pageblock-skip information.
     When compaction starts it begins from the cached restart points and will
    update the cached restart points until a page is isolated or a pageblock
    is skipped that would have been scanned by synchronous compaction.
    
    Change-Id: Ib788e0bfd803f5abdd8476b9e203a0b6420e194b
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Acked-by: Rik van Riel <riel@redhat.com>
    Cc: Richard Davies <richard@arachsys.com>
    Cc: Shaohua Li <shli@kernel.org>
    Cc: Avi Kivity <avi@redhat.com>
    Acked-by: Rafael Aquini <aquini@redhat.com>
    Cc: Fengguang Wu <fengguang.wu@intel.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Git-commit: c89511ab2f8fe2b47585e60da8af7fd213ec877e
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    [lauraa@codeaurora.org: Minor context fixup]
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit 0b4693a6d6a65b841884d991affda77fdb00d281
Author: Minchan Kim <minchan@kernel.org>
Date:   Tue Jul 31 16:43:01 2012 -0700

    mm: clean up __count_immobile_pages()
    
    The __count_immobile_pages() naming is rather awkward.  Choose a more
    clear name and add a comment.
    
    Change-Id: Ic1d8573bbc7eaa82dd5e3f9a1199ee6dd4ac9fc0
    Signed-off-by: Minchan Kim <minchan@kernel.org>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Michal Hocko <mhocko@suse.cz>
    Acked-by: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Bartlomiej Zolnierkiewicz <b.zolnierkie@samsung.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Git-commit: 80934513b230bfcf70265f2ef0fdae89fb391633
    Git-repo: git://git.kernel.org/cgit/linux/kernel/git/stable/linux-stable.git
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit 2c19c457aadc7c236a4f7df4b20e012a0fe94487
Author: Minchan Kim <minchan@kernel.org>
Date:   Tue Jul 31 16:42:59 2012 -0700

    mm: do not use page_count() without a page pin
    
    d179e84ba ("mm: vmscan: do not use page_count without a page pin") fixed
    this problem in vmscan.c but same problem is in __count_immobile_pages().
    
    I copy and paste d179e84ba's contents for description.
    
    "It is unsafe to run page_count during the physical pfn scan because
    compound_head could trip on a dangling pointer when reading
    page->first_page if the compound page is being freed by another CPU."
    
    Change-Id: I2b620c07adfa2c5a0f56219a11c042dc97428085
    Signed-off-by: Minchan Kim <minchan@kernel.org>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Michal Hocko <mhocko@suse.cz>
    Reviewed-by: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Wanpeng Li <liwp.linux@gmail.com>
    Cc: Bartlomiej Zolnierkiewicz <b.zolnierkie@samsung.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Git-commit: 97d255c816946388bab504122937730d3447c612
    Git-repo: git://git.kernel.org/cgit/linux/kernel/git/stable/linux-stable.git
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit 08339cd2923d20ec84d06be050bd294777d4f3b9
Author: Marek Szyprowski <m.szyprowski@samsung.com>
Date:   Thu Dec 20 15:05:18 2012 -0800

    mm: cma: WARN if freed memory is still in use
    
    Memory returned to free_contig_range() must have no other references.
    Let kernel to complain loudly if page reference count is not equal to 1.
    
    Change-Id: If1b84bb383d97eff441d7a1e18b874c64b7f5f85
    [rientjes@google.com: support sparsemem]
    Signed-off-by: Marek Szyprowski <m.szyprowski@samsung.com>
    Reviewed-by: Kyungmin Park <kyungmin.park@samsung.com>
    Acked-by: Michal Nazarewicz <mina86@mina86.com>
    Signed-off-by: David Rientjes <rientjes@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Git-commit: bcc2b02f4c1b36bc67272df7119b75bac78525ab
    Git-repo: git://git.kernel.org/cgit/linux/kernel/git/stable/linux-stable.git
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit eedd7f9df021504187fda4ffcfa6359a25f20896
Author: Mel Gorman <mgorman@suse.de>
Date:   Mon Oct 8 16:32:41 2012 -0700

    mm: compaction: cache if a pageblock was scanned and no pages were isolated
    
    When compaction was implemented it was known that scanning could
    potentially be excessive.  The ideal was that a counter be maintained for
    each pageblock but maintaining this information would incur a severe
    penalty due to a shared writable cache line.  It has reached the point
    where the scanning costs are a serious problem, particularly on
    long-lived systems where a large process starts and allocates a large
    number of THPs at the same time.
    
    Instead of using a shared counter, this patch adds another bit to the
    pageblock flags called PG_migrate_skip.  If a pageblock is scanned by
    either migrate or free scanner and 0 pages were isolated, the pageblock is
    marked to be skipped in the future.  When scanning, this bit is checked
    before any scanning takes place and the block skipped if set.
    
    The main difficulty with a patch like this is "when to ignore the cached
    information?" If it's ignored too often, the scanning rates will still be
    excessive.  If the information is too stale then allocations will fail
    that might have otherwise succeeded.  In this patch
    
    o CMA always ignores the information
    o If the migrate and free scanner meet then the cached information will
      be discarded if it's at least 5 seconds since the last time the cache
      was discarded
    o If there are a large number of allocation failures, discard the cache.
    
    The time-based heuristic is very clumsy but there are few choices for a
    better event.  Depending solely on multiple allocation failures still
    allows excessive scanning when THP allocations are failing in quick
    succession due to memory pressure.  Waiting until memory pressure is
    relieved would cause compaction to continually fail instead of using
    reclaim/compaction to try allocate the page.  The time-based mechanism is
    clumsy but a better option is not obvious.
    
    Change-Id: I17a4887aca9bb3d2d9d3756089ad7c9b89922727
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Acked-by: Rik van Riel <riel@redhat.com>
    Cc: Richard Davies <richard@arachsys.com>
    Cc: Shaohua Li <shli@kernel.org>
    Cc: Avi Kivity <avi@redhat.com>
    Acked-by: Rafael Aquini <aquini@redhat.com>
    Cc: Fengguang Wu <fengguang.wu@intel.com>
    Cc: Michal Nazarewicz <mina86@mina86.com>
    Cc: Bartlomiej Zolnierkiewicz <b.zolnierkie@samsung.com>
    Cc: Kyungmin Park <kyungmin.park@samsung.com>
    Cc: Mark Brown <broonie@opensource.wolfsonmicro.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Git-commit: bb13ffeb9f6bfeb301443994dfbf29f91117dfb3
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    [lauraa@codeaurora.org: Context fixup due to merging patches out of order]
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit ed0655061dd7ebb1a58591961023ae48669da5ad
Author: Mel Gorman <mgorman@suse.de>
Date:   Mon Oct 8 16:32:36 2012 -0700

    mm: compaction: acquire the zone->lock as late as possible
    
    Compaction's free scanner acquires the zone->lock when checking for
    PageBuddy pages and isolating them.  It does this even if there are no
    PageBuddy pages in the range.
    
    This patch defers acquiring the zone lock for as long as possible.  In the
    event there are no free pages in the pageblock then the lock will not be
    acquired at all which reduces contention on zone->lock.
    
    Change-Id: Iaf8b30f94e43b03ac93d751b7b97dbe0b76b87af
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Acked-by: Rik van Riel <riel@redhat.com>
    Cc: Richard Davies <richard@arachsys.com>
    Cc: Shaohua Li <shli@kernel.org>
    Cc: Avi Kivity <avi@redhat.com>
    Acked-by: Rafael Aquini <aquini@redhat.com>
    Acked-by: Minchan Kim <minchan@kernel.org>
    Tested-by: Peter Ujfalusi <peter.ujfalusi@ti.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Git-commit: f40d1e42bb988d2a26e8e111ea4c4c7bac819b7e
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit 59b0b91a34e4c4f30c4fbbfb6304d4005f16ac8d
Author: Mel Gorman <mgorman@suse.de>
Date:   Mon Oct 8 16:32:33 2012 -0700

    mm: compaction: acquire the zone->lru_lock as late as possible
    
    Richard Davies and Shaohua Li have both reported lock contention problems
    in compaction on the zone and LRU locks as well as significant amounts of
    time being spent in compaction.  This series aims to reduce lock
    contention and scanning rates to reduce that CPU usage.  Richard reported
    at https://lkml.org/lkml/2012/9/21/91 that this series made a big
    different to a problem he reported in August:
    
       http://marc.info/?l=kvm&m=134511507015614&w=2
    
    Patch 1 defers acquiring the zone->lru_lock as long as possible.
    
    Patch 2 defers acquiring the zone->lock as lock as possible.
    
    Patch 3 reverts Rik's "skip-free" patches as the core concept gets
    	reimplemented later and the remaining patches are easier to
    	understand if this is reverted first.
    
    Patch 4 adds a pageblock-skip bit to the pageblock flags to cache what
    	pageblocks should be skipped by the migrate and free scanners.
    	This drastically reduces the amount of scanning compaction has
    	to do.
    
    Patch 5 reimplements something similar to Rik's idea except it uses the
    	pageblock-skip information to decide where the scanners should
    	restart from and does not need to wrap around.
    
    I tested this on 3.6-rc6 + linux-next/akpm. Kernels tested were
    
    akpm-20120920	3.6-rc6 + linux-next/akpm as of Septeber 20th, 2012
    lesslock	Patches 1-6
    revert		Patches 1-7
    cachefail	Patches 1-8
    skipuseless	Patches 1-9
    
    Stress high-order allocation tests looked ok.  Success rates are more or
    less the same with the full series applied but there is an expectation
    that there is less opportunity to race with other allocation requests if
    there is less scanning.  The time to complete the tests did not vary that
    much and are uninteresting as were the vmstat statistics so I will not
    present them here.
    
    Using ftrace I recorded how much scanning was done by compaction and got this
    
                                3.6.0-rc6     3.6.0-rc6   3.6.0-rc6  3.6.0-rc6 3.6.0-rc6
                                akpm-20120920 lockless  revert-v2r2  cachefail skipuseless
    
    Total   free    scanned         360753976  515414028  565479007   17103281   18916589
    Total   free    isolated          2852429    3597369    4048601     670493     727840
    Total   free    efficiency        0.0079%    0.0070%    0.0072%    0.0392%    0.0385%
    Total   migrate scanned         247728664  822729112 1004645830   17946827   14118903
    Total   migrate isolated          2555324    3245937    3437501     616359     658616
    Total   migrate efficiency        0.0103%    0.0039%    0.0034%    0.0343%    0.0466%
    
    The efficiency is worthless because of the nature of the test and the
    number of failures.  The really interesting point as far as this patch
    series is concerned is the number of pages scanned.  Note that reverting
    Rik's patches massively increases the number of pages scanned indicating
    that those patches really did make a difference to CPU usage.
    
    However, caching what pageblocks should be skipped has a much higher
    impact.  With patches 1-8 applied, free page and migrate page scanning are
    both reduced by 95% in comparison to the akpm kernel.  If the basic
    concept of Rik's patches are implemened on top then scanning then the free
    scanner barely changed but migrate scanning was further reduced.  That
    said, tests on 3.6-rc5 indicated that the last patch had greater impact
    than what was measured here so it is a bit variable.
    
    One way or the other, this series has a large impact on the amount of
    scanning compaction does when there is a storm of THP allocations.
    
    This patch:
    
    Compaction's migrate scanner acquires the zone->lru_lock when scanning a
    range of pages looking for LRU pages to acquire.  It does this even if
    there are no LRU pages in the range.  If multiple processes are compacting
    then this can cause severe locking contention.  To make matters worse
    commit b2eef8c0 ("mm: compaction: minimise the time IRQs are disabled
    while isolating pages for migration") releases the lru_lock every
    SWAP_CLUSTER_MAX pages that are scanned.
    
    This patch makes two changes to how the migrate scanner acquires the LRU
    lock.  First, it only releases the LRU lock every SWAP_CLUSTER_MAX pages
    if the lock is contended.  This reduces the number of times it
    unnecessarily disables and re-enables IRQs.  The second is that it defers
    acquiring the LRU lock for as long as possible.  If there are no LRU pages
    or the only LRU pages are transhuge then the LRU lock will not be acquired
    at all which reduces contention on zone->lru_lock.
    
    Change-Id: If518f3acb4db4ba579b708889de0fa9f42366899
    [minchan@kernel.org: augment comment]
    [akpm@linux-foundation.org: tweak comment text]
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Acked-by: Rik van Riel <riel@redhat.com>
    Cc: Richard Davies <richard@arachsys.com>
    Cc: Shaohua Li <shli@kernel.org>
    Cc: Avi Kivity <avi@redhat.com>
    Acked-by: Rafael Aquini <aquini@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Git-commit: 2a1402aa044b55c2d30ab0ed9405693ef06fb07c
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit c03de5f21b56be7d525518c88435588bbef68528
Author: Mel Gorman <mgorman@suse.de>
Date:   Mon Oct 8 16:32:31 2012 -0700

    mm: compaction: Update try_to_compact_pages()kerneldoc comment
    
    Parameters were added without documentation, tut tut.
    
    Change-Id: I1355906b3a3a6e3319a0fedc8ba28c3327a0c8f2
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Git-commit: 661c4cb9b829110cb68c18ea05a56be39f75a4d2
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit 2622870dbe30b77ac24a406dd98a4312a6763562
Author: Mel Gorman <mgorman@suse.de>
Date:   Mon Oct 8 16:32:30 2012 -0700

    mm: compaction: move fatal signal check out of compact_checklock_irqsave
    
    Commit c67fe3752abe ("mm: compaction: Abort async compaction if locks
    are contended or taking too long") addressed a lock contention problem
    in compaction by introducing compact_checklock_irqsave() that effecively
    aborting async compaction in the event of compaction.
    
    To preserve existing behaviour it also moved a fatal_signal_pending()
    check into compact_checklock_irqsave() but that is very misleading.  It
    "hides" the check within a locking function but has nothing to do with
    locking as such.  It just happens to work in a desirable fashion.
    
    This patch moves the fatal_signal_pending() check to
    isolate_migratepages_range() where it belongs.  Arguably the same check
    should also happen when isolating pages for freeing but it's overkill.
    
    Change-Id: I026ab765b4160bdb6bbb8b7359be24b6159e382c
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Shaohua Li <shli@kernel.org>
    Cc: Minchan Kim <minchan@kernel.org>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Git-commit: 3cc668f4e30fbd97b3c0574d8cac7a83903c9bc7
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit 020f320b05931eef57fb9b0b194bf08ba479a6e8
Author: Shaohua Li <shli@kernel.org>
Date:   Mon Oct 8 16:32:27 2012 -0700

    mm: compaction: abort compaction loop if lock is contended or run too long
    
    isolate_migratepages_range() might isolate no pages if for example when
    zone->lru_lock is contended and running asynchronous compaction. In this
    case, we should abort compaction, otherwise, compact_zone will run a
    useless loop and make zone->lru_lock is even contended.
    
    An additional check is added to ensure that cc.migratepages and
    cc.freepages get properly drained whan compaction is aborted.
    
    Change-Id: Ia33b52655f9926d0bb2cde95492066bd8132149d
    [minchan@kernel.org: Putback pages isolated for migration if aborting]
    [akpm@linux-foundation.org: compact_zone_order requires non-NULL arg contended]
    [akpm@linux-foundation.org: make compact_zone_order() require non-NULL arg `contended']
    [minchan@kernel.org: Putback pages isolated for migration if aborting]
    Signed-off-by: Andrea Arcangeli <aarcange@redhat.com>
    Signed-off-by: Shaohua Li <shli@fusionio.com>
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Acked-by: Minchan Kim <minchan@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Git-commit: e64c5237cf6ff474cb2f3f832f48f2b441dd9979
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit e66e1ec2c535718ecf52129d8e0217722862b38a
Author: Mel Gorman <mgorman@suse.de>
Date:   Mon Oct 8 16:29:12 2012 -0700

    mm: compaction: capture a suitable high-order page immediately when it is made available
    
    While compaction is migrating pages to free up large contiguous blocks
    for allocation it races with other allocation requests that may steal
    these blocks or break them up.  This patch alters direct compaction to
    capture a suitable free page as soon as it becomes available to reduce
    this race.  It uses similar logic to split_free_page() to ensure that
    watermarks are still obeyed.
    
    Change-Id: I46fc38ca67bc50aa7a77a59255caf563f50343a9
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Reviewed-by: Rik van Riel <riel@redhat.com>
    Reviewed-by: Minchan Kim <minchan@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Git-commit: 1fb3f8ca0e9222535a39b884cb67a34628411b9f
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit 954f7a95d67ff99220b33b892e8a0495f503c72e
Author: Mel Gorman <mgorman@suse.de>
Date:   Mon Oct 8 16:29:09 2012 -0700

    mm: compaction: update comment in try_to_compact_pages
    
    Allocation success rates have been far lower since 3.4 due to commit
    fe2c2a106663 ("vmscan: reclaim at order 0 when compaction is enabled").
    This commit was introduced for good reasons and it was known in advance
    that the success rates would suffer but it was justified on the grounds
    that the high allocation success rates were achieved by aggressive
    reclaim.  Success rates are expected to suffer even more in 3.6 due to
    commit 7db8889ab05b ("mm: have order > 0 compaction start off where it
    left") which testing has shown to severely reduce allocation success
    rates under load - to 0% in one case.
    
    This series aims to improve the allocation success rates without
    regressing the benefits of commit fe2c2a106663.  The series is based on
    latest mmotm and takes into account the __GFP_NO_KSWAPD flag is going
    away.
    
    Patch 1 updates a stale comment seeing as I was in the general area.
    
    Patch 2 updates reclaim/compaction to reclaim pages scaled on the number
    	of recent failures.
    
    Patch 3 captures suitable high-order pages freed by compaction to reduce
    	races with parallel allocation requests.
    
    Patch 4 fixes the upstream commit [7db8889a: mm: have order > 0 compaction
    	start off where it left] to enable compaction again
    
    Patch 5 identifies when compacion is taking too long due to contention
    	and aborts.
    
    STRESS-HIGHALLOC
    		 3.6-rc1-akpm	  full-series
    Pass 1          36.00 ( 0.00%)    51.00 (15.00%)
    Pass 2          42.00 ( 0.00%)    63.00 (21.00%)
    while Rested    86.00 ( 0.00%)    86.00 ( 0.00%)
    
    From
    
      http://www.csn.ul.ie/~mel/postings/mmtests-20120424/global-dhp__stress-highalloc-performance-ext3/hydra/comparison.html
    
    I know that the allocation success rates in 3.3.6 was 78% in comparison
    to 36% in in the current akpm tree.  With the full series applied, the
    success rates are up to around 51% with some variability in the results.
    This is not as high a success rate but it does not reclaim excessively
    which is a key point.
    
    MMTests Statistics: vmstat
    Page Ins                                     3050912     3078892
    Page Outs                                    8033528     8039096
    Swap Ins                                           0           0
    Swap Outs                                          0           0
    
    Note that swap in/out rates remain at 0. In 3.3.6 with 78% success rates
    there were 71881 pages swapped out.
    
    Direct pages scanned                           70942      122976
    Kswapd pages scanned                         1366300     1520122
    Kswapd pages reclaimed                       1366214     1484629
    Direct pages reclaimed                         70936      105716
    Kswapd efficiency                                99%         97%
    Kswapd velocity                             1072.550    1182.615
    Direct efficiency                                99%         85%
    Direct velocity                               55.690      95.672
    
    The kswapd velocity changes very little as expected.  kswapd velocity is
    around the 1000 pages/sec mark where as in kernel 3.3.6 with the high
    allocation success rates it was 8140 pages/second.  Direct velocity is
    higher as a result of patch 2 of the series but this is expected and is
    acceptable.  The direct reclaim and kswapd velocities change very little.
    
    If these get accepted for merging then there is a difficulty in how they
    should be handled.  7db8889a ("mm: have order > 0 compaction start off
    where it left") is broken but it is already in 3.6-rc1 and needs to be
    fixed.  However, if just patch 4 from this series is applied then Jim
    Schutt's workload is known to break again as his workload also requires
    patch 5.  While it would be preferred to have all these patches in 3.6 to
    improve compaction in general, it would at least be acceptable if just
    patches 4 and 5 were merged to 3.6 to fix a known problem without breaking
    compaction completely.  On the face of it, that would force
    __GFP_NO_KSWAPD patches to be merged at the same time but I can do a
    version of this series with __GFP_NO_KSWAPD change reverted and then
    rebase it on top of this series.  That might be best overall because I
    note that the __GFP_NO_KSWAPD patch should have removed
    deferred_compaction from page_alloc.c but it didn't but fixing that causes
    collisions with this series.
    
    This patch:
    
    The comment about order applied when the check was order >
    PAGE_ALLOC_COSTLY_ORDER which has not been the case since c5a73c3d ("thp:
    use compaction for all allocation orders").  Fixing the comment while I'm
    in the general area.
    
    Change-Id: Ida65d938b78618ec098d5e511e88cf39578ba606
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Reviewed-by: Rik van Riel <riel@redhat.com>
    Reviewed-by: Minchan Kim <minchan@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Git-commit: 4ffb6335da87b51c17e7ff6495170785f21558dd
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit 508f4f9811824769582baa206be2af840d660b69
Author: Mel Gorman <mgorman@suse.de>
Date:   Tue Aug 21 16:16:17 2012 -0700

    mm: compaction: Abort async compaction if locks are contended or taking too long
    
    Jim Schutt reported a problem that pointed at compaction contending
    heavily on locks.  The workload is straight-forward and in his own words;
    
    	The systems in question have 24 SAS drives spread across 3 HBAs,
    	running 24 Ceph OSD instances, one per drive.  FWIW these servers
    	are dual-socket Intel 5675 Xeons w/48 GB memory.  I've got ~160
    	Ceph Linux clients doing dd simultaneously to a Ceph file system
    	backed by 12 of these servers.
    
    Early in the test everything looks fine
    
      procs -------------------memory------------------ ---swap-- -----io---- --system-- -----cpu-------
       r  b       swpd       free       buff      cache   si   so    bi    bo   in   cs  us sy  id wa st
      31 15          0     287216        576   38606628    0    0     2  1158    2   14   1  3  95  0  0
      27 15          0     225288        576   38583384    0    0    18 2222016 203357 134876  11 56  17 15  0
      28 17          0     219256        576   38544736    0    0    11 2305932 203141 146296  11 49  23 17  0
       6 18          0     215596        576   38552872    0    0     7 2363207 215264 166502  12 45  22 20  0
      22 18          0     226984        576   38596404    0    0     3 2445741 223114 179527  12 43  23 22  0
    
    and then it goes to pot
    
      procs -------------------memory------------------ ---swap-- -----io---- --system-- -----cpu-------
       r  b       swpd       free       buff      cache   si   so    bi    bo   in   cs  us sy  id wa st
      163  8          0     464308        576   36791368    0    0    11 22210  866  536   3 13  79  4  0
      207 14          0     917752        576   36181928    0    0   712 1345376 134598 47367   7 90   1  2  0
      123 12          0     685516        576   36296148    0    0   429 1386615 158494 60077   8 84   5  3  0
      123 12          0     598572        576   36333728    0    0  1107 1233281 147542 62351   7 84   5  4  0
      622  7          0     660768        576   36118264    0    0   557 1345548 151394 59353   7 85   4  3  0
      223 11          0     283960        576   36463868    0    0    46 1107160 121846 33006   6 93   1  1  0
    
    Note that system CPU usage is very high blocks being written out has
    dropped by 42%. He analysed this with perf and found
    
      perf record -g -a sleep 10
      perf report --sort symbol --call-graph fractal,5
        34.63%  [k] _raw_spin_lock_irqsave
                |
                |--97.30%-- isolate_freepages
                |          compaction_alloc
                |          unmap_and_move
                |          migrate_pages
                |          compact_zone
                |          compact_zone_order
                |          try_to_compact_pages
                |          __alloc_pages_direct_compact
                |          __alloc_pages_slowpath
                |          __alloc_pages_nodemask
                |          alloc_pages_vma
                |          do_huge_pmd_anonymous_page
                |          handle_mm_fault
                |          do_page_fault
                |          page_fault
                |          |
                |          |--87.39%-- skb_copy_datagram_iovec
                |          |          tcp_recvmsg
                |          |          inet_recvmsg
                |          |          sock_recvmsg
                |          |          sys_recvfrom
                |          |          system_call
                |          |          __recv
                |          |          |
                |          |           --100.00%-- (nil)
                |          |
                |           --12.61%-- memcpy
                 --2.70%-- [...]
    
    There was other data but primarily it is all showing that compaction is
    contended heavily on the zone->lock and zone->lru_lock.
    
    commit [b2eef8c0: mm: compaction: minimise the time IRQs are disabled
    while isolating pages for migration] noted that it was possible for
    migration to hold the lru_lock for an excessive amount of time. Very
    broadly speaking this patch expands the concept.
    
    This patch introduces compact_checklock_irqsave() to check if a lock
    is contended or the process needs to be scheduled. If either condition
    is true then async compaction is aborted and the caller is informed.
    The page allocator will fail a THP allocation if compaction failed due
    to contention. This patch also introduces compact_trylock_irqsave()
    which will acquire the lock only if it is not contended and the process
    does not need to schedule.
    
    Change-Id: Ia5318c923b903948072ff279dc9aed698bb6d02d
    Reported-by: Jim Schutt <jaschut@sandia.gov>
    Tested-by: Jim Schutt <jaschut@sandia.gov>
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Git-commit: c67fe3752abe6ab47639e2f9b836900c3dc3da84
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    [lauraa@codeaurora.org: Minor context fixup in isolate_migratepages_range]
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit ba8ae25b8667521c9f97add4cc77196877dcb134
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Fri Dec 18 12:17:12 2015 +0100

    vmscan: fix a small fault of me

commit f8368018c6a515384a3d92dbbce4c7e997302f04
Author: Hugh Dickins <hughd@google.com>
Date:   Tue May 29 15:06:52 2012 -0700

    mm/memcg: scanning_global_lru means mem_cgroup_disabled
    
    Although one has to admire the skill with which it has been concealed,
    scanning_global_lru(mz) is actually just an interesting way to test
    mem_cgroup_disabled().  Too many developer hours have been wasted on
    confusing it with global_reclaim(): just use mem_cgroup_disabled().
    
    Change-Id: I9f8f70b41b002cc8c4583a0a9869459da24b4fe5
    Signed-off-by: Hugh Dickins <hughd@google.com>
    Signed-off-by: Konstantin Khlebnikov <khlebnikov@openvz.org>
    Acked-by: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Acked-by: Glauber Costa <glommer@parallels.com>
    Cc: Michal Hocko <mhocko@suse.cz>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Minchan Kim <minchan@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Git-commit: c3c787e8c38557ccf44c670d73aebe630a2b1479
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit e839a934c50a4fe3383f4c25060018be3b64b85c
Author: Konstantin Khlebnikov <khlebnikov@openvz.org>
Date:   Tue May 29 15:06:54 2012 -0700

    mm: remove lru type checks from __isolate_lru_page()
    
    After patch "mm: forbid lumpy-reclaim in shrink_active_list()" we can
    completely remove anon/file and active/inactive lru type filters from
    __isolate_lru_page(), because isolation for 0-order reclaim always
    isolates pages from right lru list.  And pages-isolation for lumpy
    shrink_inactive_list() or memory-compaction anyway allowed to isolate
    pages from all evictable lru lists.
    
    Change-Id: I2a1a0325b1d193f4ca5e3ea7d5eda9b8bf7c6698
    Signed-off-by: Konstantin Khlebnikov <khlebnikov@openvz.org>
    Acked-by: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Hugh Dickins <hughd@google.com>
    Acked-by: Michal Hocko <mhocko@suse.cz>
    Cc: Glauber Costa <glommer@parallels.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Minchan Kim <minchan@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Git-commit: f3fd4a61928a5edf5b033a417e761b488b43e203
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit a461dc631b15c9076f1f8f6286f8fb8dd1db7853
Author: Minchan Kim <minchan@kernel.org>
Date:   Tue Aug 21 16:16:03 2012 -0700

    mm/compaction.c: fix deferring compaction mistake
    
    Commit aff622495c9a ("vmscan: only defer compaction for failed order and
    higher") fixed bad deferring policy but made mistake about checking
    compact_order_failed in __compact_pgdat().  So it can't update
    compact_order_failed with the new order.  This ends up preventing
    correct operation of policy deferral.  This patch fixes it.
    
    Change-Id: I8d65a5511d90e6c02bb58cd7bc5743e4327271f9
    Signed-off-by: Minchan Kim <minchan@kernel.org>
    Reviewed-by: Rik van Riel <riel@redhat.com>
    Acked-by: Mel Gorman <mel@csn.ul.ie>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Git-commit: c81758fbe0fdbbc0c74b37798f55bd9c91d5c068
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit 81a0e5d45da8455e13a5ab4589b0f82cc37dec97
Author: Minchan Kim <minchan@kernel.org>
Date:   Mon Oct 8 16:33:51 2012 -0700

    cma: decrease cc.nr_migratepages after reclaiming pagelist
    
    reclaim_clean_pages_from_list() reclaims clean pages before migration so
    cc.nr_migratepages should be updated.  Currently, there is no problem but
    it can be wrong if we try to use the value in future.
    
    Change-Id: I8b3f1238645ba1b3adcc0fe3c41e10f7074b9a96
    Signed-off-by: Minchan Kim <minchan@kernel.org>
    Acked-by: Mel Gorman <mgorman@suse.de>
    Cc: Michal Nazarewicz <mina86@mina86.com>
    Cc: Bartlomiej Zolnierkiewicz <b.zolnierkie@samsung.com>
    Cc: Marek Szyprowski <m.szyprowski@samsung.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Git-commit: beb51eaa88238daba698ad837222ad277d440b6d
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    [lauraa@codeaurora.org: Change cc to be structure instead of pointer]
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit 394e8a8181476c04059da8e14dff522b139c3343
Author: Minchan Kim <minchan@kernel.org>
Date:   Mon Oct 8 16:31:55 2012 -0700

    mm: cma: discard clean pages during contiguous allocation instead of migration
    
    Drop clean cache pages instead of migration during alloc_contig_range() to
    minimise allocation latency by reducing the amount of migration that is
    necessary.  It's useful for CMA because latency of migration is more
    important than evicting the background process's working set.  In
    addition, as pages are reclaimed then fewer free pages for migration
    targets are required so it avoids memory reclaiming to get free pages,
    which is a contributory factor to increased latency.
    
    I measured elapsed time of __alloc_contig_migrate_range() which migrates
    10M in 40M movable zone in QEMU machine.
    
    Before - 146ms, After - 7ms
    
    Change-Id: Ia527b7253bc5fa63b555ac445b676588b6def119
    [akpm@linux-foundation.org: fix nommu build]
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Signed-off-by: Minchan Kim <minchan@kernel.org>
    Reviewed-by: Mel Gorman <mgorman@suse.de>
    Cc: Marek Szyprowski <m.szyprowski@samsung.com>
    Acked-by: Michal Nazarewicz <mina86@mina86.com>
    Cc: Rik van Riel <riel@redhat.com>
    Tested-by: Kyungmin Park <kyungmin.park@samsung.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Git-commit: 02c6de8d757cb32c0829a45d81c3dfcbcafd998b
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    [lauraa@codeaurora.org: Fixups in mm/internal.h due to contexts]
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit e1d8afac416a2787ab2a0097dee777aa6a4f0c91
Author: Konstantin Khlebnikov <khlebnikov@openvz.org>
Date:   Tue May 29 15:06:59 2012 -0700

    mm/vmscan: remove update_isolated_counts()
    
    update_isolated_counts() is no longer required, because lumpy-reclaim was
    removed.  Insanity is over, now there is only one kind of inactive page.
    
    Change-Id: Ib2a40af679a00d23b22800d0e513f60838285a15
    Signed-off-by: Konstantin Khlebnikov <khlebnikov@openvz.org>
    Cc: Mel Gorman <mel@csn.ul.ie>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Acked-by: Hugh Dickins <hughd@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Git-commit: 95d918fc009072c2f88ce2e8b5db2e5abfad7c3e
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit 2bb71baa1b5030124cab78159e15e60df2369ba7
Author: Konstantin Khlebnikov <khlebnikov@openvz.org>
Date:   Tue May 29 15:06:58 2012 -0700

    mm/vmscan: push lruvec pointer into isolate_lru_pages()
    
    Move the mem_cgroup_zone_lruvec() call from isolate_lru_pages() into
    shrink_[in]active_list().  Further patches push it to shrink_zone() step
    by step.
    
    Change-Id: I59593e52a524b1b6713c0421c3ed956f78c1e1a8
    Signed-off-by: Konstantin Khlebnikov <khlebnikov@openvz.org>
    Cc: Mel Gorman <mel@csn.ul.ie>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Acked-by: Hugh Dickins <hughd@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Git-commit: 5dc35979e444b50d5551bdeb7a7abc5c69c875d0
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit d610722a6826ae38c2f71a3bcd97e2190027e086
Author: Konstantin Khlebnikov <khlebnikov@openvz.org>
Date:   Tue May 29 15:06:59 2012 -0700

    mm/vmscan: push zone pointer into shrink_page_list()
    
    It doesn't need a pointer to the cgroup - pointer to the zone is enough.
    This patch also kills the "mz" argument of page_check_references() - it is
    unused after "mm: memcg: count pte references from every member of the
    reclaimed hierarch"
    
    Change-Id: I9b219780be851f696dc5e3b7fc21889035d00313
    Signed-off-by: Konstantin Khlebnikov <khlebnikov@openvz.org>
    Cc: Mel Gorman <mel@csn.ul.ie>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Acked-by: Hugh Dickins <hughd@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Git-commit: 6a18adb35c27848195c938b0779ce882d63d3ed1
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit de44a79092c3ca0cf61d2c447d79938d2318d8ac
Author: Johannes Weiner <hannes@cmpxchg.org>
Date:   Tue May 29 15:06:25 2012 -0700

    mm: memcg: count pte references from every member of the reclaimed hierarchy
    
    The rmap walker checking page table references has historically ignored
    references from VMAs that were not part of the memcg that was being
    reclaimed during memcg hard limit reclaim.
    
    When transitioning global reclaim to memcg hierarchy reclaim, I missed
    that bit and now references from outside a memcg are ignored even during
    global reclaim.
    
    Reverting back to traditional behaviour - count all references during
    global reclaim and only mind references of the memcg being reclaimed
    during limit reclaim would be one option.
    
    However, the more generic idea is to ignore references exactly then when
    they are outside the hierarchy that is currently under reclaim; because
    only then will their reclamation be of any use to help the pressure
    situation.  It makes no sense to ignore references from a sibling memcg
    and then evict a page that will be immediately refaulted by that sibling
    which contributes to the same usage of the common ancestor under
    reclaim.
    
    The solution: make the rmap walker ignore references from VMAs that are
    not part of the hierarchy that is being reclaimed.
    
    Flat limit reclaim will stay the same, hierarchical limit reclaim will
    mind the references only to pages that the hierarchy owns.  Global
    reclaim, since it reclaims from all memcgs, will be fixed to regard all
    references.
    
    Change-Id: I3a3f39693cf5644870213df28238acf00d7417dd
    [akpm@linux-foundation.org: name the args in the declaration]
    Signed-off-by: Johannes Weiner <hannes@cmpxchg.org>
    Reported-by: Konstantin Khlebnikov <khlebnikov@openvz.org>
    Acked-by: Konstantin Khlebnikov<khlebnikov@openvz.org>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Acked-by: Michal Hocko <mhocko@suse.cz>
    Cc: Li Zefan <lizf@cn.fujitsu.com>
    Cc: Li Zefan <lizf@cn.fujitsu.com>
    Cc: Tejun Heo <tj@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Git-commit: c3ac9a8ade65ccbfd145fbff895ae8d8d62d09b0
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit e9e4f26798f70b1cd446700af5634a5f7338256d
Author: Konstantin Khlebnikov <khlebnikov@openvz.org>
Date:   Tue May 29 15:06:57 2012 -0700

    mm/vmscan: store "priority" in struct scan_control
    
    In memory reclaim some function have too many arguments - "priority" is
    one of them.  It can be stored in struct scan_control - we construct them
    on the same level.  Instead of an open coded loop we set the initial
    sc.priority, and do_try_to_free_pages() decreases it down to zero.
    
    Change-Id: I7c03b4367fe3787dfd82afe1ff21469a99bdb04f
    Signed-off-by: Konstantin Khlebnikov <khlebnikov@openvz.org>
    Cc: Mel Gorman <mel@csn.ul.ie>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Acked-by: Hugh Dickins <hughd@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Git-commit: 9e3b2f8cd340e13353a44c9a34caef2848131ed7
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit e2c390769bbb6e5ec39450ae0d1f5b1f9b11978f
Author: Rik van Riel <riel@redhat.com>
Date:   Tue May 29 15:06:18 2012 -0700

    mm: remove swap token code
    
    The swap token code no longer fits in with the current VM model.  It
    does not play well with cgroups or the better NUMA placement code in
    development, since we have only one swap token globally.
    
    It also has the potential to mess with scalability of the system, by
    increasing the number of non-reclaimable pages on the active and
    inactive anon LRU lists.
    
    Last but not least, the swap token code has been broken for a year
    without complaints, as reported by Konstantin Khlebnikov.  This suggests
    we no longer have much use for it.
    
    The days of sub-1G memory systems with heavy use of swap are over.  If
    we ever need thrashing reducing code in the future, we will have to
    implement something that does scale.
    
    Change-Id: I6d287cfc3c3206ca24da2de0c1392e5fdfcfabe8
    Signed-off-by: Rik van Riel <riel@redhat.com>
    Cc: Konstantin Khlebnikov <khlebnikov@openvz.org>
    Acked-by: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Mel Gorman <mel@csn.ul.ie>
    Cc: Hugh Dickins <hughd@google.com>
    Acked-by: Bob Picco <bpicco@meloft.net>
    Acked-by: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Git-commit: e709ffd6169ccd259eb5874e853303e91e94e829
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit ebe1cd91a048778bb0d51b8b7386af19964a0fe0
Author: Konstantin Khlebnikov <khlebnikov@openvz.org>
Date:   Tue May 29 15:06:57 2012 -0700

    mm/memcg: use vm_swappiness from target memory cgroup
    
    Use vm_swappiness from memory cgroup which is triggered this memory
    reclaim.  This is more reasonable and allows to kill one argument.
    
    Change-Id: I6aa49763a5746f021ae084885df6764bb7835a62
    [akpm@linux-foundation.org: fix build (patch skew)]
    Signed-off-by: Konstantin Khlebnikov <khlebnikov@openvz.org>
    Acked-by: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujtisu.com>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: Glauber Costa <glommer@parallels.com>
    Cc: Michal Hocko <mhocko@suse.cz>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Minchan Kim <minchan@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Git-commit: 3d58ab5c97fa2d145050242137ac39ca7d3bc2fc
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit 2e7496fd334b54048a961b77f3ab2f7118f1d9e7
Author: Konstantin Khlebnikov <khlebnikov@openvz.org>
Date:   Tue May 29 15:06:54 2012 -0700

    mm/memcg: kill mem_cgroup_lru_del()
    
    This patch kills mem_cgroup_lru_del(), we can use
    mem_cgroup_lru_del_list() instead.  On 0-order isolation we already have
    right lru list id.
    
    Change-Id: I403d40074299fb5f125603435c057071714d5b92
    Signed-off-by: Konstantin Khlebnikov <khlebnikov@openvz.org>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: Glauber Costa <glommer@parallels.com>
    Cc: Michal Hocko <mhocko@suse.cz>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Minchan Kim <minchan@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Git-commit: bbf808ed7de68fdf626fd4f9718d88cf03ce13a9
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit 66b7870cea5ec97bf236a7ac56663fe085db1d67
Author: Liam Mark <lmark@codeaurora.org>
Date:   Wed Jan 16 10:14:40 2013 -0800

    ion: tracing: add ftrace events for ion allocations
    
    Add ftrace events for ion allocations to make it easier to profile
    their performance.
    
    Change-Id: I9f32e076cd50d7d3a145353dfcef74f0f6cdf8a0
    Signed-off-by: Liam Mark <lmark@codeaurora.org>

commit f1c87225047621ec2be3df66f4f2d09d4b11a587
Author: Mel Gorman <mgorman@suse.de>
Date:   Tue May 29 15:06:20 2012 -0700

    mm: vmscan: remove reclaim_mode_t
    
    There is little motiviation for reclaim_mode_t once RECLAIM_MODE_[A]SYNC
    and lumpy reclaim have been removed.  This patch gets rid of
    reclaim_mode_t as well and improves the documentation about what
    reclaim/compaction is and when it is triggered.
    
    Change-Id: If95bc163647b1cfb93d7f3b8435060fed1e2aabf
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Acked-by: Rik van Riel <riel@redhat.com>
    Acked-by: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Cc: Konstantin Khlebnikov <khlebnikov@openvz.org>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: Ying Han <yinghan@google.com>
    Cc: Andy Whitcroft <apw@shadowen.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Git-commit: 23b9da55c5b0feb484bd5e8615f4eb1ce4169453
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit d5c18e4fb50b46a24557dddafe9060515bf9bf90
Author: Mel Gorman <mgorman@suse.de>
Date:   Tue May 29 15:06:19 2012 -0700

    mm: vmscan: do not stall on writeback during memory compaction
    
    This patch stops reclaim/compaction entering sync reclaim as this was
    only intended for lumpy reclaim and an oversight.  Page migration has
    its own logic for stalling on writeback pages if necessary and memory
    compaction is already using it.
    
    Waiting on page writeback is bad for a number of reasons but the primary
    one is that waiting on writeback to a slow device like USB can take a
    considerable length of time.  Page reclaim instead uses
    wait_iff_congested() to throttle if too many dirty pages are being
    scanned.
    
    Change-Id: I14f312b1a51ee093d9d4adda5c87e57f1b83e03d
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Acked-by: Rik van Riel <riel@redhat.com>
    Acked-by: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Cc: Konstantin Khlebnikov <khlebnikov@openvz.org>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: Ying Han <yinghan@google.com>
    Cc: Andy Whitcroft <apw@shadowen.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Git-commit: 41ac1999c3e3563e1810b14878a869c79c9368bb
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit 2a5f804f8fda026b32e9c2f66e20e843d15f5a4f
Author: Mel Gorman <mgorman@suse.de>
Date:   Tue May 29 15:06:19 2012 -0700

    mm: vmscan: remove lumpy reclaim
    
    This series removes lumpy reclaim and some stalling logic that was
    unintentionally being used by memory compaction.  The end result is that
    stalling on dirty pages during page reclaim now depends on
    wait_iff_congested().
    
    Four kernels were compared
    
      3.3.0     vanilla
      3.4.0-rc2 vanilla
      3.4.0-rc2 lumpyremove-v2 is patch one from this series
      3.4.0-rc2 nosync-v2r3 is the full series
    
    Removing lumpy reclaim saves almost 900 bytes of text whereas the full
    series removes 1200 bytes.
    
         text     data      bss       dec     hex  filename
      6740375  1927944  2260992  10929311  a6c49f  vmlinux-3.4.0-rc2-vanilla
      6739479  1927944  2260992  10928415  a6c11f  vmlinux-3.4.0-rc2-lumpyremove-v2
      6739159  1927944  2260992  10928095  a6bfdf  vmlinux-3.4.0-rc2-nosync-v2
    
    There are behaviour changes in the series and so tests were run with
    monitoring of ftrace events.  This disrupts results so the performance
    results are distorted but the new behaviour should be clearer.
    
    fs-mark running in a threaded configuration showed little of interest as
    it did not push reclaim aggressively
    
      FS-Mark Multi Threaded
                              3.3.0-vanilla       rc2-vanilla       lumpyremove-v2r3       nosync-v2r3
      Files/s  min           3.20 ( 0.00%)        3.20 ( 0.00%)        3.20 ( 0.00%)        3.20 ( 0.00%)
      Files/s  mean          3.20 ( 0.00%)        3.20 ( 0.00%)        3.20 ( 0.00%)        3.20 ( 0.00%)
      Files/s  stddev        0.00 ( 0.00%)        0.00 ( 0.00%)        0.00 ( 0.00%)        0.00 ( 0.00%)
      Files/s  max           3.20 ( 0.00%)        3.20 ( 0.00%)        3.20 ( 0.00%)        3.20 ( 0.00%)
      Overhead min      508667.00 ( 0.00%)   521350.00 (-2.49%)   544292.00 (-7.00%)   547168.00 (-7.57%)
      Overhead mean     551185.00 ( 0.00%)   652690.73 (-18.42%)   991208.40 (-79.83%)   570130.53 (-3.44%)
      Overhead stddev    18200.69 ( 0.00%)   331958.29 (-1723.88%)  1579579.43 (-8578.68%)     9576.81 (47.38%)
      Overhead max      576775.00 ( 0.00%)  1846634.00 (-220.17%)  6901055.00 (-1096.49%)   585675.00 (-1.54%)
      MMTests Statistics: duration
      Sys Time Running Test (seconds)             309.90    300.95    307.33    298.95
      User+Sys Time Running Test (seconds)        319.32    309.67    315.69    307.51
      Total Elapsed Time (seconds)               1187.85   1193.09   1191.98   1193.73
    
      MMTests Statistics: vmstat
      Page Ins                                       80532       82212       81420       79480
      Page Outs                                  111434984   111456240   111437376   111582628
      Swap Ins                                           0           0           0           0
      Swap Outs                                          0           0           0           0
      Direct pages scanned                           44881       27889       27453       34843
      Kswapd pages scanned                        25841428    25860774    25861233    25843212
      Kswapd pages reclaimed                      25841393    25860741    25861199    25843179
      Direct pages reclaimed                         44881       27889       27453       34843
      Kswapd efficiency                                99%         99%         99%         99%
      Kswapd velocity                            21754.791   21675.460   21696.029   21649.127
      Direct efficiency                               100%        100%        100%        100%
      Direct velocity                               37.783      23.375      23.031      29.188
      Percentage direct scans                           0%          0%          0%          0%
    
    ftrace showed that there was no stalling on writeback or pages submitted
    for IO from reclaim context.
    
    postmark was similar and while it was more interesting, it also did not
    push reclaim heavily.
    
      POSTMARK
                                           3.3.0-vanilla       rc2-vanilla  lumpyremove-v2r3       nosync-v2r3
      Transactions per second:               16.00 ( 0.00%)    20.00 (25.00%)    18.00 (12.50%)    17.00 ( 6.25%)
      Data megabytes read per second:        18.80 ( 0.00%)    24.27 (29.10%)    22.26 (18.40%)    20.54 ( 9.26%)
      Data megabytes written per second:     35.83 ( 0.00%)    46.25 (29.08%)    42.42 (18.39%)    39.14 ( 9.24%)
      Files created alone per second:        28.00 ( 0.00%)    38.00 (35.71%)    34.00 (21.43%)    30.00 ( 7.14%)
      Files create/transact per second:       8.00 ( 0.00%)    10.00 (25.00%)     9.00 (12.50%)     8.00 ( 0.00%)
      Files deleted alone per second:       556.00 ( 0.00%)  1224.00 (120.14%)  3062.00 (450.72%)  6124.00 (1001.44%)
      Files delete/transact per second:       8.00 ( 0.00%)    10.00 (25.00%)     9.00 (12.50%)     8.00 ( 0.00%)
    
      MMTests Statistics: duration
      Sys Time Running Test (seconds)             113.34    107.99    109.73    108.72
      User+Sys Time Running Test (seconds)        145.51    139.81    143.32    143.55
      Total Elapsed Time (seconds)               1159.16    899.23    980.17   1062.27
    
      MMTests Statistics: vmstat
      Page Ins                                    13710192    13729032    13727944    13760136
      Page Outs                                   43071140    42987228    42733684    42931624
      Swap Ins                                           0           0           0           0
      Swap Outs                                          0           0           0           0
      Direct pages scanned                               0           0           0           0
      Kswapd pages scanned                         9941613     9937443     9939085     9929154
      Kswapd pages reclaimed                       9940926     9936751     9938397     9928465
      Direct pages reclaimed                             0           0           0           0
      Kswapd efficiency                                99%         99%         99%         99%
      Kswapd velocity                             8576.567   11051.058   10140.164    9347.109
      Direct efficiency                               100%        100%        100%        100%
      Direct velocity                                0.000       0.000       0.000       0.000
    
    It looks like here that the full series regresses performance but as
    ftrace showed no usage of wait_iff_congested() or sync reclaim I am
    assuming it's a disruption due to monitoring.  Other data such as memory
    usage, page IO, swap IO all looked similar.
    
    Running a benchmark with a plain DD showed nothing very interesting.
    The full series stalled in wait_iff_congested() slightly less but stall
    times on vanilla kernels were marginal.
    
    Running a benchmark that hammered on file-backed mappings showed stalls
    due to congestion but not in sync writebacks
    
      MICRO
                                           3.3.0-vanilla       rc2-vanilla  lumpyremove-v2r3       nosync-v2r3
      MMTests Statistics: duration
      Sys Time Running Test (seconds)             308.13    294.50    298.75    299.53
      User+Sys Time Running Test (seconds)        330.45    316.28    318.93    320.79
      Total Elapsed Time (seconds)               1814.90   1833.88   1821.14   1832.91
    
      MMTests Statistics: vmstat
      Page Ins                                      108712      120708       97224      110344
      Page Outs                                  155514576   156017404   155813676   156193256
      Swap Ins                                           0           0           0           0
      Swap Outs                                          0           0           0           0
      Direct pages scanned                         2599253     1550480     2512822     2414760
      Kswapd pages scanned                        69742364    71150694    68839041    69692533
      Kswapd pages reclaimed                      34824488    34773341    34796602    34799396
      Direct pages reclaimed                         53693       94750       61792       75205
      Kswapd efficiency                                49%         48%         50%         49%
      Kswapd velocity                            38427.662   38797.901   37799.972   38022.889
      Direct efficiency                                 2%          6%          2%          3%
      Direct velocity                             1432.174     845.464    1379.807    1317.446
      Percentage direct scans                           3%          2%          3%          3%
      Page writes by reclaim                             0           0           0           0
      Page writes file                                   0           0           0           0
      Page writes anon                                   0           0           0           0
      Page reclaim immediate                             0           0           0        1218
      Page rescued immediate                             0           0           0           0
      Slabs scanned                                  15360       16384       13312       16384
      Direct inode steals                                0           0           0           0
      Kswapd inode steals                             4340        4327        1630        4323
    
      FTrace Reclaim Statistics: congestion_wait
      Direct number congest     waited                 0          0          0          0
      Direct time   congest     waited               0ms        0ms        0ms        0ms
      Direct full   congest     waited                 0          0          0          0
      Direct number conditional waited               900        870        754        789
      Direct time   conditional waited               0ms        0ms        0ms       20ms
      Direct full   conditional waited                 0          0          0          0
      KSwapd number congest     waited              2106       2308       2116       1915
      KSwapd time   congest     waited          139924ms   157832ms   125652ms   132516ms
      KSwapd full   congest     waited              1346       1530       1202       1278
      KSwapd number conditional waited             12922      16320      10943      14670
      KSwapd time   conditional waited               0ms        0ms        0ms        0ms
      KSwapd full   conditional waited                 0          0          0          0
    
    Reclaim statistics are not radically changed.  The stall times in kswapd
    are massive but it is clear that it is due to calls to congestion_wait()
    and that is almost certainly the call in balance_pgdat().  Otherwise
    stalls due to dirty pages are non-existant.
    
    I ran a benchmark that stressed high-order allocation.  This is very
    artifical load but was used in the past to evaluate lumpy reclaim and
    compaction.  Generally I look at allocation success rates and latency
    figures.
    
      STRESS-HIGHALLOC
                       3.3.0-vanilla       rc2-vanilla  lumpyremove-v2r3       nosync-v2r3
      Pass 1          81.00 ( 0.00%)    28.00 (-53.00%)    24.00 (-57.00%)    28.00 (-53.00%)
      Pass 2          82.00 ( 0.00%)    39.00 (-43.00%)    38.00 (-44.00%)    43.00 (-39.00%)
      while Rested    88.00 ( 0.00%)    87.00 (-1.00%)    88.00 ( 0.00%)    88.00 ( 0.00%)
    
      MMTests Statistics: duration
      Sys Time Running Test (seconds)             740.93    681.42    685.14    684.87
      User+Sys Time Running Test (seconds)       2922.65   3269.52   3281.35   3279.44
      Total Elapsed Time (seconds)               1161.73   1152.49   1159.55   1161.44
    
      MMTests Statistics: vmstat
      Page Ins                                     4486020     2807256     2855944     2876244
      Page Outs                                    7261600     7973688     7975320     7986120
      Swap Ins                                       31694           0           0           0
      Swap Outs                                      98179           0           0           0
      Direct pages scanned                           53494       57731       34406      113015
      Kswapd pages scanned                         6271173     1287481     1278174     1219095
      Kswapd pages reclaimed                       2029240     1281025     1260708     1201583
      Direct pages reclaimed                          1468       14564       16649       92456
      Kswapd efficiency                                32%         99%         98%         98%
      Kswapd velocity                             5398.133    1117.130    1102.302    1049.641
      Direct efficiency                                 2%         25%         48%         81%
      Direct velocity                               46.047      50.092      29.672      97.306
      Percentage direct scans                           0%          4%          2%          8%
      Page writes by reclaim                       1616049           0           0           0
      Page writes file                             1517870           0           0           0
      Page writes anon                               98179           0           0           0
      Page reclaim immediate                        103778       27339        9796       17831
      Page rescued immediate                             0           0           0           0
      Slabs scanned                                1096704      986112      980992      998400
      Direct inode steals                              223      215040      216736      247881
      Kswapd inode steals                           175331       61548       68444       63066
      Kswapd skipped wait                            21991           0           1           0
      THP fault alloc                                    1         135         125         134
      THP collapse alloc                               393         311         228         236
      THP splits                                        25          13           7           8
      THP fault fallback                                 0           0           0           0
      THP collapse fail                                  3           5           7           7
      Compaction stalls                                865        1270        1422        1518
      Compaction success                               370         401         353         383
      Compaction failures                              495         869        1069        1135
      Compaction pages moved                        870155     3828868     4036106     4423626
      Compaction move failure                        26429       23865       29742       27514
    
    Success rates are completely hosed for 3.4-rc2 which is almost certainly
    due to commit fe2c2a106663 ("vmscan: reclaim at order 0 when compaction
    is enabled").  I expected this would happen for kswapd and impair
    allocation success rates (https://lkml.org/lkml/2012/1/25/166) but I did
    not anticipate this much a difference: 80% less scanning, 37% less
    reclaim by kswapd
    
    In comparison, reclaim/compaction is not aggressive and gives up easily
    which is the intended behaviour.  hugetlbfs uses __GFP_REPEAT and would
    be much more aggressive about reclaim/compaction than THP allocations
    are.  The stress test above is allocating like neither THP or hugetlbfs
    but is much closer to THP.
    
    Mainline is now impaired in terms of high order allocation under heavy
    load although I do not know to what degree as I did not test with
    __GFP_REPEAT.  Keep this in mind for bugs related to hugepage pool
    resizing, THP allocation and high order atomic allocation failures from
    network devices.
    
    In terms of congestion throttling, I see the following for this test
    
      FTrace Reclaim Statistics: congestion_wait
      Direct number congest     waited                 3          0          0          0
      Direct time   congest     waited               0ms        0ms        0ms        0ms
      Direct full   congest     waited                 0          0          0          0
      Direct number conditional waited               957        512       1081       1075
      Direct time   conditional waited               0ms        0ms        0ms        0ms
      Direct full   conditional waited                 0          0          0          0
      KSwapd number congest     waited                36          4          3          5
      KSwapd time   congest     waited            3148ms      400ms      300ms      500ms
      KSwapd full   congest     waited                30          4          3          5
      KSwapd number conditional waited             88514        197        332        542
      KSwapd time   conditional waited            4980ms        0ms        0ms        0ms
      KSwapd full   conditional waited                49          0          0          0
    
    The "conditional waited" times are the most interesting as this is
    directly impacted by the number of dirty pages encountered during scan.
    As lumpy reclaim is no longer scanning contiguous ranges, it is finding
    fewer dirty pages.  This brings wait times from about 5 seconds to 0.
    kswapd itself is still calling congestion_wait() so it'll still stall but
    it's a lot less.
    
    In terms of the type of IO we were doing, I see this
    
      FTrace Reclaim Statistics: mm_vmscan_writepage
      Direct writes anon  sync                         0          0          0          0
      Direct writes anon  async                        0          0          0          0
      Direct writes file  sync                         0          0          0          0
      Direct writes file  async                        0          0          0          0
      Direct writes mixed sync                         0          0          0          0
      Direct writes mixed async                        0          0          0          0
      KSwapd writes anon  sync                         0          0          0          0
      KSwapd writes anon  async                    91682          0          0          0
      KSwapd writes file  sync                         0          0          0          0
      KSwapd writes file  async                   822629          0          0          0
      KSwapd writes mixed sync                         0          0          0          0
      KSwapd writes mixed async                        0          0          0          0
    
    In 3.2, kswapd was doing a bunch of async writes of pages but
    reclaim/compaction was never reaching a point where it was doing sync
    IO.  This does not guarantee that reclaim/compaction was not calling
    wait_on_page_writeback() but I would consider it unlikely.  It indicates
    that merging patches 2 and 3 to stop reclaim/compaction calling
    wait_on_page_writeback() should be safe.
    
    This patch:
    
    Lumpy reclaim had a purpose but in the mind of some, it was to kick the
    system so hard it trashed.  For others the purpose was to complicate
    vmscan.c.  Over time it was giving softer shoes and a nicer attitude but
    memory compaction needs to step up and replace it so this patch sends
    lumpy reclaim to the farm.
    
    The tracepoint format changes for isolating LRU pages with this patch
    applied.  Furthermore reclaim/compaction can no longer queue dirty pages
    in pageout() if the underlying BDI is congested.  Lumpy reclaim used
    this logic and reclaim/compaction was using it in error.
    
    Change-Id: Ib2992962c9e99cf250a7f859bb2a67034051e4d4
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Acked-by: Rik van Riel <riel@redhat.com>
    Acked-by: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Cc: Konstantin Khlebnikov <khlebnikov@openvz.org>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: Ying Han <yinghan@google.com>
    Cc: Andy Whitcroft <apw@shadowen.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Git-commit: c53919adc045bf803252e912f23028a68525753d
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit 8e0a15270ce3ed2f080693889ebe8a254f079513
Author: Konstantin Khlebnikov <khlebnikov@openvz.org>
Date:   Tue May 29 15:06:53 2012 -0700

    mm: push lru index into shrink_[in]active_list()
    
    Let's toss lru index through call stack to isolate_lru_pages(), this is
    better than its reconstructing from individual bits.
    
    Change-Id: Id07444ba97af9699ecfff1750db13cb2fee147fc
    [akpm@linux-foundation.org: fix kerneldoc, per Minchan]
    Signed-off-by: Konstantin Khlebnikov <khlebnikov@openvz.org>
    Acked-by: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Acked-by: Hugh Dickins <hughd@google.com>
    Cc: Glauber Costa <glommer@parallels.com>
    Cc: Michal Hocko <mhocko@suse.cz>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Minchan Kim <minchan@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Git-commit: 3cb9945179bd04e9282f31a1173ac11b1300c462
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit 6b50a5b89e0df62adf3f9058a5594e5eda75c3cb
Author: David S. Miller <davem@davemloft.net>
Date:   Tue Dec 15 15:39:08 2015 -0500

    bluetooth: Validate socket address length in sco_sock_bind().
    
    Change-Id: I890640975f1af64f71947b6a1820249e08f6375b
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 1ebfddb6ca254d5eb2671195d6ac5e3fa67e3689
Author: Hannes Frederic Sowa <hannes@stressinduktion.org>
Date:   Mon Dec 14 21:03:39 2015 +0000

    net: add validation for the socket syscall protocol argument
    
     reported that one could simply crash the kernel as root by
    using a simple program:
    
    	int socket_fd;
    	struct sockaddr_in addr;
    	addr.sin_port = 0;
    	addr.sin_addr.s_addr = INADDR_ANY;
    	addr.sin_family = 10;
    
    	socket_fd = socket(10,3,0x40000000);
    	connect(socket_fd , &addr,16);
    
    AF_INET, AF_INET6 sockets actually only support 8-bit protocol
    identifiers. inet_sock's skc_protocol field thus is sized accordingly,
    thus larger protocol identifiers simply cut off the higher bits and
    store a zero in the protocol fields.
    
    This could lead to e.g. NULL function pointer because as a result of
    the cut off inet_num is zero and we call down to inet_autobind, which
    is NULL for raw sockets.
    
    kernel: Call Trace:
    kernel:  [<ffffffff816db90e>] ? inet_autobind+0x2e/0x70
    kernel:  [<ffffffff816db9a4>] inet_dgram_connect+0x54/0x80
    kernel:  [<ffffffff81645069>] SYSC_connect+0xd9/0x110
    kernel:  [<ffffffff810ac51b>] ? ptrace_notify+0x5b/0x80
    kernel:  [<ffffffff810236d8>] ? syscall_trace_enter_phase2+0x108/0x200
    kernel:  [<ffffffff81645e0e>] SyS_connect+0xe/0x10
    kernel:  [<ffffffff81779515>] tracesys_phase2+0x84/0x89
    
    I found no particular commit which introduced this problem.
    
    Change-Id: I653fad90da54908144cc8916c2dccb1fa6f14eed
    CVE: CVE-2015-8543
    Cc: Cong Wang <cwang@twopensource.com>
    Reported-by:  <guoyonggang@360.cn>
    Signed-off-by: Hannes Frederic Sowa <hannes@stressinduktion.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 749fd9493008b013759a4cdf890903d9fd751251
Author: Marcelo Leitner <mleitner@redhat.com>
Date:   Mon Feb 23 11:17:13 2015 -0300

    ipv6: addrconf: validate new MTU before applying it
    
    Currently we don't check if the new MTU is valid or not and this allows
    one to configure a smaller than minimum allowed by RFCs or even bigger
    than interface own MTU, which is a problem as it may lead to packet
    drops.
    
    If you have a daemon like NetworkManager running, this may be exploited
    by remote attackers by forging RA packets with an invalid MTU, possibly
    leading to a DoS. (NetworkManager currently only validates for values
    too small, but not for too big ones.)
    
    The fix is just to make sure the new value is valid. That is, between
    IPV6_MIN_MTU and interface's MTU.
    
    Note that similar check is already performed at
    ndisc_router_discovery(), for when kernel itself parses the RA.
    
    Change-Id: I79c3b9e5b0b8f9468a975f876b9bf3f1247d9fc8
    Signed-off-by: Marcelo Ricardo Leitner <mleitner@redhat.com>
    Signed-off-by: Sabrina Dubroca <sd@queasysnail.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit e64922f1a9ca26dbd53958f77609fbc5602e519f
Author: Dmitry Torokhov <dtor@google.com>
Date:   Thu Jul 9 17:17:57 2015 -0700

    net: fix iterating over hashtable in tcp_nuke_addr()
    
    The actual size of the tcp hashinfo table is tcp_hashinfo.ehash_mask + 1
    so we need to adjust the loop accordingly to get the sockets hashed into
    the last bucket.
    
    Change-Id: I796b3c7b4a1a7fa35fba9e5192a4a403eb6e17de
    Signed-off-by: Dmitry Torokhov <dtor@google.com>

commit d423651774aa70884ffd7ece8d336c32608729d6
Author: Mark Grondona <mgrondona@llnl.gov>
Date:   Wed Sep 11 14:24:31 2013 -0700

    __ptrace_may_access() should not deny sub-threads
    
    commit 73af963f9f3036dffed55c3a2898598186db1045 upstream.
    
    __ptrace_may_access() checks get_dumpable/ptrace_has_cap/etc if task !=
    current, this can can lead to surprising results.
    
    For example, a sub-thread can't readlink("/proc/self/exe") if the
    executable is not readable.  setup_new_exec()->would_dump() notices that
    inode_permission(MAY_READ) fails and then it does
    set_dumpable(suid_dumpable).  After that get_dumpable() fails.
    
    (It is not clear why proc_pid_readlink() checks get_dumpable(), perhaps we
    could add PTRACE_MODE_NODUMPABLE)
    
    Change __ptrace_may_access() to use same_thread_group() instead of "task
    == current".  Any security check is pointless when the tasks share the
    same ->mm.
    
    Change-Id: Ia9376d54188dcb3914c5a6a7ce4ade8c3e2154cc
    Signed-off-by: Mark Grondona <mgrondona@llnl.gov>
    Signed-off-by: Ben Woodard <woodard@redhat.com>
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 478bc2140da8d16f4298f42aaf94b3c0eaa41d90
Author: Jeff Vander Stoep <jeffv@google.com>
Date:   Wed Mar 11 14:32:24 2015 -0700

    mm: reorder can_do_mlock to fix audit denial
    
    A userspace call to mmap(MAP_LOCKED) may result in the successful locking
    of memory while also producing a confusing audit log denial.  can_do_mlock
    checks capable and rlimit.  If either of these return positive
    can_do_mlock returns true.  The capable check leads to an LSM hook used by
    apparmour and selinux which produce the audit denial.  Reordering so
    rlimit is checked first eliminates the denial on success, only recording a
    denial when the lock is unsuccessful as a result of the denial.
    
    Change-Id: I8af127fb3eb6c95cfc743920f387cfa025fbfc32
    Signed-off-by: Jeff Vander Stoep <jeffv@google.com>
    Acked-by: Nick Kralevich <nnk@google.com>
    Cc: Jeff Vander Stoep <jeffv@google.com>
    Cc: Sasha Levin <sasha.levin@oracle.com>
    Cc: "Paul E. McKenney" <paulmck@linux.vnet.ibm.com>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Cc: Paul Cassella <cassella@cray.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>

commit 615086f8afd59e4eae591c2d5edf63f2fc253929
Author: David Howells <dhowells@redhat.com>
Date:   Thu Oct 15 17:21:37 2015 +0100

    KEYS: Fix crash when attempt to garbage collect an uninstantiated keyring
    
    The following sequence of commands:
    
        i=`keyctl add user a a @s`
        keyctl request2 keyring foo bar @t
        keyctl unlink $i @s
    
    tries to invoke an upcall to instantiate a keyring if one doesn't already
    exist by that name within the user's keyring set.  However, if the upcall
    fails, the code sets keyring->type_data.reject_error to -ENOKEY or some
    other error code.  When the key is garbage collected, the key destroy
    function is called unconditionally and keyring_destroy() uses list_empty()
    on keyring->type_data.link - which is in a union with reject_error.
    Subsequently, the kernel tries to unlink the keyring from the keyring names
    list - which oopses like this:
    
    	BUG: unable to handle kernel paging request at 00000000ffffff8a
    	IP: [<ffffffff8126e051>] keyring_destroy+0x3d/0x88
    	...
    	Workqueue: events key_garbage_collector
    	...
    	RIP: 0010:[<ffffffff8126e051>] keyring_destroy+0x3d/0x88
    	RSP: 0018:ffff88003e2f3d30  EFLAGS: 00010203
    	RAX: 00000000ffffff82 RBX: ffff88003bf1a900 RCX: 0000000000000000
    	RDX: 0000000000000000 RSI: 000000003bfc6901 RDI: ffffffff81a73a40
    	RBP: ffff88003e2f3d38 R08: 0000000000000152 R09: 0000000000000000
    	R10: ffff88003e2f3c18 R11: 000000000000865b R12: ffff88003bf1a900
    	R13: 0000000000000000 R14: ffff88003bf1a908 R15: ffff88003e2f4000
    	...
    	CR2: 00000000ffffff8a CR3: 000000003e3ec000 CR4: 00000000000006f0
    	...
    	Call Trace:
    	 [<ffffffff8126c756>] key_gc_unused_keys.constprop.1+0x5d/0x10f
    	 [<ffffffff8126ca71>] key_garbage_collector+0x1fa/0x351
    	 [<ffffffff8105ec9b>] process_one_work+0x28e/0x547
    	 [<ffffffff8105fd17>] worker_thread+0x26e/0x361
    	 [<ffffffff8105faa9>] ? rescuer_thread+0x2a8/0x2a8
    	 [<ffffffff810648ad>] kthread+0xf3/0xfb
    	 [<ffffffff810647ba>] ? kthread_create_on_node+0x1c2/0x1c2
    	 [<ffffffff815f2ccf>] ret_from_fork+0x3f/0x70
    	 [<ffffffff810647ba>] ? kthread_create_on_node+0x1c2/0x1c2
    
    Note the value in RAX.  This is a 32-bit representation of -ENOKEY.
    
    The solution is to only call ->destroy() if the key was successfully
    instantiated.
    
    Change-Id: Ia52370813b7e8231fdd99d2a208340af1c7b4007
    Reported-by: Dmitry Vyukov <dvyukov@google.com>
    Signed-off-by: David Howells <dhowells@redhat.com>
    Tested-by: Dmitry Vyukov <dvyukov@google.com>

commit 5711910fe7f59bb883dd29903c1819bc69116e8b
Author: David Howells <dhowells@redhat.com>
Date:   Fri Sep 25 16:30:08 2015 +0100

    KEYS: Fix race between key destruction and finding a keyring by name
    
    There appears to be a race between:
    
     (1) key_gc_unused_keys() which frees key->security and then calls
         keyring_destroy() to unlink the name from the name list
    
     (2) find_keyring_by_name() which calls key_permission(), thus accessing
         key->security, on a key before checking to see whether the key usage is 0
         (ie. the key is dead and might be cleaned up).
    
    Fix this by calling ->destroy() before cleaning up the core key data -
    including key->security.
    
    Change-Id: I4b9b89af020e6348af095e9014bf23b5eb1a9ef9
    Reported-by: Petr Matousek <pmatouse@redhat.com>
    Signed-off-by: David Howells <dhowells@redhat.com>

commit c6a3ac72398db7209ef4c56c024830682cbe8556
Author: David Howells <dhowells@redhat.com>
Date:   Tue Nov 24 21:36:31 2015 +0000

    KEYS: Fix handling of stored error in a negatively instantiated user key
    
    If a user key gets negatively instantiated, an error code is cached in the
    payload area.  A negatively instantiated key may be then be positively
    instantiated by updating it with valid data.  However, the ->update key
    type method must be aware that the error code may be there.
    
    The following may be used to trigger the bug in the user key type:
    
        keyctl request2 user user "" @u
        keyctl add user user "a" @u
    
    which manifests itself as:
    
    	BUG: unable to handle kernel paging request at 00000000ffffff8a
    	IP: [<ffffffff810a376f>] __call_rcu.constprop.76+0x1f/0x280 kernel/rcu/tree.c:3046
    	PGD 7cc30067 PUD 0
    	Oops: 0002 [#1] SMP
    	Modules linked in:
    	CPU: 3 PID: 2644 Comm: a.out Not tainted 4.3.0+ #49
    	Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS Bochs 01/01/2011
    	task: ffff88003ddea700 ti: ffff88003dd88000 task.ti: ffff88003dd88000
    	RIP: 0010:[<ffffffff810a376f>]  [<ffffffff810a376f>] __call_rcu.constprop.76+0x1f/0x280
    	 [<ffffffff810a376f>] __call_rcu.constprop.76+0x1f/0x280 kernel/rcu/tree.c:3046
    	RSP: 0018:ffff88003dd8bdb0  EFLAGS: 00010246
    	RAX: 00000000ffffff82 RBX: 0000000000000000 RCX: 0000000000000001
    	RDX: ffffffff81e3fe40 RSI: 0000000000000000 RDI: 00000000ffffff82
    	RBP: ffff88003dd8bde0 R08: ffff88007d2d2da0 R09: 0000000000000000
    	R10: 0000000000000000 R11: ffff88003e8073c0 R12: 00000000ffffff82
    	R13: ffff88003dd8be68 R14: ffff88007d027600 R15: ffff88003ddea700
    	FS:  0000000000b92880(0063) GS:ffff88007fd00000(0000) knlGS:0000000000000000
    	CS:  0010 DS: 0000 ES: 0000 CR0: 000000008005003b
    	CR2: 00000000ffffff8a CR3: 000000007cc5f000 CR4: 00000000000006e0
    	Stack:
    	 ffff88003dd8bdf0 ffffffff81160a8a 0000000000000000 00000000ffffff82
    	 ffff88003dd8be68 ffff88007d027600 ffff88003dd8bdf0 ffffffff810a39e5
    	 ffff88003dd8be20 ffffffff812a31ab ffff88007d027600 ffff88007d027620
    	Call Trace:
    	 [<ffffffff810a39e5>] kfree_call_rcu+0x15/0x20 kernel/rcu/tree.c:3136
    	 [<ffffffff812a31ab>] user_update+0x8b/0xb0 security/keys/user_defined.c:129
    	 [<     inline     >] __key_update security/keys/key.c:730
    	 [<ffffffff8129e5c1>] key_create_or_update+0x291/0x440 security/keys/key.c:908
    	 [<     inline     >] SYSC_add_key security/keys/keyctl.c:125
    	 [<ffffffff8129fc21>] SyS_add_key+0x101/0x1e0 security/keys/keyctl.c:60
    	 [<ffffffff8185f617>] entry_SYSCALL_64_fastpath+0x12/0x6a arch/x86/entry/entry_64.S:185
    
    Note the error code (-ENOKEY) in EDX.
    
    A similar bug can be tripped by:
    
        keyctl request2 trusted user "" @u
        keyctl add trusted user "a" @u
    
    This should also affect encrypted keys - but that has to be correctly
    parameterised or it will fail with EINVAL before getting to the bit that
    will crashes.
    
    Change-Id: I171d566f431c56208e1fe279f466d2d399a9ac7c
    Reported-by: Dmitry Vyukov <dvyukov@google.com>
    Signed-off-by: David Howells <dhowells@redhat.com>
    Acked-by: Mimi Zohar <zohar@linux.vnet.ibm.com>
    Signed-off-by: James Morris <james.l.morris@oracle.com>

commit 50d7287ffc28b0c9aa27317f39013cb2d3f481e4
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri Apr 20 13:05:50 2012 +0000

    arm: Use generic idle thread allocation
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Cc: Russell King <linux@arm.linux.org.uk>
    Tested-by: Frank Rowand <frank.rowand@am.sony.com>
    Link: http://lkml.kernel.org/r/20120420124557.448826362@linutronix.de
    
    Change-Id: I5d578157a22fc7accd152a4515e4dd2c9b086b75

commit ed5d5c0beec72db0cebc5fa4ddbc3901054c59bf
Author: Vinayak Menon <vinmenon@codeaurora.org>
Date:   Wed Aug 19 16:16:39 2015 +0530

    mm: vmpressure: account allocstalls only on higher pressures
    
    At present any vmpressure value is scaled up if the pages are
    reclaimed through direct reclaim. This can result in false
    vmpressure values. Consider a case where a device is booted up
    and most of the memory is occuppied by file pages. kswapd will
    make sure that high watermark is maintained. Now when a sudden
    huge allocation request comes in, the system will definitely
    have to get into direct reclaims. The vmpressures can be very low,
    but because of allocstall accounting logic even these low values
    will be scaled to values nearing 100. This can resulting in
    unnecessary LMK kills for example.
    
    Change-Id: Idd7c6724264ac89f1f68f2e9d70a32390ffca3e5
    Signed-off-by: Vinayak Menon <vinmenon@codeaurora.org>

commit 195f69a91f15ef0c582e31ce7ec3ecbc979c597b
Author: Vinayak Menon <vinmenon@codeaurora.org>
Date:   Tue Mar 31 11:06:29 2015 +0530

    mm: vmpressure: scale pressure based on reclaim context
    
    The existing calculation of vmpressure takes into account only
    the ratio of reclaimed to scanned pages, but not the time spent
    or the difficulty in reclaiming those pages. For e.g. when there
    are quite a number of file pages in the system, an allocation
    request can be satisfied by reclaiming the file pages alone. If
    such a reclaim is succesful, the vmpressure value will remain low
    irrespective of the time spent by the reclaim code to free up the
    file pages. With a feature like lowmemorykiller, killing a task
    can be faster than reclaiming the file pages alone. So if the
    vmpressure values reflect the reclaim difficulty level, clients
    can make a decision based on that, for e.g. to kill a task early.
    
    This patch monitors the number of pages scanned in the direct
    reclaim path and scales the vmpressure level according to that.
    
    Signed-off-by: Vinayak Menon <vinmenon@codeaurora.org>
    Change-Id: I6e643d29a9a1aa0814309253a8b690ad86ec0b13

commit 3017267e4096dedb600632c0d7345aefc78ffeaf
Author: Laura Abbott <lauraa@codeaurora.org>
Date:   Mon Sep 22 13:26:28 2014 -0700

    mm: Add notifier framework for showing memory
    
    There are many drivers in the kernel which can hold on
    to lots of memory. It can be useful to dump out all those
    drivers at key points in the kernel. Introduct a notifier
    framework for dumping this information. When the notifiers
    are called, drivers can dump out the state of any memory
    they may be using.
    
    Change-Id: Ifb2946964bf5d072552dd56d8d6dfdd794af6d84
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit 2169967b1b39e117346a59181af91e9ba3d3a3f9
Author: Vinayak Menon <vinmenon@codeaurora.org>
Date:   Wed Mar 4 16:38:28 2015 +0530

    mm: vmpressure: allow in-kernel clients to subscribe for events
    
    Currently, vmpressure is tied to memcg and its events are
    available only to userspace clients. This patch removes
    the dependency on CONFIG_MEMCG and adds a mechanism for
    in-kernel clients to subscribe for vmpressure events (in
    fact raw vmpressure values are delivered instead of vmpressure
    levels, to provide clients more flexibility to take actions
    on custom pressure levels which are not currently defined
    by vmpressure module).
    
    Change-Id: I38010f166546e8d7f12f5f355b5dbfd6ba04d587
    Signed-off-by: Vinayak Menon <vinmenon@codeaurora.org>

commit 3e072aa838ada9e382f8289eff600cbe4f15fc48
Author: Anton Vorontsov <anton.vorontsov@linaro.org>
Date:   Mon Apr 29 15:08:31 2013 -0700

    memcg: add memory.pressure_level events
    
    With this patch userland applications that want to maintain the
    interactivity/memory allocation cost can use the pressure level
    notifications.  The levels are defined like this:
    
    The "low" level means that the system is reclaiming memory for new
    allocations.  Monitoring this reclaiming activity might be useful for
    maintaining cache level.  Upon notification, the program (typically
    "Activity Manager") might analyze vmstat and act in advance (i.e.
    prematurely shutdown unimportant services).
    
    The "medium" level means that the system is experiencing medium memory
    pressure, the system might be making swap, paging out active file
    caches, etc.  Upon this event applications may decide to further analyze
    vmstat/zoneinfo/memcg or internal memory usage statistics and free any
    resources that can be easily reconstructed or re-read from a disk.
    
    The "critical" level means that the system is actively thrashing, it is
    about to out of memory (OOM) or even the in-kernel OOM killer is on its
    way to trigger.  Applications should do whatever they can to help the
    system.  It might be too late to consult with vmstat or any other
    statistics, so it's advisable to take an immediate action.
    
    The events are propagated upward until the event is handled, i.e.  the
    events are not pass-through.  Here is what this means: for example you
    have three cgroups: A->B->C.  Now you set up an event listener on
    cgroups A, B and C, and suppose group C experiences some pressure.  In
    this situation, only group C will receive the notification, i.e.  groups
    A and B will not receive it.  This is done to avoid excessive
    "broadcasting" of messages, which disturbs the system and which is
    especially bad if we are low on memory or thrashing.  So, organize the
    cgroups wisely, or propagate the events manually (or, ask us to
    implement the pass-through events, explaining why would you need them.)
    
    Performance wise, the memory pressure notifications feature itself is
    lightweight and does not require much of bookkeeping, in contrast to the
    rest of memcg features.  Unfortunately, as of current memcg
    implementation, pages accounting is an inseparable part and cannot be
    turned off.  The good news is that there are some efforts[1] to improve
    the situation; plus, implementing the same, fully API-compatible[2]
    interface for CONFIG_CGROUP_MEM_RES_CTLR=n case (e.g.  embedded) is also
    a viable option, so it will not require any changes on the userland
    side.
    
    [1] http://permalink.gmane.org/gmane.linux.kernel.cgroups/6291
    [2] http://lkml.org/lkml/2013/2/21/454
    
    Change-Id: Ib7b9f7986008d423b84152e2781d0402a560c95e
    [akpm@linux-foundation.org: coding-style fixes]
    [akpm@linux-foundation.org: fix CONFIG_CGROPUPS=n warnings]
    Signed-off-by: Anton Vorontsov <anton.vorontsov@linaro.org>
    Acked-by: Kirill A. Shutemov <kirill@shutemov.name>
    Acked-by: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Glauber Costa <glommer@parallels.com>
    Cc: Michal Hocko <mhocko@suse.cz>
    Cc: Luiz Capitulino <lcapitulino@redhat.com>
    Cc: Greg Thelen <gthelen@google.com>
    Cc: Leonid Moiseichuk <leonid.moiseichuk@nokia.com>
    Cc: KOSAKI Motohiro <kosaki.motohiro@gmail.com>
    Cc: Minchan Kim <minchan@kernel.org>
    Cc: Bartlomiej Zolnierkiewicz <b.zolnierkie@samsung.com>
    Cc: John Stultz <john.stultz@linaro.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    
     Conflicts:
    	mm/vmscan.c

commit d0ac2d27aa6beb2fd972abf7942685b92e6b9b2f
Author: Ruchi Kandoi <kandoiruchi@google.com>
Date:   Thu Apr 23 12:09:09 2015 -0700

    nf: IDLETIMER: Adds the uid field in the msg
    
    Message notifications contains an additional uid field. This field
    represents the uid that was responsible for waking the radio. And hence
    it is present only in notifications stating that the radio is now
    active.
    
    Change-Id: I18fc73eada512e370d7ab24fc9f890845037b729
    Signed-off-by: Ruchi Kandoi <kandoiruchi@google.com>
    Bug: 20264396

commit 1b02bd6e3efcb1fb8e55b6fb4e04d3b9181f01d5
Author: Ruchi Kandoi <kandoiruchi@google.com>
Date:   Tue Mar 25 16:43:28 2014 -0700

    nf: IDLETIMER: time-stamp and suspend/resume handling.
    
    Message notifications contains an additional timestamp field in nano seconds.
    The expiry time for the timers are modified during suspend/resume.
    If timer was supposed to expire while the system is suspended then a
    notification is sent when it resumes with the timestamp of the scheduled expiry.
    
    Removes the race condition for multiple work scheduled.
    
    Bug: 13247811
    
    Change-Id: I752c5b00225fe7085482819f975cc0eb5af89bff
    Signed-off-by: Ruchi Kandoi <kandoiruchi@google.com>

commit cb6a263c3116ee1abd20ec3e679e094242d0c1da
Author: Ruchi Kandoi <kandoiruchi@google.com>
Date:   Mon Mar 24 14:27:59 2014 -0700

    Remove build & conflict errors for commit 1d8c783dff935599870485f76fdfc76ee84799e9
    
    Signed-off-by: Ruchi Kandoi<kandoiruchi@google.com>
    Change-Id: I549228fa5b1b95d3fc2b4c62c264126923424811

commit 240e107ee9cd7944d218be5ed7eb2f278ae75f5e
Author: Ruchi Kandoi <kandoiruchi@google.com>
Date:   Mon Mar 24 13:54:42 2014 -0700

    Revert "nf: IDLETIMER: time-stamp and suspend/resume handling."
    
    This reverts commit b1027c0228924d23cc3918d43fbf9fbc21f0dbe9.
    
    Conflicts:
    	net/netfilter/xt_IDLETIMER.c
    
    Change-Id: Ie98e5f1fcf1ef3c087021d3e24396f36457b2270

commit 3afaaf647f0a1bbc8056037857f614a3e23d7b55
Author: Ruchi Kandoi <kandoiruchi@google.com>
Date:   Mon Mar 24 13:31:27 2014 -0700

    Revert "nf: Fix PM notifier list corruption"
    
    This reverts commit 5e647ffb460d9301f05281ba0a5ca6e257129c8d.
    
    Change-Id: Idb3f446ed978c116ec6cb291995d092ae5edb788

commit 1c26c2249b77b0e9c6140f2cc49a40e70a7bb42f
Author: Ruchi Kandoi <kandoiruchi@google.com>
Date:   Mon Mar 17 13:59:39 2014 -0700

    nf: Fix PM notifier list corruption
    
    unregister the pm_notifier when the netfilter is destroyed.
    check for null timer when system suspends.
    
    Bug: 13508802
    Bug: 13508117
    
    Change-Id: I8ff0c18993d8374103fae6f35183c3eae8a7e6f0
    Signed-off-by: Ruchi Kandoi <kandoiruchi@google.com>

commit 6e7cf3735ea7b9fa4c1fadc735ed267729929cfc
Author: Ruchi Kandoi <kandoiruchi@google.com>
Date:   Thu Mar 13 12:36:02 2014 -0700

    nf: time keeping changed from wall-clock to boot-time
    
    Time stamps for the last modified and delay-triggered time store
    boot-time instead of wall-clock time.
    
    Change-Id: I256ae28dafac60add3b0b924c706ecdb71f5bbbf
    Signed-off-by: Ruchi Kandoi <kandoiruchi@google.com>

commit 8f6cd605f518541abc5042d463c0d5f870919493
Author: Ruchi Kandoi <kandoiruchi@google.com>
Date:   Wed Mar 5 13:24:29 2014 -0800

    nf: IDLETIMER: time-stamp and suspend/resume handling.
    
    Message notifications contains an additional timestamp field in nano
    seconds.
    The expiry time for the timers are modified during suspend/resume.
    If timer was supposed to expire while the system is suspended then
    a notification is sent when it resumes with the timestamp of the
    scheduled expiry.
    
    Removes the race condition for multiple work scheduled.
    
    Change-Id: I52fdd2800aaf88e477df1228474b4965a9397450
    Signed-off-by: Ruchi Kandoi <kandoiruchi@google.com>
    Bug: 13247811

commit 7e2eee7b6ecdd3d741a75d2d95a3c46ea5fdcfb5
Author: Sabrina Dubroca <sd@queasysnail.net>
Date:   Thu Oct 15 12:25:00 2015 -0500

    net: add length argument to skb_copy_and_csum_datagram_iovec
    
    Without this length argument, we can read past the end of the iovec in
    memcpy_toiovec because we have no way of knowing the total length of the
    iovec's buffers.
    
    This is needed for stable kernels where 89c22d8c3b27 ("net: Fix skb
    csum races when peeking") has been backported but that don't have the
    ioviter conversion, which is almost all the stable trees <= 3.18.
    
    This also fixes a kernel crash for NFS servers when the client uses
     -onfsvers=3,proto=udp to mount the export.
    
    Change-Id: I1865e3d7a1faee42a5008a9ad58c4d3323ea4bab
    Signed-off-by: Sabrina Dubroca <sd@queasysnail.net>
    Reviewed-by: Hannes Frederic Sowa <hannes@stressinduktion.org>

commit 6a96ab3b57eb4bf0f0ee6a0cf953cda2924c4c9f
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Thu Nov 12 10:11:19 2015 -0600

    f2fs: sync to upstream
    
    https://git.kernel.org/cgit/linux/kernel/git/jaegeuk/f2fs.git/commit/?h=linux-3.4&id=994642cfdbc08f1eda7ff48504f779cbcc9e3067
    
    Change-Id: I898ed8f9df510fa24ac3f197d879a056c947662a

commit ed94ac182cf896c1943f595f76640269dd1361c5
Author: Domi Papoi <dpapoi@codeaurora.org>
Date:   Tue Aug 11 17:31:50 2015 -0400

    msm: video: Checks for code robustness
    
    Check for NULL pointer and array out of bounds
    
    Change-Id: I42fb2b6fb087e6e4a99b2783d2b68499e802541a
    Signed-off-by: Domi Papoi <dpapoi@codeaurora.org>

commit 48b3ae123f5885fa8ef83f250f690b437f22e81e
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun Nov 15 18:32:05 2015 +0100

    hid-logitech: this was already removed on an earlier linux update

commit 7746656dd7a5970f4284b2b2525eed7f15b0d95a
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun Nov 15 18:22:17 2015 +0100

    OJ2 source drop update!

commit 68561fb5d027ff1e1569041a613de0c366338c94
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Oct 22 19:00:07 2015 +0200

    Linux 3.4.110

commit 01a51916b1803c0bfadd87ad27d297b866bd9c27
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun Oct 18 22:56:39 2015 +0200

    update defconfig files

commit f5fc944d6c61b1dff9fe9368d48f03f2b7194b9c
Author: Kim, Milo <Milo.Kim@ti.com>
Date:   Thu Apr 26 22:01:10 2012 -0700

    staging: android: timed_output : disable the timed output device when the device is unregistered
    
    (Life cycle of timed output device driver)
    
    1) register the device as the timed output
    2) enable() ops is called via the sysfs
       timeout > 0 : timer is activated and device is turned on
       timeout = 0 : timer is cancelled and device is off
    3) unregister the timed output device if not used any more
    
    So the registered device should be disabled explicitly when the module is removed.
    ('disabled' means the timer is stopped and the device is turned off)
    
    Rather than implementing that code in each driver,
    just call enable() with timeout = 0 to clean up the driver.
    
    Signed-off-by: Milo(Woogyom) Kim <milo.kim@ti.com>
    Cc: Mike Lockwood <lockwood@android.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 5beaf8611e7f7b960128dc74c279ae85d5125630
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun Oct 18 20:42:29 2015 +0200

    fix all occuring compiling errors

commit 67a1ae5fbc867313392b17911df191cddbc31968
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Oct 14 22:32:17 2015 +0200

    cpufreq: some cleanup

commit d0a1aa64afa5758583dccc0cd143fdd40c8d9621
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Fri Dec 21 20:38:00 2012 -0800

    proc: Allow proc_free_inum to be called from any context
    
    While testing the pid namespace code I hit this nasty warning.
    
    [  176.262617] ------------[ cut here ]------------
    [  176.263388] WARNING: at /home/eric/projects/linux/linux-userns-devel/kernel/softirq.c:160 local_bh_enable_ip+0x7a/0xa0()
    [  176.265145] Hardware name: Bochs
    [  176.265677] Modules linked in:
    [  176.266341] Pid: 742, comm: bash Not tainted 3.7.0userns+ #18
    [  176.266564] Call Trace:
    [  176.266564]  [<ffffffff810a539f>] warn_slowpath_common+0x7f/0xc0
    [  176.266564]  [<ffffffff810a53fa>] warn_slowpath_null+0x1a/0x20
    [  176.266564]  [<ffffffff810ad9ea>] local_bh_enable_ip+0x7a/0xa0
    [  176.266564]  [<ffffffff819308c9>] _raw_spin_unlock_bh+0x19/0x20
    [  176.266564]  [<ffffffff8123dbda>] proc_free_inum+0x3a/0x50
    [  176.266564]  [<ffffffff8111d0dc>] free_pid_ns+0x1c/0x80
    [  176.266564]  [<ffffffff8111d195>] put_pid_ns+0x35/0x50
    [  176.266564]  [<ffffffff810c608a>] put_pid+0x4a/0x60
    [  176.266564]  [<ffffffff8146b177>] tty_ioctl+0x717/0xc10
    [  176.266564]  [<ffffffff810aa4d5>] ? wait_consider_task+0x855/0xb90
    [  176.266564]  [<ffffffff81086bf9>] ? default_spin_lock_flags+0x9/0x10
    [  176.266564]  [<ffffffff810cab0a>] ? remove_wait_queue+0x5a/0x70
    [  176.266564]  [<ffffffff811e37e8>] do_vfs_ioctl+0x98/0x550
    [  176.266564]  [<ffffffff810b8a0f>] ? recalc_sigpending+0x1f/0x60
    [  176.266564]  [<ffffffff810b9127>] ? __set_task_blocked+0x37/0x80
    [  176.266564]  [<ffffffff810ab95b>] ? sys_wait4+0xab/0xf0
    [  176.266564]  [<ffffffff811e3d31>] sys_ioctl+0x91/0xb0
    [  176.266564]  [<ffffffff810a95f0>] ? task_stopped_code+0x50/0x50
    [  176.266564]  [<ffffffff81939199>] system_call_fastpath+0x16/0x1b
    [  176.266564] ---[ end trace 387af88219ad6143 ]---
    
    It turns out that spin_unlock_bh(proc_inum_lock) is not safe when
    put_pid is called with another spinlock held and irqs disabled.
    
    For now take the easy path and use spin_lock_irqsave(proc_inum_lock)
    in proc_free_inum and spin_loc_irq in proc_alloc_inum(proc_inum_lock).
    
    Signed-off-by: "Eric W. Biederman" <ebiederm@xmission.com>
    Bug: 22173056
    Backport: commits 0f7aa3c to this one are backport of mnt namespace
    Signed-off-by: Thierry Strudel <tstrudel@google.com>
    (cherry picked from commit dfb2ea45becb198beeb75350d0b7b7ad9076a38f)
    
    Change-Id: If9bb99bae09aa8a4bf302585d77e85d0f1a2c54b

commit 81a407c78bbc97e79ffcfdaaa844c25312dcec6e
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Wed Jun 15 10:21:48 2011 -0700

    proc: Usable inode numbers for the namespace file descriptors.
    
    Assign a unique proc inode to each namespace, and use that
    inode number to ensure we only allocate at most one proc
    inode for every namespace in proc.
    
    A single proc inode per namespace allows userspace to test
    to see if two processes are in the same namespace.
    
    This has been a long requested feature and only blocked because
    a naive implementation would put the id in a global space and
    would ultimately require having a namespace for the names of
    namespaces, making migration and certain virtualization tricks
    impossible.
    
    We still don't have per superblock inode numbers for proc, which
    appears necessary for application unaware checkpoint/restart and
    migrations (if the application is using namespace file descriptors)
    but that is now allowd by the design if it becomes important.
    
    I have preallocated the ipc and uts initial proc inode numbers so
    their structures can be statically initialized.
    
    Signed-off-by: Eric W. Biederman <ebiederm@xmission.com>
    (cherry picked from commit 98f842e675f96ffac96e6c50315790912b2812be)
    
    Change-Id: I1d88fe77f3f209df97e056a0a6f47d48d035a552

commit 493b3e310f24c3f92b0961e672cce7d87c521b8e
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Sat Jun 18 17:48:18 2011 -0700

    proc: Fix the namespace inode permission checks.
    
    Change the proc namespace files into symlinks so that
    we won't cache the dentries for the namespace files
    which can bypass the ptrace_may_access checks.
    
    To support the symlinks create an additional namespace
    inode with it's own set of operations distinct from the
    proc pid inode and dentry methods as those no longer
    make sense.
    
    Signed-off-by: Eric W. Biederman <ebiederm@xmission.com>
    (cherry picked from commit bf056bfa80596a5d14b26b17276a56a0dcb080e5)
    
    Change-Id: Ia39cad87181734e8936a8f0f4bc8b0a1d3432b69

commit 57ce08dc4801ad2b81e9d3ada4c7d5ab414c0840
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Fri Jun 17 13:33:20 2011 -0700

    proc: Generalize proc inode allocation
    
    Generalize the proc inode allocation so that it can be
    used without having to having to create a proc_dir_entry.
    
    This will allow namespace file descriptors to remain light
    weight entitities but still have the same inode number
    when the backing namespace is the same.
    
    Acked-by: Serge E. Hallyn <serge.hallyn@ubuntu.com>
    Signed-off-by: Eric W. Biederman <ebiederm@xmission.com>
    (cherry picked from commit 33d6dce607573b5fd7a43168e0d91221b3ca532b)
    
    Change-Id: I05a34dc62d1b22422c87091ffc72771c38762390

commit bec796e0044bdafd91c00cd32c1ad58596c81dfb
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Thu Jul 26 21:42:03 2012 -0700

    vfs: Allow unprivileged manipulation of the mount namespace.
    
    - Add a filesystem flag to mark filesystems that are safe to mount as
      an unprivileged user.
    
    - Add a filesystem flag to mark filesystems that don't need MNT_NODEV
      when mounted by an unprivileged user.
    
    - Relax the permission checks to allow unprivileged users that have
      CAP_SYS_ADMIN permissions in the user namespace referred to by the
      current mount namespace to be allowed to mount, unmount, and move
      filesystems.
    
    Acked-by: "Serge E. Hallyn" <serge@hallyn.com>
    Signed-off-by: "Eric W. Biederman" <ebiederm@xmission.com>
    (cherry picked from commit 0c55cfc4166d9a0f38de779bd4d75a90afbe7734)
    
    Change-Id: I6fffb9b2583d763dc28e7b8097ae8a150cce4093

commit 15d5fa7d5c0b0ee70cd5e4d5c2f4fbbe4180891d
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Tue Jul 31 13:13:04 2012 -0700

    vfs: Only support slave subtrees across different user namespaces
    
    Sharing mount subtress with mount namespaces created by unprivileged
    users allows unprivileged mounts created by unprivileged users to
    propagate to mount namespaces controlled by privileged users.
    
    Prevent nasty consequences by changing shared subtrees to slave
    subtress when an unprivileged users creates a new mount namespace.
    
    Acked-by: Serge Hallyn <serge.hallyn@canonical.com>
    Signed-off-by: "Eric W. Biederman" <ebiederm@xmission.com>
    (cherry picked from commit 7a472ef4be8387bc05a42e16309b02c8ca943a40)
    
    Change-Id: I361627720c1a0b435d675a1e05437043b601f76c

commit 1e1dded6ebfafc375c86e4f5efb592f3022f1d11
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Thu Jul 26 21:08:32 2012 -0700

    vfs: Add a user namespace reference from struct mnt_namespace
    
    This will allow for support for unprivileged mounts in a new user namespace.
    
    Acked-by: "Serge E. Hallyn" <serge@hallyn.com>
    Signed-off-by: "Eric W. Biederman" <ebiederm@xmission.com>
    (cherry picked from commit 771b1371686e0a63e938ada28de020b9a0040f55)
    
    Change-Id: I8d0b3a2d99558680211771129074855f00b761a6

commit 471421292e6a17610d8e808ab82bcf6df276e68f
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Sun Mar 7 18:49:36 2010 -0800

    vfs: Add setns support for the mount namespace
    
    setns support for the mount namespace is a little tricky as an
    arbitrary decision must be made about what to set fs->root and
    fs->pwd to, as there is no expectation of a relationship between
    the two mount namespaces.  Therefore I arbitrarily find the root
    mount point, and follow every mount on top of it to find the top
    of the mount stack.  Then I set fs->root and fs->pwd to that
    location.  The topmost root of the mount stack seems like a
    reasonable place to be.
    
    Bind mount support for the mount namespace inodes has the
    possibility of creating circular dependencies between mount
    namespaces.  Circular dependencies can result in loops that
    prevent mount namespaces from every being freed.  I avoid
    creating those circular dependencies by adding a sequence number
    to the mount namespace and require all bind mounts be of a
    younger mount namespace into an older mount namespace.
    
    Add a helper function proc_ns_inode so it is possible to
    detect when we are attempting to bind mound a namespace inode.
    
    Acked-by: Serge Hallyn <serge.hallyn@canonical.com>
    Signed-off-by: Eric W. Biederman <ebiederm@xmission.com>
    (cherry picked from commit 8823c079ba7136dc1948d6f6dcb5f8022bde438e)
    
    Change-Id: I0dc36f55f7180a1fd7488963071ae0e376b6c867

commit 37dd052d63d1ae77e5c115a4940c75cf081fb0c5
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Fri Sep 21 08:19:02 2012 -0400

    do_add_mount()/umount -l races
    
    normally we deal with lock_mount()/umount races by checking that
    mountpoint to be is still in our namespace after lock_mount() has
    been done.  However, do_add_mount() skips that check when called
    with MNT_SHRINKABLE in flags (i.e. from finish_automount()).  The
    reason is that ->mnt_ns may be a temporary namespace created exactly
    to contain automounts a-la NFS4 referral handling.  It's not the
    namespace of the caller, though, so check_mnt() would fail here.
    We still need to check that ->mnt_ns is non-NULL in that case,
    though.
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>
    (cherry picked from commit 156cacb1d0d36b0d0582d9e798e58e0044f516b3)
    
    Change-Id: Ie8792ac8ab5595a4773709676b12eecae588e551

commit 77483b8e658de6dd7e44ef1cd0b2a78a39040e1a
Author: Josef Bacik <josef@redhat.com>
Date:   Mon Mar 26 09:59:21 2012 -0400

    fs: introduce inode operation ->update_time
    
    Btrfs has to make sure we have space to allocate new blocks in order to modify
    the inode, so updating time can fail.  We've gotten around this by having our
    own file_update_time but this is kind of a pain, and Christoph has indicated he
    would like to make xfs do something different with atime updates.  So introduce
    ->update_time, where we will deal with i_version an a/m/c time updates and
    indicate which changes need to be made.  The normal version just does what it
    has always done, updates the time and marks the inode dirty, and then
    filesystems can choose to do something different.
    
    I've gone through all of the users of file_update_time and made them check for
    errors with the exception of the fault code since it's complicated and I wasn't
    quite sure what to do there, also Jan is going to be pushing the file time
    updates into page_mkwrite for those who have it so that should satisfy btrfs and
    make it not a big deal to check the file_update_time() return code in the
    generic fault path. Thanks,
    
    Signed-off-by: Josef Bacik <josef@redhat.com>
    (cherry picked from commit c3b2da314834499f34cba94f7053e55f6d6f92d8)
    
    Change-Id: I2b6c71d99924d2e1167781022327e809d772f91d

commit e98954dc8b246729ccb7b9ec7f3430a9d4d44564
Author: David Howells <dhowells@redhat.com>
Date:   Mon Jun 25 12:55:28 2012 +0100

    VFS: Comment mount following code
    
    Add comments describing what the directions "up" and "down" mean and ref count
    handling to the VFS mount following family of functions.
    
    Signed-off-by: Valerie Aurora <vaurora@redhat.com> (Original author)
    Signed-off-by: David Howells <dhowells@redhat.com>
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>
    (cherry picked from commit f015f1267b23d3530d3f874243fb83cb5f443005)
    
    Change-Id: I37b3599bf1a6e81a62f8c9f713a47a1d6201412a

commit 8c76cdb0660427aefae5dedbf353a3d4839865c4
Author: David Howells <dhowells@redhat.com>
Date:   Mon Jun 25 12:55:18 2012 +0100

    VFS: Make clone_mnt()/copy_tree()/collect_mounts() return errors
    
    copy_tree() can theoretically fail in a case other than ENOMEM, but always
    returns NULL which is interpreted by callers as -ENOMEM.  Change it to return
    an explicit error.
    
    Also change clone_mnt() for consistency and because union mounts will add new
    error cases.
    
    Thanks to Andreas Gruenbacher <agruen@suse.de> for a bug fix.
    [AV: folded braino fix by Dan Carpenter]
    
    Original-author: Valerie Aurora <vaurora@redhat.com>
    Signed-off-by: David Howells <dhowells@redhat.com>
    Cc: Valerie Aurora <valerie.aurora@gmail.com>
    Cc: Andreas Gruenbacher <agruen@suse.de>
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>
    (cherry picked from commit be34d1a3bc4b6f357a49acb55ae870c81337e4f0)
    
    Change-Id: I296a0221eb0aac574cb9c2aa31b9325cce9eed27

commit 75037bd3f082bb0d13684db36d14c61bbb093f32
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Sat Jun 9 01:16:59 2012 -0400

    get rid of magic in proc_namespace.c
    
    don't rely on proc_mounts->m being the first field; container_of()
    is there for purpose.  No need to bother with ->private, while
    we are at it - the same container_of will do nicely.
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>
    (cherry picked from commit 6ce6e24e72233073c8ead9419fc5040d44803dae)
    
    Change-Id: I6ab6761c574981a04ab96d9c1a4c01db6c0d77d3

commit 58427d067ea831d6e88d9bb1669eeb5cb743394f
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Sat Jun 9 00:59:08 2012 -0400

    get rid of ->mnt_longterm
    
    it's enough to set ->mnt_ns of internal vfsmounts to something
    distinct from all struct mnt_namespace out there; then we can
    just use the check for ->mnt_ns != NULL in the fast path of
    mntput_no_expire()
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>
    (cherry picked from commit f7a99c5b7c8bd3d3f533c8b38274e33f3da9096e)
    
    Conflicts:
    	fs/dcache.c
    
    Change-Id: Ie5ba7c7f2a56e69d213463aa9d16abec9c0683d8

commit b03317edaaea75c1c60899bb81db66dbec33544e
Author: Andi Kleen <ak@linux.intel.com>
Date:   Tue May 8 13:32:02 2012 +0930

    brlocks/lglocks: API cleanups
    
    lglocks and brlocks are currently generated with some complicated macros
    in lglock.h.  But there's no reason to not just use common utility
    functions and put all the data into a common data structure.
    
    In preparation, this patch changes the API to look more like normal
    function calls with pointers, not magic macros.
    
    The patch is rather large because I move over all users in one go to keep
    it bisectable.  This impacts the VFS somewhat in terms of lines changed.
    But no actual behaviour change.
    
    [akpm@linux-foundation.org: checkpatch fixes]
    Signed-off-by: Andi Kleen <ak@linux.intel.com>
    Cc: Al Viro <viro@zeniv.linux.org.uk>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Rusty Russell <rusty@rustcorp.com.au>
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>
    
    (cherry picked from commit 962830df366b66e71849040770ae6ba55a8b4aec)
    
    Conflicts:
    	fs/dcache.c
    	fs/namespace.c
    
    Change-Id: Ic2892645a95585c419d79f95db46b0a581f9ac1e

commit d58c9b1e0630f5cf162fdb6df2ad89dfea70eae6
Author: Andi Kleen <ak@linux.intel.com>
Date:   Tue May 8 13:32:24 2012 +0930

    brlocks/lglocks: turn into functions
    
    lglocks and brlocks are currently generated with some complicated macros
    in lglock.h.  But there's no reason to not just use common utility
    functions and put all the data into a common data structure.
    
    Since there are at least two users it makes sense to share this code in a
    library.  This is also easier maintainable than a macro forest.
    
    This will also make it later possible to dynamically allocate lglocks and
    also use them in modules (this would both still need some additional, but
    now straightforward, code)
    
    [akpm@linux-foundation.org: checkpatch fixes]
    Signed-off-by: Andi Kleen <ak@linux.intel.com>
    Cc: Al Viro <viro@zeniv.linux.org.uk>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Rusty Russell <rusty@rustcorp.com.au>
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>
    
    (cherry picked from commit eea62f831b8030b0eeea8314eed73b6132d1de26)
    
    Conflicts:
    	kernel/Makefile
    
    Change-Id: I3341efdc7d8610b075469bac2b521dab0b3d82d3

commit dc8fa715c96a28947a3620d7371016ea2cdd14f5
Author: Rusty Russell <rusty@rustcorp.com.au>
Date:   Tue May 8 13:29:45 2012 +0930

    lglock: remove online variants of lock
    
    Optimizing the slow paths adds a lot of complexity.  If you need to
    grab every lock often, you have other problems.
    
    Signed-off-by: Rusty Russell <rusty@rustcorp.com.au>
    Acked-by: Nick Piggin <npiggin@kernel.dk>
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>
    (cherry picked from commit 9dd6fa03ab31bb57cee4623a689d058d222fbe68)
    
    Change-Id: I4dca40af18091db3ce0fb2e298c7d39121a94b08

commit e52014216bc610e9ce35ec3ed801e3ac84323dac
Author: Ruchi Kandoi <kandoiruchi@google.com>
Date:   Mon Apr 6 17:53:17 2015 -0700

    proc: uid: Changes the thread notifier to profile event notifier.
    
    In order to keep the code consistent with all other platforms, the
    thread notifier is changed to profile event notifier.
    
    Change-Id: I5b996c789927b42dacba10af6fe81a21866e2c8f

commit 391adc6f10d780e92ad7352a2884af0b125ab62b
Author: Jin Qian <jinqian@google.com>
Date:   Mon May 11 17:57:52 2015 -0700

    proc: uid_cputime: fix show_uid_stat permission
    
    Change-Id: I8d364a8c1a1d97797ba157ebf02dc48c49d28cd4
    Signed-off-by: Jin Qian <jinqian@google.com>

commit 76c99c84a01f2d691a03d774111fe4e990516737
Author: jinqian <jinqian@google.com>
Date:   Wed Mar 11 10:44:50 2015 -0700

    proc: uid: Adds accounting for the cputimes per uid.
    
    Adds proc files /proc/uid_cputime/show_uid_stat and
    /proc/uid_cputime/remove_uid_range.
    
    show_uid_stat lists the total utime and stime for the active as well as
    terminated processes for each of the uids.
    
    Writing a range of uids to remove_uid_range will delete the accounting
    for all the uids within that range.
    Change-Id: Ie1ee76caea7ef4f343fd37e0ffa00556ec85eb3a

commit 58c7491e4c91e595a454ff4e5257c1454ded3462
Author: Ruchi Kandoi <kandoiruchi@google.com>
Date:   Tue Oct 21 13:55:04 2014 -0700

    cpufreq: Avoid using global variable total_cpus
    
    The change is to compile on kernels where cpufreq stats are compiled as
    a module (CONFIG_CPU_FREQ_STAT=m), because total_cpus is not exported for
    module use.
    
    Reported-By: Emilio Lpez <elopez93@gmail.com>
    Signed-off-by: Ruchi Kandoi <kandoiruchi@google.com>
    Change-Id: I4f3c74f0fac5e8d9449655b26bf3b407b0fe4290

commit b3bd4cae6e69e9fc0d7411d402db5f471c550b87
Author: Minsung Kim <ms925.kim@samsung.com>
Date:   Wed Jun 25 19:44:50 2014 +0900

    cpufreq: fix sleeping in atomic context when realloc freq_table for all_time_in_state
    
    Commit 40cf2f8 (cpufreq: Persist cpufreq time in state data across hotplug)
    causes the following call trace to be spit on boot:
    
    BUG: sleeping function called from invalid context at mm/slub.c:936
    in_atomic(): 1, irqs_disabled(): 0, pid: 1, name: swapper/0
    CPU: 6 PID: 1 Comm: swapper/0 Not tainted 3.10.9-20140624.172707-eng-gd6c0f69-dirty #50
    Backtrace:
    [<c0012270>] (dump_backtrace+0x0/0x10c) from [<c001256c>] (show_stack+0x18/0x1c)
     r6:ffff1788 r5:c0c020c0 r4:e609c000 r3:00000000
    [<c0012554>] (show_stack+0x0/0x1c) from [<c07a2970>] (dump_stack+0x20/0x28)
    [<c07a2950>] (dump_stack+0x0/0x28) from [<c0057678>] (__might_sleep+0x104/0x120)
    [<c0057574>] (__might_sleep+0x0/0x120) from [<c00ff000>] (__kmalloc_track_caller+0x144/0x274)
     r6:00000000 r5:e609c000 r4:e6802140
    [<c00feebc>] (__kmalloc_track_caller+0x0/0x274) from [<c00da098>] (krealloc+0x58/0xb0)
    [<c00da040>] (krealloc+0x0/0xb0) from [<c050266c>] (cpufreq_allstats_create+0x120/0x204)
     r8:e4c4ff00 r7:c0d266b8 r6:0013d620 r5:e4c4e600 r4:00000001
    r3:e535d6d0
    [<c050254c>] (cpufreq_allstats_create+0x0/0x204) from [<c0502e38>] (cpufreq_stat_notifier_policy+0xb8/0xd0)
    [<c0502d80>] (cpufreq_stat_notifier_policy+0x0/0xd0) from [<c00517cc>] (notifier_call_chain+0x4c/0x8c)
     r5:00000000 r4:fffffffe
    [<c0051780>] (notifier_call_chain+0x0/0x8c) from [<c00519fc>] (__blocking_notifier_call_chain+0x50/0x68)
     r8:c0cd4d00 r7:00000002 r6:e609dd7c r5:ffffffff r4:c0d25a4c
    r3:ffffffff
    [<c00519ac>] (__blocking_notifier_call_chain+0x0/0x68) from [<c0051a34>] (blocking_notifier_call_chain+0x20/0x28)
     r7:c0e24f30 r6:00000000 r5:e53e1e00 r4:e609dd7c
    [<c0051a14>] (blocking_notifier_call_chain+0x0/0x28) from [<c0500fec>] (__cpufreq_set_policy+0xc0/0x1d0)
    [<c0500f2c>] (__cpufreq_set_policy+0x0/0x1d0) from [<c0501308>] (cpufreq_add_dev_interface+0x20c/0x270)
     r7:00000008 r6:00000000 r5:e53e1e00 r4:e53e1e58
    [<c05010fc>] (cpufreq_add_dev_interface+0x0/0x270) from [<c05016a8>] (cpufreq_add_dev+0x33c/0x420)
    [<c050136c>] (cpufreq_add_dev+0x0/0x420) from [<c03604a4>] (subsys_interface_register+0x80/0xbc)
    [<c0360424>] (subsys_interface_register+0x0/0xbc) from [<c050035c>] (cpufreq_register_driver+0x8c/0x194)
    
    Change-Id: If77a656d0ea60a8fc4083283d104509fa6c07f8f
    Signed-off-by: Minsung Kim <ms925.kim@samsung.com>

commit e9290a0799f8060f5f65573f1bccf505706226b5
Author: Cristoforo Cataldo <cristoforo.cataldo@gmail.com>
Date:   Thu May 9 18:56:06 2013 +0200

    cpufreq: LulzActive CPU governor
    
    A new interactive governor developed by Tegrak.
    For more information please visit:
    http://tegrak2x.blogspot.com/2011/11/lulzactive-governor-v2.html

commit 716b5e6449c0df709a069960d5ca2179052fcb30
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Mon Oct 12 14:46:02 2015 +0200

    cpufreq: fix uberdemand for my kernel source, update defconfig after the last updates

commit dccf2f71c807eb645d26a0448e3ffb890424efa6
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Mon Oct 12 14:03:29 2015 +0200

    jf_eur: fix sensor ID, it should be 7 instead of 0

commit ab060a0ba74c1e3a7d231cae0da730eecd38152a
Author: myfluxi <linflux@arcor.de>
Date:   Fri Oct 18 21:20:22 2013 +0200

    block: deadline: Optimize for non-rotational
    
    Change-Id: Ib36df79cf0cab0aff5e0a67fd92265ff29fda39e

commit 6eb3d2995dc8a56073fbc51522ec15f7ea4aac76
Author: Paul Keith <javelinanddart@gmail.com>
Date:   Mon Oct 5 19:27:16 2015 -0500

    jf: Fix Board File Spacing for Multi Phase Hooks

commit 3b393ff05fa406b0615a8cb632aed12da18f2fb6
Author: Paul Keith <javelinanddart@gmail.com>
Date:   Mon Oct 5 19:23:48 2015 -0500

    uberdemand: Add Second Phase Hooks
    * And Use a Config Macro for 2 Phase Freqs Instead of Hardcoding

commit d41e03ca2063cce8eaa2e52be1cc21f914aa4ce7
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Mon Oct 12 13:54:48 2015 +0200

    cpufreq: add uberdemand governor

commit 76a7bfd397d62081d750b8c8525127ad8312a054
Author: Eric Dumazet <edumazet@google.com>
Date:   Thu Sep 17 08:38:00 2015 -0700

    tcp_cubic: do not set epoch_start in the future
    
    Tracking idle time in bictcp_cwnd_event() is imprecise, as epoch_start
    is normally set at ACK processing time, not at send time.
    
    Doing a proper fix would need to add an additional state variable,
    and does not seem worth the trouble, given CUBIC bug has been there
    forever before Jana noticed it.
    
    Let's simply not set epoch_start in the future, otherwise
    bictcp_update() could overflow and CUBIC would again
    grow cwnd too fast.
    
    This was detected thanks to a packetdrill test Neal wrote that was flaky
    before applying this fix.
    
    Change-Id: Ifd1e4be175824a31619ff4c1dc973f82346b799d
    Fixes: 30927520dbae ("tcp_cubic: better follow cubic curve after idle period")
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: Neal Cardwell <ncardwell@google.com>
    Signed-off-by: Yuchung Cheng <ycheng@google.com>
    Cc: Jana Iyengar <jri@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit efd649dad44ecb2d9d1ac3c6f2411e4eac75d26a
Author: Eric Dumazet <edumazet@google.com>
Date:   Wed Sep 9 21:55:07 2015 -0700

    tcp_cubic: better follow cubic curve after idle period
    
    Jana Iyengar found an interesting issue on CUBIC :
    
    The epoch is only updated/reset initially and when experiencing losses.
    The delta "t" of now - epoch_start can be arbitrary large after app idle
    as well as the bic_target. Consequentially the slope (inverse of
    ca->cnt) would be really large, and eventually ca->cnt would be
    lower-bounded in the end to 2 to have delayed-ACK slow-start behavior.
    
    This particularly shows up when slow_start_after_idle is disabled
    as a dangerous cwnd inflation (1.5 x RTT) after few seconds of idle
    time.
    
    Jana initial fix was to reset epoch_start if app limited,
    but Neal pointed out it would ask the CUBIC algorithm to recalculate the
    curve so that we again start growing steeply upward from where cwnd is
    now (as CUBIC does just after a loss). Ideally we'd want the cwnd growth
    curve to be the same shape, just shifted later in time by the amount of
    the idle period.
    
    Change-Id: I5a6b57d38d85c1e685835061888e719d240350dc
    Reported-by: Jana Iyengar <jri@google.com>
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: Yuchung Cheng <ycheng@google.com>
    Signed-off-by: Neal Cardwell <ncardwell@google.com>
    Cc: Stephen Hemminger <stephen@networkplumber.org>
    Cc: Sangtae Ha <sangtae.ha@gmail.com>
    Cc: Lawrence Brakmo <lawrence@brakmo.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit a3344e7d4050fe57b8d27aed6ae34a7039948d55
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Tue Oct 6 14:39:02 2015 +0200

    also fix DEVICE_ATTR for GPE build

commit 1ac7fca582bc22d000f2b953a77508ac85496368
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Fri Jun 12 10:37:08 2015 +0200

    Fix all wrong DEVICE_ATTR sets in active drivers.

commit 9f3ff84fe9999eba16992a1c579f170ec26ca0bd
Author: Dorimanx <yuri@bynet.co.il>
Date:   Thu Jun 11 19:29:33 2015 +0300

    Fix all wrong DEVICE_ATTR sets in active drivers. I SEE YOU!! HAHA!
    
    examples:
    
    4>------------[ cut here ]------------
    <4>WARNING: at drivers/base/core.c:572 device_create_file+0xa4/0xa8()
    <4>Attribute fuelrst: read permission without 'show'
    <4>Modules linked in:
    <4>[<c010ce0c>] (unwind_backtrace) from [<c018d5f8>] (warn_slowpath_common+0x60/0x80)
    <4>[<c018d5f8>] (warn_slowpath_common) from [<c018d64c>] (warn_slowpath_fmt+0x34/0x4c)
    <4>[<c018d64c>] (warn_slowpath_fmt) from [<c04aa3c0>] (device_create_file+0xa4/0xa8)
    <4>[<c04aa3c0>] (device_create_file) from [<c0a3b3d4>] (max17048_probe+0x3b8/0x5e8)
    <4>[<c0a3b3d4>] (max17048_probe) from [<c0657c34>] (i2c_device_probe+0xb4/0x104)
    <4>[<c0657c34>] (i2c_device_probe) from [<c04ae788>] (driver_probe_device+0xa8/0x258)
    <4>[<c04ae788>] (driver_probe_device) from [<c04aea38>] (__driver_attach+0x9c/0xa0)
    <4>[<c04aea38>] (__driver_attach) from [<c04aca78>] (bus_for_each_dev+0x70/0xb0)
    <4>[<c04aca78>] (bus_for_each_dev) from [<c04ade08>] (bus_add_driver+0x1f4/0x280)
    <4>[<c04ade08>] (bus_add_driver) from [<c04af0e8>] (driver_register+0x84/0x154)
    <4>[<c04af0e8>] (driver_register) from [<c065a034>] (i2c_register_driver+0x34/0xc8)
    <4>[<c065a034>] (i2c_register_driver) from [<c01005cc>] (do_one_initcall+0x138/0x190)
    <4>[<c01005cc>] (do_one_initcall) from [<c0f00bc8>] (kernel_init+0x13c/0x1d8)
    <4>[<c0f00bc8>] (kernel_init) from [<c01073fc>] (kernel_thread_exit+0x0/0x8)
    <4>---[ end trace 72054ea705535a6a ]---
    
    <4>WARNING: at drivers/base/core.c:572 device_create_file+0xa4/0xa8()
    <4>Attribute anx7730: read permission without 'show'
    <4>Modules linked in:
    <4>[<c010ce0c>] (unwind_backtrace) from [<c018d5f8>] (warn_slowpath_common+0x60/0x80)
    <4>[<c018d5f8>] (warn_slowpath_common) from [<c018d64c>] (warn_slowpath_fmt+0x34/0x4c)
    <4>[<c018d64c>] (warn_slowpath_fmt) from [<c04aa3c0>] (device_create_file+0xa4/0xa8)
    <4>[<c04aa3c0>] (device_create_file) from [<c0423e40>] (anx7808_i2c_probe+0x494/0x890)
    <4>[<c0423e40>] (anx7808_i2c_probe) from [<c0657c34>] (i2c_device_probe+0xb4/0x104)
    <4>[<c0657c34>] (i2c_device_probe) from [<c04ae788>] (driver_probe_device+0xa8/0x258)
    <4>[<c04ae788>] (driver_probe_device) from [<c04aea38>] (__driver_attach+0x9c/0xa0)
    <4>[<c04aea38>] (__driver_attach) from [<c04aca78>] (bus_for_each_dev+0x70/0xb0)
    <4>[<c04aca78>] (bus_for_each_dev) from [<c04ade08>] (bus_add_driver+0x1f4/0x280)
    <4>[<c04ade08>] (bus_add_driver) from [<c04af0e8>] (driver_register+0x84/0x154)
    <4>[<c04af0e8>] (driver_register) from [<c065a034>] (i2c_register_driver+0x34/0xc8)
    <4>[<c065a034>] (i2c_register_driver) from [<c0f1d138>] (anx7808_init_async+0x14/0x34)
    <4>[<c0f1d138>] (anx7808_init_async) from [<c01b269c>] (async_run_entry_fn+0x90/0x1cc)
    <4>[<c01b269c>] (async_run_entry_fn) from [<c01a5438>] (process_one_work+0x1dc/0x468)
    <4>[<c01a5438>] (process_one_work) from [<c01a613c>] (worker_thread+0x148/0x490)
    <4>[<c01a613c>] (worker_thread) from [<c01aaebc>] (kthread+0xd8/0xf0)
    <4>[<c01aaebc>] (kthread) from [<c01073fc>] (kernel_thread_exit+0x0/0x8)
    <4>---[ end trace 72054ea705535a68 ]---
    <4>------------[ cut here ]------------
    <4>WARNING: at drivers/base/core.c:572 device_create_file+0xa4/0xa8()
    <4>Attribute anx7808: read permission without 'show'
    <4>Modules linked in:
    <4>[<c010ce0c>] (unwind_backtrace) from [<c018d5f8>] (warn_slowpath_common+0x60/0x80)
    <4>[<c018d5f8>] (warn_slowpath_common) from [<c018d64c>] (warn_slowpath_fmt+0x34/0x4c)
    <4>[<c018d64c>] (warn_slowpath_fmt) from [<c04aa3c0>] (device_create_file+0xa4/0xa8)
    <4>[<c04aa3c0>] (device_create_file) from [<c0423e40>] (anx7808_i2c_probe+0x494/0x890)
    <4>[<c0423e40>] (anx7808_i2c_probe) from [<c0657c34>] (i2c_device_probe+0xb4/0x104)
    <4>[<c0657c34>] (i2c_device_probe) from [<c04ae788>] (driver_probe_device+0xa8/0x258)
    <4>[<c04ae788>] (driver_probe_device) from [<c04aea38>] (__driver_attach+0x9c/0xa0)
    <4>[<c04aea38>] (__driver_attach) from [<c04aca78>] (bus_for_each_dev+0x70/0xb0)
    <4>[<c04aca78>] (bus_for_each_dev) from [<c04ade08>] (bus_add_driver+0x1f4/0x280)
    <4>[<c04ade08>] (bus_add_driver) from [<c04af0e8>] (driver_register+0x84/0x154)
    <4>[<c04af0e8>] (driver_register) from [<c065a034>] (i2c_register_driver+0x34/0xc8)
    <4>[<c065a034>] (i2c_register_driver) from [<c0f1d138>] (anx7808_init_async+0x14/0x34)
    <4>[<c0f1d138>] (anx7808_init_async) from [<c01b269c>] (async_run_entry_fn+0x90/0x1cc)
    <4>[<c01b269c>] (async_run_entry_fn) from [<c01a5438>] (process_one_work+0x1dc/0x468)
    <4>[<c01a5438>] (process_one_work) from [<c01a613c>] (worker_thread+0x148/0x490)
    <4>[<c01a613c>] (worker_thread) from [<c01aaebc>] (kthread+0xd8/0xf0)
    <4>[<c01aaebc>] (kthread) from [<c01073fc>] (kernel_thread_exit+0x0/0x8)
    <4>---[ end trace 72054ea705535a69 ]---
    
    and much more. just fix the perms :)
    
    Conflicts:
    	drivers/media/platform/msm/vidc/msm_v4l2_vidc.c
    	drivers/misc/pm8xxx-cradle.c
    	drivers/power/max17048_battery.c
    	drivers/power/smb349_charger.c
    	drivers/usb/gadget/u_lgeusb.c
    	drivers/video/slimport/slimport.c

commit 3ecb16b480310c84346853b516ac8c5f6f96786a
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Fri Oct 2 13:37:57 2015 +0200

    intellidemand as default governor

commit d5d1fe6fcbfb928c44c07c890540db956f9cae99
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Fri Oct 2 11:28:29 2015 +0200

    Enable arch power switch

commit cecf9916cc6a5f31163de8b50d95f29a3f2dbfc9
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Aug 21 13:18:23 2012 -0700

    workqueue: make deferrable delayed_work initializer names consistent
    
    Initalizers for deferrable delayed_work are confused.
    
    * __DEFERRED_WORK_INITIALIZER()
    * DECLARE_DEFERRED_WORK()
    * INIT_DELAYED_WORK_DEFERRABLE()
    
    Rename them to
    
    * __DEFERRABLE_WORK_INITIALIZER()
    * DECLARE_DEFERRABLE_WORK()
    * INIT_DEFERRABLE_WORK()
    
    This patch doesn't cause any functional changes.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>

commit 930bb791926935fc70d0f5d823563d6290fdbdfb
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Fri Oct 2 11:00:22 2015 +0200

    1. Fix all compiling errors after disabling sec_debug_mdp, second update darkness, nightmare and pagasusq cpufreqs, merged from Alucards branch
    (This was not my work at all)

commit f4827e51cec4f7fe726a64ce5a6f54b8d69cc844
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Fri Oct 2 10:39:31 2015 +0200

    another small update of defconfig

commit 197a28a522bf1a96c4bc3e2c01eb49239b19f5d5
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Fri Oct 2 09:38:53 2015 +0200

    update defconfigs

commit 2b0cc3e22b6b87be359f23cfa75edd53dbc54458
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Oct 1 15:20:03 2015 +0200

    second fix for GCC 5.2 compilers, with my first variant here: https://github.com/Tkkg1994/Hulk-Kernel-V2/commit/218f968abab9ca125708fc7919c9b36956c9069a it did break some youtube things
    
    Signed-off-by: Tkkg1994 <luca.grifo@outlook.com>

commit 9223039db0ea45fbd2a2c10070f04ec0b915fb42
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Oct 1 14:55:25 2015 +0200

    Revert "fix all compiling errors/warning that come with GCC 5.2"
    
    This reverts commit 218f968abab9ca125708fc7919c9b36956c9069a.

commit ceadf0dbcefc15331ada4f44b65def2e33182fe2
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Sep 30 13:51:49 2015 +0200

    Revert "staging: android: timed_output : disable the timed output device when the device is unregistered"
    
    This reverts commit 7cc411895f5ee08ae3c88a370a8b1646cc19b37f.

commit d55eda14c0e4176bf5b00277545b0dede895ecf6
Author: Tim Bird <tim.bird@am.sony.com>
Date:   Thu May 10 15:11:36 2012 -0700

    staging: android: logger: Allocate logs dynamically at boot (v3)
    
    This changes the log initialization to be dynamic, but still
    at boot time.  These changes are a predecessor to implementing
    runtime allocation and freeing of logs, to make the Android logger
    less hard-coded.
    
    Change from a fixed set of static log structures, to allocation
    at init time into a list.  Return proper error numbers on log
    allocation failure.
    
    Cc: Brian Swetland <swetland@google.com>
    Signed-off-by: Tim Bird <tim.bird@am.sony.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit bef6e32e04c0f2dd34035a7a1d8e86be917f22f3
Author: Sachin Kamat <sachin.kamat@linaro.org>
Date:   Tue Jun 5 16:40:10 2012 +0530

    staging: Android: Fix some checkpatch warnings
    
    Warnings reported by checkpatch.pl have been fixed.
    
    Cc: Brian Swetland <swetland@google.com>
    Signed-off-by: Sachin Kamat <sachin.kamat@linaro.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 7cc411895f5ee08ae3c88a370a8b1646cc19b37f
Author: Kim, Milo <Milo.Kim@ti.com>
Date:   Thu Apr 26 22:01:10 2012 -0700

    staging: android: timed_output : disable the timed output device when the device is unregistered
    
    (Life cycle of timed output device driver)
    
    1) register the device as the timed output
    2) enable() ops is called via the sysfs
       timeout > 0 : timer is activated and device is turned on
       timeout = 0 : timer is cancelled and device is off
    3) unregister the timed output device if not used any more
    
    So the registered device should be disabled explicitly when the module is removed.
    ('disabled' means the timer is stopped and the device is turned off)
    
    Rather than implementing that code in each driver,
    just call enable() with timeout = 0 to clean up the driver.
    
    Signed-off-by: Milo(Woogyom) Kim <milo.kim@ti.com>
    Cc: Mike Lockwood <lockwood@android.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit a3ac97bbeb945d2e5edd9fe946e237ffc6efebe9
Author: Tim Bird <tim.bird@am.sony.com>
Date:   Thu May 10 14:22:59 2012 -0700

    staging: android: logger: Fix some sparse and whitespace issues
    
    Fix a few sparse warnings, and improve whitespace.
    
    Cc: Brian Swetland <swetland@google.com>
    Signed-off-by: Tim Bird <tim.bird@am.sony.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 0ce47354eee0c3dbdb2e4587ff04456b2c574943
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Tue Sep 29 13:48:02 2015 +0200

    why is there an "f"?

commit b23c6f4d08bd873d248020a397a0cad94cf9f167
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Tue Sep 29 20:31:33 2015 +0200

    Revert "init: Support power off charging all bootloaders"
    
    This reverts commit 587d48ca98e3579e895460e271cca5ba6b181ec8.

commit 6bcfcb6b7e24726f6c56a013d7231970073615a0
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Mon Sep 28 16:19:37 2015 +0200

    update defconfig

commit b1fd7fd6768d08ee39ba358cbb6bd237b7b3a9a8
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Wed Sep 23 10:36:34 2015 +0200

    msm_hotplug and msm_rq_stats: removed hp_io_is_busy parameter!

commit d3b7d50ac2981e08191d48f6591739ac215249eb
Author: Dorimanx <yuri@bynet.co.il>
Date:   Mon Sep 21 01:26:35 2015 +0300

    CPU HOTPLUGS: convert all to use state notifier suspend.
    
    also converted all to use more simple work queue
    
    Conflicts:
    	arch/arm/mach-msm/bricked_hotplug.c
    	arch/arm/mach-msm/msm_hotplug.c

commit 429e50dcc3fa797caa7731b593f0c551b3acac8b
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun Sep 27 20:06:25 2015 +0200

    disable sec debug and restrict rooting, fix compiling errors because of sec debug disabled

commit e9a420ef289d2ce06e984613b378c99ff031d206
Author: Spegelius <spegelius@gmail.com>
Date:   Wed Aug 5 21:11:37 2015 +0300

    Fix jactive sensors
    
    - samsung_hardware was not properly defined with GT-I9295 device
    
    Change-Id: Ie8f069212362e7589c54b71f261add07603fda46

commit a612dd98900d95fa0a32fc6e88aa13bbdd5ca021
Author: alucard24 <dmbaoh2@gmail.com>
Date:   Sun Aug 9 18:03:49 2015 +0200

    Sensors: we try to consider all unknown phone models as GT-I9505 one.
    I don't have their kernel source codes so I can't implement changes......
    I need more time for studying......

commit f292abf7ee839b6b3419298b80dd8a24ca5d471d
Author: Arne Coucheron <arco68@gmail.com>
Date:   Mon Jun 23 00:21:54 2014 +0200

    init: Support power off charging all bootloaders
    
    Change-Id: I76ecfccb6ac52c422e5dba476bec6128b1292a17

commit 8bcad4811d77fde12d52d7cfb1e02951e2d85dc4
Author: cyanogen <shade@chemlab.org>
Date:   Wed Jul 30 19:26:25 2014 -0500

    Fix compilation issues with CONFIG_SEC_DEBUG disabled.
    
    Change-Id: I3ad7ba20b58f3030f291e540020a5c3749c4a42b

commit b48a85219f51809f0212073e0d70f540a2b23782
Author: Arne Coucheron <arco68@gmail.com>
Date:   Fri Sep 4 12:42:53 2015 -0500

    jf: fix restart from charging mode?
    
    Change-Id: Ieb9b89d56527b2b74f5b973aa647c7394f020cde

commit 0d469d1c37c4793a9e750ade3c65ddcc105a0c52
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun Sep 27 00:36:28 2015 +0200

    Linux 3.4.109

commit 90f76205c1fcbc03e38051c5c185f0acab0e0fe2
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Sep 24 18:43:29 2015 +0200

    make selinux permanent permissive and add the right values for limiter

commit eb4ced59ed9c691c2e473d7907e4876cc5a881e4
Author: FNU Ramendra <rramendr@codeaurora.org>
Date:   Mon Aug 5 12:23:01 2013 -0600

    msm: rpc: Fix uninitialized union in rpc router close function
    
    Initialize the rr_control_msg union to prevent the local users to obtain
    sensitive information from kernel memory in msm_rpcrouter_close function.
    
    CRs-Fixed: 515623
    Change-Id: Ife87010eb81e8840c9b1bf5d8aeb941c90020eac
    Signed-off-by: FNU Ramendra <rramendr@codeaurora.org>

commit 70776bd9cdaf8fb7561dea026e9480dc670dc972
Author: FNU Ramendra <rramendr@codeaurora.org>
Date:   Fri Jul 26 14:42:26 2013 -0600

    msm: rpc: Fix uninitialized union in rpc router code
    
    Initialize the rr_control_msg union to prevent the local users to obtain
    sensitive information from kernel memory.
    
    CRs-Fixed: 515623
    Change-Id: I778e6544fab22a2e2d87acaa7192e063dae4fbc7
    Signed-off-by: FNU Ramendra <rramendr@codeaurora.org>

commit 7eaa4e13c0d134f53a9436023ef1f4811a76937e
Author: William Clark <wclark@codeaurora.org>
Date:   Mon Mar 9 12:07:49 2015 -0700

    qseecom: avoid buffer over-write when copying app_name
    
    Change memcpy to strlcpy to only copy the string for app_name.
    
    Change-Id: I46cf34c2d2fdbf24e9e65008555f762761c81dd7
    Signed-off-by: William Clark <wclark@codeaurora.org>

commit 3036cc9a5f571927cd85d166d63921be2f0b0732
Author: Zhen Kong <zkong@codeaurora.org>
Date:   Fri Sep 12 12:58:13 2014 -0700

    qseecom: Free ion memory even if keymaster app is not unloaded
    
    Make a change to free the ion memory allocated for keymaster client
    even if keymaster app doesn't need to be unloaded from TZ, so as to
    avoid memory leak.
    
    CRs-Fixed: 724638
    Change-Id: I3ddb68c8a3f96b0a44afc3efc63e93f0a8be51bb
    Signed-off-by: Zhen Kong <zkong@codeaurora.org>

commit efc69d906785fcebab9c89f9eb3e471c58343c8c
Author: Zhen Kong <zkong@codeaurora.org>
Date:   Fri Oct 31 11:33:34 2014 -0700

    qseecom: Save appname in qseecom when loading app by kernel client
    
    We've made changes on app session managements to save app name in
    qseecom when loading app by userspace client. This change is to save
    app name when loading app by kernel client, then qseecom can compare
    the app name correctly when sending commands to TZ.
    
    CRs-Fixed: 748491
    Change-Id: I341a1a89f0e8a45056be7a5ce0a6a540842bc5dd
    Signed-off-by: Zhen Kong <zkong@codeaurora.org>

commit 6fbbb858d7b19c6a162051d5844efef1f37bc6ce
Author: William Clark <wclark@codeaurora.org>
Date:   Thu Jul 3 15:32:13 2014 -0700

    qseecom: change the number of bytes to compare app names
    
    keymaster app has two names with different length: "keymaster" and
    "keymaste". To compare the app names correctly, the shorter name
    length is choosen for memcmp in __qseecom_send_cmd.
    
    Change-Id: I5f0dff7f9756362be40cc64985b65bc2268d734f
    Signed-off-by: Zhen Kong <zkong@codeaurora.org>
    Signed-off-by: William Clark <wclark@codeaurora.org>

commit ad7f4b9bc17c33255deff571f64aac78f4cc2f9b
Author: Zhen Kong <zkong@codeaurora.org>
Date:   Sat Jun 28 14:29:30 2014 -0700

    qseecom: fix an issue related to search app when unloading an app
    
    Make a change in qseecom_unload_app to stop the execution of app
    searching when an app is founded, even if this app is in use with
    ref_cnt greater than one.
    
    Change-Id: I0328252668ef1069ec514bfb913b927a7da76594
    Signed-off-by: Zhen Kong <zkong@codeaurora.org>
    Signed-off-by: William Clark <wclark@codeaurora.org>

commit 85da23157b7332bb53730487d593d35de67cb173
Author: Zhen Kong <zkong@codeaurora.org>
Date:   Fri May 9 18:27:53 2014 -0700

    qseecom: Add support to handle multiple app load & unload
    
    Media server is associated with multiple apps on TZ-side.
    Unloading a single app for clean up is not enought. QSEECOM
    needs to handle the app load & unload of multiple apps
    associated with the client. During app_load & app_unload
    along with app_id, app_name should also be checked. Besides,
    keymaster will not be unloaded.
    
    Change-Id: Iea53362a21e359e54e40a0a93688a05264cfde5c
    Signed-off-by: Hariprasad Dhalinarasimha <hnamgund@codeaurora.org>
    Signed-off-by: Zhen Kong <zkong@codeaurora.org>

commit ec493c5c5b7ec698e835efcc5f555517a31dd876
Author: AnilKumar Chimata <anilc@codeaurora.org>
Date:   Thu Sep 5 18:00:28 2013 +0530

    qseecom: Fix issues with failure to release client handle
    
    In some scenarios when qseecom_release is called with the incorrect
    client handle type. Hence proper cleanup is not done for the memory
    allocated to the device handle or to unload the app.
    
    This change addresses the cleanup issues related to qseecom client in
    qseecom_release call by ensuring the handle type is set correctly and
    cleanup is done properly based on client type.
    
    Change-Id: I126aa431ce635ae98100b774e3d92b704635d199
    Signed-off-by: AnilKumar Chimata <anilc@codeaurora.org>

commit 21673212adaf1234933c1b0877b828d1d4f2531c
Author: Zhen Kong <zkong@codeaurora.org>
Date:   Fri Mar 29 17:28:31 2013 -0700

    qseecom: Fix issue with incomplete command exiting prematurely
    
    A process waiting on a signal can be awaken by any signals. We
    need to only continue processing when the condition of the
    wait event is met.
    
    Change-Id: Ib2102babbb505876f89b04399729e6ff5a475605
    Signed-off-by: Mona Hossain <mhossain@codeaurora.org>
    Signed-off-by: Zhen Kong <zkong@codeaurora.org>

commit 2897a88735f9f9e1b366cd09b42785331a87d351
Author: Hariprasad Dhalinarasimha <hnamgund@codeaurora.org>
Date:   Fri Jun 21 18:40:50 2013 -0700

    qseecom: Unload the app upon receiving qseecom_release.
    
    Upon receiving qseecom_release() app corresponding to the app id
    needs to be shutdown.This is applicable to user space clients as well.
    
    Change-Id: Ie659a9409f8470b7ed4a5088e6779b50c5dc1cca
    Signed-off-by: Hariprasad Dhalinarasimha <hnamgund@codeaurora.org>
    Signed-off-by: Zhen Kong <zkong@codeaurora.org>

commit fb9c488aa28aa04a407094e85e2a5b68549de996
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun Sep 20 19:56:35 2015 +0200

    enable GPU badass and GPU OC for standard kernel version

commit c58a0b3aa8ce51b8c1c31d6d42cda7efe17e1a3a
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun Sep 20 02:38:53 2015 +0200

    Revert "init: Support power off charging all bootloaders"
    
    This reverts commit c299c23079904604d86031ac55edb516f4ecfb68.

commit 35084da9a7724d8da8af847531688d982084fa51
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun Sep 20 02:32:37 2015 +0200

    update both OC and non OC defconfig

commit 69655df69b78ffced44540b9f1c474d21e2ee1e2
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun Sep 20 02:30:20 2015 +0200

    fix cpu idle in zzmoove

commit bdcdc65419481d458a9967b7b9a61808b110fe31
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun Sep 20 02:16:44 2015 +0200

    cpufreq: add lazy governor, optimized for this source
    credits go to @Cl3Kener and @javelinanddart

commit 25dd4686bc549381b02b7efe1fffd8e4cd594bba
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun Sep 20 02:09:59 2015 +0200

    add missing badass defines

commit c4ead589fdc4219a420abea5cc342e06495bedec
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun Sep 20 02:05:28 2015 +0200

    fix compiling errors with new cpugovs, mainly because of cpu time idle

commit 9b7863ee4f49a957f448f3909c9b3fb50b8974c8
Author: flar2 <asegaert@gmail.com>
Date:   Tue Mar 11 20:21:33 2014 -0400

    ElementalX Governor
    
    Signed-off-by: flar2 <asegaert@gmail.com>
    Signed-off-by: Paul Keith <javelinanddart@gmail.com>

commit bd2e1c3bf819291ff88f82dd1afd206e25b53ee0
Author: TripNRaVeR <noerialexeew@live.nl>
Date:   Tue Jan 14 03:01:54 2014 +0100

    TripNDroid CPU Governor
    
    Signed-off-by: TripNRaVeR <noerialexeew@live.nl>
    Signed-off-by: Paul Keith <javelinanddart@gmail.com>

commit 74c460faa70df7b261c052999eca1d7ee57ca144
Author: Dave Savage <bedalus@gmail.com>
Date:   Tue Feb 3 18:28:26 2015 +0000

    Preservative Governor
    
    Signed-off-by: Dave Savage <bedalus@gmail.com>

commit 3b581279562b50a54a5330a99053aabc60958fa7
Author: Paul Keith <javelinanddart@gmail.com>
Date:   Wed Jul 8 10:59:54 2015 -0500

    Add missing badass 2 and 3 phase hooks
    
    Signed-off-by: Paul Keith <javelinanddart@gmail.com>

commit 247e893b6541efe5fe415ae5e889530d0b2a307a
Author: Paul Keith <javelinanddart@gmail.com>
Date:   Mon Sep 14 18:08:24 2015 -0500

    jf: Recalculate Badass GPU Control for Adreno 320
    * Let's be a little more accurate this time
    
    Signed-off-by: Paul Keith <javelinanddart@gmail.com>

commit 8ffe2208af4c773606103820ff1d3350aadaea04
Author: Paul Keith <javelinanddart@gmail.com>
Date:   Wed Aug 5 17:25:59 2015 -0500

    cpufreq: badass: Add protection code in case it's a Samsung Kernel
    * Kanged from the stock ondemand governor

commit acc9122fe0e9999b1f918b1878275014d744e85d
Author: Paul Keith <javelinanddart@gmail.com>
Date:   Wed Aug 5 16:25:16 2015 -0500

    cpufreq: badass: Sync with Upstream Ondemand Governor
    with removed cpu_idle
    
    Signed-off-by: Paul Keith <javelinanddart@gmail.com>

commit e7454873a10607d55cde977e1680ced14e68e564
Author: Paul Keith <javelinanddart@gmail.com>
Date:   Tue Mar 3 16:03:11 2015 -0600

    Tune Badass GPU Control for the Adreno 320
    * It was originally meant for Adreno 220 which is clocked at 266MHz
      The Adreno 320 is clocked at at 450MHz
      It's roughly clocked 1.7 times higher
      Therefore increase the thresholds by 1.7x and round
    
    Signed-off-by: Paul Keith <javelinanddart@gmail.com>

commit 60a80f39bbadae53290bbd536447acbe7e2079a3
Author: Paul Keith <javelinanddart@gmail.com>
Date:   Sat May 2 23:35:25 2015 -0500

    Badass GPU Control, missing from Uber Kernel
    
    Signed-off-by: Paul Keith <javelinanddart@gmail.com>

commit 311a5306f98a2c1746a13b5c116736491113e8f0
Author: alucard24 <dmbaoh2@gmail.com>
Date:   Mon Jun 1 15:47:42 2015 +0200

    jf: sensors: use global var to adapt changes for all variants
    
    Developed to mainly fix the compass bug which affects International or Verizon phones, caused by using an unique platform source for all the jf variants and depending on which calibration is set in board-jf_eur.
    These two variants share the same board revision but their compass is soldered differently, therefore calibration is different: if set good for Verizon the International doesn't work and vice versa.
    This commit fixes this behavior by retrieving the samsung.hardware from cmdline and setting correct sensor parameters according to the phone variant found.
    
    Change-Id: I8ae2ad768ab3184a224e8f9f4a6d2b11a8647f0b

commit 4b248d42ca3ab464a29175607ad842a308660267
Author: Dan Pasanen <dan.pasanen@gmail.com>
Date:   Sun Jul 27 18:51:07 2014 -0500

    sensorhub: use system revison rather than defconfig for variants
    
    Change-Id: I66889432883c1ca1e07784f34aa3bcdbd2cdf21d

commit 61dc3843ff0c5f1bf0d2c8fdf6b4e5bb1fcc0a3f
Author: Salva Peir <speiro@ai2.upv.es>
Date:   Wed Apr 30 19:48:02 2014 +0200

    media: media-device: fix infoleak in ioctl media_enum_entities()
    
    commit e6a623460e5fc960ac3ee9f946d3106233fd28d8 upstream.
    
    This fixes CVE-2014-1739.
    
    Signed-off-by: Salva Peir <speiro@ai2.upv.es>
    Acked-by: Laurent Pinchart <laurent.pinchart@ideasonboard.com>
    Signed-off-by: Mauro Carvalho Chehab <m.chehab@samsung.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 79b694f56cdef16f960e8a53c086097bcc522ecf
Author: Jeremy Gebben <jgebben@codeaurora.org>
Date:   Mon Jan 21 14:09:15 2013 -0700

    iommu: msm: check range before mapping for IOMMU-v1
    
    Make sure iommu_map_range() does not leave a partial
    mapping on error if part of the range is already mapped.
    
    Change-Id: I108b45ce8935b73ecb65f375930fe5e00b8d91eb
    Signed-off-by: Jeremy Gebben <jgebben@codeaurora.org>

commit 4c76d096a1fb591dacc8127d7539e8e4e3cbd3d4
Author: Olav Haugan <ohaugan@codeaurora.org>
Date:   Wed Jun 19 11:17:41 2013 -0700

    iommu: msm: Handle unmapping of PTE properly
    
    The unmap api is currently not handling unmapping of page table
    entries (PTE) properly. The generic function that calls the msm
    unmap API expects the unmap call to unmap as much as possible
    and then return the amount that was unmapped.
    In addition the unmap function does not support an arbitrary input
    length. However, the function that calls the msm unmap function
    assumes that this is supported.
    
    Both these issues can cause mappings to not be unmapped which will
    cause subsequent mappings to fail because the mapping already exists.
    
    Change-Id: I638d5c38673abe297a701de9b7209c962564e1f1
    Signed-off-by: Olav Haugan <ohaugan@codeaurora.org>

commit 5e496942168fdbb408ca8b851c2af62ba42b94e6
Author: Laura Abbott <lauraa@codeaurora.org>
Date:   Mon Apr 22 10:01:55 2013 -0700

    iommu: msm: Don't treat address 0 as an error case
    
    Currently, the iommu page table code treats a scattergather
    list with physical address 0 as an error. This may not be
    correct in all cases. Physical address 0 is a valid part
    of the system and may be used for valid page allocations.
    Nothing else in the system checks for physical address 0
    for error so don't treat it as an error.
    
    Change-Id: Ie9f0dae9dace4fff3b1c3449bc89c3afdd2e63a0
    CRs-Fixed: 478304
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit 67b5a74701828c0a49b4b660da6a5ab17ae75bdf
Author: Kevin Matlage <kmatlage@codeaurora.org>
Date:   Fri Feb 1 12:41:04 2013 -0700

    iommu: msm: Let IOMMUv1 use all possible page sizes
    
    Allow the IOMMUv1 to use 16M, 1M, 64K or 4K iommu
    pages when physical and virtual addresses are
    appropriately aligned. This can reduce TLB misses
    when large buffers are mapped.
    
    Change-Id: Iffcaa04097fc3877962f3954d73a6ba448dca20b
    Signed-off-by: Kevin Matlage <kmatlage@codeaurora.org>

commit c81a9d7859eba96f30e37e2cb6593fcebe912115
Author: Jeremy Gebben <jgebben@codeaurora.org>
Date:   Fri Apr 19 13:45:03 2013 -0600

    iommu: msm: prevent partial mappings on error
    
    If msm_iommu_map_range() fails mid way through the va
    range with an error, clean up the PTEs that have already
    been created so they are not leaked.
    
    Change-Id: Ie929343cd6e36cade7b2cc9b4b4408c3453e6b5f
    CRs-Fixed: 478304
    Signed-off-by: Jeremy Gebben <jgebben@codeaurora.org>

commit 6254adc93b5cb64694b1832708b3b6cedaf7dac7
Author: Jeremy Gebben <jgebben@codeaurora.org>
Date:   Fri Apr 19 13:45:03 2013 -0600

    iommu: msm: prevent partial mappings on error
    
    If msm_iommu_map_range() fails mid way through the va
    range with an error, clean up the PTEs that have already
    been created so they are not leaked.
    
    Change-Id: Ie929343cd6e36cade7b2cc9b4b4408c3453e6b5f
    CRs-Fixed: 478304
    Signed-off-by: Jeremy Gebben <jgebben@codeaurora.org>

commit 209c529e086095fb56b2324cadc189e3ecd2c146
Author: Sakshi Agrawal <sakshia@codeaurora.org>
Date:   Tue Oct 9 16:14:28 2012 -0700

    iommu: Make termination of stalled transaction optional
    
    When pagefault happens do not cancel the faulting transaction if
    the registered fault handler returns EBUSY error. This way
    drivers can control when they want to resume the transaction.
    
    Change-Id: Ia4563da073ab04174803101c3b8ec82b0571850e
    Signed-off-by: Shubhraprakash Das <sadas@codeaurora.org>
    Signed-off-by: Sakshi Agrawal <sakshia@codeaurora.org>

commit 613d1b155961edd244f8c4be30e92db90c68d58d
Author: Sakshi Agrawal <sakshia@codeaurora.org>
Date:   Fri Oct 5 14:03:45 2012 -0600

    iommu/msm: check range before mapping.
    
    Make sure iommu_map_range() does not leave a partial
    mapping on error if part of the range is already
    mapped.
    
    Change-Id: I0ddeb0e0169b579f1efdeca4071fce4ee75a11f8
    Signed-off-by: Jeremy Gebben <jgebben@codeaurora.org>
    Signed-off-by: Sakshi Agrawal <sakshia@codeaurora.org>

commit fa13d557d32a527a0d52cac1cb64ed52b3a32369
Author: Roman Birg <roman@cyngn.com>
Date:   Mon Aug 18 14:04:44 2014 -0700

    jf: report SW_LID instead of SW_FLIP
    
    Change-Id: Iebffbabdbb3748eec4f887ebd227c67adf01d8ef
    Signed-off-by: Roman Birg <roman@cyngn.com>
    Signed-off-by: Scott Mertz <smertz@cyngn.com>

commit 3f91b83546ccd0b6f6c54fca809a9e23068a63f7
Author: Dan Pasanen <dan.pasanen@gmail.com>
Date:   Sun Sep 7 01:02:12 2014 -0500

    Fix warnings for gcc 4.9
    
    Change-Id: Id1126120e92c2da10f3b027b99374f83585ad9f2

commit 20846cc173d3b7daff6638eb300d1fe5a4a1380d
Author: Mona Hossain <mhossain@codeaurora.org>
Date:   Tue Mar 26 14:05:49 2013 -0700

    crypto: msm: Fix XO shutdown issues
    
    ce-core_src clk is being left on preventing device from going
    into low power mode. Furthere more, clks are enabled without
    checking if the handle is valid.
    This fix removes enabling of ce_crore_src_clk. Also checks if
    clk handle is valid before enabling them.
    
    Change-Id: Ieb64b4b11385838ba18dd05d47ccc978e5b1bdf1
    Signed-off-by: Mona Hossain <mhossain@codeaurora.org>

commit 26b6ca753bb599574962f1e9ebff35c1b23d6d37
Author: Mona Hossain <mhossain@codeaurora.org>
Date:   Thu Mar 21 10:24:58 2013 -0700

    crypto: msm: Refactor/clean-up clock code
    
    Add clk enable/disable and init/deinit APIs.
    These APIs will be used in future to allow clk scaling and
    gating by clients.
    
    Change-Id: I08503972b9ed77633e14d81711aa92c61df7a0ad
    Signed-off-by: Mona Hossain <mhossain@codeaurora.org>

commit 8b25fd514c815898ce889e113d355fbb358896db
Author: Ramesh Masavarapu <rameshm@codeaurora.org>
Date:   Thu Sep 6 08:42:52 2012 -0700

    crypto: Enable AXI clock for Crypto Engine.
    
    Currently the AXI clock is not enabled for the crypto driver.
    Without this clock enabled, data will not be transferred
    between the DDR and the Crypto Engine.This fix enables the
    AXI clock.
    
    Signed-off-by: Ramesh Masavarapu <rameshm@codeaurora.org>

commit fca8305630a0b1e1823acf7d143b48e59df266a3
Author: Arne Coucheron <arco68@gmail.com>
Date:   Mon Jun 23 00:21:54 2014 +0200

    init: Support power off charging all bootloaders
    
    Change-Id: I76ecfccb6ac52c422e5dba476bec6128b1292a17

commit 5f41c954dc5ee8c1e554927d2536631144c12528
Author: Dan Pasanen <dan.pasanen@gmail.com>
Date:   Sat Aug 30 18:57:50 2014 -0500

    leds-an30259a: POWERING pattern: match blue and green patterns
    
    * gives a more uniform pulse effect
    
    Change-Id: If563d7089a65a9d8c0c739288e3e9ab19a83a78d

commit 746abc2fc7262bcece69886d428a1fec552a1ff7
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Fri Sep 18 15:38:39 2015 +0200

    Revert "scripts: Add sortextable to sort the kernel's exception table."
    
    This reverts commit a877271913ee3cd2c0c37845d4248ec691a7a259.

commit 124d9c22755bc5c7e9876834d41cb78aba067626
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Fri Sep 18 15:38:30 2015 +0200

    Revert "scripts/sortextable: Handle relative entries, and other cleanups"
    
    This reverts commit 54abec2cc10b02fccd981e9f5f44f088cc44b2aa.

commit 1e2e9eff79e6c6f9c596076f49eef84c1cc1f902
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Fri Sep 18 15:38:15 2015 +0200

    Revert "scripts/sortextable: support objects with more than 64K sections."
    
    This reverts commit f70f917b26545cf7d6235b1f9d375130d11be04e.

commit fc5f0040a9379ed5a46dea83a34ea9786b91b1da
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Fri Sep 18 15:38:03 2015 +0200

    Revert "scripts: remove unused function in sortextable.c"
    
    This reverts commit a4815cc204c1e3a95862702ea22e4d2118c5d56f.

commit ab8ba6764a7bc40f1f920c8b659dcfdc5089d93f
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Fri Sep 18 15:37:51 2015 +0200

    Revert "kbuild/extable: Hook up sortextable into the build system."
    
    This reverts commit 433a90af08492b7a8956ff37fee84a2b5081aa0a.

commit 9a434a05aa8f3d60f29e9dc46d2f724bfc9aa9f6
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Fri Sep 18 15:37:35 2015 +0200

    Revert "scripts/sortextable: silence script output"
    
    This reverts commit 0bd650d28f013a595834d4058ffa258e7a6adf32.

commit d73a2a1ca706c59f201faeedec03e96a2c3b96ca
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Fri Sep 18 15:37:19 2015 +0200

    Revert "scripts/sortextable: suppress warning: `relocs_size' may be used uninitialized"
    
    This reverts commit 37b77af9cd54d972ea9dbe7c7f6b6c7ab7b6375c.

commit 218f968abab9ca125708fc7919c9b36956c9069a
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Fri Sep 18 14:50:59 2015 +0200

    fix all compiling errors/warning that come with GCC 5.2
    
    Signed-off-by: Tkkg1994 <luca.grifo@outlook.com>

commit 6ccdb20431f2aea7487c3619466a163d0e49ff7c
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Fri Sep 18 14:15:12 2015 +0200

    Revert "ARM: Section based HYP idmap"
    
    This reverts commit 0cb810ffa22ae14b0c8c15e4aa57a6f8aa6b710c.

commit 82862d88391f514153a6fd046de4d4b624ad7e3c
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Fri Sep 18 14:15:09 2015 +0200

    Revert "ARM: KVM: enforce maximum size for identity mapped code"
    
    This reverts commit 9b502b1342e747b54185bd0d7e4e40f79bce816d.

commit 05cdb6970f41b6e7ce5f12a3ca0760f34dbaa400
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Fri Sep 18 14:13:34 2015 +0200

    switch to selfcompiled 5.2 toolchain

commit 36659ae775ae94c06caf860c231f334476b442dc
Author: Pranav Vashi <neobuddy89@gmail.com>
Date:   Thu Apr 2 04:19:21 2015 -0700

    cpufreq: Add Frequency limiter driver
    
    * Per-CPU total control.
    
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 065c8408d7c2d318f0bc73a64a3ca8a37100b008
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Fri Sep 18 13:57:51 2015 +0200

    Revert "msm: Add MSM limiter driver"
    
    This reverts commit fd7d28865ac28b86db75834930d9b9608ecb421e.

commit 5951851d0670376a1a98d3a023f091eae90d7898
Author: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
Date:   Sat Sep 7 01:24:06 2013 +0530

    cpufreq: Use signed type for 'ret' variable, to store negative error values
    
    There are places where the variable 'ret' is declared as unsigned int
    and then used to store negative return values such as -EINVAL. Fix them
    by declaring the variable as a signed quantity.
    
    Signed-off-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Git-commit: 5136fa56582beadb7fa71eb30bc79148bfcba5c1
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Junjie Wu <junjiew@codeaurora.org>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 14db4f440540f8f1a0d9eda74135970ad3496335
Author: Pranav Vashi <neobuddy89@gmail.com>
Date:   Wed Feb 4 05:00:47 2015 +0530

    cpufreq: Restore policy min-max properly
    
    * 5echo2@xda reported min-max frequencies per core
      do not stick at their values and reset to stock.
    * The issue actually is whenever a core was unplugged
      and plugged, we are not restoring saved frequencies.
    * This was a bug introduced by recent simplications
      in cpufreq driver. Squash it with fire!
    
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit a982342dc170d947fd2b000a8152f48bc507157f
Author: Viresh Kumar <viresh.kumar@linaro.org>
Date:   Tue Jan 29 14:39:08 2013 +0000

    cpufreq: Simplify cpufreq_add_dev()
    
    Currently cpufreq_add_dev() firsts allocates policy, calls
    driver->init() and then checks if this CPU is already managed or not.
    And if it is already managed, its policy is freed.
    
    We can save all this if we somehow know that CPU is managed or not in
    advance.  policy->related_cpus contains the list of all valid sibling
    CPUs of policy->cpu. We can check this to see if the current CPU is
    already managed.
    
    From now on, platforms don't really need to set related_cpus from
    their init() routines, as the same work is done by core too.
    
    If a platform driver needs to set the related_cpus mask with some
    additional CPUs, other than CPUs present in policy->cpus, they are
    free to do it, though, as we don't override anything.
    
    Signed-off-by: Viresh Kumar <viresh.kumar@linaro.org>
    Tested-by: Shawn Guo <shawn.guo@linaro.org>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>
    
    Conflicts:
    	drivers/cpufreq/cpufreq.c

commit 82bc00e7a713c28077c3a5e7933ef9e8b24908fe
Author: myfluxi <linflux@arcor.de>
Date:   Thu Jan 8 12:10:11 2015 +0100

    cpufreq: Add policy notifiers
    
    We don't want to abuse CPUFREQ_GOV_START/STOP here.
    
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 828fca34e93bd1758c47e53d74b0c2d7fb93e245
Author: Dorimanx <yuri@bynet.co.il>
Date:   Tue Sep 1 19:31:02 2015 +0300

     Tune UKSM to work with Android Devices
    
    uksm: tweak CPU usage
    uksm: fixes
    oops
    uksm: squashed updates
    
    Thanks to @DecimalMan
    uksm: make conservative default
    UKSM: Add Battery Gov, tune, and fix build
    
    This commit does the following
    - Adds a true battery preset
    - Renames current presets to be more representative of what they actually do
    - Fixes presets not being added due to extra commas
    Create more UKSM governors
    
    Add Governors for Higher CPU usage (HighCPU) for more merging, and low cpu usage (Battery) for less battery drain
    
    Signed-off-by: frap129 <frap129@gmail.com>
    uksm: tune more, last tuning was a bit low
    
    https://github.com/AOSPA-legacy/android_kernel_oneplus_msm8974/commit/68a473fb32e829a170d491fcc6aed3a87dea964e

commit d60c1fa08ba2732e14d310a1cce423f9b9e50c32
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Fri Sep 18 13:31:01 2015 +0200

    update defconfig file

commit 5454ee57a468899844a5423095324de0493ff864
Author: Tim Gardner <tim.gardner@canonical.com>
Date:   Mon Oct 13 15:54:20 2014 -0700

    scripts/sortextable: suppress warning: `relocs_size' may be used uninitialized
    
    In file included from scripts/sortextable.c:194:0:
    scripts/sortextable.c: In function `main':
    scripts/sortextable.h:176:3: warning: `relocs_size' may be used uninitialized in this function [-Wmaybe-uninitialized]
       memset(relocs, 0, relocs_size);
       ^
    scripts/sortextable.h:106:6: note: `relocs_size' was declared here
      int relocs_size;
          ^
    In file included from scripts/sortextable.c:192:0:
    scripts/sortextable.h:176:3: warning: `relocs_size' may be used uninitialized in this function [-Wmaybe-uninitialized]
       memset(relocs, 0, relocs_size);
       ^
    scripts/sortextable.h:106:6: note: `relocs_size' was declared here
      int relocs_size;
          ^
    
    gcc 4.9.1
    
    Signed-off-by: Tim Gardner <tim.gardner@canonical.com>
    Reviewed-by: Jamie Iles <jamie.iles@oracle.com>
    
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit e3eb060e65c9b233fa3ebba76b0f9a43645f72ff
Author: Arnd Bergmann <arnd@arndb.de>
Date:   Tue Nov 27 09:18:00 2012 +0000

    scripts/sortextable: silence script output
    
    The exception table sorter outputs one line every time
    it gets called, e.g. 'sort done marker at 66dc00', which
    is slightly annoying when doing 'make -s' which is otherwise
    completely silent. Since that output is not helpful to
    most people building the kernel, turn it off by default.
    
    Signed-off-by: Arnd Bergmann <arnd@arndb.de>
    Acked-by: David Daney <david.daney@cavium.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>

commit 1c11860add0e32330869de85a0566f6c7a51f12f
Author: David Daney <david.daney@cavium.com>
Date:   Thu Apr 19 14:59:57 2012 -0700

    kbuild/extable: Hook up sortextable into the build system.
    
    Define a config variable BUILDTIME_EXTABLE_SORT to control build time
    sorting of the kernel's exception table.
    
    Patch Makefile to do the sorting when BUILDTIME_EXTABLE_SORT is
    selected.
    
    Signed-off-by: David Daney <david.daney@cavium.com>
    Link: http://lkml.kernel.org/r/1334872799-14589-4-git-send-email-ddaney.cavm@gmail.com
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

commit 8238b577603a36022ca06f18b1d3ce8301112f55
Author: Ramkumar Ramachandra <artagnon@gmail.com>
Date:   Wed Jul 10 23:33:38 2013 +0530

    scripts: remove unused function in sortextable.c
    
    Signed-off-by: Ramkumar Ramachandra <artagnon@gmail.com>
    Acked-by: David Daney <david.daney@cavium.com>
    Signed-off-by: Michal Marek <mmarek@suse.cz>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit ef8ba8823d5e3a1daa9a08625ff43be772081616
Author: Jamie Iles <jamie.iles@oracle.com>
Date:   Tue Nov 12 15:06:51 2013 -0800

    scripts/sortextable: support objects with more than 64K sections.
    
    Building with a large config and -ffunction-sections results in a large
    number of sections and sortextable needs to be able to handle that.
    Implement support for > 64K sections as modpost does.
    
    Signed-off-by: Jamie Iles <jamie.iles@oracle.com>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Michal Marek <mmarek@suse.cz>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 7ef38b49cc557504c5dc48fd2b5ae9c5e908d581
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Fri Apr 12 19:12:04 2013 +0100

    ARM: KVM: enforce maximum size for identity mapped code
    
    We're about to move to an init procedure where we rely on the
    fact that the init code fits in a single page. Make sure we
    align the idmap text on a vector alignment, and that the code is
    not bigger than a single page.
    
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Christoffer Dall <cdall@cs.columbia.edu>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit f1cb3f93c9879b30d52942708a504e5c02ba9ef0
Author: Christoffer Dall <c.dall@virtualopensystems.com>
Date:   Sun Jan 20 18:43:10 2013 -0500

    ARM: Section based HYP idmap
    
    Add a method (hyp_idmap_setup) to populate a hyp pgd with an
    identity mapping of the code contained in the .hyp.idmap.text
    section.
    
    Offer a method to drop this identity mapping through
    hyp_idmap_teardown.
    
    Make all the above depend on CONFIG_ARM_VIRT_EXT and CONFIG_ARM_LPAE.
    
    Reviewed-by: Will Deacon <will.deacon@arm.com>
    Reviewed-by: Marcelo Tosatti <mtosatti@redhat.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Christoffer Dall <c.dall@virtualopensystems.com>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit e1aa15d3a6ddcaecd2259709489454d235f15206
Author: Yalin Wang <Yalin.Wang@sonymobile.com>
Date:   Fri Sep 26 03:30:59 2014 +0100

    ARM: 8168/1: extend __init_end to a page align address
    
    This patch changes the __init_end address to a
    page align address, so that free_initmem() can
    free the whole .init section, because if the end
    address is not page aligned, it will round down to
    a page align address, then the tail unligned page
    will not be freed.
    
    Signed-off-by: wang <yalin.wang2010@gmail.com>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 460057aef49a9a73455582ec335dc4ed5d075f37
Author: David Daney <david.daney@cavium.com>
Date:   Tue Apr 24 11:23:14 2012 -0700

    scripts/sortextable: Handle relative entries, and other cleanups
    
    x86 is now using relative rather than absolute addresses in its
    exception table, so we add a sorter for these.  If there are
    relocations on the __ex_table section, they are redundant and probably
    incorrect after the sort, so they are zeroed out leaving them valid
    and consistent.
    
    Also use the unaligned safe accessors from tools/{be,le}_byteshift.h
    
    Signed-off-by: David Daney <david.daney@cavium.com>
    Link: http://lkml.kernel.org/r/1335291795-26693-2-git-send-email-ddaney.cavm@gmail.com
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

commit 59491b1e0619fa6f9a9db825e3accdf27849fc87
Author: David Daney <david.daney@cavium.com>
Date:   Thu Apr 19 14:59:55 2012 -0700

    scripts: Add sortextable to sort the kernel's exception table.
    
    Using this build-time sort saves time booting as we don't have to burn
    cycles sorting the exception table.
    
    Signed-off-by: David Daney <david.daney@cavium.com>
    Link: http://lkml.kernel.org/r/1334872799-14589-2-git-send-email-ddaney.cavm@gmail.com
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

commit 4156c69723795e88d2ce824fe7cf3f7f7136e574
Author: Jussi Kivilinna <jussi.kivilinna@mbnet.fi>
Date:   Fri Sep 21 10:26:52 2012 +0300

    crypto: testmgr - make test_aead also test 'dst != src' code paths
    
    Currrently test_aead uses same buffer for destination and source. However
    in any places, 'dst != src' take different path than 'dst == src' case.
    
    Therefore make test_aead also run tests with destination buffer being
    different than source buffer.
    
    Signed-off-by: Jussi Kivilinna <jussi.kivilinna@mbnet.fi>
    Acked-by: David S. Miller <davem@davemloft.net>
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>

commit 8d501ab7fb1e8420fb0f8cdd772a86e39bd4eb7c
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Sep 17 02:35:03 2015 +0200

    Revert "alarm: convert to later alarm system"
    
    This reverts commit b330465a62e27640fb18a29a985ec2a4bad2e298.

commit f9e5c8401cad4c902c2039827459aae255ca9391
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Sep 16 23:41:58 2015 +0200

    motor: this reverts an old patch

commit d8561ce77beb89f38d9e51b99b0bc1c8dfd268e6
Author: Daniel Lezcano <daniel.lezcano@linaro.org>
Date:   Wed Mar 27 10:22:10 2013 +0000

    cpuidle: initialize the broadcast timer framework
    
    The commit 89878baa73f0f1c679355006bd8632e5d78f96c2 introduced
    the CPUIDLE_FLAG_TIMER_STOP flag where we specify a specific idle
    state stops the local timer.
    
    Now use this flag to check at init time if one state will need
    the broadcast timer and, in this case, setup the broadcast timer
    framework. That prevents multiple code duplication in the drivers.
    
    Signed-off-by: Daniel Lezcano <daniel.lezcano@linaro.org>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit b4f013ad04d8dfac0a769e2f6917801581a47830
Author: Daniel Lezcano <daniel.lezcano@linaro.org>
Date:   Thu Mar 21 12:21:31 2013 +0000

    cpuidle : handle clockevent notify from the cpuidle framework
    
    When a cpu enters a deep idle state, the local timers are stopped and
    the time framework falls back to the timer device used as a broadcast
    timer.
    
    The different cpuidle drivers are calling clockevents_notify ENTER/EXIT
    when the idle state stops the local timer.
    
    Add a new flag CPUIDLE_FLAG_TIMER_STOP which can be set by the cpuidle
    drivers. If the flag is set, the cpuidle core code takes care of the
    notification on behalf of the driver to avoid pointless code duplication.
    
    Signed-off-by: Daniel Lezcano <daniel.lezcano@linaro.org>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Santosh Shilimkar <santosh.shilimkar@ti.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit 7d0eb7c81b85458ce8d7dbbe152253e3e845cf60
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Sep 16 23:33:14 2015 +0200

    Revert "power: PM QoS support for bounded constraints"
    
    This reverts commit bcf9ba1dc097ad784596de06d8e1092fa46449de.

commit 275c6994af024b72a6c6f9f69cd54e2496355129
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Sep 16 23:21:11 2015 +0200

    Revert "Power: pmqos: Add emc freq pmqos constraint"
    
    This reverts commit 983e543622130c1bb8d56dfb483110a8c86c593a.

commit 5ed2d63d32e3bc6a7dd9feacba70858763dfad70
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Sep 16 23:20:18 2015 +0200

    Revert "PM QoS: Fix pmqos notifiers in pm_qos_enabled_set"
    
    This reverts commit 00aef0f5e88c31988b9be3365b1343d7b4bdb39c.

commit f7a747187a40184443eeef78737cd9b5e211b873
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Sep 16 23:19:57 2015 +0200

    Revert "PM / QoS: Introduce new classes: DMA-Throughput and DVFS-Latency"
    
    This reverts commit b4ec2e1b2b4f200594061a94803b504f233c1c95.

commit 721c958dd84ed5cac79a3e76c414eeb9092a54d3
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Sep 16 23:17:13 2015 +0200

    Revert "fix kernel/power/qos.c"
    
    This reverts commit 489578e327c626235c054b59e0ce8375e36f7e72.

commit 2a6a624cd3fb0307d51a428bac5590c81bc5f868
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Sep 16 23:16:56 2015 +0200

    Revert "QoS: Enhance framework to support cpu/irq specific QoS requests"
    
    This reverts commit 95185e3194c671c421d621b8926dcf137e559edd.

commit 38415247ba1c534bb61a3f1d3d444d7d6497a6a7
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Sep 16 23:16:52 2015 +0200

    Revert "qos: Pass the list of cpus with affected qos to notifer"
    
    This reverts commit 48fe937c951fc15bab6af96b823814b2ee731e8f.

commit de7aa6e38371c460e2ada03fc83b714e1a1cde38
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Sep 16 23:15:13 2015 +0200

    Revert "power: Prefer min over max for online cpus"
    
    This reverts commit 4392db45a882e945e080a9f2836e566babfd3204.

commit 7b2d3b1cfce1ba3af297431766b7ff9cefd8932d
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Sep 16 23:15:03 2015 +0200

    Revert "power: Fix coverity error"
    
    This reverts commit 1011d143951f98b483ca0f266f5597b7322d188d.

commit 56f397b2873e8c0df1da9d3c40b41be6f3c8890c
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Sep 16 13:32:06 2015 +0200

    uksm: disable by default

commit 0a00db8bb4b824d5ed98f09aa19abe8cdb49d2a3
Author: Andrew Morton <akpm@linux-foundation.org>
Date:   Mon Apr 29 15:08:08 2013 -0700

    kernel/cpuset.c: use register_hotmemory_notifier()
    
    Use the new interface, remove one ifdef.  No code size changes.
    
    We could/should have been using __meminit/__meminitdata here but there's
    now no point in doing that because all this code is elided at compile time.
    
    Cc: Li Zefan <lizefan@huawei.com>
    Cc: Tejun Heo <tj@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit 48dd873cefec98d89b7a01e9fa8a6261427cf024
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Wed Feb 25 18:56:13 2015 +0100

    locking/rtmutex: Drop usage of __HAVE_ARCH_CMPXCHG
    
    The rtmutex code is the only user of __HAVE_ARCH_CMPXCHG and we have a few
    other user of cmpxchg() which do not care about __HAVE_ARCH_CMPXCHG. This
    define was first introduced in 23f78d4a0 ("[PATCH] pi-futex: rt mutex core")
    which is v2.6.18. The generic cmpxchg was introduced later in 068fbad288
    ("Add cmpxchg_local to asm-generic for per cpu atomic operations") which is
    v2.6.25.
    Back then something was required to get rtmutex working with the fast
    path on architectures without cmpxchg and this seems to be the result.
    
    It popped up recently on rt-users because ARM (v6+) does not define
    __HAVE_ARCH_CMPXCHG (even that it implements it) which results in slower
    locking performance in the fast path.
    To put some numbers on it: preempt -RT, am335x, 10 loops of
    100000 invocations of rt_spin_lock() + rt_spin_unlock() (time "total" is
    the average of the 10 loops for the 100000 invocations, "loop" is
    "total / 100000 * 1000"):
    
         cmpxchg |    slowpath used  ||    cmpxchg used
                 |   total   | loop  ||   total    | loop
         --------|-----------|-------||------------|-------
         ARMv6   | 9129.4 us | 91 ns ||  3311.9 us |  33 ns
         generic | 9360.2 us | 94 ns || 10834.6 us | 108 ns
         ----------------------------||--------------------
    
    Forcing it to generic cmpxchg() made things worse for the slowpath and
    even worse in cmpxchg() path. It boils down to 14ns more per lock+unlock
    in a cache hot loop so it might not be that much in real world.
    The last test was a substitute for pre ARMv6 machine but then I was able
    to perform the comparison on imx28 which is ARMv5 and therefore is
    always is using the generic cmpxchg implementation. And the numbers:
    
                  |   total     | loop
         -------- |-----------  |--------
         slowpath | 263937.2 us | 2639 ns
         cmpxchg  |  16934.2 us |  169 ns
         --------------------------------
    
    The numbers are larger since the machine is slower in general. However,
    letting rtmutex use cmpxchg() instead the slowpath seem to improve things.
    
    Since from the ARM (tested on am335x + imx28) point of view always
    using cmpxchg() in rt_mutex_lock() + rt_mutex_unlock() makes sense I
    would drop the define.
    
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: will.deacon@arm.com
    Cc: linux-arm-kernel@lists.infradead.org
    Link: http://lkml.kernel.org/r/20150225175613.GE6823@linutronix.de
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 7543efa910463118bb965164011b7e3409bca579
Author: Davidlohr Bueso <dave@stgolabs.net>
Date:   Tue May 19 10:24:55 2015 -0700

    locking/rtmutex: Implement lockless top-waiter wakeup
    
    Mark the task for later wakeup after the wait_lock has been released.
    This way, once the next task is awoken, it will have a better chance
    to of finding the wait_lock free when continuing executing in
    __rt_mutex_slowlock() when trying to acquire the rtmutex, calling
    try_to_take_rt_mutex(). Upon contended scenarios, other tasks attempting
    take the lock may acquire it first, right after the wait_lock is released,
    but (a) this can also occur with the current code, as it relies on the
    spinlock fairness, and (b) we are dealing with the top-waiter anyway,
    so it will always take the lock next.
    
    Signed-off-by: Davidlohr Bueso <dbueso@suse.de>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Mike Galbraith <umgwanakikbuti@gmail.com>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Cc: Davidlohr Bueso <dave@stgolabs.net>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/1432056298-18738-2-git-send-email-dave@stgolabs.net
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit dfa375e6a53d764528dbe6eb95d71e20546052ae
Author: Vignesh Radhakrishnan <vigneshr@codeaurora.org>
Date:   Mon May 11 16:41:54 2015 +0530

    smpboot: use kmemleak_not_leak for smpboot_thread_data
    
    Kmemleak reports the following memory leak :
    
        [<ffffffc0002faef8>] create_object+0x140/0x274
        [<ffffffc000cc3598>] kmemleak_alloc+0x80/0xbc
        [<ffffffc0002f707c>] kmem_cache_alloc_trace+0x148/0x1d8
        [<ffffffc00024504c>] __smpboot_create_thread.part.2+0x2c/0xec
        [<ffffffc0002452b4>] smpboot_register_percpu_thread+0x90/0x118
        [<ffffffc0016067c0>] spawn_ksoftirqd+0x1c/0x30
        [<ffffffc000200824>] do_one_initcall+0xb0/0x14c
        [<ffffffc001600820>] kernel_init_freeable+0x84/0x1e0
        [<ffffffc000cc273c>] kernel_init+0x10/0xcc
        [<ffffffc000203bbc>] ret_from_fork+0xc/0x50
    
    This memory allocated here points to smpboot_thread_data.
    Data is used as an argument for this kthread.
    
    This will be used when smpboot_thread_fn runs. Therefore,
    is not a leak.
    
    Call kmemleak_not_leak for smpboot_thread_data pointer
    to ensure that kmemleak doesn't report it as a memory
    leak.
    
    Signed-off-by: Vignesh Radhakrishnan <vigneshr@codeaurora.org>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit b6edbc6ddc29b17d396567e81ecfff0b620c6570
Author: Randy Dunlap <rdunlap@xenotime.net>
Date:   Thu May 31 16:26:15 2012 -0700

    kmod.c: fix kernel-doc warning
    
    Warning(kernel/kmod.c:419): No description found for parameter 'depth'
    
    Signed-off-by: Randy Dunlap <rdunlap@xenotime.net>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit ca013bb2d52df140eea86d6a34ae5e7d1616f551
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Wed Oct 10 20:09:44 2012 -0400

    make sure that kernel_thread() callbacks call do_exit() themselves
    
    Most of them never returned anyway - only two functions had to be
    changed.  That allows to simplify their callers a whole lot.
    
    Note that this does *not* apply to kthread_run() callbacks - all of
    those had been called from the same kernel_thread() callback, which
    did do_exit() already.  This is strictly about very few low-level
    kernel_thread() callbacks (there are only 6 of those, mostly as part
    of kthread.h and kmod.h exported mechanisms, plus kernel_init()
    itself).
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

commit e67352ec3f17a3f641458f583c0c4e2bacc8a85d
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Jan 22 16:48:03 2013 -0800

    async, kmod: warn on synchronous request_module() from async workers
    
    Synchronous requet_module() from an async worker can lead to deadlock
    because module init path may invoke async_synchronize_full().  The
    async worker waits for request_module() to complete and the module
    loading waits for the async task to finish.  This bug happened in the
    block layer because of default elevator auto-loading.
    
    Block layer has been updated not to do default elevator auto-loading
    and it has been decided to disallow synchronous request_module() from
    async workers.
    
    Trigger WARN_ON_ONCE() on synchronous request_module() from async
    workers.
    
    For more details, please refer to the following thread.
    
      http://thread.gmane.org/gmane.linux.kernel/1420814
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Reported-by: Alex Riesen <raa.lkml@gmail.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Arjan van de Ven <arjan@linux.intel.com>

commit f152b28f4ff1efb2ef6fc8f47ac38e3d9cb17164
Author: hongfeng <hongfeng@marvell.com>
Date:   Thu Oct 4 17:12:25 2012 -0700

    poweroff: fix bug in orderly_poweroff()
    
    orderly_poweroff is trying to poweroff platform in two steps:
    
    step 1: Call user space application to poweroff
    step 2: If user space poweroff fail, then do a force power off if force param
            is set.
    
    The bug here is, step 1 is always successful with param UMH_NO_WAIT, which obey
    the design goal of orderly_poweroff.
    
    We have two choices here:
    UMH_WAIT_EXEC which means wait for the exec, but not the process;
    UMH_WAIT_PROC which means wait for the process to complete.
    we need to trade off the two choices:
    
    If using UMH_WAIT_EXEC, there is potential issue comments by Serge E.
    Hallyn: The exec will have started, but may for whatever (very unlikely)
    reason fail.
    
    If using UMH_WAIT_PROC, there is potential issue comments by Eric W.
    Biederman: If the caller is not running in a kernel thread then we can
    easily get into a case where the user space caller will block waiting for
    us when we are waiting for the user space caller.
    
    Thanks for their excellent ideas, based on the above discussion, we
    finally choose UMH_WAIT_EXEC, which is much more safe, if the user
    application really fails, we just complain the application itself, it
    seems a better choice here.
    
    Signed-off-by: Feng Hong <hongfeng@marvell.com>
    Acked-by: Kees Cook <keescook@chromium.org>
    Acked-by: Serge Hallyn <serge.hallyn@canonical.com>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Acked-by: "Rafael J. Wysocki" <rjw@sisk.pl>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit 9e6f96894c34a4e2f61b8f43d7923f38216582bd
Author: Patrick Daly <pdaly@codeaurora.org>
Date:   Thu Jun 25 19:21:28 2015 -0700

    PM / Wakeup: Use rcu callbacks for better performance
    
    Use rcu to free objects in wakeup_source_unregister(). These objects must
    be allocated through wakeup_source_register().
    
    Replacing synchronize_rcu() with call_rcu() allows multiple calls to
    wakeup_source_unregister() to be combined into a single grace period.
    
    Signed-off-by: Patrick Daly <pdaly@codeaurora.org>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit ee84f4db41df1ee9f3c5a9bb38348c97f42ea23e
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Fri May 1 08:27:50 2015 -0700

    sched: Implement lockless wake-queues
    
    This is useful for locking primitives that can effect multiple
    wakeups per operation and want to avoid lock internal lock contention
    by delaying the wakeups until we've released the lock internal locks.
    
    Alternatively it can be used to avoid issuing multiple wakeups, and
    thus save a few cycles, in packet processing. Queue all target tasks
    and wakeup once you've processed all packets. That way you avoid
    waking the target task multiple times if there were multiple packets
    for the same task.
    
    Properties of a wake_q are:
    - Lockless, as queue head must reside on the stack.
    - Being a queue, maintains wakeup order passed by the callers. This can
      be important for otherwise, in scenarios where highly contended locks
      could affect any reliance on lock fairness.
    - A queued task cannot be added again until it is woken up.
    
    This patch adds the needed infrastructure into the scheduler code
    and uses the new wake_list to delay the futex wakeups until
    after we've released the hash bucket locks.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    [tweaks, adjustments, comments, etc.]
    Signed-off-by: Davidlohr Bueso <dbueso@suse.de>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Chris Mason <clm@fb.com>
    Cc: Davidlohr Bueso <dave@stgolabs.net>
    Cc: George Spelvin <linux@horizon.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Manfred Spraul <manfred@colorfullife.com>
    Cc: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Link: http://lkml.kernel.org/r/1430494072-30283-2-git-send-email-dave@stgolabs.net
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 47e3d1dd7037adb5599b9837d0e1323dbcd0f9ae
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Tue Sep 15 02:42:07 2015 +0200

    lazyplug: because of the implementation of the current cpuboost driver, wakeup_boost is conflicting with the defined extern bool wakeup_boost in cpu.h
    Can be found here: https://github.com/Tkkg1994/Hulk-Kernel-V2/blob/TW_Lollipop/include/linux/cpu.h#L282
    To avoid problems if CONFIG_CPU_BOOST=y, rename the function to wakeup_boost_lazy
    
    Signed-off-by: Tkkg1994 <luca.grifo@outlook.com>

commit b06df405ae1b29a3bdb2fbcc1d8b60ca6f0df6ed
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Tue Sep 15 02:25:25 2015 +0200

    update defconfig after latest merges

commit 440758ff0cf0c8d06fbf82e8caf7ae67fcc067cd
Author: Kuninori Morimoto <kuninori.morimoto.gx@renesas.com>
Date:   Tue May 29 18:46:06 2012 -0700

    driver core: fixup reversed deferred probe order
    
    If driver requests probe deferral,
    it will be added to deferred_probe_pending_list
    by driver_deferred_probe_add(), but, it used list_add().
    Because of that, deferred probe will be run as reversed order.
    This patch uses list_add_tail(), and solved this issue.
    
    Signed-off-by: Kuninori Morimoto <kuninori.morimoto.gx@renesas.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit f4462dab117df745be531a3e937bb0cf1076b579
Author: Joe Perches <joe@perches.com>
Date:   Sun Oct 28 01:05:41 2012 -0700

    drivers: base: Convert dev_printk(KERN_<LEVEL> to dev_<level>(
    
    dev_<level> calls take less code than dev_printk(KERN_<LEVEL>
    and reducing object size is good.
    
    Signed-off-by: Joe Perches <joe@perches.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 2fa445949502e04931400e56819f0e1a021a470b
Author: Dmitry Torokhov <dmitry.torokhov@gmail.com>
Date:   Sat Feb 23 13:11:14 2013 -0800

    devres: allow adding custom actions to the stack
    
    Sometimes drivers need to execute one-off actions in their error handling
    or device teardown paths. An example would be toggling a GPIO line to
    reset the controlled device into predefined state.
    
    To allow performing such actions when using managed resources let's allow
    adding them to stack/group of devres resources.
    
    Acked-by: Tejun Heo <tj@kernel.org>
    Acked-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Signed-off-by: Dmitry Torokhov <dmitry.torokhov@gmail.com>

commit 36019cee1cf34fc69998d20976a16eb93e72aef2
Author: Ming Lei <ming.lei@canonical.com>
Date:   Sat Aug 4 12:01:26 2012 +0800

    driver core: devres: introduce devres_for_each_res
    
    This patch introduces one devres API of devres_for_each_res
    so that the device's driver can iterate each resource it has
    interest in.
    
    The firmware loader will use the API to get each firmware name
    from the device instance.
    
    Signed-off-by: Ming Lei <ming.lei@canonical.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 6b1ca7fc5ab1fcede4710224cd30de4505afae54
Author: Mark Brown <broonie@opensource.wolfsonmicro.com>
Date:   Thu May 3 18:15:12 2012 +0100

    devres: Clarify documentation for devres_destroy()
    
    It's not massively obvious (at least to me) that removing and freeing a
    resource does not involve calling the release function for the resource
    but rather only removes the management of it. Make the documentation more
    explicit.
    
    Signed-off-by: Mark Brown <broonie@opensource.wolfsonmicro.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 48b058b6c88c1eae4422db871b6c6a2c49dbfb5c
Author: Mark Brown <broonie@opensource.wolfsonmicro.com>
Date:   Thu May 3 18:15:13 2012 +0100

    devres: Add devres_release()
    
    APIs using devres frequently want to implement a "remove and free the
    resource" operation so it seems sensible that they should be able to
    just have devres do the freeing for them since that's a big part of what
    devres is all about.
    
    Signed-off-by: Mark Brown <broonie@opensource.wolfsonmicro.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 92618e2dc7cc73b21d4ca2b4f73a4194e86f9994
Author: Park Ju Hyung <qkrwngud825@gmail.com>
Date:   Sat Sep 5 22:10:39 2015 +0900

    lazyplug: separate thread for reducing wake-up delay
    
    Signed-off-by: Park Ju Hyung <qkrwngud825@gmail.com>

commit 0d30cd89bee1d28b641f55159d428672f36ff97a
Author: Park Ju Hyung <qkrwngud825@gmail.com>
Date:   Sun Sep 6 01:35:03 2015 +0900

    lazyplug: add mutex & boolean check for lazy mode
    
    Multiple callers, instances can call lazyplug_enter_lazy().
    Fix race-conditions via mutex & boolean.
    
    Thanks to hsk from Matcl for debugging this.
    
    Signed-off-by: Park Ju Hyung <qkrwngud825@gmail.com>

commit 86f59ccc313c090f5321506ca6474503f30742ff
Author: Park Ju Hyung <qkrwngud825@gmail.com>
Date:   Sat Sep 5 22:04:46 2015 +0900

    Introduce Lazyplug
    
    Other hotplugging methods including mpdecision and intelli_plug focuses
    on how should we turn off CPU cores. They hotplugs the individual CPU
    cores based on the current load divided by thread capacity.
    Lazyplug takes a whole new approach on how we should do hotplugging
    based on the foundation of the other side of the coin;
    Linuxs hotplugging is very inefficient.
    
    Current hotplugging code on Linux is a total waste of CPU cycles and
    delays, so rather than hotplugging and hurt performance & battery life,
    just leaving the CPU cores on might be a better choice. This kind of
    approach is spreading out more and more.
    Samsung has been using this method for a very long time with big.LITTLE
    devices and recent Nexus 6 firmware also does the similar thing.
    
    Lazyplug just leaves them on, most of the time. It also tries to solve
    some problems with the Always on approach. On situations such as video
    playback, turning on all CPU cores is not battery friendly. So Lazyplug
    *does* actually turns off CPU cores, but only when idle state is long
    enough(to reduce the number of CPU core switchings) and when the device
    has its screen off(determination is done via earlysuspend or
    powersuspend because framebuffer API causes troubles on hotplugging CPU
    cores).
    
    Basic methodology :
    Lazyplug uses majority of the codes from intelli_plug by faux123 to
    determine when to turn off CPU cores. If the system has been idle for
    (DEF_SAMPLING_MS * DEF_IDLE_COUNT)ms, it turns off the CPU cores. And if
    the next poll determines 1 core isnt enough, it fires up all CPU cores
    (instead of selective CPU cores; which is the traditional intelli_plugs
    method).
    Lazyplug also takes touch-screen input events to fire up CPU cores to
    minimize noticeable performance degradation.
    Theres also a lazy mode for *not* aggressively turning on CPU cores
    on scenario such as video playback. For example, if you hook up
    lazyplug_enter_lazy() to the video session open function, Lazyplug wont
    aggressively turn on CPU cores and tries to handle it with 1 CPU core.
    
    * TODO :
    * Dual-core mode : YouTube video playback is mostly single-threaded.
    * It usually hovers around 10% ~ 30% of total CPU usage on quad-core
    * device. That means 1 CPU core might not be enough to handle it, but
    * also turning on all CPU cores is unnecessarily wasting power.
    
    Signed-off-by: Park Ju Hyung <qkrwngud825@gmail.com>

commit 33d2771af097ae3b13ada2065743b2a6d079110b
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Tue Aug 4 01:54:36 2015 +0200

    rename clock-generic stuff

commit 17958002ace23e87cf12e6556375c8e2af7112ad
Author: Vikram Mulukutla <markivx@codeaurora.org>
Date:   Mon Sep 23 17:48:21 2013 -0700

    msm: clock-generic: Add get_rate op for ext clocks
    
    The ext clock type may sometimes need to report the
    rate of its parent. Add the get_rate op to allow this.
    
    Change-Id: Id22e6a52aa36422fe4583d9c062a962f3399b6c4
    Signed-off-by: Vikram Mulukutla <markivx@codeaurora.org>

commit 446d1a85a6a36713ae974fa7348089dd5203b688
Author: Vikram Mulukutla <markivx@codeaurora.org>
Date:   Fri Jul 19 18:57:28 2013 -0700

    msm: clock-generic: Introduce an ext_clk clock type
    
    Some clock controllers have input clock signals from
    other clock controllers or hardware blocks. To model
    these exterior signals, introduce an ext_clk type that
    just passes requests to its parent clock. The clock
    controller using the ext_clk type will have to do a
    clk_get to get a handle to the external clock, and set
    that as the parent on the ext_clk clock.
    
    Change-Id: I762f7b4663d5a55d95e5d3b6808004b77b58e2da
    Signed-off-by: Vikram Mulukutla <markivx@codeaurora.org>

commit 59b916c57a8f80fed368ea2bc2a56787f38363e5
Author: Saravana Kannan <skannan@codeaurora.org>
Date:   Sat Jun 29 17:34:51 2013 -0700

    msm: clock-generic: Update the rate field on mux parent change.
    
    Changing the parent of a mux changes its rate to the rate of the new
    parent.  So, update the rate field of the clock when changing the parent of
    the clock.
    
    Change-Id: Ib219a5eb025c5bde922eed1864ce5296c7e84a8d
    Signed-off-by: Saravana Kannan <skannan@codeaurora.org>

commit c2d58a1e44b95aa5c6eb06f0505ee848dddd9046
Author: Saravana Kannan <skannan@codeaurora.org>
Date:   Tue Sep 3 17:54:29 2013 -0700

    msm: clock-generic: Refactor round rate code
    
    Use helper APIs to simplify div_clk and mux_clk round rate code.
    
    Change-Id: I148e53819f8ec73271d127b92780c7ae231cb126
    Signed-off-by: Saravana Kannan <skannan@codeaurora.org>

commit b4fe14371415b80c58a0b43d2778d6b321bd2fd2
Author: Saravana Kannan <skannan@codeaurora.org>
Date:   Fri Jun 28 12:29:14 2013 -0700

    msm: clock-generic: Add support for recursive set parent to mux_clk
    
    When there are a bunch of nested muxes and a consumer calls
    clk_set_parent() on the top level mux, it's possible that the consumer
    doesn't care if the new parent is an immediate parent to the top level mux
    and just wants the output from the new parent. So, recursively search for
    the new parent (if recursive set parent is enabled for the mux) and get the
    output of the new parent even if it has to go through multiple levels of
    consecutive muxes.
    
    Change-Id: Ibf76f11587e7c00fe1d34ce956e5a73c8db73e65
    Signed-off-by: Saravana Kannan <skannan@codeaurora.org>

commit b797fc1ff52b33ef25307e93961121c4a299464d
Author: Saravana Kannan <skannan@codeaurora.org>
Date:   Thu Jun 27 21:08:08 2013 -0700

    msm: clock-generic: Fix race condition between mux set rate and en/dis
    
    Some mux implementations might switch to/from a low power parent as part of
    their disable/enable ops. This can race with the temporary switch to the
    "safe parent" that's done during a mux set rate. So, grab the enable lock
    while temporarily switching to the safe parent.
    
    The onus of remembering the last requested set_mux_sel and setting it back
    to that when the mux is enabled is still the responsibility of the specific
    implementation.
    
    Change-Id: I9ead3334b29ce7d219415cef562424d6455a348d
    Signed-off-by: Saravana Kannan <skannan@codeaurora.org>

commit a491b7d137ed30da68a9ba3c7e97ffcf33415da3
Author: Saravana Kannan <skannan@codeaurora.org>
Date:   Tue Jun 25 21:53:10 2013 -0700

    msm: clock-generic: Make ops options for div_clk
    
    Fixed divider clocks have no need for any clk_div_ops. So, make the entire
    ops optional for div_clk. With this in mind, add more NULL pointer checks
    for ops.
    
    Change-Id: Ic5e2069a9a3c2c2cadf9285e1f1b9774cc5bf8f9
    Signed-off-by: Saravana Kannan <skannan@codeaurora.org>

commit 48ee6ba4f3c2aa9cfef5a2bf25c28d6ba8b51d59
Author: Saravana Kannan <skannan@codeaurora.org>
Date:   Wed Jun 12 02:04:01 2013 -0700

    msm: clock-generic: Handle round rate return values greater than LONG_MAX
    
    The return type of clk_round_rate() is long. So, technically it can return
    only rates as large as LONG_MAX (~2.1 GHz). But CPU clocks can already go
    past 2.1 GHz and any return value greater than LONG_MAX will appear as a
    negative number to the caller.
    
    So, make the clock-generic implementation a bit more selective/intelligent
    about treating negative return values from clk_round_rate() as errors. If
    the returned negative number is not one of the standard error values
    (IS_ERR_VALUE), then treat the negative long number as an unsigned long
    rate.
    
    Change-Id: I27383bdb74ed6986667fee681005adec7b16bde9
    Signed-off-by: Saravana Kannan <skannan@codeaurora.org>

commit 6ec5089b07f41a912a39c2369ef796c30ff57158
Author: Saravana Kannan <skannan@codeaurora.org>
Date:   Mon Apr 29 23:52:37 2013 -0700

    msm: clock-generic: Add generic library for divider clocks
    
    The generic divider clock library can support fixed divider clocks and
    divider clocks that can support a range of divider values.
    
    Change-Id: I9c03a4b01abad3f7f73d56f89a9e8caef30067c3
    Signed-off-by: Saravana Kannan <skannan@codeaurora.org>

commit c965d8dda8bb66ddeffc0c0cd6723cd673f81be7
Author: Saravana Kannan <skannan@codeaurora.org>
Date:   Wed Apr 24 21:51:21 2013 -0700

    msm: clock-generic: Add generic library for mux clocks
    
    The clk_ops defined in this library can't be used directly, but they
    implement majority of the code necessary to handle common clock types. The
    HW specific operations are handled by calling ops implemented by the HW
    specific driver. The clk structs defined in this library contain fields
    that are not used by the helper library, but commonly needed by the HW
    specific ops. They also provide a *priv field to store other uncommon data.
    
    To start with, this file implement generic ops for a mux.
    
    Change-Id: Ibc44b95934f402dbcbb1996d77a61d40d9e53f20
    Signed-off-by: Saravana Kannan <skannan@codeaurora.org>

commit 74ca4f07dce19c4c8d2c89d142011c28792b183c
Author: Saravana Kannan <skannan@codeaurora.org>
Date:   Tue Sep 3 16:54:58 2013 -0700

    msm: clock: Add is_better_rate() helper function
    
    The is_better_rate() function is meant for use in clk_round_rate()
    implementations to check if one of the new possible rates is better than
    the current best match for the round rate request.
    
    Change-Id: Ifca1840b15ec6faf6031af35ca9d219f6b32c3bb
    Signed-off-by: Saravana Kannan <skannan@codeaurora.org>

commit 04620f851549d04f24387f703a02bb52efc79050
Author: Junjie Wu <junjiew@codeaurora.org>
Date:   Tue Dec 24 15:30:30 2013 -0800

    msm-clock-provider: Add CLK_LIST macro
    
    Clock lookup table is deprecated, but a list of clocks are still needed
    for clock registration. Create a new macro for listing clocks.
    
    Change-Id: I36497949dfa26ce4510c32d2966708800359e993
    Signed-off-by: Junjie Wu <junjiew@codeaurora.org>

commit a2e409ec5e892e29f527764c8e881b809d9adc4c
Author: Vikram Mulukutla <markivx@codeaurora.org>
Date:   Tue Oct 22 17:59:32 2013 -0700

    msm: clk-provider: Move clock headers to include/linux/clk
    
    Architecutural changes in the ARM Linux kernel tree mandate the
    eventual removal of the mach-* directories. Move the
    mach/clk-provider and mach/clk header to include/linux/clk.
    
    Change-Id: I495f8332bf5d0d09ccfb236c819dea2bacb13542
    Signed-off-by: Vikram Mulukutla <markivx@codeaurora.org>
    Signed-off-by: Junjie Wu <junjiew@codeaurora.org>
    
    Conflicts:
    	arch/arm/mach-msm/board-8084.c
    	arch/arm/mach-msm/board-8092.c
    	arch/arm/mach-msm/board-8226.c
    	arch/arm/mach-msm/board-8610.c
    	arch/arm/mach-msm/board-fsm9900.c
    	arch/arm/mach-msm/board-krypton.c
    	arch/arm/mach-msm/board-samarium.c
    	arch/arm/mach-msm/clock-alpha-pll.h
    	arch/arm/mach-msm/clock-dsi-8610.c
    	arch/arm/mach-msm/clock-dsi-8610.h
    	arch/arm/mach-msm/clock-generic.c
    	arch/arm/mach-msm/clock-krait-8974.c
    	arch/arm/mach-msm/clock-krait.c
    	arch/arm/mach-msm/clock-krait.h
    	arch/arm/mach-msm/clock-krypton.c
    	arch/arm/mach-msm/clock-local2.c
    	arch/arm/mach-msm/clock-mdss-8974.c
    	arch/arm/mach-msm/clock-rpm.h
    	arch/arm/mach-msm/gdsc.c
    	arch/arm/mach-msm/include/mach/clock-generic.h
    	arch/arm/mach-msm/mpm-of.c
    	arch/arm/mach-msm/pcie.c
    	arch/arm/mach-msm/pil-q6v5-lpass.c
    	arch/arm/mach-msm/pil-q6v5-mss.c
    	arch/arm/mach-msm/pil-q6v5.c
    	arch/arm/mach-msm/pil-vpu.c
    	drivers/media/platform/msm/broadcast/ensigma_uccp330.c
    	drivers/media/platform/msm/camera_v2/jpeg_10/msm_jpeg_platform.c
    	drivers/media/platform/msm/camera_v2/pproc/cpp/msm_cpp.c
    	drivers/usb/dwc3/dwc3-msm.c
    	drivers/usb/host/ehci-msm-hsic.c
    	drivers/usb/host/ehci-msm2.c
    	drivers/usb/host/xhci-msm-hsic.c
    	drivers/usb/phy/phy-msm-ssusb-qmp.c
    	drivers/usb/phy/phy-msm-usb.c
    	drivers/video/msm/mdp.c
    	drivers/video/msm/mdss/dsi_io_v2.c
    	drivers/video/msm/mdss/mdp3.c
    	drivers/video/msm/mdss/mdss_edp_aux.c
    	drivers/video/msm/mdss/mdss_qpic.c
    
    Conflicts:
    	arch/arm/mach-msm/clock.c
    	drivers/mmc/host/msm_sdcc.c

commit f4259697f3bbd8d7fa27b5703a5b54fe106d5bac
Author: Junjie Wu <junjiew@codeaurora.org>
Date:   Fri Dec 6 18:19:09 2013 -0800

    clkdev: Return -EPROBE_DEFER when clock is not found
    
    If clock provider registers later than its users, clk_get might fail.
    Allow drivers to do deferred probing instead of failing them permanently.
    
    Change-Id: Ibdb3442a65672ae1202934963cb3479dc7aea8e1
    Signed-off-by: Junjie Wu <junjiew@codeaurora.org>

commit 8b0dc67cdb38cf51b4c4982a27caa344403da69d
Author: Junjie Wu <junjiew@codeaurora.org>
Date:   Fri Dec 6 17:24:48 2013 -0800

    clkdev: Remove of_clk_get error message
    
    of_clk_get_by_name will print an error message if it can't find the
    clock from device tree. However, that doesn't indicate clk_get will
    fail. It can still succeed in clk_get_sys. Remove the error message
    to avoid confusion.
    
    Driver calling clk_get should check return value and print error
    messages in their code.
    
    Change-Id: I0627fa85cbb6260c4d5b0e8f51470198c82840ee
    Signed-off-by: Junjie Wu <junjiew@codeaurora.org>

commit e72acddf9276dd4f2ba64bc9a799c6387e31f72c
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Mon Aug 3 17:43:49 2015 +0200

    Remove a function that was defined twice, once here and the other time in clk-devres

commit 298dc521ebdcd77983733ca5ae26867a082f7f3b
Author: Shawn Guo <shawn.guo@linaro.org>
Date:   Tue Mar 27 15:23:21 2012 +0800

    clk: remove unnecessary EXPORT_SYMBOL_GPL
    
    It makes no sense to have EXPORT_SYMBOL_GPL on static functions.
    
    Signed-off-by: Shawn Guo <shawn.guo@linaro.org>
    Signed-off-by: Mike Turquette <mturquette@linaro.org>

commit eed9e1de3943023c87f7ee12280b70998e3303de
Author: Mark Brown <broonie@sirena.org.uk>
Date:   Wed Sep 19 12:43:21 2012 +0100

    ARM: 7537/1: clk: Fix release in devm_clk_put()
    
    Surprisingly devres_destroy() doesn't call the destructor for the
    resource it is destroying, use the newly added devres_release() instead
    to fix this.
    
    Signed-off-by: Mark Brown <broonie@opensource.wolfsonmicro.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

commit d101e2b88922db4cb387a9429d1e87673433f597
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Tue Sep 15 01:53:43 2015 +0200

    fix missing function in barry allen

commit 7e5a10c9e2325358298b72d03b373704e9611158
Author: Lars-Peter Clausen <lars@metafoo.de>
Date:   Tue Sep 11 19:56:23 2012 +0100

    ARM: 7534/1: clk: Make the managed clk functions generically available
    
    The managed clk functions are currently only available when the generic clk
    lookup framework is build. But the managed clk functions are merely wrappers
    around clk_get and clk_put and do not depend on any specifics of the generic
    lookup functions and there are still quite a few custom implementations of the
    clk API. So make the managed functions available whenever the clk API is
    implemented.
    
    The patch also removes the custom implementation of devm_clk_get for the
    coldfire platform.
    
    Signed-off-by: Lars-Peter Clausen <lars@metafoo.de>
    Acked-by: Greg Ungerer <gerg@uclinux.org>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

commit 5674eb7843030fce22962c3353bd9c4061e05488
Author: Sascha Hauer <s.hauer@pengutronix.de>
Date:   Thu May 3 15:36:14 2012 +0530

    clk: add a fixed factor clock
    
    Having fixed factors/dividers in hardware is a common pattern, so
    add a basic clock type doing this. It basically describes a fixed
    factor clock using a nominator and a denominator.
    
    Signed-off-by: Sascha Hauer <s.hauer@pengutronix.de>
    Reviewed-by: Viresh Kumar <viresh.kumar@st.com>
    Tested-by: Shawn Guo <shawn.guo@linaro.org>
    [mturquette@linaro.org: constify parent_names in static init macro]
    [mturquette@linaro.org: copy/paste bug from mux in static init macro]
    [mturquette@linaro.org: fix error handling in clk_register_fixed_factor]
    [mturquette@linaro.org: improve division accuracy; thanks to Saravana]
    Signed-off-by: Mike Turquette <mturquette@linaro.org>

commit d429cd66c806042a5a0fdbb0056ff5c0c5b4f599
Author: Rob Herring <rob.herring@calxeda.com>
Date:   Wed Jul 18 11:52:23 2012 +0800

    clk: fix compile for OF && !COMMON_CLK
    
    With commit 766e6a4ec602d0c107 (clk: add DT clock binding support),
    compiling with OF && !COMMON_CLK is broken.
    
    Reported-by: Alexandre Pereira da Silva <aletes.xgr@gmail.com>
    Reported-by: Prashant Gaikwad <pgaikwad@nvidia.com>
    Signed-off-by: Rob Herring <rob.herring@calxeda.com>
    Signed-off-by: Mike Turquette <mturquette@linaro.org>

commit e758184457df2aa353c098805e6c1758b5c1c3ce
Author: Shawn Guo <shawn.guo@linaro.org>
Date:   Wed Jul 18 11:52:22 2012 +0800

    clk: fix clk_get on of_clk_get_by_name return check
    
    The commit 766e6a4 (clk: add DT clock binding support) plugs device
    tree clk lookup of_clk_get_by_name into clk_get, and fall on non-DT
    lookup clk_get_sys if DT lookup fails.
    
    The return check on of_clk_get_by_name takes (clk != NULL) as a
    successful DT lookup.  But it's not the case.  For any system that
    does not define clk lookup in device tree, ERR_PTR(-ENOENT) will be
    returned, and consequently, all the client drivers calling clk_get
    in their probe functions will fail to probe with error code -ENOENT
    returned.
    
    Fix the issue by checking of_clk_get_by_name return with !IS_ERR(clk),
    and update of_clk_get and of_clk_get_by_name for !CONFIG_OF build
    correspondingly.
    
    Signed-off-by: Shawn Guo <shawn.guo@linaro.org>
    Acked-by: Rob Herring <rob.herring@calxeda.com>
    Tested-by: Marek Vasut <marex@denx.de>
    Tested-by: Lauri Hintsala <lauri.hintsala@bluegiga.com>
    Signed-off-by: Mike Turquette <mturquette@linaro.org>

commit 5e21ae344602eb6daeac85f3b32dcc61a957a8a0
Author: Grant Likely <grant.likely@secretlab.ca>
Date:   Mon Apr 9 14:50:06 2012 -0500

    clk: add DT clock binding support
    
    Based on work 1st by Ben Herrenschmidt and Jeremy Kerr, then by Grant
    Likely, this patch adds support to clk_get to allow drivers to retrieve
    clock data from the device tree.
    
    Platforms scan for clocks in DT with of_clk_init and a match table, and
    the register a provider through of_clk_add_provider. The provider's
    clk_src_get function will be called when a device references the
    provider's OF node for a clock reference.
    
    v6 (Rob Herring):
        - Return error values instead of NULL to match clock framework
          expectations
    
    v5 (Rob Herring):
        - Move from drivers/of into common clock subsystem
        - Squashed "dt/clock: add a simple provider get function" and
          "dt/clock: add function to get parent clock name"
        - Rebase to 3.4-rc1
        - Drop CONFIG_OF_CLOCK and just use CONFIG_OF
        - Add missing EXPORT_SYMBOL to various functions
        - s/clock-output-name/clock-output-names/
        - Define that fixed-clock binding is a single output
    
    v4 (Rob Herring):
        - Rework for common clk subsystem
        - Add of_clk_get_parent_name function
    
    v3: - Clarified documentation
    
    v2: - fixed errant ';' causing compile error
        - Editorial fixes from Shawn Guo
        - merged in adding lookup to clkdev
        - changed property names to match established convention. After
          working with the binding a bit it really made more sense to follow the
          lead of 'reg', 'gpios' and 'interrupts' by making the input simply
          'clocks' & 'clock-names' instead of 'clock-input-*', and to only use
          clock-output* for the producer nodes. (Sorry Shawn, this will mean
          you need to change some code, but it should be trivial)
        - Add ability to inherit clocks from parent nodes by using an empty
          'clock-ranges' property.  Useful for busses.  I could use some feedback
          on the new property name, 'clock-ranges' doesn't feel right to me.
    
    Signed-off-by: Grant Likely <grant.likely@secretlab.ca>
    Signed-off-by: Rob Herring <rob.herring@calxeda.com>
    Reviewed-by: Shawn Guo <shawn.guo@freescale.com>
    Cc: Sascha Hauer <kernel@pengutronix.de>
    Signed-off-by: Mike Turquette <mturquette@linaro.org>

commit 0b0b0bf40fc8382b1de0790baa740b30f796bc50
Author: Mark Brown <broonie@sirena.org.uk>
Date:   Thu Apr 5 11:42:09 2012 +0100

    ARM: 7376/1: clkdev: Implement managed clk_get()
    
    Allow clk API users to simplify their cleanup paths by providing a
    managed version of clk_get() and clk_put().
    
    Signed-off-by: Mark Brown <broonie@opensource.wolfsonmicro.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

commit 1e663f9d2ee3fb91cb4af96bef1903e3e2949118
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Tue Sep 15 01:34:54 2015 +0200

    Revert "msm: clock: Introduce device tree lookups" because we don't have this function (sadly)
    
    This reverts commit bf857833da965ec7fbe203f0c6510c59a18afde4.

commit 2377151416f8be6a543de51ba6d55bd002818a10
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Tue Aug 4 16:44:24 2015 +0200

    cpufreq: add arteractive governor and update pm-data to get this working :)
    all credits are going to @arter97, thank you dude
    
    Signed-off-by: Tkkg1994 <luca.grifo@outlook.com>

commit 0c9cc96bc45d2a9aaaad1954f3fb9134fc3efeba
Author: viresh kumar <viresh.kumar@st.com>
Date:   Thu Apr 19 04:23:25 2012 +0100

    ARM: 7392/1: CLKDEV: Optimize clk_find()
    
    clk_find must return as soon as it gets the correct clock. Currently it check
    all clocks until it found a lookup with both dev_id and con_id matching.
    
    If only one of them is passed, then we don't actually need to wait for both of
    them to match. We can quit as soon as the requested id (dev_id or con_id)
    matches.
    
    Signed-off-by: Viresh Kumar <viresh.kumar@st.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

commit 320e4db0642acdb79b9a5e751ea0e6542e6904ce
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Wed May 2 09:30:32 2012 +0100

    CLKDEV: provide helpers for common clock framework
    
    The common clock framework allocates clocks dynamically.  Provide a
    set of helpers to streamline the clkdev registration of the clock
    lookups to avoid repetitive code sequences.
    
    Reviewed-by: Viresh Kumar <viresh.kumar@st.com>
    Tested-by: Shawn Guo <shawn.guo@linaro.org>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

commit a8d70f828c5a8995ab11b26d2317cb3469ac1b18
Author: Vikram Mulukutla <markivx@codeaurora.org>
Date:   Fri Jun 14 15:28:10 2013 -0700

    msm: clock-rpm: Correct is_enabled implementation for RPM-SMD clocks
    
    Clocks using the RPM-SMD driver have no way to query
    clock state from the RPM. Therefore do not use the
    get_rate function to figure out the enable-state of
    the clock.
    
    Change-Id: I4d142c716a4e41e54185e4ed4fe9feb33a1c2427
    Signed-off-by: Vikram Mulukutla <markivx@codeaurora.org>

commit ceb20f4252fe311f142d26898c6a258b6fe54206
Author: Vikram Mulukutla <markivx@codeaurora.org>
Date:   Sat Oct 12 11:27:33 2013 -0700

    msm: clock: Introduce device tree lookups
    
    Add a function of_msm_clock_register that registers a
    clock provider with the clock DT framework. Provide a
    new CLK_LOOKUP_OF macro to link up clock lookup entries
    with DT clock indices.
    
    Change-Id: I66ab29df116341583db5f812b2dc5db3a65e15b5
    Signed-off-by: Vikram Mulukutla <markivx@codeaurora.org>

commit c59b2f4a1960b9cdbe27e41dc3adda2035f68ff6
Author: Vikram Mulukutla <markivx@codeaurora.org>
Date:   Mon Sep 23 17:49:42 2013 -0700

    msm: clock-debug: Allow dividers between clocks and measurement logic
    
    Some debug implementations output a divided clock specifically
    for the measurement circuitry; therefore the measured output
    is an integer fraction of the original clock rate. Account for
    this by dividing the rate on the clock being measured by the
    rate set on the parent of the measure clock.
    
    Change-Id: I07e791670ccfb3eaa29eab76cdbd2bfabce62a38
    Signed-off-by: Vikram Mulukutla <markivx@codeaurora.org>

commit c078026ff80d5197b0e81570f60abbd7a0933bae
Author: Vikram Mulukutla <markivx@codeaurora.org>
Date:   Mon Sep 23 13:30:40 2013 -0700

    msm: clock-debug: Enable debug muxes en route to clock being measured
    
    MSM clock debug muxes have gates that need to be enabled
    to allow clock signals to be output to the measurement
    circuitry. Since we want to ensure that a clock that is
    assumed to be off in software is also not outputting a
    signal, it is not possible to call clk_enable on the
    measure clock. Therefore call the enable op on all the
    clocks upstream of the measure clock, except for the clock
    being measured. Also, leave the clocks enabled to allow
    measurement using a scope.
    
    Change-Id: Ia8016edd460d03b6a0686c58788457aeb6c9e2c5
    Signed-off-by: Vikram Mulukutla <markivx@codeaurora.org>

commit 2c4be47cdfc81313c8138c20e495fa51b670abeb
Author: Vikram Mulukutla <markivx@codeaurora.org>
Date:   Thu Jul 11 12:20:18 2013 -0700

    msm: clock-debug: Modify measure logic to account for multiple CCs
    
    In the near future, multiple clock controllers will be
    registered, each with their own lookup tables. Modify
    the debugfs measure node creation logic to account for
    this. The assumption is that there will be one measure
    clock that will apply to all clock controllers.
    
    Change-Id: I8d2d7a6df2a7a0228bdc7fbc743d5207f2a1bab0
    Signed-off-by: Vikram Mulukutla <markivx@codeaurora.org>

commit 4d1a3a1802d2769b35074193941b42513deeb803
Author: Patrick Daly <pdaly@codeaurora.org>
Date:   Thu Sep 26 18:14:51 2013 -0700

    msm: clock: Add a new API to print registers for debugging
    
    clk_debug_print_hw() will format a list of register names and values, and
    output the result to either the kernel log or a seq_file.
    
    Change-Id: Ic225eb0261818faaee91b42955f2a517e927a30b
    Signed-off-by: Patrick Daly <pdaly@codeaurora.org>

commit 672da24b66f821ac397584b8b49108407e01d98f
Author: Xiaogang Cui <xiaogang@codeaurora.org>
Date:   Sat Jul 13 06:45:45 2013 +0800

    msm: clock-debug: Add support to show all the enabled clocks
    
    Add a new debugfs file <debugfs>/clk/enabled_clocks to show all the
    currently enabled clock, their rate and their dependency chain.
    
    Example output:
    cat /sys/kernel/debug/clk/enabled_clocks
    Enabled clocks:
            gcc_lpass_q6_axi_clk:1:1 [0]
            gcc_mss_q6_bimc_axi_clk:1:1 [0]
            gcc_mss_cfg_ahb_clk:1:1 [0]
            gcc_boot_rom_ahb_clk:1:1 [0]
            xo_a_clk:2:2 [19200000]
            gpll0_ao:2:2 [600000000]
            kpss_ahb_clk_src:1:1 [19200000] ->
    xo_a_clk:2:2 [19200000]
            bimc_msmbus_a_clk:1:1 [400000000] ->
    bimc_a_clk:1:1 [400000000]
            qdss_clk:4:4 [1]
            qdss_clk:4:4 [1]
            qdss_clk:4:4 [1]
            qdss_clk:4:4 [1]
            qdss_clk:4:4 [1]
               ....
    Enabled clock count: 42
    
    Change-Id: I6eda59c7e61b77518676e3aa93895ccb792176ac
    Signed-off-by: Xiaogang Cui <xiaogang@codeaurora.org>
    
    Conflicts:
    	arch/arm/mach-msm/clock-debug.c

commit d9c8131b36815a7e1ed1f67ed1625c658825d582
Author: xiaogang <xiaogang@codeaurora.org>
Date:   Fri Jul 5 03:34:17 2013 +0800

    msm: clock-debug: Update fmax_rates to print uV/uA values for each level
    
    Example output after change:
    /sys/kernel/debug/clk/a7sspll $ cat fmax_rates
                      reg 0     reg 1
             freq        uV        uV
                0         0         1
       1000000000   1800000         4
     [ 1900000000   1800000         5]
                0   1800000         7
    
    Change-Id: I1563cd8f77d479e0c6c67ee3c93a66fbafb6d6f7
    Signed-off-by: Xiaogang Cui <xiaogang@codeaurora.org>

commit 6dab634e131a6924bccda62b45faf6a3d3a93d0e
Author: Vikram Mulukutla <markivx@codeaurora.org>
Date:   Thu Oct 17 12:36:54 2013 -0700

    msm: clock: Use CLKFLAG_NO_RATE_CACHE in clk_set_parent
    
    Some use cases involve the rate of a parent changing
    from under a child. Therefore the set_parent op needs
    a chance to update the rate field. Use the
    CLKFLAG_NO_RATE_CACHE to allow the clk_set_parent to
    go through to the set_parent op.
    
    Change-Id: Icaf48915322539c84ef522621e35dafebe08b23b
    Signed-off-by: Vikram Mulukutla <markivx@codeaurora.org>

commit 8b7f35e671e238c9c3018f988745eff60abe9685
Author: Patrick Daly <pdaly@codeaurora.org>
Date:   Tue Oct 1 16:51:22 2013 -0700

    msm: clock: Check that round_rate is less than fmax
    
    HW has maximum frequency requirements. Ensure that the clock frequency
    returned from clk_round_rate() is within these limits.
    
    Change-Id: I0d4ed5851fc1f9f76c0ad0622cc6609e64e5311f
    Signed-off-by: Patrick Daly <pdaly@codeaurora.org>

commit 2691feb358294c008426b97c8c139d4e37fc7796
Author: Vikram Mulukutla <markivx@codeaurora.org>
Date:   Mon Sep 23 19:41:48 2013 -0700

    msm: clock: Allow set_parent on clocks without the set_parent op
    
    Some use cases require that set_parent succeed on a clock
    that actually has just one parent. If a clock doesn't expose
    a set_parent op, but the requested parent is the same as the
    current parent, return success.
    
    Change-Id: I0cf3f1e6e982de5cc86eb408ca71a62466710ef1
    Signed-off-by: Vikram Mulukutla <markivx@codeaurora.org>

commit 6fe5eb87f561e70d113de7c712e081f02d8e3bb3
Author: Saravana Kannan <skannan@codeaurora.org>
Date:   Fri Jun 28 12:23:12 2013 -0700

    msm: clock: Don't update clk->parent in clk_set_parent()
    
    Any change to the parent field has to be done between __clk_pre_reparent()
    and __clk_post_reparent() to avoid race conditions. So, stop setting the
    parent field and leave it to the ops->set_parent() implementations to
    update the parent field. All current implementations of ops->set_parent()
    already do so.
    
    Change-Id: Idc41bee306881a885335ecb322a21509231048cc
    Signed-off-by: Saravana Kannan <skannan@codeaurora.org>

commit 2d843cc77cc1cd9576f248f9afdd42e01f621e03
Author: Junjie Wu <junjiew@codeaurora.org>
Date:   Mon Aug 12 16:37:25 2013 -0700

    msm: clock: Call regulator_enable/disable when updating vdd
    
    When all clocks on a regulator are disabled, regulator_disable should
    be called.  Likewise, when first clock is prepared, regulator_enable
    should be called.
    
    Add regulator_enable/disable function during voltage voting.
    
    Change-Id: I791e6faffe58870990a7865b7f241ec53f50012b
    Signed-off-by: Junjie Wu <junjiew@codeaurora.org>

commit 40502fb97ed641ee0aaf3e78f83d9e8c90b85cb8
Author: Patrick Daly <pdaly@codeaurora.org>
Date:   Tue Jul 16 14:19:54 2013 -0700

    msm: clock: Keep proper track of current voltage level
    
    Fix a bug where vdd_class->cur_level is not updated when regulator api
    calls are successful. The impact of the bug is unnecessary calls to
    regulator APIs but not additional RPM calls since the regulator APIs
    will optimize out repeated requests for the same voltage/current.
    
    In case the regulator APIs fail, this bug might also result in
    undervolting. But that's an unlikely case in a real product.
    
    Change-Id: Ibe3574b6b72fcf898a5bfe9b5b253a8d0e0125ae
    Signed-off-by: Patrick Daly <pdaly@codeaurora.org>

commit c26051c7130e1fa95fb73a7462580424d2905fab
Author: Saravana Kannan <skannan@codeaurora.org>
Date:   Wed Mar 20 21:22:05 2013 -0700

    msm: clock: Add pre_set_rate and post_set_rate ops
    
    Some clocks might need to perform additional steps before/after a voltage
    changes happen during a rate change. To accomodate such clocks and similar
    requirements from new clock types in the future, add generic
    pre/post_set_rate ops that are called before any HW state change is done
    for set rate.
    
    Change-Id: I07e4f79136028aaae5dee6d66d18b45c445f325b
    Signed-off-by: Saravana Kannan <skannan@codeaurora.org>

commit d337d39099fa7fb78c61654f3b685aa9dd80923a
Author: Vikram Mulukutla <markivx@codeaurora.org>
Date:   Mon Dec 10 20:51:22 2012 -0800

    msm: clock: Allow msm_clock_init to be invoked multiple times
    
    In preparation for a series of changes that move MSM clock
    controllers to the Linux device model, allow msm_clock_init
    to be invoked multiple times (once for each clock controller).
    
    Change-Id: I0d56612c660f020ea762feb468234813cff0db22
    Signed-off-by: Vikram Mulukutla <markivx@codeaurora.org>
    
    Conflicts:
    	arch/arm/mach-msm/clock-debug.c

commit f3c91776aea622f11eb0d5001f6240c9c7f41617
Author: Vikram Mulukutla <markivx@codeaurora.org>
Date:   Wed Apr 10 14:24:38 2013 -0700

    msm: clock-debug: Add support to set a parent via debugfs
    
    To enable debugging of clocks that implement the
    clk_set_parent API, add support in debugfs to set
    the parent like so:
    
    cd <debugfs>/clk/<clock_name>
    echo "parent_clock_name" > parent
    
    Change-Id: Icf1781b52ab6a366951f23a3c7b6cc9445442201
    Signed-off-by: Vikram Mulukutla <markivx@codeaurora.org>

commit 146a2a9e2a51707127f124eaaeb791e856b1f776
Author: Saravana Kannan <skannan@codeaurora.org>
Date:   Thu Sep 27 16:19:07 2012 -0700

    clock-debug: Add support to query current parent.
    
    Add a "parent" debugfs file for each clock that can be used to query the
    current parent of a clock. The file contains the name of the parent clock
    if it exists. Otherwise, contains "None".
    
    Change-Id: I5ba1dcd344e9a2a1eec7f2bae18cec3f9b7b2338
    Signed-off-by: Saravana Kannan <skannan@codeaurora.org>

commit 1184e0e0f6cddfd675c6cc26250e52458c09c9dc
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Mon Aug 3 13:35:30 2015 +0200

    more adjustments for the new cpufreq

commit aa35b909b14504d85d05867189c1b4bcdce25a67
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Mon Aug 3 13:29:09 2015 +0200

    cpufreq: add barry_allen governor v1.0, adjusted for this kernel source
    thanks to @javilonas for his work!

commit a9e9f263d89e35d11c199422aa902d19fe512558
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Tue Sep 15 00:50:31 2015 +0200

    Revert lowmemory killer to stock (this is killing the wifi!)

commit a5e2dc663c51fc2e29ab67ba000683e4c7773b79
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Tue Sep 15 00:51:22 2015 +0200

    Revert "mm, oom: change type of oom_score_adj to short"
    
    This reverts commit b48d61d9ae2e996dc090d2794aa2ed35b8a4c61b.

commit b48d61d9ae2e996dc090d2794aa2ed35b8a4c61b
Author: David Rientjes <rientjes@google.com>
Date:   Tue Dec 11 16:02:54 2012 -0800

    mm, oom: change type of oom_score_adj to short
    
    The maximum oom_score_adj is 1000 and the minimum oom_score_adj is -1000,
    so this range can be represented by the signed short type with no
    functional change.  The extra space this frees up in struct signal_struct
    will be used for per-thread oom kill flags in the next patch.
    
    Signed-off-by: David Rientjes <rientjes@google.com>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Reviewed-by: Michal Hocko <mhocko@suse.cz>
    Cc: Anton Vorontsov <anton.vorontsov@linaro.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    
    Conflicts:
    	drivers/staging/android/lowmemorykiller.c

commit 166e7bb2f3561b58f40599c28fae94cf2e71ad76
Author: Hong-Mei Li <a21834@motorola.com>
Date:   Fri Mar 27 14:15:25 2015 -0700

    drivers:lmk: Fix null pointer issue
    
    On some race, the tsk that lmk is using may be deleted from the RB tree
    by other thread, and rb_next would return a NULL if we use this tsk to
    get next. For this case, we need to skip this round of shrink and wait
    for the next turn. Otherwise, tsk would trigger NULL pointer panic.
    
    Signed-off-by: Hong-Mei Li <a21834@motorola.com>
    Reviewed-on: http://gerrit.mot.com/729556
    SLTApproved: Slta Waiver <sltawvr@motorola.com>
    SME-Granted: SME Approvals Granted
    Tested-by: Jira Key <jirakey@motorola.com>
    Reviewed-by: Yi-Wei Zhao <gbjc64@motorola.com>
    Submit-Approved: Jira Key <jirakey@motorola.com>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit b8c78d5fc91183fada03e6c044081cb498980587
Author: Hong-Mei Li <a21834@motorola.com>
Date:   Thu Mar 12 19:14:26 2015 -0700

    drivers:lmk: Fix double delete issue
    
    someone may change a process's oom_score_adj by proc fs, even though the
    process has exited. In that case, the task was deleted from the rb tree
    already, and the redundant deleting would trigger rb_erase panic finally.
    
    In this patch, we make sure to clear the node after deteting and check
    its empty status before rb_erase.
    
    Signed-off-by: Hong-Mei Li <a21834@motorola.com>
    Reviewed-on: http://gerrit.mot.com/725306
    SLTApproved: Slta Waiver <sltawvr@motorola.com>
    SME-Granted: SME Approvals Granted
    Tested-by: Jira Key <jirakey@motorola.com>
    Reviewed-by: Sheng-Zhe Zhao <a18689@motorola.com>
    Submit-Approved: Jira Key <jirakey@motorola.com>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 77f01c7a21354975426ee5e8c21e32a56d7c9398
Author: Laura Abbott <lauraa@codeaurora.org>
Date:   Tue Sep 23 19:48:30 2014 -0700

    lowmemorykiller: Dump out slab state information
    
    Leaks from the slab allocator are a very common cause of
    out of memory issues and excessive lowmemorykiller killing.
    Dump out the slab statistics as well to help with debugging.
    
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 46bc733455afa561902106276aef67a5a01de13e
Author: Yi-wei Zhao <gbjc64@motorola.com>
Date:   Fri Feb 13 17:33:04 2015 -0800

    staging:android:lmk: read rb tree root with spinlock
    
    there is racing condition: after reading rb tree root, it might be changed
    by other tasks before adding new node. it can lead to rb tree corruption.
    This patch is to avoid this race condition.
    
    Signed-off-by: Yi-wei Zhao <gbjc64@motorola.com>
    Reviewed-on: http://gerrit.mot.com/715645
    Tested-by: Jira Key <jirakey@motorola.com>
    Reviewed-by: Sheng-Zhe Zhao <a18689@motorola.com>
    SLTApproved: Christopher Fries <cfries@motorola.com>
    Submit-Approved: Jira Key <jirakey@motorola.com>
    Reviewed-on: http://gerrit.mot.com/719823
    SLTApproved: Slta Waiver <sltawvr@motorola.com>
    Reviewed-by: Upgrade Build <dbbuild@motorola.com>
    Submit-Approved: Upgrade Build <dbbuild@motorola.com>
    Tested-by: Upgrade Build <dbbuild@motorola.com>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 1984d55b6a9512ed560ae4e1b005abf86b9b59ec
Author: alucard24 <dmbaoh2@gmail.com>
Date:   Mon Sep 14 19:37:25 2015 +0200

    alucard, darkness and nightmare govs: small changes!
    
    Conflicts:
    	drivers/cpufreq/cpufreq_alucard.c

commit b057ddd4ebf5a41c126b57fb94ccf70950c2a371
Author: alucard24 <dmbaoh2@gmail.com>
Date:   Sat Aug 15 14:03:30 2015 +0200

    alucard, darkness and nightmare cpu govs: adjusted sampling time!
    
    Conflicts:
    	drivers/cpufreq/cpufreq_alucard.c

commit 82b8acd6d09115bd61c0d455d2c11d3b18cee358
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Thu Sep 3 17:44:31 2015 +0200

    alucard and nightmare cpu govs: small change!
    
    Conflicts:
    	drivers/cpufreq/cpufreq_alucard.c

commit 0177b357bf32a847c4aceafa188351535c237a93
Author: Jason Hrycay <jason.hrycay@motorola.com>
Date:   Wed Apr 15 12:08:54 2015 -0500

    lowmemorykiller: Check tgid with rcu lock held
    
    When checking if the current task shares the same task group ID of the
    task which is signalled for MEMDIE, ensure we do so within the rcu lock
    to avoid potential race condition of the memory pointed to by tsk
    getting free'd and/or reusued and we end up with a false evaluation of
    current.tgid matching, thus killing a task that maybe should not be
    killed.
    
    Signed-off-by: Jason Hrycay <jason.hrycay@motorola.com>
    Reviewed-on: http://gerrit.mot.com/735795
    SLTApproved: Slta Waiver <sltawvr@motorola.com>
    SME-Granted: SME Approvals Granted
    Tested-by: Jira Key <jirakey@motorola.com>
    Reviewed-by: Hong-Mei Li <a21834@motorola.com>
    Reviewed-by: Christopher Fries <cfries@motorola.com>
    Submit-Approved: Jira Key <jirakey@motorola.com>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 157a64b4d433d321779d6dbf8dab18191fc31454
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Mon Sep 14 22:10:36 2015 +0200

    cleanup of lowmemory killer

commit fbe6f158dd7e1ab8a0f73b45c36a0baba360c856
Author: Dorimanx <yuri@bynet.co.il>
Date:   Fri Jun 19 13:21:34 2015 +0300

    HOTPLUGS: enabled CPU 0-2 on screen OFF so we have 3 cores when in standby.
    
    Default LP is all cores online in standby. when suspending and no wakelocks.
    only boot CPU is online cpu0 and sleeping.
    
    i have found that when we have only core0 and on limited freq.
    and play hifi sound via stream wifi/data cpu is 100% on core0
    and other system services start to die. and kernel panic on mem allocations
    and OOM and watchdog barks...
    
    Give system what it's needs.
    now load is split per core, and system is relaxed.

commit 747b429382d45ad2511ba9f52b168a0a4cffb685
Author: Dorimanx <yuri@bynet.co.il>
Date:   Wed Jun 17 12:03:46 2015 +0300

    HOTPLUGS: add boost on hotplug start. and core down on stop.

commit 8a8a69d4971b2bd6642c97c5205b4db9563128d0
Author: Dorimanx <yuri@bynet.co.il>
Date:   Thu Jun 11 16:52:49 2015 +0300

    HOTPLUGS: fix bug, and correct the CPU down at suspend.
    
    Now 0,1 can stay online.
    
    this fix this warning:
    <3>CPU0 hotplug is not supported
    <6>_cpu_down: attempt to take down CPU 0 failed

commit 4a0f643353f6da609d384e14875f1f81adb93b78
Author: Weijie Yang <weijie.yang@samsung.com>
Date:   Fri Feb 14 14:03:56 2014 +0800

    staging: android: lowmemorykiller: set TIF_MEMDIE before send kill sig
    
    Set TIF_MEMDIE tsk_thread flag before send kill signal to the
    selected thread. This is to fit a usual code sequence and avoid
    potential race issue.
    
    Signed-off-by: Weijie Yang <weijie.yang@samsung.com>
    Acked-by: David Rientjes <rientjes@google.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 5ae97a23ee00aa4a98635ed153076d24e91fdf03
Author: Pranav Vashi <neobuddy89@gmail.com>
Date:   Sat Apr 5 21:19:09 2014 +0530

    staging:android:lmk: Use for_each_thread helper
    
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit b029fd56b6b96f639dd6531ef37b313b009a317c
Author: Susheel Khiani <skhiani@codeaurora.org>
Date:   Thu Feb 12 19:00:15 2015 +0530

    lowmemorykiller: Do proper NULL checks
    
    Pointer other_free is getting dereferenced without
    performing proper NULL checks which may cause issue.
    Do proper NULL checks at all points before dereferencing
    it.
    
    Signed-off-by: Susheel Khiani <skhiani@codeaurora.org>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit ffd8a26e492a2945b5577ab999bba6238e00b442
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun Aug 2 16:06:30 2015 +0200

     staging: android: lmk: add rcu lock while test task flag
    
    test_task_flag() need to be guarded by rcu_read_lock(), because
    
     test_task_flag()->
       while_each_thread()->
         next_thread()-> list_entry_rcu()
    
    Signed-off-by: Yi-wei Zhao <gbjc64@motorola.com>
    Reviewed-on: http://gerrit.pcs.mot.com/608283
    SLTApproved: Slta Waiver <sltawvr@motorola.com>
    Tested-by: Jira Key <jirakey@motorola.com>
    Reviewed-by: Hong-Mei Li <a21834@motorola.com>
    Reviewed-by: Yuanyuan Zhong <zyy@motorola.com>
    Reviewed-by: Jeffrey Carlyle <jeff.carlyle@motorola.com>
    Submit-Approved: Jira Key <jirakey@motorola.com>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 54a201e9bd4ba544558dc60726b8d7529c8c0cc4
Author: Yuanyuan Zhong <zyy@motorola.com>
Date:   Sun Dec 8 21:23:33 2013 -0600

    staging: android: lmk: check TIF directly
    
    When trying to fix that lmk can't kill an exiting task
    (commit 895304e926352e0272964a421bd6271e47616ab3
    (CR) staging: android: lmk: skip if killed by lmk),
    additional check was added for the TIF_MEMDIE flag.
    However the task lock has already been taken when examining
    the task. The same helper function which tries to grab the
    task lock again will be deadlocked.
    Change to check the thread flag of thread group leader directly.
    
    Signed-off-by: Yuanyuan Zhong <zyy@motorola.com>
    Reviewed-on: http://gerrit.pcs.mot.com/587751
    SLTApproved: Slta Waiver <sltawvr@motorola.com>
    Tested-by: Jira Key <jirakey@motorola.com>
    Reviewed-by: Yi-Wei Zhao <gbjc64@motorola.com>
    Submit-Approved: Jira Key <jirakey@motorola.com>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 0d36491ab50f7f780005c119f83fd786829602f3
Author: Yuanyuan Zhong <zyy@motorola.com>
Date:   Wed Nov 20 18:21:02 2013 -0600

    staging: android: lmk: skip if killed by lmk
    
    A task can be selected to kill when it's already exiting. Sending
    signal to the task at that state won't set TIF_SIGPENDING for it.
    After that, every task including threads in the exiting task itself
    running into lmk will compete for the mutex, then spend some time
    waiting for the exiting task to release memory or try to send signal
    to the same task again.
    We want the exiting task to exit as early as possible but it's slowed
    down greatly by the mutex and waiting. Skip lmk if current thread
    group leader is exiting and is killed by lmk. Also delegate TIF_MEMDIE
    to current task.
    
    Signed-off-by: Yuanyuan Zhong <zyy@motorola.com>
    Reviewed-on: http://gerrit.pcs.mot.com/584015
    SLTApproved: Slta Waiver <sltawvr@motorola.com>
    Tested-by: Jira Key <jirakey@motorola.com>
    Reviewed-by: Check Patch <CHEKPACH@motorola.com>
    Reviewed-by: Klocwork kwcheck <klocwork-kwcheck@sourceforge.mot.com>
    Reviewed-by: Russell Knize <rknize@motorola.com>
    Submit-Approved: Jira Key <jirakey@motorola.com>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 37ca4ed0798d5beb8f33aaf8524337794f2fc004
Author: Hong-Mei Li <a21834@motorola.com>
Date:   Fri Jul 12 01:07:25 2013 +0800

    staging: android: lmk: check free memory when tasks switch to background
    
    Add more free-memory check point.
    This patch trigger low memory killer to scan free memory when a process is moved
    to background task groups.
    
    Signed-off-by: Hong-Mei Li <a21834@motorola.com>
    Reviewed-on: http://gerrit.pcs.mot.com/557973
    Tested-by: Jira Key <jirakey@motorola.com>
    Reviewed-by: Yi-Wei Zhao <gbjc64@motorola.com>
    SLTApproved: Yi-Wei Zhao <gbjc64@motorola.com>
    Submit-Approved: Jira Key <jirakey@motorola.com>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 8ae9ffb32219a9274470d6e0777d5c399553e8da
Author: Yi-wei Zhao <gbjc64@motorola.com>
Date:   Tue May 7 12:00:16 2013 -0500

    trace: memkill: provide fixed zone info columns.
    
    Maintain kernel trace log to always have fixed number of columns.
    
    propagated_from: (CR)
    
    Signed-off-by: Yi-wei Zhao <gbjc64@motorola.com>
    Reviewed-on: http://gerrit.pcs.mot.com/537078
    SLT-Approved: Slta Waiver <sltawvr@motorola.com>
    Tested-by: Jira Key <jirakey@motorola.com>
    Reviewed-by: David Ding <dding@motorola.com>
    Reviewed-by: Dong-Dong Yang <gdb746@motorola.com>
    Reviewed-by: Jeff Thompson <jthomps1@motorola.com>
    Reviewed-by: Klocwork kwcheck <klocwork-kwcheck@sourceforge.mot.com>
    Reviewed-by: Check Patch <CHEKPACH@motorola.com>
    Submit-Approved: Jira Key <jirakey@motorola.com>
    SLT-Approved: Slta Waiver <sltawvr@motorola.com>
    Tested-by: Jira Key <jirakey@motorola.com>
    Reviewed-by: Klocwork kwcheck <klocwork-kwcheck@sourceforge.mot.com>
    Reviewed-by: Check Patch <CHEKPACH@motorola.com>
    Reviewed-by: Jason Hrycay <jason.hrycay@motorola.com>
    Submit-Approved: Jira Key <jirakey@motorola.com>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit be6d0a5443122508a08063f7f9d4dbe2aeb39039
Author: Yuanyuan Zhong <a6510c@motorola.com>
Date:   Thu Apr 11 16:43:49 2013 -0500

    trace: memkill: per-zone page state for LMK
    
    LMK considers memory status per zone. This change appends
    gfp mask and per-zone free/file page state to LMK kill tracing.
    A field <Node_id>:<Zone_id>:<Nr_free>:<Nr_page> is printed for
    each zone.
    
    Signed-off-by: Yuanyuan Zhong <a6510c@motorola.com>
    Signed-off-by: Jeffrey Carlyle <jeff.carlyle@motorola.com>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit abd4c306375c1aa6f25be8b2b04497024fc890c8
Author: Jeffrey Carlyle <jeff.carlyle@motorola.com>
Date:   Thu Sep 27 00:56:42 2012 -0500

    trace: allow memkill tracing to be selectively enabled
    
    Signed-off-by: Jeffrey Carlyle <jeff.carlyle@motorola.com>
    Reviewed-on: http://gerrit.pcs.mot.com/482673
    Reviewed-on: http://gerrit.pcs.mot.com/513906
    Tested-by: Jira Key <jirakey@motorola.com>
    Reviewed-by: Klocwork kwcheck <klocwork-kwcheck@sourceforge.mot.com>
    Reviewed-by: Thirumarai Selvi S <a16462@motorola.com>
    Reviewed-by: Check Patch <CHEKPACH@motorola.com>
    Submit-Approved: Jira Key <jirakey@motorola.com>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit a5794f71c375ad19446772ef4a89c76b6accd25e
Author: Liam Mark <lmark@codeaurora.org>
Date:   Tue Jun 3 13:33:18 2014 -0700

    mm, oom: make dump_tasks public
    
    Allow other functions to dump the list of tasks.
    Useful for when debugging memory leaks.
    
    Bug: 17871993
    Change-Id: I76c33a118a9765b4c2276e8c76de36399c78dbf6
    Signed-off-by: Liam Mark <lmark@codeaurora.org>
    Signed-off-by: Naveen Ramaraj <nramaraj@codeaurora.org>

commit 9053dbaadd4c0190be9ce9c8d4dda830926321fd
Author: Jeffrey Carlyle <jeff.carlyle@motorola.com>
Date:   Fri Aug 31 03:55:52 2012 -0500

    trace: memkill: trace LMK and OOM kills
    
    How to enable tracer:
    
      # mount -t debugfs nodev /sys/kernel/debug
      # echo 1 > /sys/kernel/debug/tracing/tracing_on
      # echo 1 > /sys/kernel/debug/tracing/events/memkill/enable
      # cat /sys/kernel/debug/tracing/trace
    
    Event format:
    
      oom_kill: <pid> <comm> <total_vm> <anon_rss> <file_rss> <oom_score_adj> <order> <points> <cached> <freeswap>
      oom_kill_shared: <pid> <comm>
      lmk_kill: <pid> <comm> <selected_oom_adj> <selected_tasksize> <min_adj> <cached> <freeswap>
    
    Signed-off-by: Jeffrey Carlyle <jeff.carlyle@motorola.com>
    Tested-by: Jira Key <jirakey@motorola.com>
    Reviewed-by: Jason Hrycay <jason.hrycay@motorola.com>
    Reviewed-by: Klocwork kwcheck <klocwork-kwcheck@sourceforge.mot.com>
    Reviewed-by: Thirumarai Selvi S <a16462@motorola.com>
    Reviewed-by: Check Patch <CHEKPACH@motorola.com>
    Reviewed-by: Christopher Fries <qcf001@motorola.com>
    Submit-Approved: Jira Key <jirakey@motorola.com>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 0282bc797ff1f881713a66e7f966eb18e866c50a
Author: Yi-wei Zhao <gbjc64@motorola.com>
Date:   Fri Dec 14 16:34:53 2012 -0600

    staging: android: lowmemorykiller: select a new task to kill
    
    Under certain circumstances, a process may take time to handle a SIGKILL.
    When lowmemkiller is called again shortly after, it would pick the same
    process to kill over and over, so that we cann't get free memory for long time.
    
    Solution is to check fatal_signal_pending() on the selected task, and if it's
    already pending, select a new task to kill.
    
    Signed-off-by: Tianshui Shi <kfp634@motorola.com>
    Tested-by: Jira Key <JIRAKEY@motorola.com>
    Reviewed-by: Yi-Wei Zhao <gbjc64@motorola.com>
    Reviewed-by: Jason Hrycay <jason.hrycay@motorola.com>
    Reviewed-by: Jeffrey Carlyle <jeff.carlyle@motorola.com>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 9af0e0240e216181d3ef51b7f8e291fcb38419f0
Author: Liam Mark <lmark@codeaurora.org>
Date:   Fri Aug 30 12:10:39 2013 -0700

    android/lowmemorykiller: Account for total_swapcache_pages
    
    The lowmemorykiller relies on NR_FILE_PAGES when measureing
    the amount of reclaimable memory in the system.
    However when swap is enabled swap cache pages are counted
    in NR_FILE_PAGES, and swap cache pages aren't as reclaimable
    in low memory as file cache pages. Therefore a large swap
    cache can result in the lowmemorykiller not running and
    an OOM occuring.
    
    In order to ensure the lowmemorykiller properly evaluates the
    amount of reclaimable memory don't count the swap cache pages.
    
    Signed-off-by: Liam Mark <lmark@codeaurora.org>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit be3c1533a210e5ce40e31506434b8d8a98e1cc7e
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun Aug 2 15:50:25 2015 +0200

     lowmemorykiller: enhance debug information
    
    Add extra debug information to make it easier to both determine
    why the lowmemorykiller killed a process and to help find the source
    of memory leaks.
    
    Also increase the debug level for "select" statements to help prevent
    flooding the log.
    
    Bug: 17871993
    Change-Id: I3b6876c5ecdf192ecc271aed3f37579f66d47a08
    Signed-off-by: Liam Mark <lmark@codeaurora.org>
    Signed-off-by: Naveen Ramaraj <nramaraj@codeaurora.org>

commit 2a242e836ff0da77b8d00494a4bc35b64c2accad
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun Aug 2 15:43:11 2015 +0200

     lowmemorykiller: make default lowmemorykiller debug message useful
    
    lowmemorykiller debug messages are inscrutable and mostly useful
    for debugging the lowmemorykiller, not explaining why a process
    was killed.  Make the messages more useful by prefixing them
    with "lowmemorykiller: " and explaining in more readable terms
    what was killed, who it was killed for, and why it was killed.
    
    The messages now look like:
    [   76.997631] lowmemorykiller: Killing 'droid.gallery3d' (2172), adj 1000,
    [   76.997635]    to free 27436kB on behalf of 'kswapd0' (29) because
    [   76.997638]    cache 122624kB is below limit 122880kB for oom_score_adj 1000
    [   76.997641]    Free memory is -53356kB above reserved
    
    A negative number for free memory above reserved means some of the
    reserved memory has been used and is being regenerated by kswapd,
    which is likely what called the shrinkers.
    
    Bug: 17871993
    Change-Id: I1fe983381e73e124b90aa5d91cb66e55eaca390f
    Signed-off-by: Colin Cross <ccross@android.com>
    Signed-off-by: Naveen Ramaraj <nramaraj@codeaurora.org>

commit 7cb7b7c0b6892351e07622200769f2fb0707bff3
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Mon Sep 14 22:02:22 2015 +0200

    add support for GPE

commit 97b6970388637c4c5742c8be0da28299628bbdfc
Author: Naveen Ramaraj <nramaraj@codeaurora.org>
Date:   Tue Sep 30 13:14:45 2014 -0700

    lowmemorykiller: Account for highmem during kswapd reclaim
    
    Currently most memory reclaim is done through kswapd.
    Since kswapd uses a gfp mask of GFP_KERNEL, and because
    the lowmemorykiller is zone aware, the lowmemorykiller will
    ignore highmem most of the time.
    This results in the lowmemorykiller being overly aggressive.
    
    The fix to this issue is to allow the lowmemorykiller to
    count highmem when being called by the kswapd if the lowmem
    watermarks are satisfied.
    
    Bug: 17626969
    Change-Id: I938644584f374763d10d429d835e74daa4854a38
    Signed-off-by: Liam Mark <lmark@codeaurora.org>
    Signed-off-by: Naveen Ramaraj <nramaraj@codeaurora.org>

commit 6adec0b4e8c69a78ae51cf6f7e47ef456315a3f1
Author: Liam Mark <lmark@codeaurora.org>
Date:   Wed Mar 27 12:34:51 2013 -0700

    android/lowmemorykiller: Selectively count free CMA pages
    
    In certain memory configurations there can be a large number of
    CMA pages which are not suitable to satisfy certain memory
    requests.
    
    This large number of unsuitable pages can cause the
    lowmemorykiller to not kill any tasks because the
    lowmemorykiller counts all free pages.
    In order to ensure the lowmemorykiller properly evaluates the
    free memory only count the free CMA pages if they are suitable
    for satisfying the memory request.
    
    Change-Id: I7f06d53e2d8cfe7439e5561fe6e5209ce73b1c90
    CRs-fixed: 437016
    Signed-off-by: Liam Mark <lmark@codeaurora.org>

commit 3f044d59f40af17075f1820c503915ca65eeca20
Author: seungho1.park <seungho1.park@lge.com>
Date:   Tue Jul 24 10:20:44 2012 +0900

    android: lowmemorykiller: add lmk parameters tunning code.
    
    There are cases that LMK doesn't run, even when it must run.
    It is due to LMK shrinker not considering memory status per zone.
    So add LMK parameters(other_free, other_file) tunnig code to
    consider target zone of LMK shrinker.
    
    Change-Id: I6f1f8660d5da920a0e3af45a160499965032081d
    Git-commit: 22d990a58fc17b3f0155e15eb2dc3efa037bea1c
    Git-repo: https://android.googlesource.com/kernel/common/
    [ohaugan@codeaurora.org: Fix compilation issues]
    Signed-off-by: Olav Haugan <ohaugan@codeaurora.org>

commit d7d3affce8fe52fd51bd878e4e995771d65c0eb2
Author: Liam Mark <lmark@codeaurora.org>
Date:   Thu Feb 7 14:31:36 2013 -0800

    android/lowmemorykiller: Ignore tasks with freed mm
    
    A killed task can stay in the task list long after its
    memory has been returned to the system, therefore
    ignore any tasks whose mm struct has been freed.
    
    Change-Id: I76394b203b4ab2312437c839976f0ecb7b6dde4e
    CRs-fixed: 450383
    Signed-off-by: Liam Mark <lmark@codeaurora.org>

commit 750b736cd84a173380ab486d250336e9b6ce1c9d
Author: Liam Mark <lmark@codeaurora.org>
Date:   Fri Jan 25 12:40:18 2013 -0800

    android/lowmemorykiller: Wait for memory to be freed
    
    The memory reclaim code needs to give time to the system to
    return the memory from a killed process otherwise the memory
    reclaim code could run continuously, in multiple threads,
    which could starve both the watchdog thread and the thread
    which is responsible for returning the memory from the
    killed process.
    
    Change-Id: Ieded4bfe038ca936247fa4b638070e979b02eaa1
    CRs-fixed: 447740
    Signed-off-by: Liam Mark <lmark@codeaurora.org>

commit 8c146112087f26b1b78fc003cee4cbc0243dee14
Author: Liam Mark <lmark@codeaurora.org>
Date:   Thu Sep 20 14:42:28 2012 -0700

    android/lowmemorykiller: Check all tasks for death pending
    
    The lowmemorykiller uses the TIF_MEMDIE flag to help ensure it doesn't
    kill another task until the memory from the previously killed task has
    been returned to the system.
    
    However the lowmemorykiller does not currently look at tasks who do not
    have a tasks->mm, but just because a process doesn't have a tasks->mm
    does not mean that the task's memory has been fully returned to the
    system yet.
    
    In order to prevent the lowmemorykiller from unnecessarily killing
    multiple applications in a row the lowmemorykiller has been changed to
    ensure that previous killed tasks are no longer in the process list
    before attempting to kill another task.
    
    Change-Id: I7d8a8fd39ca5625e6448ed2efebfb621f6e93845
    Signed-off-by: Liam Mark <lmark@codeaurora.org>

commit 3c47258f67d06392602ba8119e323a5f2f68fe9a
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun Aug 2 17:43:35 2015 +0200

    add missing endif, sorry :P

commit c14e028b4a9652d7539d13bbd158b0000063a2af
Author: alucard24 <dmbaoh2@gmail.com>
Date:   Sun Aug 2 15:09:20 2015 +0200

    alucard, darkness and nightmare govs: modified sampling time calculation!

commit 466c173d2f20ea51b67a64dc1118c1fe548a6319
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sat Aug 1 18:10:46 2015 +0200

    Add google play edition barcode

commit 0195ba4b56a33acfaf005f5b3d25c4dea7e8398d
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Mon Sep 14 21:57:49 2015 +0200

    update gitignore and defconfigs

commit 9a753fd4884a288e5626b0b8f721672d1a8476ac
Author: Michael Kerrisk <mtk.manpages@gmail.com>
Date:   Tue Jul 17 21:37:27 2012 +0200

    PM: Rename CAP_EPOLLWAKEUP to CAP_BLOCK_SUSPEND
    
    As discussed in
    http://thread.gmane.org/gmane.linux.kernel/1249726/focus=1288990,
    the capability introduced in 4d7e30d98939a0340022ccd49325a3d70f7e0238
    to govern EPOLLWAKEUP seems misnamed: this capability is about governing
    the ability to suspend the system, not using a particular API flag
    (EPOLLWAKEUP). We should make the name of the capability more general
    to encourage reuse in related cases. (Whether or not this capability
    should also be used to govern the use of /sys/power/wake_lock is a
    question that needs to be separately resolved.)
    
    This patch renames the capability to CAP_BLOCK_SUSPEND. In order to ensure
    that the old capability name doesn't make it out into the wild, could you
    please apply and push up the tree to ensure that it is incorporated
    for the 3.5 release.
    
    Signed-off-by: Michael Kerrisk <mtk.manpages@gmail.com>
    Acked-by: Serge Hallyn <serge.hallyn@canonical.com>
    Signed-off-by: Rafael J. Wysocki <rjw@sisk.pl>

commit 9ef347969b71d3443b98e3727d7e398b6c6de8c6
Author: Rafael J. Wysocki <rjw@sisk.pl>
Date:   Mon May 21 21:20:48 2012 +0200

    epoll: Fix user space breakage related to EPOLLWAKEUP
    
    Commit 4d7e30d (epoll: Add a flag, EPOLLWAKEUP, to prevent
    suspend while epoll events are ready) caused some applications to
    malfunction, because they set the bit corresponding to the new
    EPOLLWAKEUP flag in their eventpoll flags and they don't have the
    new CAP_EPOLLWAKEUP capability.
    
    To prevent that from happening, change epoll_ctl() to clear
    EPOLLWAKEUP in epds.events if the caller doesn't have the
    CAP_EPOLLWAKEUP capability instead of failing and returning an
    error code, which allows the affected applications to function
    normally.
    
    Reported-and-tested-by: Jiri Slaby <jslaby@suse.cz>
    Signed-off-by: Rafael J. Wysocki <rjw@sisk.pl>

commit 105c6c9f05e1cba532bed58714dec02754590fdd
Author: Arve Hjnnevg <arve@android.com>
Date:   Tue May 1 21:33:34 2012 +0200

    epoll: Add a flag, EPOLLWAKEUP, to prevent suspend while epoll events are ready
    
    When an epoll_event, that has the EPOLLWAKEUP flag set, is ready, a
    wakeup_source will be active to prevent suspend. This can be used to
    handle wakeup events from a driver that support poll, e.g. input, if
    that driver wakes up the waitqueue passed to epoll before allowing
    suspend.
    
    Signed-off-by: Arve Hjnnevg <arve@android.com>
    Reviewed-by: NeilBrown <neilb@suse.de>
    Signed-off-by: Rafael J. Wysocki <rjw@sisk.pl>

commit 231a33616693ef0d90b64ef8d16e81363410e3df
Author: Rafael J. Wysocki <rjw@sisk.pl>
Date:   Thu Jul 19 00:00:58 2012 +0200

    PM / Sleep: Require CAP_BLOCK_SUSPEND to use wake_lock/wake_unlock
    
    Require processes wanting to use the wake_lock/wake_unlock sysfs
    files to have the CAP_BLOCK_SUSPEND capability, which also is
    required for the eventpoll EPOLLWAKEUP flag to be effective, so that
    all interfaces related to blocking autosleep depend on the same
    capability.
    
    Signed-off-by: Rafael J. Wysocki <rjw@sisk.pl>
    Cc: stable@vger.kernel.org
    Acked-by: Michael Kerrisk <mtk.man-pages@gmail.com>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 2369c3f562304a2cc57cb4ed1dfe4ab34f2f86a4
Author: Jan Beulich <JBeulich@suse.com>
Date:   Mon Sep 10 12:05:18 2012 +0000

    properly __init-annotate pm_sysrq_init()
    
    This is used only as argument to subsys_initcall().
    
    Signed-off-by: Jan Beulich <jbeulich@suse.com>
    Signed-off-by: Rafael J. Wysocki <rjw@sisk.pl>

commit e51cf418c04147508cba242e98fdec6993911572
Author: zhangwei(Jovi) <jovi.zhangwei@huawei.com>
Date:   Tue Apr 30 15:28:52 2013 -0700

    power/sysrq: fix inconstistent help message of sysrq key
    
    Currently help message of /proc/sysrq-trigger highlight its
    upper-case characters, like below:
    
          SysRq : HELP : loglevel(0-9) reBoot Crash terminate-all-tasks(E)
          memory-full-oom-kill(F) kill-all-tasks(I) ...
    
    this would confuse user trigger sysrq by upper-case character, which is
    inconsistent with the real lower-case character registed key.
    
    This inconsistent help message will also lead more confused when
    26 upper-case letters put into use in future.
    
    This patch fix power off sysrq key: "poweroff(o)"
    
    Signed-off-by: zhangwei(Jovi) <jovi.zhangwei@huawei.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit c61d6d5e03f120f188dcb237f3a58c22caccfdaf
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Jul 18 18:15:46 2012 -0700

    Make wait_for_device_probe() also do scsi_complete_async_scans()
    
    Commit a7a20d103994 ("sd: limit the scope of the async probe domain")
    make the SCSI device probing run device discovery in it's own async
    domain.
    
    However, as a result, the partition detection was no longer synchronized
    by async_synchronize_full() (which, despite the name, only synchronizes
    the global async space, not all of them).  Which in turn meant that
    "wait_for_device_probe()" would not wait for the SCSI partitions to be
    parsed.
    
    And "wait_for_device_probe()" was what the boot time init code relied on
    for mounting the root filesystem.
    
    Now, most people never noticed this, because not only is it
    timing-dependent, but modern distributions all use initrd.  So the root
    filesystem isn't actually on a disk at all.  And then before they
    actually mount the final disk filesystem, they will have loaded the
    scsi-wait-scan module, which not only does the expected
    wait_for_device_probe(), but also does scsi_complete_async_scans().
    
    [ Side note: scsi_complete_async_scans() had also been partially broken,
      but that was fixed in commit 43a8d39d0137 ("fix async probe
      regression"), so that same commit a7a20d103994 had actually broken
      setups even if you used scsi-wait-scan explicitly ]
    
    Solve this problem by just moving the scsi_complete_async_scans() call
    into wait_for_device_probe().  Everybody who wants to wait for device
    probing to finish really wants the SCSI probing to complete, so there's
    no reason not to do this.
    
    So now "wait_for_device_probe()" really does what the name implies, and
    properly waits for device probing to finish.  This also removes the now
    unnecessary extra calls to scsi_complete_async_scans().
    
    Reported-and-tested-by: Artem S. Tashkinov <t.artem@mailcity.com>
    Cc: Dan Williams <dan.j.williams@gmail.com>
    Cc: Alan Stern <stern@rowland.harvard.edu>
    Cc: James Bottomley <jbottomley@parallels.com>
    Cc: Borislav Petkov <bp@amd64.org>
    Cc: linux-scsi <linux-scsi@vger.kernel.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    
    Conflicts:
    	drivers/scsi/scsi_wait_scan.c

commit bc33899d0e39719d329e5158fba224c2ee87c498
Author: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
Date:   Sat Jun 16 15:30:45 2012 +0200

    ftrace: Disable function tracing during suspend/resume and hibernation, again
    
    If function tracing is enabled for some of the low-level suspend/resume
    functions, it leads to triple fault during resume from suspend, ultimately
    ending up in a reboot instead of a resume (or a total refusal to come out
    of suspended state, on some machines).
    
    This issue was explained in more detail in commit f42ac38c59e0a03d (ftrace:
    disable tracing for suspend to ram). However, the changes made by that commit
    got reverted by commit cbe2f5a6e84eebb (tracing: allow tracing of
    suspend/resume & hibernation code again). So, unfortunately since things are
    not yet robust enough to allow tracing of low-level suspend/resume functions,
    suspend/resume is still broken when ftrace is enabled.
    
    So fix this by disabling function tracing during suspend/resume & hibernation.
    
    Signed-off-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Cc: stable@vger.kernel.org
    Signed-off-by: Rafael J. Wysocki <rjw@sisk.pl>
    
    Conflicts:
    	kernel/power/suspend.c

commit 9ded593d8fefba722afb996dcf9f2ebaf1258d28
Author: Bojan Smojver <bojan@rexursive.com>
Date:   Sat Jun 16 00:09:58 2012 +0200

    PM / Hibernate: Enable suspend to both for in-kernel hibernation.
    
    It is often useful to suspend to memory after hibernation image has been
    written to disk. If the battery runs out or power is otherwise lost, the
    computer will resume from the hibernated image. If not, it will resume
    from memory and hibernation image will be discarded.
    
    Signed-off-by: Bojan Smojver <bojan@rexursive.com>
    Signed-off-by: Rafael J. Wysocki <rjw@sisk.pl>

commit 7831dad37779315c33a6372b2cd921bfa3b0473c
Author: Minho Ban <mhban@samsung.com>
Date:   Mon May 14 21:45:31 2012 +0200

    PM / Hibernate: Use get_gendisk to verify partition if resume_file is integer format
    
    Sometimes resume= parameter comes in integer style (e.g. major:minor)
    and then name_to_dev_t can not detect partition properly. (especially
    async device like usb, mmc).
    
    This patch calls get_gendisk() if resumewait is true and resume_file
    is in integer format to work around this problem.
    
    Signed-off-by: Minho Ban <mhban@samsung.com>
    Signed-off-by: Rafael J. Wysocki <rjw@sisk.pl>

commit fe724b04a5c8f772c9b45d550363b50b1feb3ee6
Author: Jesse Barnes <jbarnes@virtuousgeek.org>
Date:   Mon Feb 4 13:37:20 2013 +0000

    PM: make VT switching to the suspend console optional v3
    
    KMS drivers can potentially restore the display configuration without
    userspace help.  Such drivers can can call a new funciton,
    pm_vt_switch_required(false) if they support this feature.  In that
    case, the PM layer won't VT switch to the suspend console at suspend
    time and then back to the original VT on resume, but rather leave things
    alone for a nicer looking suspend and resume sequence.
    
    v2: make a function so we can handle multiple drivers (Alan)
    v3: use a list to track device requests (Rafael)
    v4: Squash in build fix from Jesse for CONFIG_VT_CONSOLE_SLEEP=n
    v5: Squash in patch from Wu Fengguang to add a few missing static
    qualifiers.
    v6: Add missing EXPORT_SYMBOL.
    
    Signed-off-by: Jesse Barnes <jbarnes@virtuousgeek.org>
    Reviewed-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com> (v3)
    Signed-off-by: Daniel Vetter <daniel.vetter@ffwll.ch>

commit 8b5e652f23509826da656b72338251867cd90e6d
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sat Aug 1 15:53:48 2015 +0200

    kernel: add missing suspend time in makefile

commit 158094f1877aaf7ba40d5174d2f18528ffaababa
Author: Liu ShuoX <shuox.liu@intel.com>
Date:   Thu Jul 11 16:03:45 2013 +0800

    PM / Sleep: avoid 'autosleep' in shutdown progress
    
    commit e5248a111bf4048a9f3fab1a9c94c4630a10592a upstream.
    
    Prevent automatic system suspend from happening during system
    shutdown by making try_to_suspend() check system_state and return
    immediately if it is not SYSTEM_RUNNING.
    
    This prevents the following breakage from happening (scenario from
    Zhang Yanmin):
    
     Kernel starts shutdown and calls all device driver's shutdown
     callback.  When a driver's shutdown is called, the last wakelock is
     released and suspend-to-ram starts.  However, as some driver's shut
     down callbacks already shut down devices and disabled runtime pm,
     the suspend-to-ram calls driver's suspend callback without noticing
     that device is already off and causes crash.
    
    [rjw: Changelog]
    Signed-off-by: Liu ShuoX <shuox.liu@intel.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 6d1657b6a260374b9cdd318f484ce22b291765c8
Author: Iliyan Malchev <malchev@google.com>
Date:   Mon Feb 23 19:43:51 2015 -0800

    power: add partial-resume framework
    
    Partial resume refers to the concept of not waking up userspace when the kernel
    comes out of suspend for certain types of events that we wish to discard.  An
    example is a network packet that can be disacarded in the kernel, or spurious
    wakeup event that we wish to ignore.  Partial resume allows drivers to register
    callbacks, one one hand, and provides hooks into the PM's suspend/resume
    mechanism, on the other.
    
    When a device resumes from suspend, the core suspend/resume code invokes
    partialresume to check to see if the set of wakeup interrupts all have
    matching handlers. If this is not the case, the PM subsystem can continue to
    resume as before.  If all of the wakeup sources have matching handlers, then
    those are invoked in turn (and can block), and if all of them reach consensus
    that the reason for the wakeup can be ignored, they say so to the PM subsystem,
    which goes right back into suspend.  This latter support is implemented in a
    separate change.
    
    Signed-off-by: Iliyan Malchev <malchev@google.com>
    Change-Id: Id50940bb22a550b413412264508d259f7121d442
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 4bfa64af406b28f42b6b2cd1e8e172d1ca9a9b17
Author: Josh Triplett <josh@joshtriplett.org>
Date:   Sun Nov 18 21:27:55 2012 -0800

    drivers/base/core.c: Mark to_root_device static
    
    Nothing outside of drivers/base/core.c references this function.
    
    Signed-off-by: Josh Triplett <josh@joshtriplett.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 71e35cd81594b43a2461da3900141343c5abcebe
Author: ShuoX Liu <shuox.liu@intel.com>
Date:   Fri Nov 23 15:14:12 2012 +0800

    driver core: use initcall_debug to control shutdown info
    
    syscore_shutdown uses initcall_debug to control the debug info output.
    Its a good programming. But device_shutdown doesnt. The patch changes
    device_shutdown to follow the style.
    
    Signed-off-by: Yanmin Zhang <yanmin_zhang@linux.intel.com>
    Signed-off-by: ShuoX Liu <shuox.liu@intel.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit f09233ecca3ab4f71a098443f23075d52ec75252
Author: LongX Zhang <longx.zhang@intel.com>
Date:   Thu Oct 25 00:21:28 2012 +0200

    driver core / PM: move the calling to device_pm_remove behind the calling to bus_remove_device
    
    We hit an hang issue when removing a mmc device on Medfield Android phone by sysfs interface.
    
    device_pm_remove will call pm_runtime_remove which would disable
    runtime PM of the device. After that pm_runtime_get* or
    pm_runtime_put* will be ignored. So if we disable the runtime PM
    before device really be removed, drivers' _remove callback may
    access HW even pm_runtime_get* fails. That is bad.
    
    Consider below call sequence when removing a device:
    device_del => device_pm_remove
                 => class_intf->remove_dev(dev, class_intf)  => pm_runtime_get_sync/put_sync
                 => bus_remove_device => device_release_driver => pm_runtime_get_sync/put_sync
    
    remove_dev might call pm_runtime_get_sync/put_sync.
    Then, generic device_release_driver also calls pm_runtime_get_sync/put_sync.
    Since device_del => device_pm_remove firstly, later _get_sync wouldn't really wake up the device.
    
    I git log -p to find the patch which moves the calling to device_pm_remove ahead.
    It's below patch:
    
    commit  775b64d2b6ca37697de925f70799c710aab5849a
    Author: Rafael J. Wysocki <rjw@sisk.pl>
    Date:   Sat Jan 12 20:40:46 2008 +0100
    
         PM: Acquire device locks on suspend
    
         This patch reorganizes the way suspend and resume notifications are
         sent to drivers.  The major changes are that now the PM core acquires
         every device semaphore before calling the methods, and calls to
         device_add() during suspends will fail, while calls to device_del()
         during suspends will block.
    
         It also provides a way to safely remove a suspended device with the
         help of the PM core, by using the device_pm_schedule_removal() callback
         introduced specifically for this purpose, and updates two drivers (msr
         and cpuid) that need to use it.
    
    As device_pm_schedule_removal is deleted by another patch, we need also revert other parts of the patch,
    i.e. move the calling of device_pm_remove after the calling to bus_remove_device.
    
    Signed-off-by: LongX Zhang <longx.zhang@intel.com>
    Acked-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit f8bf96b44b307e973aa1a7a37d1f0f9ea4f86670
Author: Borislav Petkov <bp@alien8.de>
Date:   Tue Oct 9 19:52:05 2012 +0200

    drivers/base: Add a DEVICE_BOOL_ATTR macro
    
    ... which, analogous to DEVICE_INT_ATTR provides functionality to
    set/clear bools. Its purpose is to be used where values need to be used
    as booleans in configuration context.
    
    Next patch uses this.
    
    Signed-off-by: Borislav Petkov <bp@alien8.de>
    Acked-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Acked-by: Tony Luck <tony.luck@intel.com>

commit fd80707d22112c4b47b9ff72f0d4114d4e196b36
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Mon Sep 14 21:50:44 2015 +0200

    namei: fix compiling

commit 266f138b9419caa8b858358b2f501fc960c4efa3
Author: Aristeu Rozanski <aris@redhat.com>
Date:   Thu Aug 23 16:53:28 2012 -0400

    xattr: extract simple_xattr code from tmpfs
    
    Extract in-memory xattr APIs from tmpfs. Will be used by cgroup.
    
    $ size vmlinux.o
       text    data     bss     dec     hex filename
    4658782  880729 5195032 10734543         a3cbcf vmlinux.o
    $ size vmlinux.o
       text    data     bss     dec     hex filename
    4658957  880729 5195032 10734718         a3cc7e vmlinux.o
    
    v7:
    - checkpatch warnings fixed
    - Implement the changes requested by Hugh Dickins:
    	- make simple_xattrs_init and simple_xattrs_free inline
    	- get rid of locking and list reinitialization in simple_xattrs_free,
    	  they're not needed
    v6:
    - no changes
    v5:
    - no changes
    v4:
    - move simple_xattrs_free() to fs/xattr.c
    v3:
    - in kmem_xattrs_free(), reinitialize the list
    - use simple_xattr_* prefix
    - introduce simple_xattr_add() to prevent direct list usage
    
    Original-patch-by: Li Zefan <lizefan@huawei.com>
    Cc: Li Zefan <lizefan@huawei.com>
    Cc: Hillf Danton <dhillf@gmail.com>
    Cc: Lennart Poettering <lpoetter@redhat.com>
    Acked-by: Hugh Dickins <hughd@google.com>
    Signed-off-by: Li Zefan <lizefan@huawei.com>
    Signed-off-by: Aristeu Rozanski <aris@redhat.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>
    
    Conflicts:
    	mm/shmem.c

commit e66c8a8bde1b14bbbb53f7e83ef7d8781d87a35e
Author: David Howells <dhowells@redhat.com>
Date:   Tue Oct 2 19:24:29 2012 +0100

    KEYS: Make the session and process keyrings per-thread
    
    Make the session keyring per-thread rather than per-process, but still
    inherited from the parent thread to solve a problem with PAM and gdm.
    
    The problem is that join_session_keyring() will reject attempts to change the
    session keyring of a multithreaded program but gdm is now multithreaded before
    it gets to the point of starting PAM and running pam_keyinit to create the
    session keyring.  See:
    
    	https://bugs.freedesktop.org/show_bug.cgi?id=49211
    
    The reason that join_session_keyring() will only change the session keyring
    under a single-threaded environment is that it's hard to alter the other
    thread's credentials to effect the change in a multi-threaded program.  The
    problems are such as:
    
     (1) How to prevent two threads both running join_session_keyring() from
         racing.
    
     (2) Another thread's credentials may not be modified directly by this process.
    
     (3) The number of threads is uncertain whilst we're not holding the
         appropriate spinlock, making preallocation slightly tricky.
    
     (4) We could use TIF_NOTIFY_RESUME and key_replace_session_keyring() to get
         another thread to replace its keyring, but that means preallocating for
         each thread.
    
    A reasonable way around this is to make the session keyring per-thread rather
    than per-process and just document that if you want a common session keyring,
    you must get it before you spawn any threads - which is the current situation
    anyway.
    
    Whilst we're at it, we can the process keyring behave in the same way.  This
    means we can clean up some of the ickyness in the creds code.
    
    Basically, after this patch, the session, process and thread keyrings are about
    inheritance rules only and not about sharing changes of keyring.
    
    Reported-by: Mantas M. <grawity@gmail.com>
    Signed-off-by: David Howells <dhowells@redhat.com>
    Tested-by: Ray Strode <rstrode@redhat.com>
    
    Conflicts:
    	security/keys/keyctl.c

commit 2ec5bdd0b590bc192a9ae382562b237b3fdbd057
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Fri May 11 10:59:08 2012 +1000

    keys: change keyctl_session_to_parent() to use task_work_add()
    
    Change keyctl_session_to_parent() to use task_work_add() and move
    key_replace_session_keyring() logic into task_work->func().
    
    Note that we do task_work_cancel() before task_work_add() to ensure that
    only one work can be pending at any time.  This is important, we must not
    allow user-space to abuse the parent's ->task_works list.
    
    The callback, replace_session_keyring(), checks PF_EXITING.  I guess this
    is not really needed but looks better.
    
    As a side effect, this fixes the (unlikely) race.  The callers of
    key_replace_session_keyring() and keyctl_session_to_parent() lack the
    necessary barriers, the parent can miss the request.
    
    Now we can remove task_struct->replacement_session_keyring and related
    code.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Acked-by: David Howells <dhowells@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Richard Kuo <rkuo@codeaurora.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Alexander Gordeev <agordeev@redhat.com>
    Cc: Chris Zankel <chris@zankel.net>
    Cc: David Smith <dsmith@redhat.com>
    Cc: "Frank Ch. Eigler" <fche@redhat.com>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Larry Woodman <lwoodman@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

commit de6e3f6ad79f66f83734c6394fb7e2e4193c3ac6
Author: Alan Cox <alan@linux.intel.com>
Date:   Fri Sep 28 12:20:02 2012 +0100

    keys: Fix unreachable code
    
    We set ret to NULL then test it. Remove the bogus test
    
    Signed-off-by: Alan Cox <alan@linux.intel.com>
    Signed-off-by: David Howells <dhowells@redhat.com>

commit 100fe963b6468d90520fe5a86fde4b0db6ac7ad1
Author: Kent Yoder <key@linux.vnet.ibm.com>
Date:   Thu Jun 7 13:47:14 2012 -0500

    tpm: Move tpm_get_random api into the TPM device driver
    
    Move the tpm_get_random api from the trusted keys code into the TPM
    device driver itself so that other callers can make use of it. Also,
    change the api slightly so that the number of bytes read is returned in
    the call, since the TPM command can potentially return fewer bytes than
    requested.
    
    Acked-by: David Safford <safford@linux.vnet.ibm.com>
    Reviewed-by: H. Peter Anvin <hpa@linux.intel.com>
    Signed-off-by: Kent Yoder <key@linux.vnet.ibm.com>

commit 9b0499c605555cc569210fa1d5e66e84bfa00539
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Apr 14 10:06:31 2013 -0700

    Add file_ns_capable() helper function for open-time capability checking
    
    Nothing is using it yet, but this will allow us to delay the open-time
    checks to use time, without breaking the normal UNIX permission
    semantics where permissions are determined by the opener (and the file
    descriptor can then be passed to a different process, or the process can
    drop capabilities).
    
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit 8dc5abf6e0c52ff2021428caac9942e5fbb4ecd3
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Mon Nov 14 16:24:06 2011 -0800

    userns: Replace the hard to write inode_userns with inode_capable.
    
    This represents a change in strategy of how to handle user namespaces.
    Instead of tagging everything explicitly with a user namespace and bulking
    up all of the comparisons of uids and gids in the kernel,  all uids and gids
    in use will have a mapping to a flat kuid and kgid spaces respectively.  This
    allows much more of the existing logic to be preserved and in general
    allows for faster code.
    
    In this new and improved world we allow someone to utiliize capabilities
    over an inode if the inodes owner mapps into the capabilities holders user
    namespace and the user has capabilities in their user namespace.  Which
    is simple and efficient.
    
    Moving the fs uid comparisons to be comparisons in a flat kuid space
    follows in later patches, something that is only significant if you
    are using user namespaces.
    
    Acked-by: Serge Hallyn <serge.hallyn@canonical.com>
    Signed-off-by: Eric W. Biederman <ebiederm@xmission.com>

commit 9d42363654e4e66679f33e0b0a9c38bb43c3fd59
Author: Shaohua Li <shli@kernel.org>
Date:   Mon Oct 8 16:32:19 2012 -0700

    readahead: fault retry breaks mmap file read random detection
    
    .fault now can retry.  The retry can break state machine of .fault.  In
    filemap_fault, if page is miss, ra->mmap_miss is increased.  In the second
    try, since the page is in page cache now, ra->mmap_miss is decreased.  And
    these are done in one fault, so we can't detect random mmap file access.
    
    Add a new flag to indicate .fault is tried once.  In the second try, skip
    ra->mmap_miss decreasing.  The filemap_fault state machine is ok with it.
    
    I only tested x86, didn't test other archs, but looks the change for other
    archs is obvious, but who knows :)
    
    Signed-off-by: Shaohua Li <shaohua.li@fusionio.com>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Wu Fengguang <fengguang.wu@intel.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit 53f784f20be90adf4b833f915ada340669fdc52c
Author: Maxim Patlasov <mpatlasov@parallels.com>
Date:   Fri Oct 26 19:50:04 2012 +0400

    mm: minor cleanup of iov_iter_single_seg_count()
    
    The function does not modify iov_iter which 'i' points to.
    
    Signed-off-by: Maxim Patlasov <mpatlasov@parallels.com>
    Signed-off-by: Miklos Szeredi <mszeredi@suse.cz>

commit 6227081e899ea7fa289f461d25e0359b4b8cf215
Author: Boaz Harrosh <bharrosh@panasas.com>
Date:   Thu May 31 16:26:15 2012 -0700

    kmod: convert two call sites to call_usermodehelper_fns()
    
    Both kernel/sys.c && security/keys/request_key.c where inlining the exact
    same code as call_usermodehelper_fns(); So simply convert these sites to
    directly use call_usermodehelper_fns().
    
    Signed-off-by: Boaz Harrosh <bharrosh@panasas.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Tetsuo Handa <penguin-kernel@I-love.SAKURA.ne.jp>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit faa462a4f3cb744a8b5440c335d4b2fe877d7062
Author: David Howells <dhowells@redhat.com>
Date:   Mon May 21 12:32:13 2012 +0100

    KEYS: Fix some sparse warnings
    
    Fix some sparse warnings in the keyrings code:
    
     (1) compat_keyctl_instantiate_key_iov() should be static.
    
     (2) There were a couple of places where a pointer was being compared against
         integer 0 rather than NULL.
    
     (3) keyctl_instantiate_key_common() should not take a __user-labelled iovec
         pointer as the caller must have copied the iovec to kernel space.
    
     (4) __key_link_begin() takes and __key_link_end() releases
         keyring_serialise_link_sem under some circumstances and so this should be
         declared.
    
         Note that adding __acquires() and __releases() for this doesn't help cure
         the warnings messages - something only commenting out both helps.
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    Signed-off-by: James Morris <james.l.morris@oracle.com>

commit 35ee49696e1687919d5640fec4bf8d3b71f18f0c
Author: David Howells <dhowells@redhat.com>
Date:   Tue May 15 14:11:11 2012 +0100

    KEYS: Don't check for NULL key pointer in key_validate()
    
    Don't bother checking for NULL key pointer in key_validate() as all of the
    places that call it will crash anyway if the relevant key pointer is NULL by
    the time they call key_validate().  Therefore, the checking must be done prior
    to calling here.
    
    Whilst we're at it, simplify the key_validate() function a bit and mark its
    argument const.
    
    Reported-by: Dan Carpenter <dan.carpenter@oracle.com>
    Signed-off-by: David Howells <dhowells@redhat.com>
    cc: Dan Carpenter <dan.carpenter@oracle.com>
    Signed-off-by: James Morris <james.l.morris@oracle.com>

commit 629b49b17aa84b7be79321363933d7e80d1ba5c5
Author: David Howells <dhowells@redhat.com>
Date:   Fri May 11 10:56:56 2012 +0100

    KEYS: Announce key type (un)registration
    
    Announce the (un)registration of a key type in the core key code rather than
    in the callers.
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    Acked-by: Mimi Zohar <zohar@us.ibm.com>

commit 46bd0b14c5dce1df686b91dacb6a1a8685994abb
Author: David Howells <dhowells@redhat.com>
Date:   Fri May 11 10:56:56 2012 +0100

    KEYS: Reorganise keys Makefile
    
    Reorganise the keys directory Makefile to put all the core bits together and
    the type-specific bits after.
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    Acked-by: Mimi Zohar <zohar@us.ibm.com>

commit 7cc04eb379b2ecdc497118e7d82f5d2b93202c16
Author: David Howells <dhowells@redhat.com>
Date:   Fri May 11 10:56:56 2012 +0100

    KEYS: Move the key config into security/keys/Kconfig
    
    Move the key config into security/keys/Kconfig as there are going to be a lot
    of key-related options.
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    Acked-by: Mimi Zohar <zohar@us.ibm.com>

commit 435f5e73380542a53179907790fa4cb478e022fa
Author: David Howells <dhowells@redhat.com>
Date:   Fri May 11 10:56:56 2012 +0100

    KEYS: Do LRU discard in full keyrings
    
    Do an LRU discard in keyrings that are full rather than returning ENFILE.  To
    perform this, a time_t is added to the key struct and updated by the creation
    of a link to a key and by a key being found as the result of a search.  At the
    completion of a successful search, the keyrings in the path between the root of
    the search and the first found link to it also have their last-used times
    updated.
    
    Note that discarding a link to a key from a keyring does not necessarily
    destroy the key as there may be references held by other places.
    
    An alternate discard method that might suffice is to perform FIFO discard from
    the keyring, using the spare 2-byte hole in the keylist header as the index of
    the next link to be discarded.
    
    This is useful when using a keyring as a cache for DNS results or foreign
    filesystem IDs.
    
    This can be tested by the following.  As root do:
    
    	echo 1000 >/proc/sys/kernel/keys/root_maxkeys
    
    	kr=`keyctl newring foo @s`
    	for ((i=0; i<2000; i++)); do keyctl add user a$i a $kr; done
    
    Without this patch ENFILE should be reported when the keyring fills up.  With
    this patch, the keyring discards keys in an LRU fashion.  Note that the stored
    LRU time has a granularity of 1s.
    
    After doing this, /proc/key-users can be observed and should show that most of
    the 2000 keys have been discarded:
    
    	[root@andromeda ~]# cat /proc/key-users
    	    0:   517 516/516 513/1000 5249/20000
    
    The "513/1000" here is the number of quota-accounted keys present for this user
    out of the maximum permitted.
    
    In /proc/keys, the keyring shows the number of keys it has and the number of
    slots it has allocated:
    
    	[root@andromeda ~]# grep foo /proc/keys
    	200c64c4 I--Q--     1 perm 3b3f0000     0     0 keyring   foo: 509/509
    
    The maximum is (PAGE_SIZE - header) / key pointer size.  That's typically 509
    on a 64-bit system and 1020 on a 32-bit system.
    
    Signed-off-by: David Howells <dhowells@redhat.com>

commit e2f7c31246e09ef79cbc0b6dc289d35ad30722f7
Author: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
Date:   Wed Feb 6 15:59:18 2013 -0800

    TTY: mark tty_get_device call with the proper const values
    
    Micha's previous patch missed this tty check to fix up the
    class_find_device() arguments.
    
    Reported-by: kbuild test robot <fengguang.wu@intel.com>
    Cc: Micha Mirosaw <mirq-linux@rere.qmqm.pl>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 26908bd656bc5673184e50797ca7dbc660c34351
Author: Anton Vorontsov <anton.vorontsov@linaro.org>
Date:   Tue Jul 31 04:39:30 2012 -0700

    thermal: Constify 'type' argument for the registration routine
    
    thermal_zone_device_register() does not modify 'type' argument, so it is
    safe to declare it as const. Otherwise, if we pass a const string, we are
    getting the ugly warning:
    
    CC drivers/power/power_supply_core.o
    drivers/power/power_supply_core.c: In function 'psy_register_thermal':
    drivers/power/power_supply_core.c:204:6: warning: passing argument 1 of 'thermal_zone_device_register' discards 'const' qualifier from pointer target type [enabled by default]
    include/linux/thermal.h:140:29: note: expected 'char *' but argument is of type 'const char *'
    
    Signed-off-by: Anton Vorontsov <anton.vorontsov@linaro.org>
    Acked-by: Jean Delvare <khali@linux-fr.org>

commit cc59eb7b7bf88bf0446b28fb2715bcdc31151df8
Author: Stephen Warren <swarren@nvidia.com>
Date:   Thu Nov 8 16:12:28 2012 -0800

    block: partition: msdos: provide UUIDs for partitions
    
    The MSDOS/MBR partition table includes a 32-bit unique ID, often referred
    to as the NT disk signature.  When combined with a partition number within
    the table, this can form a unique ID similar in concept to EFI/GPT's
    partition UUID.  Constructing and recording this value in struct
    partition_meta_info allows MSDOS partitions to be referred to on the
    kernel command-line using the following syntax:
    
    root=PARTUUID=0002dd75-01
    
    Signed-off-by: Stephen Warren <swarren@nvidia.com>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Will Drewry <wad@chromium.org>
    Cc: Kay Sievers <kay.sievers@vrfy.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

commit f30cecf8e962ac89e29697f27d065a3acefcbf5e
Author: Stephen Warren <swarren@nvidia.com>
Date:   Thu Nov 8 16:12:27 2012 -0800

    init: reduce PARTUUID min length to 1 from 36
    
    Reduce the minimum length for a root=PARTUUID= parameter to be considered
    valid from 36 to 1.  EFI/GPT partition UUIDs are always exactly 36
    characters long, hence the previous limit.  However, the next patch will
    support DOS/MBR UUIDs too, which have a different, shorter, format.
    Instead of validating any particular length, just ensure that at least
    some non-empty value was given by the user.
    
    Also, consider a missing UUID value to be a parsing error, in the same
    vein as if /PARTNROFF exists and can't be parsed.  As such, make both
    error cases print a message and disable rootwait.  Convert to pr_err while
    we're at it.
    
    Signed-off-by: Stephen Warren <swarren@nvidia.com>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Will Drewry <wad@chromium.org>
    Cc: Kay Sievers <kay.sievers@vrfy.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

commit 7b0c3ad7b4b0ea125e7a9d3bbb1e70f3ad7a903b
Author: Stephen Warren <swarren@nvidia.com>
Date:   Thu Nov 8 16:12:25 2012 -0800

    block: store partition_meta_info.uuid as a string
    
    This will allow other types of UUID to be stored here, aside from true
    UUIDs.  This also simplifies code that uses this field, since it's usually
    constructed from a, used as a, or compared to other, strings.
    
    Note: A simplistic approach here would be to set uuid_str[36]=0 whenever a
    /PARTNROFF option was found to be present.  However, this modifies the
    input string, and causes subsequent calls to devt_from_partuuid() not to
    see the /PARTNROFF option, which causes different results.  In order to
    avoid misleading future maintainers, this parameter is marked const.
    
    Signed-off-by: Stephen Warren <swarren@nvidia.com>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Will Drewry <wad@chromium.org>
    Cc: Kay Sievers <kay.sievers@vrfy.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>
    
    Conflicts:
    	init/do_mounts.c

commit 4ce6f6b26b32d73467e72b9e55a7c49336271358
Author: Jeff Layton <jlayton@redhat.com>
Date:   Wed Oct 10 15:25:26 2012 -0400

    vfs: allocate page instead of names_cache buffer in mount_block_root
    
    First, it's incorrect to call putname() after __getname_gfp() since the
    bare __getname_gfp() call skips the auditing code, while putname()
    doesn't.
    
    mount_block_root allocates a PATH_MAX buffer via __getname_gfp, and then
    calls get_fs_names to fill the buffer. That function can call
    get_filesystem_list which assumes that that buffer is a full page in
    size. On arches where PAGE_SIZE != 4k, then this could potentially
    overrun.
    
    In practice, it's hard to imagine the list of filesystem names even
    approaching 4k, but it's best to be safe. Just allocate a page for this
    purpose instead.
    
    With this, we can also remove the __getname_gfp() definition since there
    are no more callers.
    
    Signed-off-by: Jeff Layton <jlayton@redhat.com>
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

commit 31b88df22333cddc0b745dd95e024e8e37b68b0f
Author: Micha Mirosaw <mirq-linux@rere.qmqm.pl>
Date:   Fri Feb 1 20:40:17 2013 +0100

    driver-core: constify data for class_find_device()
    
    All in-kernel users of class_find_device() don't really need mutable
    data for match callback.
    
    In two places (kernel/power/suspend_test.c, drivers/scsi/osd/osd_uld.c)
    this patch changes match callbacks to use const search data.
    
    The const is propagated to rtc_class_open() and power_supply_get_by_name()
    parameters.
    
    Note that there's a dev reference leak in suspend_test.c that's not
    touched in this patch.
    
    Signed-off-by: Micha Mirosaw <mirq-linux@rere.qmqm.pl>
    Acked-by: Grant Likely <grant.likely@secretlab.ca>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    
    Conflicts:
    	init/do_mounts.c

commit 0baa1c7bfbc3fa17753bc5bbfaa0b3a7fa43f6fe
Author: Ramakrishna Pallala <ramakrishna.pallala@intel.com>
Date:   Fri Nov 30 13:57:46 2012 +0530

    power_supply: Add watchdog and safety timer expiries under PROP_HEALTH_*
    
    As most of the charger chips come with two kinds of safety features
    related to timing:
    
    1. Watchdog Timer (interms of seconds/mins)
    2. Safety Timer (interms of hours)
    
    This patch adds these to fault causes in POWER_SUPPLY_PROP_HEALTH_* enums
    so that whenever there is either watchdog timeout or safety timer timeout
    driver could notify the user space accurately about the fault and will
    also be helpful for debug.
    
    Signed-off-by: Ramakrishna Pallala <ramakrishna.pallala@intel.com>
    Signed-off-by: Anton Vorontsov <anton@enomsg.org>

commit c3619b5465f99bc2d9cd7472319b67d65bfecabf
Author: Ramakrishna Pallala <ramakrishna.pallala@intel.com>
Date:   Tue Oct 9 22:25:59 2012 +0530

    power_supply: Register power supply for thermal cooling device
    
    This patch registers the power supply as a cooling device if the power
    supply has support for charge throttling.
    
    Now with this change low level drivers need not register with thermal
    framework as it is automatically done by power supply framework.
    
    Signed-off-by: Ramakrishna Pallala <ramakrishna.pallala@intel.com>
    Signed-off-by: Anton Vorontsov <anton.vorontsov@linaro.org>

commit 7fe5a95edf717350b4c74667167c3c88d145994f
Author: Ramakrishna Pallala <ramakrishna.pallala@intel.com>
Date:   Tue Oct 9 22:25:29 2012 +0530

    power_supply: Add support for CHARGE_CONTROL_* attributes
    
    Add support for power supply attributes CHARGE_CONTROL_LIMIT
    and CHARGE_CONTROL_LIMIT_MAX.
    
    These new attributes will enable the user space to implement
    custom charging algorithms based on platform state.
    
    Signed-off-by: Ramakrishna Pallala <ramakrishna.pallala@intel.com>
    Signed-off-by: Anton Vorontsov <anton.vorontsov@linaro.org>

commit dc8a1ef5e604956004c1a67338c85031a8d7136b
Author: Ramakrishna Pallala <ramakrishna.pallala@intel.com>
Date:   Mon Jul 30 12:49:21 2012 +0530

    power_supply: Add new power supply properties CHARGE_CURRENT/VOLTAGE_MAX
    
    There are different types of chargers avalibale like AC, Solar, USB,
    etc..  Even in USB we have different types SDP/DCP/CDP/ACA and all these
    chargers have different o/p ratings. For example SDP supports only 500mA
    of charge current whereas AC charger can support upto 8A or more.
    
    Similarly batteries also come with charge current and voltage ratings
    and these ratings vary depending on its capacity and the technology
    used.
    
    This patch adds two new power supply properties
    CONSTANT_CHARGE_CURRENT_MAX and CONSTANT_CHARGE_CURRENT_MAX.
    
    Signed-off-by: Ramakrishna Pallala <ramakrishna.pallala@intel.com>
    Signed-off-by: Anton Vorontsov <anton.vorontsov@linaro.org>

commit 07f8cbe313db526c65a1f1427af6f8fdd81a2627
Author: Ramakrishna Pallala <ramakrishna.pallala@intel.com>
Date:   Thu Aug 23 06:50:21 2012 +0530

    power_supply: Add new power supply AUTHENTIC property
    
    It is possible that users can use non-standard chargers or use invalid
    batteries especially with mobile devices.
    
    This patch adds a new power supply property called 'AUTHENTIC' to
    indicate this to the user(user space).
    
    Signed-off-by: Ramakrishna Pallala <ramakrishna.pallala@intel.com>
    Signed-off-by: Anton Vorontsov <anton.vorontsov@linaro.org>

commit b38fd704334fef07ee227fcf9496e3b34427aa71
Author: Ramakrishna Pallala <ramakrishna.pallala@intel.com>
Date:   Thu Jul 5 16:59:12 2012 +0530

    power_supply: Add min/max alert properties for CAPACITY, TEMP, TEMP_AMBIENT
    
    Minimum and maximum alerts on power supply properties will help or allow
    the user space to "proactively" create policies like connect/disconnect
    charger or stop/start the user apps based on capacity or temperature
    parameters.
    
    These parameters can be used to avoid unnecessary polling from user space
    and even from kernel space if the underlying HW can support INT triggers
    (ex: max17042/47).
    
    This patch adds the following power supply alert type properties:
    
     CAPACITY_ALERT_MIN
     CAPACITY_ALERT_MAX
     TEMP_ALERT_MIN
     TEMP_ALERT_MAX
     TEMP_AMBIENT_ALERT_MIN
     TEMP_AMBIENT_ALERT_MAX
    
    Signed-off-by: Ramakrishna Pallala <ramakrishna.pallala@intel.com>
    Signed-off-by: Anton Vorontsov <anton.vorontsov@linaro.org>

commit 283a42a60e0c1f567a73de8ffae94fcfdcf4c174
Author: Ramakrishna Pallala <ramakrishna.pallala@intel.com>
Date:   Sun May 6 18:16:44 2012 +0530

    power_supply: Add constant charge_current and charge_voltage properties
    
    Constant Charge Current(CC) is charging parameter which limit the
    maximum current which can be pumped into the battery during charge cycle.
    
    Constant Charge Voltage(CV) is also charging parameter which limit the
    maximum voltage that battery can reach during charge cycle.
    
    It is very common practice that at low or high temperatures we
    do not charge the batteries upto it's fullest charge voltage
    to avoid battery and user safety issues.
    
    These sysfs properties will be useful for debug and to implement
    certain user space policies like "Charging limited due to OverTemp".
    
    Signed-off-by: Ramakrishna Pallala <ramakrishna.pallala@intel.com>
    Signed-off-by: Anton Vorontsov <cbouatmailru@gmail.com>

commit df70acf1d9eb7c9c27969d054ab366ff6b2cd907
Author: Jenny TC <jenny.tc@intel.com>
Date:   Wed May 9 20:36:47 2012 +0530

    power_supply: Register battery as a thermal zone
    
    Battery and charger contribute to Thermals in most of the embedded
    devices. So, it makes sense to identify them as Thermal zones in a
    particular platform.
    
    This patch registers a thermal zone if the power supply is reporting
    a temperature property. The thermal zone will be used by platform's
    thermal management solution.
    
    Signed-off-by: Jenny TC <jenny.tc@intel.com>
    Signed-off-by: Anton Vorontsov <cbouatmailru@gmail.com>

commit cf33d53781ab2d0c0617e0c56787e1b0ba4905fb
Author: Anton Vorontsov <cbouatmailru@gmail.com>
Date:   Fri May 4 21:06:19 2012 -0700

    power_supply: Make the core a boolean instead of a tristate
    
    On Mon, Apr 02, 2012 at 01:53:23PM +1000, Benjamin Herrenschmidt wrote:
    > > drivers/built-in.o: In function `.nouveau_pm_trigger':
    > > (.text+0xa56e8): undefined reference to `.power_supply_is_system_supplied'
    > >
    > > nouveau probably needs to depends on CONFIG_POWER_SUPPLY to force a module
    > > build with the latter is =m
    >
    > Ok, not that trivial...
    >
    > The problem is more like POWER_SUPPLY should be a bool, not a tristate.
    >
    > If you think about it: you don't want things like nouveau to depend on a
    > random subsystem like that, people will never get it. In fact,
    > POWER_SUPPLY provides empty inline stubs when not enabled, so that's
    > really designed to not have depends...
    >
    > However that -cannot- work if POWER_SUPPLY is modular and the drivers
    > who use it are not.
    >
    > The only fixes here that make sense I can think of
    > that don't also involve Kconfig horrors are:
    >
    >  - Ugly: in power_supply.h, use the extern variant if
    >
    >       defined(CONFIG_POWER_SUPPLY) ||
    >        (defined(CONFIG_POWER_SUPPLY_MODULE) && defined(MODULE))
    >
    > IE. use the stub if power supply is a module and what is being built is
    > built-in. Of course that's not only ugly, it somewhat sucks from a user
    > perspective as the subsystem now exists but can't be used by some
    > drivers...
    >
    >  - Better: Just make the bloody thing a bool :-) The power supply
    > framework itself is small enough, just make it a boolean option and
    > avoid the problem entirely. The actual power supply sub drivers can
    > remain modular of course.
    
    Suggested-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Anton Vorontsov <cbouatmailru@gmail.com>

commit 06f4dad22f793eaae9a8f015bae4df5a6b2ac5b1
Author: Ramakrishna Pallala <ramakrishna.pallala@intel.com>
Date:   Tue Apr 10 16:21:20 2012 +0530

    power_supply: Add voltage_ocv property and use it for max17042 driver
    
    This adds a new sysfs file called 'voltage_ocv' which gives the
    Open Circuit Voltage of the battery.
    
    This property can be used for platform shutdown policies and
    can be useful for initial capacity estimations.
    
    Note: This patch is generated against linux-next branch.
    
    Signed-off-by: Ramakrishna Pallala <ramakrishna.pallala@intel.com>
    Signed-off-by: Anton Vorontsov <anton.vorontsov@linaro.org>

commit 5b5c7de0d43b7a997e18c295567ad64774e4c85f
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Jul 30 18:22:00 2015 +0200

    tick-sched: fix compiling errors

commit 7c11e8c30473c49f8a0077d8727b132e97c7727e
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Wed Jan 16 18:02:04 2013 +0100

    cputime: Move default nsecs_to_cputime() to jiffies based cputime file
    
    If the architecture doesn't provide an implementation of
    nsecs_to_cputime(), the cputime accounting core uses a
    default one that converts the nanoseconds to jiffies. However
    this only makes sense if we use the jiffies based cputime.
    
    For now it doesn't matter much because this API is only
    called on code that uses jiffies based cputime accounting.
    
    But the code may evolve and this API may be used more
    broadly in the future. Keeping this default implementation
    around is very error prone as it may introduce a bug and
    hide it on architectures that don't override this API.
    
    Fix this by moving this definition to the jiffies based
    cputime headers as it is the only place where it belongs to.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Li Zhong <zhong@linux.vnet.ibm.com>
    Cc: Namhyung Kim <namhyung.kim@lge.com>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Paul Gortmaker <paul.gortmaker@windriver.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>

commit 035a9f7de96fe5e788863134f8c5d962768f819c
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Tue Nov 13 14:20:55 2012 +0100

    cputime: Use accessors to read task cputime stats
    
    This is in preparation for the full dynticks feature. While
    remotely reading the cputime of a task running in a full
    dynticks CPU, we'll need to do some extra-computation. This
    way we can account the time it spent tickless in userspace
    since its last cputime snapshot.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Li Zhong <zhong@linux.vnet.ibm.com>
    Cc: Namhyung Kim <namhyung.kim@lge.com>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Paul Gortmaker <paul.gortmaker@windriver.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>

commit 03d44a41c1fec4bc509fffe197aa1bc842207145
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Tue Jan 15 13:25:07 2013 +0100

    cputime: Librarize per nsecs resolution cputime definitions
    
    The full dynticks cputime accounting that we'll soon introduce
    will rely on sched_clock(). And its clock can have a per
    nanosecond granularity.
    
    To prepare for this, we need to have a cputime_t implementation
    that has this precision.
    
    ia64 virtual cputime accounting already uses that granularity
    so all we need is to librarize its implementation in the asm
    generic headers.
    
    Also librarize the default per jiffy granularity cputime_t
    as well so that we can easily pick either implementation
    depending on the cputime accounting config we choose.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Li Zhong <zhong@linux.vnet.ibm.com>
    Cc: Namhyung Kim <namhyung.kim@lge.com>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Paul Gortmaker <paul.gortmaker@windriver.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>

commit 6153ee160e0ee36c318725de9d9ec4c11a434c51
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Wed Nov 28 17:00:57 2012 +0100

    cputime: Comment cputime's adjusting code
    
    The reason for the scaling and monotonicity correction performed
    by cputime_adjust() may not be immediately clear to the reviewer.
    
    Add some comments to explain what happens there.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Paul Gortmaker <paul.gortmaker@windriver.com>

commit d1d87dfd454beef7473b9eaa0ce3f73898f3fdeb
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Thu Nov 22 00:58:35 2012 +0100

    cputime: Consolidate cputime adjustment code
    
    task_cputime_adjusted() and thread_group_cputime_adjusted()
    essentially share the same code. They just don't use the same
    source:
    
    * The first function uses the cputime in the task struct and the
    previous adjusted snapshot that ensures monotonicity.
    
    * The second adds the cputime of all tasks in the group and the
    previous adjusted snapshot of the whole group from the signal
    structure.
    
    Just consolidate the common code that does the adjustment. These
    functions just need to fetch the values from the appropriate
    source.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Paul Gortmaker <paul.gortmaker@windriver.com>

commit fc55deb348e370ddbbcbd7bc1c9395f3a1697dbe
Author: Nick Kossifidis <mickflemm@gmail.com>
Date:   Sun Dec 16 22:18:11 2012 -0500

    random: Mix cputime from each thread that exits to the pool
    
    When a thread exits mix it's cputime (userspace + kernelspace) to the entropy pool.
    
    We don't know how "random" this is, so we use add_device_randomness that doesn't mess
    with entropy count.
    
    Signed-off-by: Nick Kossifidis <mickflemm@gmail.com>
    Signed-off-by: Theodore Ts'o <tytso@mit.edu>

commit b48323ed09bdba2727b8a611518b5cd4dad6f3dd
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Jul 30 18:11:10 2015 +0200

    Revert "time: Convert CONFIG_GENERIC_TIME_VSYSCALL to CONFIG_GENERIC_TIME_VSYSCALL_OLD"
    
    This reverts commit 424a9223d106bef46e77fe777d28f0b784997d30.

commit 40e32a60b4590cb6362127314bec431ff706c910
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Jul 30 18:09:47 2015 +0200

    Revert "time: Introduce new GENERIC_TIME_VSYSCALL"
    
    This reverts commit e6527072c83c68fa2dc6ff9140723569df06b085.

commit 48d2c570534b20dfd4b02a7f05554be25fb3aab6
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Jul 30 18:03:56 2015 +0200

    Revert "time: Only do nanosecond rounding on GENERIC_TIME_VSYSCALL_OLD systems" This reverts commit d9cc60715200143c7230b340e41a4daf26fce068.

commit b59826fe7793f9972865d03f8e4433c1a0fac3c3
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Jul 30 18:03:33 2015 +0200

    Revert "time: Fix 1ns/tick drift w/ GENERIC_TIME_VSYSCALL_OLD"
    
    This reverts commit ddc6a0ddd9aa7c64ded4b66c181cd92fd4755579.

commit db2f700525d5da352da6ab7e0599cab3f80e9d41
Author: Brandon Berhent <bbedward@gmail.com>
Date:   Tue May 12 11:01:46 2015 -0400

    msm: zen_decision: Don't set default settings in zd_probe

commit 2aa4111776adfb66a07dcac05a61c0cbabb3e325
Author: Brandon Berhent <bbedward@gmail.com>
Date:   Wed May 6 17:49:17 2015 -0400

    msm: zen_decision: I'm a dummy - Correctly allow bat_threshold_ignore to be changed in the userspace...

commit 9a27d22b30708359ace287792f06d4328ac532c8
Author: Brandon Berhent <bbedward@gmail.com>
Date:   Wed May 6 12:40:33 2015 -0400

    msm: zen_decision: Missed f(x) name change

commit 5e0a2728d373fed25fdf81d6fa11d4d6999016c8
Author: Brandon Berhent <bbedward@gmail.com>
Date:   Wed May 6 11:51:36 2015 -0400

    msm: zen_decision: Only call INIT_DELAYED_WORK once - Only call it once, not every time we want to run a thread - Rename some functions for clarity

commit 245269f6d2b1ecac9e9d1060aee7a993d82d4951
Author: Brandon Berhent <bbedward@gmail.com>
Date:   Wed May 6 00:25:35 2015 -0400

    msm: zen_decision: change queue_delayed_work_on to queue_delayed_work - Not necessary to explicitly queue work on CPU0 anymore

commit 949c3f123cf75a5bad1d47f0d50c6f9a4ad95c33
Author: Brandon Berhent <bbedward@gmail.com>
Date:   Tue May 5 21:28:16 2015 -0400

    msm: zen_decision: Use WQ_UNBOUND and max_active = 1 when allocating WQ - Set WQ_UNBOUND flag on alloc_workqueue which gives the CPU scheduler freedom to move it to other CPUs - Pass 1 as max_active, ensure only 1 task is running at a time from the WQ

commit baf0b07a1dafaa73434290faad543d7e24f4a3df
Author: Brandon Berhent <bbedward@gmail.com>
Date:   Tue May 5 14:53:00 2015 -0400

    msm: zen_decision: remove unnecessary logic - The iteration over offline CPUs doesn't need an explicit check for CPU0 because CPU0 will never be part of the iteration.

commit 22fcd12650ddb25d9148da2f01afe7552b920d19
Author: Brandon Berhent <bbedward@gmail.com>
Date:   Mon May 4 18:17:07 2015 -0400

    msm_zen_decision: some cleanups - Cleanup some commnets and Kconfig description

commit 5dc2ea28824d5f41d878ab70e5eeb1bd76dab028
Author: Brandon Berhent <bbedward@gmail.com>
Date:   Mon May 4 17:45:37 2015 -0400

    msm: zen_decision v2.0 - Different functionality than v1.0, the following is the complete functionality: - On screen ON, queue worker thread to bring all CPUs online after wake_wait_time - If current "battery" device level is <= bat_threshold_ignore, don't do any CPU_UP work to account for MSMThermal default activity
    
    Tunables in 2.0 (/sys/kernel/zen_decision):
    - enabled (0 or 1): enable or disable the driver
    - wake_wait_time (0-60000): How long to wait before executing CPU_UP work
    - bat_thresold_ignore (0-100, 0=disabled): Don't do any CPU_UP work up to this battery charge level (e.g. if set at 15, and battery level is 15% or lower then don't attempt to bring CPUs online when screen comes on)

commit 978357c2fe760fb0013815687d08f61f7385f870
Author: Brandon Berhent <bbedward@gmail.com>
Date:   Mon Apr 27 22:04:47 2015 -0400

    msm_zen_decision: disable by default - What this does now with screen off/on doesn't really have a point. I thought MPDec was messing with this before. - Leaving for future expansion

commit c18c86f2b47d6bf0e6eaa1221ca194916892997e
Author: Brandon Berhent <bbedward@gmail.com>
Date:   Mon Apr 27 21:03:16 2015 -0400

    msm_zen_decision: use per-cpu mutex to synchronize hotplug operations

commit 4b94764b2c268ec830cc4960b6b11b9d4bd9f760
Author: Brandon Berhent <bbedward@gmail.com>
Date:   Mon Apr 27 17:48:51 2015 -0400

    msm_zen_decision: General fixes and cleanup - Actually use "enabled" to disable the driver - Actually allow suspend_wait_time to change > 0 - Restrict suspend_wait_time to a maximum of 60s

commit bc8ce4dcea122d007bc7a795d1aed426a129709e
Author: Brandon Berhent <bbedward@gmail.com>
Date:   Mon Apr 27 16:14:29 2015 -0400

    msm: add msm_zen_decision
    This adds a simple driver intended to allow the MPDecision service to be disabled.
    The driver is easily extendable, but currently it:
    - Can be enabled/disabled in /sys/kernel/zen_decision/enabled
    - On screen power OFF it removes all cpu cores after suspend_wait_time
    - On screen power ON it enables all cpu cores.
    - /sys/kernel/zen_decision/suspend_wait_time (ms) refers to how long we wait after screen is off to remove the cores (default 5s). This just to avoid removing cores in excess.

commit 61e17b703780aa59c5af68b68bd1eee2bbf2466d
Author: Feng Tang <feng.tang@intel.com>
Date:   Tue Mar 12 11:56:46 2013 +0800

    clocksource: Add new feature flag CLOCK_SOURCE_SUSPEND_NONSTOP
    
    Some x86 processors have a TSC clocksource, which continues to run
    even when system is suspended. Also most OMAP platforms have a
    32 KHz timer which has similar capability. Add a feature flag so that
    it could be utilized.
    
    Signed-off-by: Feng Tang <feng.tang@intel.com>
    Signed-off-by: John Stultz <john.stultz@linaro.org>

commit 6162eef1286f85c076f4d7bb8a10980150f41336
Author: John Stultz <john.stultz@linaro.org>
Date:   Tue Dec 10 17:18:18 2013 -0800

    timekeeping: Avoid possible deadlock from clock_was_set_delayed
    
    commit 6fdda9a9c5db367130cf32df5d6618d08b89f46a upstream.
    
    As part of normal operaions, the hrtimer subsystem frequently calls
    into the timekeeping code, creating a locking order of
      hrtimer locks -> timekeeping locks
    
    clock_was_set_delayed() was suppoed to allow us to avoid deadlocks
    between the timekeeping the hrtimer subsystem, so that we could
    notify the hrtimer subsytem the time had changed while holding
    the timekeeping locks. This was done by scheduling delayed work
    that would run later once we were out of the timekeeing code.
    
    But unfortunately the lock chains are complex enoguh that in
    scheduling delayed work, we end up eventually trying to grab
    an hrtimer lock.
    
    Sasha Levin noticed this in testing when the new seqlock lockdep
    enablement triggered the following (somewhat abrieviated) message:
    
    [  251.100221] ======================================================
    [  251.100221] [ INFO: possible circular locking dependency detected ]
    [  251.100221] 3.13.0-rc2-next-20131206-sasha-00005-g8be2375-dirty #4053 Not tainted
    [  251.101967] -------------------------------------------------------
    [  251.101967] kworker/10:1/4506 is trying to acquire lock:
    [  251.101967]  (timekeeper_seq){----..}, at: [<ffffffff81160e96>] retrigger_next_event+0x56/0x70
    [  251.101967]
    [  251.101967] but task is already holding lock:
    [  251.101967]  (hrtimer_bases.lock#11){-.-...}, at: [<ffffffff81160e7c>] retrigger_next_event+0x3c/0x70
    [  251.101967]
    [  251.101967] which lock already depends on the new lock.
    [  251.101967]
    [  251.101967]
    [  251.101967] the existing dependency chain (in reverse order) is:
    [  251.101967]
    -> #5 (hrtimer_bases.lock#11){-.-...}:
    [snipped]
    -> #4 (&rt_b->rt_runtime_lock){-.-...}:
    [snipped]
    -> #3 (&rq->lock){-.-.-.}:
    [snipped]
    -> #2 (&p->pi_lock){-.-.-.}:
    [snipped]
    -> #1 (&(&pool->lock)->rlock){-.-...}:
    [  251.101967]        [<ffffffff81194803>] validate_chain+0x6c3/0x7b0
    [  251.101967]        [<ffffffff81194d9d>] __lock_acquire+0x4ad/0x580
    [  251.101967]        [<ffffffff81194ff2>] lock_acquire+0x182/0x1d0
    [  251.101967]        [<ffffffff84398500>] _raw_spin_lock+0x40/0x80
    [  251.101967]        [<ffffffff81153e69>] __queue_work+0x1a9/0x3f0
    [  251.101967]        [<ffffffff81154168>] queue_work_on+0x98/0x120
    [  251.101967]        [<ffffffff81161351>] clock_was_set_delayed+0x21/0x30
    [  251.101967]        [<ffffffff811c4bd1>] do_adjtimex+0x111/0x160
    [  251.101967]        [<ffffffff811e2711>] compat_sys_adjtimex+0x41/0x70
    [  251.101967]        [<ffffffff843a4b49>] ia32_sysret+0x0/0x5
    [  251.101967]
    -> #0 (timekeeper_seq){----..}:
    [snipped]
    [  251.101967] other info that might help us debug this:
    [  251.101967]
    [  251.101967] Chain exists of:
      timekeeper_seq --> &rt_b->rt_runtime_lock --> hrtimer_bases.lock#11
    
    [  251.101967]  Possible unsafe locking scenario:
    [  251.101967]
    [  251.101967]        CPU0                    CPU1
    [  251.101967]        ----                    ----
    [  251.101967]   lock(hrtimer_bases.lock#11);
    [  251.101967]                                lock(&rt_b->rt_runtime_lock);
    [  251.101967]                                lock(hrtimer_bases.lock#11);
    [  251.101967]   lock(timekeeper_seq);
    [  251.101967]
    [  251.101967]  *** DEADLOCK ***
    [  251.101967]
    [  251.101967] 3 locks held by kworker/10:1/4506:
    [  251.101967]  #0:  (events){.+.+.+}, at: [<ffffffff81154960>] process_one_work+0x200/0x530
    [  251.101967]  #1:  (hrtimer_work){+.+...}, at: [<ffffffff81154960>] process_one_work+0x200/0x530
    [  251.101967]  #2:  (hrtimer_bases.lock#11){-.-...}, at: [<ffffffff81160e7c>] retrigger_next_event+0x3c/0x70
    [  251.101967]
    [  251.101967] stack backtrace:
    [  251.101967] CPU: 10 PID: 4506 Comm: kworker/10:1 Not tainted 3.13.0-rc2-next-20131206-sasha-00005-g8be2375-dirty #4053
    [  251.101967] Workqueue: events clock_was_set_work
    
    So the best solution is to avoid calling clock_was_set_delayed() while
    holding the timekeeping lock, and instead using a flag variable to
    decide if we should call clock_was_set() once we've released the locks.
    
    This works for the case here, where the do_adjtimex() was the deadlock
    trigger point. Unfortuantely, in update_wall_time() we still hold
    the jiffies lock, which would deadlock with the ipi triggered by
    clock_was_set(), preventing us from calling it even after we drop the
    timekeeping lock. So instead call clock_was_set_delayed() at that point.
    
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Prarit Bhargava <prarit@redhat.com>
    Cc: Richard Cochran <richardcochran@gmail.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Sasha Levin <sasha.levin@oracle.com>
    Reported-by: Sasha Levin <sasha.levin@oracle.com>
    Tested-by: Sasha Levin <sasha.levin@oracle.com>
    Signed-off-by: John Stultz <john.stultz@linaro.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    
    Conflicts:
    	kernel/time/timekeeping.c

commit d7bf00ed5fb0d9b002957d013e7b0f977539d4f2
Author: John Stultz <john.stultz@linaro.org>
Date:   Wed Dec 11 19:10:36 2013 -0800

    timekeeping: Fix missing timekeeping_update in suspend path
    
    commit 330a1617b0a6268d427aa5922c94d082b1d3e96d upstream.
    
    Since 48cdc135d4840 (Implement a shadow timekeeper), we have to
    call timekeeping_update() after any adjustment to the timekeeping
    structure in order to make sure that any adjustments to the structure
    persist.
    
    In the timekeeping suspend path, we udpate the timekeeper
    structure, so we should be sure to update the shadow-timekeeper
    before releasing the timekeeping locks. Currently this isn't done.
    
    In most cases, the next time related code to run would be
    timekeeping_resume, which does update the shadow-timekeeper, but
    in an abundence of caution, this patch adds the call to
    timekeeping_update() in the suspend path.
    
    Cc: Sasha Levin <sasha.levin@oracle.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Prarit Bhargava <prarit@redhat.com>
    Cc: Richard Cochran <richardcochran@gmail.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: John Stultz <john.stultz@linaro.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 60fbf3c4ee978beab39a8bf9047183c882599d4f
Author: John Stultz <john.stultz@linaro.org>
Date:   Tue Dec 10 17:13:35 2013 -0800

    timekeeping: Fix CLOCK_TAI timer/nanosleep delays
    
    commit 04005f6011e3b504cd4d791d9769f7cb9a3b2eae upstream.
    
    A think-o in the calculation of the monotonic -> tai time offset
    results in CLOCK_TAI timers and nanosleeps to expire late (the
    latency is ~2x the tai offset).
    
    Fix this by adding the tai offset from the realtime offset instead
    of subtracting.
    
    Cc: Sasha Levin <sasha.levin@oracle.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Prarit Bhargava <prarit@redhat.com>
    Cc: Richard Cochran <richardcochran@gmail.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: John Stultz <john.stultz@linaro.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 7bc9e9c2a37c711fe192dbfc4ea743e83effb00d
Author: John Stultz <john.stultz@linaro.org>
Date:   Wed Dec 11 18:50:25 2013 -0800

    timekeeping: Fix lost updates to tai adjustment
    
    commit f55c07607a38f84b5c7e6066ee1cfe433fa5643c upstream.
    
    Since 48cdc135d4840 (Implement a shadow timekeeper), we have to
    call timekeeping_update() after any adjustment to the timekeeping
    structure in order to make sure that any adjustments to the structure
    persist.
    
    Unfortunately, the updates to the tai offset via adjtimex do not
    trigger this update, causing adjustments to the tai offset to be
    made and then over-written by the previous value at the next
    update_wall_time() call.
    
    This patch resovles the issue by calling timekeeping_update()
    right after setting the tai offset.
    
    Cc: Sasha Levin <sasha.levin@oracle.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Prarit Bhargava <prarit@redhat.com>
    Cc: Richard Cochran <richardcochran@gmail.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: John Stultz <john.stultz@linaro.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit a8471dc8ba5a67e24049c610e257b2dc76d0554a
Author: Geert Uytterhoeven <geert@linux-m68k.org>
Date:   Fri May 3 23:27:07 2013 +0200

    ntp: Remove unused variable flags in __hardpps
    
    kernel/time/ntp.c: In function __hardpps:
    kernel/time/ntp.c:877: warning: unused variable flags
    
    commit a076b2146fabb0894cae5e0189a8ba3f1502d737 ("ntp: Remove ntp_lock,
    using the timekeeping locks to protect ntp state") removed its users,
    but not the actual variable.
    
    Signed-off-by: Geert Uytterhoeven <geert@linux-m68k.org>
    Signed-off-by: John Stultz <john.stultz@linaro.org>

commit 3b703297e075e1221a160f21e5c18d92658a98a2
Author: John Stultz <john.stultz@linaro.org>
Date:   Fri Mar 22 11:52:03 2013 -0700

    ntp: Remove ntp_lock, using the timekeeping locks to protect ntp state
    
    In order to properly handle the NTP state in future changes to the
    timekeeping lock management, this patch moves the management of
    all of the ntp state under the timekeeping locks.
    
    This allows us to remove the ntp_lock.
    
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Richard Cochran <richardcochran@gmail.com>
    Cc: Prarit Bhargava <prarit@redhat.com>
    Signed-off-by: John Stultz <john.stultz@linaro.org>
    
    Conflicts:
    	kernel/time/ntp.c

commit 526d5b8bb90f7cbd82e40b2cb9055ecc6308b8a9
Author: John Stultz <john.stultz@linaro.org>
Date:   Wed Sep 11 16:50:56 2013 -0700

    timekeeping: Fix HRTICK related deadlock from ntp lock changes
    
    commit 7bd36014460f793c19e7d6c94dab67b0afcfcb7f upstream.
    
    Gerlando Falauto reported that when HRTICK is enabled, it is
    possible to trigger system deadlocks. These were hard to
    reproduce, as HRTICK has been broken in the past, but seemed
    to be connected to the timekeeping_seq lock.
    
    Since seqlock/seqcount's aren't supported w/ lockdep, I added
    some extra spinlock based locking and triggered the following
    lockdep output:
    
    [   15.849182] ntpd/4062 is trying to acquire lock:
    [   15.849765]  (&(&pool->lock)->rlock){..-...}, at: [<ffffffff810aa9b5>] __queue_work+0x145/0x480
    [   15.850051]
    [   15.850051] but task is already holding lock:
    [   15.850051]  (timekeeper_lock){-.-.-.}, at: [<ffffffff810df6df>] do_adjtimex+0x7f/0x100
    
    <snip>
    
    [   15.850051] Chain exists of: &(&pool->lock)->rlock --> &p->pi_lock --> timekeeper_lock
    [   15.850051]  Possible unsafe locking scenario:
    [   15.850051]
    [   15.850051]        CPU0                    CPU1
    [   15.850051]        ----                    ----
    [   15.850051]   lock(timekeeper_lock);
    [   15.850051]                                lock(&p->pi_lock);
    [   15.850051] lock(timekeeper_lock);
    [   15.850051] lock(&(&pool->lock)->rlock);
    [   15.850051]
    [   15.850051]  *** DEADLOCK ***
    
    The deadlock was introduced by 06c017fdd4dc48451a ("timekeeping:
    Hold timekeepering locks in do_adjtimex and hardpps") in 3.10
    
    This patch avoids this deadlock, by moving the call to
    schedule_delayed_work() outside of the timekeeper lock
    critical section.
    
    Reported-by: Gerlando Falauto <gerlando.falauto@keymile.com>
    Tested-by: Lin Ming <minggr@gmail.com>
    Signed-off-by: John Stultz <john.stultz@linaro.org>
    Cc: Mathieu Desnoyers <mathieu.desnoyers@efficios.com>
    Link: http://lkml.kernel.org/r/1378943457-27314-1-git-send-email-john.stultz@linaro.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit faa9eee79a35f0d2cf20d0c3201f45e1ceec7d98
Author: Zoran Markovic <zoran.markovic@linaro.org>
Date:   Fri May 17 11:24:05 2013 -0700

    timekeeping: Correct run-time detection of persistent_clock.
    
    Since commit 31ade30692dc9680bfc95700d794818fa3f754ac, timekeeping_init()
    checks for presence of persistent clock by attempting to read a non-zero
    time value. This is an issue on platforms where persistent_clock (instead
    is implemented as a free-running counter (instead of an RTC) starting
    from zero on each boot and running during suspend. Examples are some ARM
    platforms (e.g. PandaBoard).
    
    An attempt to read such a clock during timekeeping_init() may return zero
    value and falsely declare persistent clock as missing. Additionally, in
    the above case suspend times may be accounted twice (once from
    timekeeping_resume() and once from rtc_resume()), resulting in a gradual
    drift of system time.
    
    This patch does a run-time correction of the issue by doing the same check
    during timekeeping_suspend().
    
    A better long-term solution would have to return error when trying to read
    non-existing clock and zero when trying to read an uninitialized clock, but
    that would require changing all persistent_clock implementations.
    
    This patch addresses the immediate breakage, for now.
    
    Cc: John Stultz <john.stultz@linaro.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Feng Tang <feng.tang@intel.com>
    Cc: stable@vger.kernel.org
    Signed-off-by: Zoran Markovic <zoran.markovic@linaro.org>
    [jstultz: Tweaked commit message and subject]
    Signed-off-by: John Stultz <john.stultz@linaro.org>

commit 120742eb50627122fcbc5557cd31d63c090732b6
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Apr 22 09:37:04 2013 +0200

    timekeeping: Update tk->cycle_last in resume
    
    commit 7ec98e15aa (timekeeping: Delay update of clock->cycle_last)
    forgot to update tk->cycle_last in the resume path. This results in a
    stale value versus clock->cycle_last and prevents resume in the worst
    case.
    
    Reported-by: Jiri Slaby <jslaby@suse.cz>
    Reported-and-tested-by: Borislav Petkov <bp@alien8.de>
    Acked-by: John Stultz <john.stultz@linaro.org>
    Cc: Linux-pm mailing list <linux-pm@lists.linux-foundation.org>
    Link: http://lkml.kernel.org/r/alpine.LFD.2.02.1304211648150.21884@ionos
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

commit a676ab73fe24ab17682c24851d2bf2843711f4a0
Author: John Stultz <john.stultz@linaro.org>
Date:   Wed Apr 10 12:41:49 2013 -0700

    timekeeping: Make sure to notify hrtimers when TAI offset changes
    
    Now that we have CLOCK_TAI timers, make sure we notify hrtimer
    code when TAI offset is changed.
    
    Signed-off-by: John Stultz <john.stultz@linaro.org>
    Link: http://lkml.kernel.org/r/1365622909-953-1-git-send-email-john.stultz@linaro.org
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

commit 2fddb0b20ef65f5e19b4069e584cef145aa54af6
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Feb 21 22:51:40 2013 +0000

    timekeeping: Shorten seq_count region
    
    Shorten the seqcount write hold region to the actual update of the
    timekeeper and the related data (e.g vsyscall).
    
    On a contemporary x86 system this reduces the maximum latencies on
    Preempt-RT from 8us to 4us on the non-timekeeping cores.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: John Stultz <john.stultz@linaro.org>

commit 6b5aeaf5a33cd3f35475198f3b557f0dac2a2b15
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Feb 21 22:51:40 2013 +0000

    timekeeping: Implement a shadow timekeeper
    
    Use the shadow timekeeper to do the update_wall_time() adjustments and
    then copy it over to the real timekeeper.
    
    Keep the shadow timekeeper in sync when updating stuff outside of
    update_wall_time().
    
    This allows us to limit the timekeeper_seq hold time to the update of
    the real timekeeper and the vsyscall data in the next patch.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: John Stultz <john.stultz@linaro.org>
    
    Conflicts:
    	kernel/time/timekeeping.c

commit f2acfbca35e5299e5f8960b17fb2c3b7f64ad75b
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Feb 21 22:51:39 2013 +0000

    timekeeping: Delay update of clock->cycle_last
    
    For calculating the new timekeeper values store the new cycle_last
    value in the timekeeper and update the clock->cycle_last just when we
    actually update the new values.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: John Stultz <john.stultz@linaro.org>

commit 6682851168b7e57e6bc03f36d2f23e3887856d98
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Feb 21 22:51:38 2013 +0000

    timekeeping: Store cycle_last value in timekeeper struct as well
    
    For implementing a shadow timekeeper and a split calculation/update
    region we need to store the cycle_last value in the timekeeper and
    update the value in the clocksource struct only in the update region.
    
    Add the extra storage to the timekeeper.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: John Stultz <john.stultz@linaro.org>

commit 49e29c6e4858077b880146e47afa3d1daf258823
Author: John Stultz <john.stultz@linaro.org>
Date:   Fri Mar 22 14:20:03 2013 -0700

    timekeeping: Simplify tai updating from do_adjtimex
    
    Since we are taking the timekeeping locks, just go ahead
    and update any tai change directly, rather then dropping
    the lock and calling a function that will just take it again.
    
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Richard Cochran <richardcochran@gmail.com>
    Cc: Prarit Bhargava <prarit@redhat.com>
    Signed-off-by: John Stultz <john.stultz@linaro.org>

commit e3583369018f70b7ead2837156016a39a3fc5b85
Author: John Stultz <john.stultz@linaro.org>
Date:   Fri Mar 22 11:37:28 2013 -0700

    timekeeping: Hold timekeepering locks in do_adjtimex and hardpps
    
    In moving the NTP state to be protected by the timekeeping locks,
    be sure to acquire the timekeeping locks prior to calling
    ntp functions.
    
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Richard Cochran <richardcochran@gmail.com>
    Cc: Prarit Bhargava <prarit@redhat.com>
    Signed-off-by: John Stultz <john.stultz@linaro.org>

commit 33351a9e5680ece53b59b80abb4223f5c779344b
Author: John Stultz <john.stultz@linaro.org>
Date:   Fri Mar 22 15:04:13 2013 -0700

    timekeeping: Move ADJ_SETOFFSET to top level do_adjtimex()
    
    Since ADJ_SETOFFSET adjusts the timekeeping state, process
    it as part of the top level do_adjtimex() function in
    timekeeping.c.
    
    This avoids deadlocks that could occur once we change the
    ntp locking rules.
    
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Richard Cochran <richardcochran@gmail.com>
    Cc: Prarit Bhargava <prarit@redhat.com>
    Signed-off-by: John Stultz <john.stultz@linaro.org>
    
    Conflicts:
    	kernel/time/ntp.c

commit d50d651043ceadacc11e292984b1e52f7743a365
Author: John Stultz <john.stultz@linaro.org>
Date:   Fri Mar 22 12:28:15 2013 -0700

    ntp: Rework do_adjtimex to take timespec and tai arguments
    
    In order to change the locking rules, we need to provide
    the timespec and tai values rather then having the ntp
    logic acquire these values itself.
    
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Richard Cochran <richardcochran@gmail.com>
    Cc: Prarit Bhargava <prarit@redhat.com>
    Signed-off-by: John Stultz <john.stultz@linaro.org>
    
    Conflicts:
    	kernel/time/ntp.c

commit e6139a2245e4c661b0e60efdacae405fe68149b8
Author: John Stultz <john.stultz@linaro.org>
Date:   Fri Mar 22 12:08:52 2013 -0700

    ntp: Move timex validation to timekeeping do_adjtimex call.
    
    Move logic that does not need the ntp state to be done
    in the timekeeping do_adjtimex() call.
    
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Richard Cochran <richardcochran@gmail.com>
    Cc: Prarit Bhargava <prarit@redhat.com>
    Signed-off-by: John Stultz <john.stultz@linaro.org>

commit 0b1b1c0e7681bbea4bb2ce2ab31e13bb34a9ba09
Author: John Stultz <john.stultz@linaro.org>
Date:   Fri Mar 22 11:31:29 2013 -0700

    ntp: Move do_adjtimex() and hardpps() functions to timekeeping.c
    
    In preparation for changing the ntp locking rules, move
    do_adjtimex and hardpps accessor functions to timekeeping.c,
    but keep the code logic in ntp.c.
    
    This patch also introduces a ntp_internal.h file so timekeeping
    specific interfaces of ntp.c can be more limitedly shared with
    timekeeping.c.
    
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Richard Cochran <richardcochran@gmail.com>
    Cc: Prarit Bhargava <prarit@redhat.com>
    Signed-off-by: John Stultz <john.stultz@linaro.org>

commit 877afbc8c86046c4b6e6e70b27dc769765958b57
Author: John Stultz <john.stultz@linaro.org>
Date:   Fri Mar 22 11:59:04 2013 -0700

    ntp: Split out timex validation from do_adjtimex
    
    Split out the timex validation done in do_adjtimex into a separate
    function. This will help simplify logic in following patches.
    
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Richard Cochran <richardcochran@gmail.com>
    Cc: Prarit Bhargava <prarit@redhat.com>
    Signed-off-by: John Stultz <john.stultz@linaro.org>

commit 6a180b4e5ad760c41cbfc7fcbf4421c97eba186b
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Apr 10 11:14:55 2012 +0200

    ntp: Make ntp_lock raw
    
    seconds_overflow() is called from hard interrupt context even on
    Preempt-RT. This requires the lock to be a raw_spinlock.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: John Stultz <john.stultz@linaro.org>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 8c1b8a4aaa11048ba849b2d99e746dc6adc89760
Author: Prarit Bhargava <prarit@redhat.com>
Date:   Fri Feb 8 17:59:53 2013 -0500

    time, Fix setting of hardware clock in NTP code
    
    At init time, if the system time is "warped" forward in warp_clock()
    it will differ from the hardware clock by sys_tz.tz_minuteswest.  This time
    difference is not taken into account when ntp updates the hardware clock,
    and this causes the system time to jump forward by this offset every reboot.
    
    The kernel must take this offset into account when writing the system time
    to the hardware clock in the ntp code.  This patch adds
    persistent_clock_is_local which indicates that an offset has been applied
    in warp_clock() and accounts for the "warp" before writing the hardware
    clock.
    
    x86 does not have this problem as rtc writes are software limited to a
    +/-15 minute window relative to the current rtc time.  Other arches, such
    as powerpc, however do a full synchronization of the system time to the
    rtc and will see this problem.
    
    [v2]: generated against tip/timers/core
    
    Signed-off-by: Prarit Bhargava <prarit@redhat.com>
    Cc: John Stultz <john.stultz@linaro.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: John Stultz <john.stultz@linaro.org>

commit 1c4f7879e8dcf9d6650cb38e9f2182d13ec65da7
Author: Jason Gunthorpe <jgunthorpe@obsidianresearch.com>
Date:   Mon Dec 17 14:30:53 2012 -0700

    NTP: Add a CONFIG_RTC_SYSTOHC configuration
    
    The purpose of this option is to allow ARM/etc systems that rely on the
    class RTC subsystem to have the same kind of automatic NTP based
    synchronization that we have on PC platforms. Today ARM does not
    implement update_persistent_clock and makes extensive use of the class
    RTC system.
    
    When enabled CONFIG_RTC_SYSTOHC will provide a generic
    rtc_update_persistent_clock that stores the current time in the RTC and
    is intended complement the existing CONFIG_RTC_HCTOSYS option that loads
    the RTC at boot.
    
    Like with RTC_HCTOSYS the platform's update_persistent_clock is used
    first, if it works. Platforms with mixed class RTC and non-RTC drivers
    need to return ENODEV when class RTC should be used. Such an update for
    PPC is included in this patch.
    
    Long term, implementations of update_persistent_clock should migrate to
    proper class RTC drivers and use CONFIG_RTC_SYSTOHC instead.
    
    Tested on ARM kirkwood and PPC405
    
    Signed-off-by: Jason Gunthorpe <jgunthorpe@obsidianresearch.com>
    Signed-off-by: John Stultz <john.stultz@linaro.org>
    
    Conflicts:
    	arch/powerpc/kernel/time.c

commit eca76a1c8af533883c80a418bfbad37c32b905b7
Author: Fengguang Wu <fengguang.wu@intel.com>
Date:   Mon Mar 25 12:24:24 2013 -0700

    timekeeping: __timekeeping_set_tai_offset can be static
    
    Yet again, the kbuild test robot saves the day, noting
    I left out defining __timekeeping_set_tai_offset as
    static. It even sent me this patch.
    
    Reported-by: Fengguang Wu <fengguang.wu@intel.com>
    Signed-off-by: John Stultz <john.stultz@linaro.org>

commit bf95b5b2d79f815b1c65be6eb2b1b334e44138f7
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Feb 21 22:51:38 2013 +0000

    timekeeping: Split timekeeper_lock into lock and seqcount
    
    We want to shorten the seqcount write hold time. So split the seqlock
    into a lock and a seqcount.
    
    Open code the seqwrite_lock in the places which matter and drop the
    sequence counter update where it's pointless.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    [jstultz: Merge fixups from CLOCK_TAI collisions]
    Signed-off-by: John Stultz <john.stultz@linaro.org>

commit 6f56388690547ec220b71d87905b4220fcb103e6
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Feb 21 22:51:37 2013 +0000

    timekeeping: Move lock out of timekeeper struct
    
    Make the lock a separate entity. Preparatory patch for shadow
    timekeeper structure.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    [Merged with CLOCK_TAI changes]
    Signed-off-by: John Stultz <john.stultz@linaro.org>

commit c115f5567aef38565fab6bee7825df75a4469264
Author: Marcelo Tosatti <mtosatti@redhat.com>
Date:   Tue Nov 27 23:28:59 2012 -0200

    time: export time information for KVM pvclock
    
    As suggested by John, export time data similarly to how its
    done by vsyscall support. This allows KVM to retrieve necessary
    information to implement vsyscall support in KVM guests.
    
    Acked-by: John Stultz <johnstul@us.ibm.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

commit c9fd6fa87abc23965633a7b992f94beabe5e8484
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Feb 21 22:51:36 2013 +0000

    timekeeping: Make jiffies_lock internal
    
    Nothing outside of the timekeeping core needs that lock.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: John Stultz <john.stultz@linaro.org>

commit fdedc97c841a9654021edc030c056ec36dd4fb45
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Feb 21 22:51:36 2013 +0000

    timekeeping: Calc stuff once
    
    Calculate the cycle interval shifted value once. No functional change,
    just makes the code more readable.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: John Stultz <john.stultz@linaro.org>

commit f62a58c5530b7f00e59a9fb1d24693f98ddb873a
Author: John Stultz <john.stultz@linaro.org>
Date:   Mon Jan 21 17:00:11 2013 -0800

    hrtimer: Add hrtimer support for CLOCK_TAI
    
    Add hrtimer support for CLOCK_TAI, as well as posix timer interfaces.
    
    Signed-off-by: John Stultz <john.stultz@linaro.org>

commit a1da654a82877c98bdd18e4fdff80e47dc579dcb
Author: John Stultz <john.stultz@linaro.org>
Date:   Thu May 3 12:43:40 2012 -0700

    timekeeping: Add CLOCK_TAI clockid
    
    This add a CLOCK_TAI clockid and the needed accessors.
    
    CC: Thomas Gleixner <tglx@linutronix.de>
    CC: Eric Dumazet <eric.dumazet@gmail.com>
    CC: Richard Cochran <richardcochran@gmail.com>
    Signed-off-by: John Stultz <john.stultz@linaro.org>

commit 0c1364426cda35a0d7c7efefd1583483fe835d74
Author: John Stultz <john.stultz@linaro.org>
Date:   Thu May 3 12:30:07 2012 -0700

    timekeeping: Move TAI managment into timekeeping core from ntp
    
    Currently NTP manages the TAI offset. Since there's plans for a
    CLOCK_TAI clockid, push the TAI management into the timekeeping
    core.
    
    CC: Thomas Gleixner <tglx@linutronix.de>
    CC: Eric Dumazet <eric.dumazet@gmail.com>
    CC: Richard Cochran <richardcochran@gmail.com>
    Signed-off-by: John Stultz <john.stultz@linaro.org>
    
    Conflicts:
    	kernel/time/timekeeping.c

commit 0c28049728f3048f9b858f3b4dec4226c308cd60
Author: Feng Tang <feng.tang@intel.com>
Date:   Tue Mar 12 11:56:48 2013 +0800

    timekeeping: utilize the suspend-nonstop clocksource to count suspended time
    
    There are some new processors whose TSC clocksource won't stop during
    suspend. Currently, after system resumes, kernel will use persistent
    clock or RTC to compensate the sleep time, but with these nonstop
    clocksources, we could skip the special compensation from external
    sources, and just use current clocksource for time recounting.
    
    This can solve some time drift bugs caused by some not-so-accurate or
    error-prone RTC devices.
    
    The current way to count suspended time is first try to use the persistent
    clock, and then try the RTC if persistent clock can't be used. This
    patch will change the trying order to:
    	suspend-nonstop clocksource -> persistent clock -> RTC
    
    When counting the sleep time with nonstop clocksource, use an accurate way
    suggested by Jason Gunthorpe to cover very large delta cycles.
    
    Signed-off-by: Feng Tang <feng.tang@intel.com>
    [jstultz: Small optimization, avoiding re-reading the clocksource]
    Signed-off-by: John Stultz <john.stultz@linaro.org>

commit 41f373c704b690ba2725a92cdcfccd690d6d5628
Author: Feng Tang <feng.tang@intel.com>
Date:   Wed Jan 16 00:09:47 2013 +0800

    timekeeping: Add persistent_clock_exist flag
    
    In current kernel, there are several places which need to check
    whether there is a persistent clock for the platform. Current check
    is done by calling the read_persistent_clock() and validating its
    return value.
    
    So one optimization is to do the check only once in timekeeping_init(),
    and use a flag persistent_clock_exist to record it.
    
    v2: Add a has_persistent_clock() helper function, as suggested by John.
    
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: John Stultz <john.stultz@linaro.org>
    Signed-off-by: Feng Tang <feng.tang@intel.com>
    Signed-off-by: John Stultz <john.stultz@linaro.org>

commit 1ae907e86fc65b84fc5130ea70f5b7745837f87f
Author: Martin Schwidefsky <schwidefsky@de.ibm.com>
Date:   Fri Nov 22 11:44:51 2013 -0800

    time: Fix 1ns/tick drift w/ GENERIC_TIME_VSYSCALL_OLD
    
    commit 4be77398ac9d948773116b6be4a3c91b3d6ea18c upstream.
    
    Since commit 1e75fa8be9f (time: Condense timekeeper.xtime
    into xtime_sec - merged in v3.6), there has been an problem
    with the error accounting in the timekeeping code, such that
    when truncating to nanoseconds, we round up to the next nsec,
    but the balancing adjustment to the ntp_error value was dropped.
    
    This causes 1ns per tick drift forward of the clock.
    
    In 3.7, this logic was isolated to only GENERIC_TIME_VSYSCALL_OLD
    architectures (s390, ia64, powerpc).
    
    The fix is simply to balance the accounting and to subtract the
    added nanosecond from ntp_error. This allows the internal long-term
    clock steering to keep the clock accurate.
    
    While this fix removes the regression added in 1e75fa8be9f, the
    ideal solution is to move away from GENERIC_TIME_VSYSCALL_OLD
    and use the new VSYSCALL method, which avoids entirely the
    nanosecond granular rounding, and the resulting short-term clock
    adjustment oscillation needed to keep long term accurate time.
    
    [ jstultz: Many thanks to Martin for his efforts identifying this
      	   subtle bug, and providing the fix. ]
    
    Originally-from: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Paul Turner <pjt@google.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Richard Cochran <richardcochran@gmail.com>
    Cc: Prarit Bhargava <prarit@redhat.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1385149491-20307-1-git-send-email-john.stultz@linaro.org
    Signed-off-by: John Stultz <john.stultz@linaro.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit eed35ecbba18ad01ccad25363ebbec906a014136
Author: John Stultz <john.stultz@linaro.org>
Date:   Tue Feb 28 16:50:11 2012 -0800

    time: Kill xtime_lock, replacing it with jiffies_lock
    
    Now that timekeeping is protected by its own locks, rename
    the xtime_lock to jifffies_lock to better describe what it
    protects.
    
    CC: Thomas Gleixner <tglx@linutronix.de>
    CC: Eric Dumazet <eric.dumazet@gmail.com>
    CC: Richard Cochran <richardcochran@gmail.com>
    Signed-off-by: John Stultz <john.stultz@linaro.org>
    
    Conflicts:
    	kernel/time/tick-sched.c

commit 2f330ccd69ee204c573b7c4d4fb947a3e6857a3b
Author: Lars-Peter Clausen <lars@metafoo.de>
Date:   Thu Oct 18 11:34:41 2012 +0200

    time/jiffies: Make clocksource_jiffies static
    
    Commit f1b8274 ("clocksource: Cleanup clocksource selection") removed all
    external references to clocksource_jiffies so there is no need to have the
    symbol globally visible.
    
    Fixes the following sparse warning:
      CHECK   kernel/time/jiffies.c kernel/time/jiffies.c:61:20: warning: symbol 'clocksource_jiffies' was not declared. Should it be static?
    
    Signed-off-by: Lars-Peter Clausen <lars@metafoo.de>
    Signed-off-by: John Stultz <john.stultz@linaro.org>

commit f9b74ac056b9a077ff618ff47cf07b07e300d115
Author: Chuansheng Liu <chuansheng.liu@intel.com>
Date:   Thu Oct 25 01:07:35 2012 +0800

    tick: Correct the comments for tick_sched_timer()
    
    In the comments of function tick_sched_timer(), the sentence
    "timer->base->cpu_base->lock held" is not right.
    
    In function __run_hrtimer(), before call timer->function(),
    the cpu_base->lock has been unlocked.
    
    Signed-off-by: liu chuansheng <chuansheng.liu@intel.com>
    Cc: fei.li@intel.com
    Cc: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/1351098455.15558.1421.camel@cliu38-desktop-build
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

commit 25a3021285aa1a8bec4e381d69953fa165381567
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Mon Oct 15 16:17:16 2012 +0200

    tick: Conditionally build nohz specific code in tick handler
    
    This optimize a bit the high res tick sched handler.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>

commit e2ddba111764e1b7a79bded61224bbc07b37cf08
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Mon Oct 15 02:43:03 2012 +0200

    tick: Consolidate tick handling for high and low res handlers
    
    Besides unifying code, this also adds the idle check before
    processing idle accounting specifics on the low res handler.
    This way we also generalize this part of the nohz code for
    !CONFIG_HIGH_RES_TIMERS to prepare for the adaptive tickless
    features.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    
    Conflicts:
    	kernel/time/tick-sched.c

commit e9ca7165595ddd6f25d583564b7c60ee628cc35b
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Mon Oct 15 02:03:27 2012 +0200

    tick: Consolidate timekeeping handling code
    
    Unify the duplicated timekeeping handling code of low and high res tick
    sched handlers.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>

commit 72af15cf620c78736cc62aac41e00d2b34ab6104
Author: John Stultz <john.stultz@linaro.org>
Date:   Tue Sep 4 15:38:12 2012 -0400

    time: Only do nanosecond rounding on GENERIC_TIME_VSYSCALL_OLD systems
    
    We only do rounding to the next nanosecond so we don't see minor
    1ns inconsistencies in the vsyscall implementations. Since we're
    changing the vsyscall implementations to avoid this, conditionalize
    the rounding only to the GENERIC_TIME_VSYSCALL_OLD architectures.
    
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Paul Turner <pjt@google.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Richard Cochran <richardcochran@gmail.com>
    Cc: Prarit Bhargava <prarit@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: John Stultz <john.stultz@linaro.org>

commit f8c83bd34bc2e18ab7629e46effb942a2144ebba
Author: John Stultz <john.stultz@linaro.org>
Date:   Tue Sep 11 19:58:13 2012 -0400

    time: Introduce new GENERIC_TIME_VSYSCALL
    
    Now that we moved everyone over to GENERIC_TIME_VSYSCALL_OLD,
    introduce the new declaration and config option for the new
    update_vsyscall method.
    
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Paul Turner <pjt@google.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Richard Cochran <richardcochran@gmail.com>
    Cc: Prarit Bhargava <prarit@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: John Stultz <john.stultz@linaro.org>

commit b4259ebb0e46365272bfd557147cc5e1c9cc8f75
Author: John Stultz <john.stultz@linaro.org>
Date:   Tue Sep 4 15:34:21 2012 -0400

    time: Convert CONFIG_GENERIC_TIME_VSYSCALL to CONFIG_GENERIC_TIME_VSYSCALL_OLD
    
    To help migrate archtectures over to the new update_vsyscall method,
    redfine CONFIG_GENERIC_TIME_VSYSCALL as CONFIG_GENERIC_TIME_VSYSCALL_OLD
    
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Paul Turner <pjt@google.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Richard Cochran <richardcochran@gmail.com>
    Cc: Prarit Bhargava <prarit@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: John Stultz <john.stultz@linaro.org>

commit 0b44a75a0ef5669bfb28b5ea95620e56fb0ace96
Author: John Stultz <john.stultz@linaro.org>
Date:   Tue Sep 4 15:27:48 2012 -0400

    time: Move update_vsyscall definitions to timekeeper_internal.h
    
    Since users will need to include timekeeper_internal.h, move
    update_vsyscall definitions to timekeeper_internal.h.
    
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Paul Turner <pjt@google.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Richard Cochran <richardcochran@gmail.com>
    Cc: Prarit Bhargava <prarit@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: John Stultz <john.stultz@linaro.org>

commit a7fabe0127ac0cbd9ab5064b011ffe09c6071d6d
Author: John Stultz <john.stultz@linaro.org>
Date:   Tue Sep 4 15:12:07 2012 -0400

    time: Move timekeeper structure to timekeeper_internal.h for vsyscall changes
    
    We're going to need to access the timekeeper in update_vsyscall,
    so make the structure available for those who need it.
    
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Paul Turner <pjt@google.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Richard Cochran <richardcochran@gmail.com>
    Cc: Prarit Bhargava <prarit@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: John Stultz <john.stultz@linaro.org>

commit 11a50a25cadb28c9f9e1153ca85ec42773516a43
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu May 28 18:52:34 2015 +0200

    gpu: remove unused 550MHz step

commit 2fe881af0630bdbfd79298a0f97373a46220d108
Author: John Stultz <john.stultz@linaro.org>
Date:   Tue Sep 4 12:42:27 2012 -0400

    jiffies: Remove compile time assumptions about CLOCK_TICK_RATE
    
    CLOCK_TICK_RATE is used to accurately caclulate exactly how
    a tick will be at a given HZ.
    
    This is useful, because while we'd expect NSEC_PER_SEC/HZ,
    the underlying hardware will have some granularity limit,
    so we won't be able to have exactly HZ ticks per second.
    
    This slight error can cause timekeeping quality problems
    when using the jiffies or other jiffies driven clocksources.
    Thus we currently use compile time CLOCK_TICK_RATE value to
    generate SHIFTED_HZ and NSEC_PER_JIFFIES, which we then use
    to adjust the jiffies clocksource to correct this error.
    
    Unfortunately though, since CLOCK_TICK_RATE is a compile
    time value, and the jiffies clocksource is registered very
    early during boot, there are a number of cases where there
    are different possible hardware timers that have different
    tick rates. This causes problems in cases like ARM where
    there are numerous different types of hardware, each having
    their own compile-time CLOCK_TICK_RATE, making it hard to
    accurately support different hardware with a single kernel.
    
    For the most part, this doesn't matter all that much, as not
    too many systems actually utilize the jiffies or jiffies driven
    clocksource. Usually there are other highres clocksources
    who's granularity error is negligable.
    
    Even so, we have some complicated calcualtions that we do
    everywhere to handle these edge cases.
    
    This patch removes the compile time SHIFTED_HZ value, and
    introduces a register_refined_jiffies() function. This results
    in the default jiffies clock as being assumed a perfect HZ
    freq, and allows archtectures that care about jiffies accuracy
    to call register_refined_jiffies() with the tick rate, specified
    dynamically at boot.
    
    This allows us, where necessary, to not have a compile time
    CLOCK_TICK_RATE constant, simplifies the jiffies code, and
    still provides a way to have an accurate jiffies clock.
    
    NOTE: Since this patch does not add register_refinied_jiffies()
    calls for every arch, it may cause time quality regressions
    in some cases. Its likely these will not be noticable, but
    if they are an issue, adding the following to the end of
    setup_arch() should resolve the regression:
    	register_refinied_jiffies(CLOCK_TICK_RATE)
    
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Richard Cochran <richardcochran@gmail.com>
    Cc: Prarit Bhargava <prarit@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: John Stultz <john.stultz@linaro.org>

commit b5e629b4e22dd522a63cc291af24443fda0c090e
Author: John Stultz <john.stultz@linaro.org>
Date:   Tue Sep 11 19:26:03 2012 -0400

    time: Fix timeekeping_get_ns overflow on 32bit systems
    
    Daniel Lezcano reported seeing multi-second stalls from
    keyboard input on his T61 laptop when NOHZ and CPU_IDLE
    were enabled on a 32bit kernel.
    
    He bisected the problem down to commit
    1e75fa8be9fb6 ("time: Condense timekeeper.xtime into xtime_sec").
    
    After reproducing this issue, I narrowed the problem down
    to the fact that timekeeping_get_ns() returns a 64bit
    nsec value that hasn't been accumulated. In some cases
    this value was being then stored in timespec.tv_nsec
    (which is a long).
    
    On 32bit systems, with idle times larger then 4 seconds
    (or less, depending on the value of xtime_nsec), the
    returned nsec value would overflow 32bits. This limited
    kept time from increasing, causing timers to not expire.
    
    The fix is to make sure we don't directly store the
    result of timekeeping_get_ns() into a tv_nsec field,
    instead using a 64bit nsec value which can then be
    added into the timespec via timespec_add_ns().
    
    Reported-and-bisected-by: Daniel Lezcano <daniel.lezcano@linaro.org>
    Tested-by: Daniel Lezcano <daniel.lezcano@linaro.org>
    Signed-off-by: John Stultz <john.stultz@linaro.org>
    Acked-by: Prarit Bhargava <prarit@redhat.com>
    Cc: Richard Cochran <richardcochran@gmail.com>
    Link: http://lkml.kernel.org/r/1347405963-35715-1-git-send-email-john.stultz@linaro.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    
    Conflicts:
    	kernel/time/timekeeping.c

commit 75c085e825f408aec3193085ca0a5f48110b09ba
Author: Rafael J. Wysocki <rjw@sisk.pl>
Date:   Mon Aug 6 01:40:41 2012 +0200

    timekeeping: Add suspend and resume of clock event devices
    
    Some clock event devices, for example such that belong to PM domains,
    need to be handled in a spcial way during the timekeeping suspend
    and resume (which takes place in the system core, or "syscore",
    stages of system power transitions) in analogy with clock sources.
    
    Introduce .suspend() and .resume() callbacks for clock event devices
    that will be executed by timekeeping_suspend/_resume(), respectively,
    next the the clock sources' .suspend() and .resume() callbacks.
    
    Signed-off-by: Rafael J. Wysocki <rjw@sisk.pl>

commit 10ebd53d8e5ec0d0b4043e7d1a6b0f1a1ae11ce2
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Tue Jun 30 13:57:12 2015 +0200

    time: rename the last timekeeper. to tk->

commit 93ef47bdde3c8cafd44d1e7cafe79b7005819974
Author: John Stultz <john.stultz@linaro.org>
Date:   Tue Aug 21 20:30:48 2012 -0400

    time: Avoid potential shift overflow with large shift values
    
    Andreas Schwab noticed that the 1 << tk->shift could overflow if the
    shift value was greater than 30, since 1 would be a 32bit long on
    32bit architectures. This issue was introduced by 1e75fa8be (time:
    Condense timekeeper.xtime into xtime_sec)
    
    Use 1ULL instead to ensure we don't overflow on the shift.
    
    Reported-by: Andreas Schwab <schwab@linux-m68k.org>
    Signed-off-by: John Stultz <john.stultz@linaro.org>
    Cc: Prarit Bhargava <prarit@redhat.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Link: http://lkml.kernel.org/r/1345595449-34965-4-git-send-email-john.stultz@linaro.org
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

commit 6bbda5144bd65617cc6365ffb46aa6a88a323b8a
Author: Andreas Schwab <schwab@linux-m68k.org>
Date:   Tue Aug 21 20:30:47 2012 -0400

    time: Fix casting issue in timekeeping_forward_now
    
    arch_gettimeoffset returns a u32 value which when shifted by tk->shift
    can overflow. This issue was introduced with 1e75fa8be (time: Condense
    timekeeper.xtime into xtime_sec)
    
    Cast it to u64 first.
    
    Signed-off-by: Andreas Schwab <schwab@linux-m68k.org>
    Signed-off-by: John Stultz <john.stultz@linaro.org>
    Cc: Prarit Bhargava <prarit@redhat.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Link: http://lkml.kernel.org/r/1345595449-34965-3-git-send-email-john.stultz@linaro.org
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

commit e8e09f96f0bf626f11e476a57e667b2c29b2ba73
Author: John Stultz <john.stultz@linaro.org>
Date:   Tue Aug 21 20:30:46 2012 -0400

    time: Ensure we normalize the timekeeper in tk_xtime_add
    
    Andreas noticed problems with resume on specific hardware after commit
    1e75fa8b (time: Condense timekeeper.xtime into xtime_sec) combined
    with commit b44d50dca (time: Fix casting issue in tk_set_xtime and
    tk_xtime_add)
    
    After some digging I realized we aren't normalizing the timekeeper
    after the add. Add the missing normalize call.
    
    Reported-by: Andreas Schwab <schwab@linux-m68k.org>
    Tested-by: Andreas Schwab <schwab@linux-m68k.org>
    Signed-off-by: John Stultz <john.stultz@linaro.org>
    Cc: Prarit Bhargava <prarit@redhat.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Link: http://lkml.kernel.org/r/1345595449-34965-2-git-send-email-john.stultz@linaro.org
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

commit 1f92e4ab19837c7a0650784428eb79f6c5bd95fc
Author: John Stultz <john.stultz@linaro.org>
Date:   Wed Aug 8 15:36:20 2012 -0400

    time: Improve sanity checking of timekeeping inputs
    
    Unexpected behavior could occur if the time is set to a value large
    enough to overflow a 64bit ktime_t (which is something larger then the
    year 2262).
    
    Also unexpected behavior could occur if large negative offsets are
    injected via adjtimex.
    
    So this patch improves the sanity check timekeeping inputs by
    improving the timespec_valid() check, and then makes better use of
    timespec_valid() to make sure we don't set the time to an invalid
    negative value or one that overflows ktime_t.
    
    Note: This does not protect from setting the time close to overflowing
    ktime_t and then letting natural accumulation cause the overflow.
    
    Reported-by: CAI Qian <caiqian@redhat.com>
    Reported-by: Sasha Levin <levinsasha928@gmail.com>
    Signed-off-by: John Stultz <john.stultz@linaro.org>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Prarit Bhargava <prarit@redhat.com>
    Cc: Zhouping Liu <zliu@redhat.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: stable@vger.kernel.org
    Link: http://lkml.kernel.org/r/1344454580-17031-1-git-send-email-john.stultz@linaro.org
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

commit 8d0d0f3652ad1a0de082dacc8cb12695ad2307e3
Author: Ingo Molnar <mingo@kernel.org>
Date:   Sat Aug 4 21:21:14 2012 +0200

    time: Fix adjustment cleanup bug in timekeeping_adjust()
    
    Tetsuo Handa reported that sporadically the system clock starts
    counting up too quickly which is enough to confuse the hangcheck
    timer to print a bogus stall warning.
    
    Commit 2a8c0883 "time: Move xtime_nsec adjustment underflow handling
    timekeeping_adjust" overlooked this exit path:
    
            } else
                    return;
    
    which should really be a proper exit sequence, fixing the bug as a
    side effect.
    
    Also make the flow more readable by properly balancing curly
    braces.
    
    Reported-by: Tetsuo Handa <penguin-kernel@I-love.SAKURA.ne.jp> wrote:
    Tested-by: Tetsuo Handa <penguin-kernel@I-love.SAKURA.ne.jp> wrote:
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Cc: john.stultz@linaro.org
    Cc: a.p.zijlstra@chello.nl
    Cc: richardcochran@gmail.com
    Cc: prarit@redhat.com
    Link: http://lkml.kernel.org/r/20120804192114.GA28347@gmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 18a603e0be9ee5437c1f2deb98b23317d23da542
Author: John Stultz <john.stultz@linaro.org>
Date:   Fri Jul 27 14:48:13 2012 -0400

    time: Remove all direct references to timekeeper
    
    Ingo noted that the numerous timekeeper.value references made
    the timekeeping code ugly and caused many long lines that
    had to be broken up. He recommended replacing timekeeper.value
    references with tk->value.
    
    This patch provides a local tk value for all top level time
    functions and sets it to &timekeeper. Then all timekeeper
    access is done via a tk pointer.
    
    Signed-off-by: John Stultz <john.stultz@linaro.org>
    Cc: Prarit Bhargava <prarit@redhat.com>
    Link: http://lkml.kernel.org/r/1343414893-45779-6-git-send-email-john.stultz@linaro.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    
    Conflicts:
    	kernel/time/timekeeping.c

commit fd5c59df22467f1613b6900753fb321f92e92d8d
Author: John Stultz <john.stultz@linaro.org>
Date:   Fri Jul 27 14:48:12 2012 -0400

    time: Clean up offs_real/wall_to_mono and offs_boot/total_sleep_time updates
    
    For performance reasons, we maintain ktime_t based duplicates of
    wall_to_monotonic (offs_real) and total_sleep_time (offs_boot).
    
    Since large problems could occur (such as the resume regression
    on 3.5-rc7, or the leapsecond hrtimer issue) if these value
    pairs were to be inconsistently updated, this patch this cleans
    up how we modify these value pairs to ensure we are always
    consistent.
    
    As a side-effect this is also more efficient as we only
    caulculate the duplicate values when they are changed,
    rather then every update_wall_time call.
    
    This also provides WARN_ONs to detect if future changes break
    the invariants.
    
    Signed-off-by: John Stultz <john.stultz@linaro.org>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Richard Cochran <richardcochran@gmail.com>
    Cc: Prarit Bhargava <prarit@redhat.com>
    Link: http://lkml.kernel.org/r/1343414893-45779-5-git-send-email-john.stultz@linaro.org
    [ Cleaned up minor style issues. ]
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    
    Conflicts:
    	kernel/time/timekeeping.c

commit a8324033ec3ea01edd163551e0b2ac9407838b1d
Author: John Stultz <john.stultz@linaro.org>
Date:   Fri Jul 27 14:48:11 2012 -0400

    time: Clean up stray newlines
    
    Ingo noted inconsistent newline usage between functions.
    This patch cleans those up.
    
    Signed-off-by: John Stultz <john.stultz@linaro.org>
    Cc: Prarit Bhargava <prarit@redhat.com>
    Link: http://lkml.kernel.org/r/1343414893-45779-4-git-send-email-john.stultz@linaro.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit a1a3c309d24bda722d803e588a10bab936161110
Author: John Stultz <john.stultz@linaro.org>
Date:   Fri Jul 27 14:48:10 2012 -0400

    time/jiffies: Rename ACTHZ to SHIFTED_HZ
    
    Ingo noted that ACTHZ is a confusing name, and requested it
    be renamed, so this patch renames ACTHZ to SHIFTED_HZ to
    better describe it.
    
    Signed-off-by: John Stultz <john.stultz@linaro.org>
    Cc: Prarit Bhargava <prarit@redhat.com>
    Link: http://lkml.kernel.org/r/1343414893-45779-3-git-send-email-john.stultz@linaro.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit f90ffdb32c319fd1fd4a4e72ce01afca6f0368bd
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Fri Jul 27 14:48:09 2012 -0400

    time/jiffies: Allow CLOCK_TICK_RATE to be undefined
    
    CLOCK_TICK_RATE is a legacy constant that defines the timer
    device's granularity. On hardware with particularly coarse
    granularity, this constant is used to reduce accumulated
    time error when using jiffies as a clocksource, by calculating
    the hardware's actual tick length rather then just assuming
    it is 1sec/HZ.
    
    However, for the most part this is unnecessary, as most modern
    systems don't use jiffies for their clocksource, and their
    tick device is sufficiently fine grained to avoid major error.
    
    Thus, this patch allows an architecture to not define
    CLOCK_TICK_RATE, in which case ACTHZ defaults to (HZ << 8).
    
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Acked-by: Arnd Bergmann <arnd@arndb.de>
    Cc: Richard Cochran <richardcochran@gmail.com>
    Cc: Prarit Bhargava <prarit@redhat.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    [ Commit log & intention tweaks ]
    Signed-off-by: John Stultz <john.stultz@linaro.org>
    Link: http://lkml.kernel.org/r/1343414893-45779-2-git-send-email-john.stultz@linaro.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 5ba19f660429278f5229708a20d8d6a38134f069
Author: John Stultz <john.stultz@linaro.org>
Date:   Mon Jul 23 16:22:37 2012 -0400

    time: Fix casting issue in tk_set_xtime and tk_xtime_add
    
    commit 1e75fa8b (time: Condense timekeeper.xtime into xtime_sec)
    introduced helper functions which apply a timespec to the core
    internal timekeeper data. The internal storage type is u64. The
    timespec tv_nsec value must be shifted before set or added to the
    internal value. tv_nsec is a long, which is 32bit on a 32bit system,
    so without casting tv_nsec to u64 we lose the bits which are shifted
    over the 32bit boundary.
    
    Add the proper typecasts.
    
    Reported-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Tested-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Signed-off-by: John Stultz <john.stultz@linaro.org>
    Acked-by: Prarit Bhargava <prarit@redhat.com>
    Link: http://lkml.kernel.org/r/1343074957-16541-1-git-send-email-john.stultz@linaro.org
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

commit 3aab520b2f96cd75f24eaddf3a5a6b09416d4624
Author: John Stultz <john.stultz@linaro.org>
Date:   Fri Jul 13 01:21:57 2012 -0400

    time: Rework timekeeping functions to take timekeeper ptr as argument
    
    As part of cleaning up the timekeeping code, this patch converts
    a number of internal functions to takei a timekeeper ptr as an
    argument, so that the internal functions don't access the global
    timekeeper structure directly. This allows for further optimizations
    to reduce lock hold time later.
    
    This patch has been updated to include more consistent usage of the
    timekeeper value, by making sure it is always passed as a argument
    to non top-level functions.
    
    Signed-off-by: John Stultz <john.stultz@linaro.org>
    Reviewed-by: Ingo Molnar <mingo@kernel.org>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Richard Cochran <richardcochran@gmail.com>
    Cc: Prarit Bhargava <prarit@redhat.com>
    Link: http://lkml.kernel.org/r/1342156917-25092-9-git-send-email-john.stultz@linaro.org
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    
    Conflicts:
    	kernel/time/timekeeping.c

commit 4c3ce68d9176d4d174312e58defbd4d18f81ec11
Author: John Stultz <john.stultz@linaro.org>
Date:   Fri Jul 13 01:21:56 2012 -0400

    time: Move xtime_nsec adjustment underflow handling timekeeping_adjust
    
    When we make adjustments speeding up the clock, its possible
    for xtime_nsec to underflow. We already handle this properly,
    but we do so from update_wall_time() instead of the more logical
    timekeeping_adjust(), where the possible underflow actually
    occurs.
    
    Thus, move the correction logic to the timekeeping_adjust, which
    is the function that causes the issue. Making update_wall_time()
    more readable.
    
    Signed-off-by: John Stultz <johnstul@us.ibm.com>
    Reviewed-by: Ingo Molnar <mingo@kernel.org>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Richard Cochran <richardcochran@gmail.com>
    Cc: Prarit Bhargava <prarit@redhat.com>
    Link: http://lkml.kernel.org/r/1342156917-25092-8-git-send-email-john.stultz@linaro.org
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

commit 7779742430c3e33f5ac17c9a57fcec67d6251aac
Author: John Stultz <john.stultz@linaro.org>
Date:   Fri Jul 13 01:21:55 2012 -0400

    time: Move arch_gettimeoffset() usage into timekeeping_get_ns()
    
    Since we call arch_gettimeoffset() in all the accessor
    functions, move arch_gettimeoffset() calls into
    timekeeping_get_ns() and timekeeping_get_ns_raw() to simplify
    the code.
    
    This also makes the code easier to maintain as we don't have to
    worry about forgetting the arch_gettimeoffset() as has happened
    in the past.
    
    Signed-off-by: John Stultz <johnstul@us.ibm.com>
    Reviewed-by: Ingo Molnar <mingo@kernel.org>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Richard Cochran <richardcochran@gmail.com>
    Cc: Prarit Bhargava <prarit@redhat.com>
    Link: http://lkml.kernel.org/r/1342156917-25092-7-git-send-email-john.stultz@linaro.org
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

commit 30297128e542be0bf57d79aef6249b87af32ebdc
Author: John Stultz <john.stultz@linaro.org>
Date:   Fri Jul 13 01:21:54 2012 -0400

    time: Refactor accumulation of nsecs to secs
    
    We do the exact same logic moving nsecs to secs in the
    timekeeper in multiple places, so condense this into a
    single function.
    
    Signed-off-by: John Stultz <john.stultz@linaro.org>
    Reviewed-by: Ingo Molnar <mingo@kernel.org>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Richard Cochran <richardcochran@gmail.com>
    Cc: Prarit Bhargava <prarit@redhat.com>
    Link: http://lkml.kernel.org/r/1342156917-25092-6-git-send-email-john.stultz@linaro.org
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    
    Conflicts:
    	kernel/time/timekeeping.c

commit 91a854d2d25d00fdb107cfe371041457b0de8fe7
Author: John Stultz <john.stultz@linaro.org>
Date:   Fri Jul 13 01:21:53 2012 -0400

    time: Condense timekeeper.xtime into xtime_sec
    
    The timekeeper struct has a xtime_nsec, which keeps the
    sub-nanosecond remainder.  This ends up being somewhat
    duplicative of the timekeeper.xtime.tv_nsec value, and we
    have to do extra work to keep them apart, copying the full
    nsec portion out and back in over and over.
    
    This patch simplifies some of the logic by taking the timekeeper
    xtime value and splitting it into timekeeper.xtime_sec and
    reuses the timekeeper.xtime_nsec for the sub-second portion
    (stored in higher res shifted nanoseconds).
    
    This simplifies some of the accumulation logic. And will
    allow for more accurate timekeeping once the vsyscall code
    is updated to use the shifted nanosecond remainder.
    
    Signed-off-by: John Stultz <john.stultz@linaro.org>
    Reviewed-by: Ingo Molnar <mingo@kernel.org>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Richard Cochran <richardcochran@gmail.com>
    Cc: Prarit Bhargava <prarit@redhat.com>
    Link: http://lkml.kernel.org/r/1342156917-25092-5-git-send-email-john.stultz@linaro.org
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    
    Conflicts:
    	kernel/time/timekeeping.c

commit baa695a7fef39ce2a18ee2085c640b7346f800dc
Author: John Stultz <john.stultz@linaro.org>
Date:   Fri Jul 13 01:21:52 2012 -0400

    time: Explicitly use u32 instead of int for shift values
    
    Ingo noted that using a u32 instead of int for shift values
    would be better to make sure the compiler doesn't unnecessarily
    use complex signed arithmetic.
    
    Signed-off-by: John Stultz <john.stultz@linaro.org>
    Reviewed-by: Ingo Molnar <mingo@kernel.org>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Richard Cochran <richardcochran@gmail.com>
    Cc: Prarit Bhargava <prarit@redhat.com>
    Link: http://lkml.kernel.org/r/1342156917-25092-4-git-send-email-john.stultz@linaro.org
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

commit 53ebc9860ca9fdc67427ee7d54b8455bf6c7f61b
Author: John Stultz <john.stultz@linaro.org>
Date:   Fri Jul 13 01:21:51 2012 -0400

    time: Whitespace cleanups per Ingo%27s requests
    
    Ingo noted a number of places where there is inconsistent
    use of whitespace. This patch tries to address the main
    culprits.
    
    Signed-off-by: John Stultz <john.stultz@linaro.org>
    Reviewed-by: Ingo Molnar <mingo@kernel.org>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Richard Cochran <richardcochran@gmail.com>
    Cc: Prarit Bhargava <prarit@redhat.com>
    Link: http://lkml.kernel.org/r/1342156917-25092-3-git-send-email-john.stultz@linaro.org
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

commit a856aaef1618df6d8d0152ac6832ee1f38b79f41
Author: Magnus Damm <magnus.damm@gmail.com>
Date:   Wed May 9 23:39:34 2012 +0900

    clockevents: Make clockevents_config() a global symbol
    
    Make clockevents_config() into a global symbol to allow it to be used
    by compiled-in clockevent drivers. This is needed by drivers that want
    to update the timer frequency after registration time.
    
    Signed-off-by: Magnus Damm <damm@opensource.se>
    Tested-by: Simon Horman <horms@verge.net.au>
    Cc: arnd@arndb.de
    Cc: johnstul@us.ibm.com
    Cc: rjw@sisk.pl
    Cc: lethal@linux-sh.org
    Cc: gregkh@linuxfoundation.org
    Cc: olof@lixom.net
    Cc: Magnus Damm <magnus.damm@gmail.com>
    Link: http://lkml.kernel.org/r/20120509143934.27521.46553.sendpatchset@w520
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

commit af17ee36b218eb3a6f5ad396a019194a064a0e81
Author: Richard Cochran <richardcochran@gmail.com>
Date:   Fri Apr 27 10:12:41 2012 +0200

    ntp: Fix a stale comment and a few stray newlines.
    
    Signed-off-by: Richard Cochran <richardcochran@gmail.com>
    Signed-off-by: John Stultz <john.stultz@linaro.org>

commit d65eff6451e07976f43f1932cb4def104c41148e
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon May 21 23:16:18 2012 +0200

    timers: Fixup the Kconfig consolidation fallout
    
    Sigh, I missed to check which architecture Kconfig files actually
    include the core Kconfig file. There are a few which did not. So we
    broke them.
    
    Instead of adding the includes to those, we are better off to move the
    include to init/Kconfig like we did already with irqs and others.
    
    This does not change anything for the architectures using the old
    style periodic timer mode. It just solves the build wreckage there.
    
    For those architectures which use the clock events infrastructure it
    moves the include of the core Kconfig file to "General setup" which is
    a way more logical place than having it at random locations specified
    by the architecture specific Kconfigs.
    
    Reported-by: Ingo Molnar <mingo@kernel.org>
    Cc: Anna-Maria Gleixner <anna-maria@glx-um.de>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

commit 4234cb940acf90d390cc1b846105fd3f43bff3d9
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri May 18 09:59:57 2012 +0200

    timers: Provide generic Kconfig switches
    
    We really don't want all the arch code defining stuff
    over and over.
    
    [ anna-maria: Added missing GENERIC_CMOS_UPDATE switch ]
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Anna-Maria Gleixner <anna-maria@glx-um.de>
    Cc: Paul Mundt <lethal@linux-sh.org>
    Link: http://lkml.kernel.org/r/1337529587.3208.2.camel@dionysos
    Acked-by: Sam Ravnborg <sam@ravnborg.org>

commit ced2e84528613fbfd81a7e7a02e76b41ec3cf2d2
Author: Richard Cochran <richardcochran@gmail.com>
Date:   Fri Apr 27 10:12:42 2012 +0200

    timekeeping: Fix a few minor newline issues.
    
    Fix a few minor newline issues.
    
    Signed-off-by: Richard Cochran <richardcochran@gmail.com>
    Signed-off-by: John Stultz <john.stultz@linaro.org>

commit 13c70384e966c14e1adb78ded9f4af64fe99a810
Author: Miklos Szeredi <mszeredi@suse.cz>
Date:   Wed Aug 15 12:55:22 2012 +0200

    audit: clean up refcounting in audit-tree
    
    Drop the initial reference by fsnotify_init_mark early instead of
    audit_tree_freeing_mark() at destroy time.
    
    In the cases we destroy the mark before we drop the initial reference we need to
    get rid of the get_mark that balances the put_mark in audit_tree_freeing_mark().
    
    Signed-off-by: Miklos Szeredi <mszeredi@suse.cz>

commit 37c40dd3c1bcdbad17447356620330fdb814609a
Author: Daniel Wagner <daniel.wagner@bmw-carit.de>
Date:   Wed Sep 12 16:12:05 2012 +0200

    cgroup: Wrap subsystem selection macro
    
    Before we are able to define all subsystem ids at compile time we need
    a more fine grained control what gets defined when we include
    cgroup_subsys.h. For example we define the enums for the subsystems or
    to declare for struct cgroup_subsys (builtin subsystem) by including
    cgroup_subsys.h and defining SUBSYS accordingly.
    
    Currently, the decision if a subsys is used is defined inside the
    header by testing if CONFIG_*=y is true. By moving this test outside
    of cgroup_subsys.h we are able to control it on the include level.
    
    This is done by introducing IS_SUBSYS_ENABLED which then is defined
    according the task, e.g. is CONFIG_*=y or CONFIG_*=m.
    
    Signed-off-by: Daniel Wagner <daniel.wagner@bmw-carit.de>
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Acked-by: Li Zefan <lizefan@huawei.com>
    Acked-by: Neil Horman <nhorman@tuxdriver.com>
    Cc: Gao feng <gaofeng@cn.fujitsu.com>
    Cc: Jamal Hadi Salim <jhs@mojatatu.com>
    Cc: John Fastabend <john.r.fastabend@intel.com>
    Cc: netdev@vger.kernel.org
    Cc: cgroups@vger.kernel.org

commit b7141e8f438606381b48377e5c3d61523ab4074f
Author: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
Date:   Tue Jul 31 16:42:12 2012 -0700

    mm/hugetlb: add new HugeTLB cgroup
    
    Implement a new controller that allows us to control HugeTLB allocations.
    The extension allows to limit the HugeTLB usage per control group and
    enforces the controller limit during page fault.  Since HugeTLB doesn't
    support page reclaim, enforcing the limit at page fault time implies that,
    the application will get SIGBUS signal if it tries to access HugeTLB pages
    beyond its limit.  This requires the application to know beforehand how
    much HugeTLB pages it would require for its use.
    
    The charge/uncharge calls will be added to HugeTLB code in later patch.
    Support for cgroup removal will be added in later patches.
    
    [akpm@linux-foundation.org: s/CONFIG_CGROUP_HUGETLB_RES_CTLR/CONFIG_MEMCG_HUGETLB/g]
    [akpm@linux-foundation.org: s/CONFIG_MEMCG_HUGETLB/CONFIG_CGROUP_HUGETLB/g]
    Reviewed-by: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Hillf Danton <dhillf@gmail.com>
    Reviewed-by: Michal Hocko <mhocko@suse.cz>
    Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit 2eafbc0d8287ce42555f722acd07e559f39e1ca2
Author: Aristeu Rozanski <aris@redhat.com>
Date:   Thu Aug 23 16:53:31 2012 -0400

    cgroup: rename subsys_bits to subsys_mask
    
    In a previous discussion, Tejun Heo suggested to rename references to
    subsys_bits (added_bits, removed_bits, etc) by something more meaningful.
    
    Cc: Li Zefan <lizefan@huawei.com>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: Hillf Danton <dhillf@gmail.com>
    Cc: Lennart Poettering <lpoetter@redhat.com>
    Signed-off-by: Aristeu Rozanski <aris@redhat.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

commit 8c0810de7a0895df45e3a902a157fd3406c5ff82
Author: Aristeu Rozanski <aris@redhat.com>
Date:   Thu Aug 23 16:53:30 2012 -0400

    cgroup: add xattr support
    
    This is one of the items in the plumber's wish list.
    
    For use cases:
    
    >> What would the use case be for this?
    >
    > Attaching meta information to services, in an easily discoverable
    > way. For example, in systemd we create one cgroup for each service, and
    > could then store data like the main pid of the specific service as an
    > xattr on the cgroup itself. That way we'd have almost all service state
    > in the cgroupfs, which would make it possible to terminate systemd and
    > later restart it without losing any state information. But there's more:
    > for example, some very peculiar services cannot be terminated on
    > shutdown (i.e. fakeraid DM stuff) and it would be really nice if the
    > services in question could just mark that on their cgroup, by setting an
    > xattr. On the more desktopy side of things there are other
    > possibilities: for example there are plans defining what an application
    > is along the lines of a cgroup (i.e. an app being a collection of
    > processes). With xattrs one could then attach an icon or human readable
    > program name on the cgroup.
    >
    > The key idea is that this would allow attaching runtime meta information
    > to cgroups and everything they model (services, apps, vms), that doesn't
    > need any complex userspace infrastructure, has good access control
    > (i.e. because the file system enforces that anyway, and there's the
    > "trusted." xattr namespace), notifications (inotify), and can easily be
    > shared among applications.
    >
    > Lennart
    
    v7:
    - no changes
    v6:
    - remove user xattr namespace, only allow trusted and security
    v5:
    - check for capabilities before setting/removing xattrs
    v4:
    - no changes
    v3:
    - instead of config option, use mount option to enable xattr support
    
    Original-patch-by: Li Zefan <lizefan@huawei.com>
    Cc: Li Zefan <lizefan@huawei.com>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: Hillf Danton <dhillf@gmail.com>
    Cc: Lennart Poettering <lpoetter@redhat.com>
    Signed-off-by: Li Zefan <lizefan@huawei.com>
    Signed-off-by: Aristeu Rozanski <aris@redhat.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

commit 57e4e29f75c1a8032932388d444c347832c919f9
Author: Aristeu Rozanski <aris@redhat.com>
Date:   Thu Aug 23 16:53:29 2012 -0400

    cgroup: revise how we re-populate root directory
    
    When remounting cgroupfs with some subsystems added to it and some
    removed, cgroup will remove all the files in root directory and then
    re-popluate it.
    
    What I'm doing here is, only remove files which belong to subsystems that
    are to be unbinded, and only create files for newly-added subsystems.
    The purpose is to have all other files untouched.
    
    This is a preparation for cgroup xattr support.
    
    v7:
    - checkpatch warnings fixed
    v6:
    - no changes
    v5:
    - no changes
    v4:
    - refactored cgroup_clear_directory() to not use cgroup_rm_file()
    - instead of going thru the list of files, get the file list using the
      subsystems
    - use 'subsys_mask' instead of {added,removed}_bits and made
      cgroup_populate_dir() to match the parameters with cgroup_clear_directory()
    v3:
    - refresh patches after recent refactoring
    
    Original-patch-by: Li Zefan <lizefan@huawei.com>
    Cc: Li Zefan <lizefan@huawei.com>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: Hillf Danton <dhillf@gmail.com>
    Cc: Lennart Poettering <lpoetter@redhat.com>
    Signed-off-by: Li Zefan <lizefan@huawei.com>
    Signed-off-by: Aristeu Rozanski <aris@redhat.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

commit db173455f61cc264de8a8a4e1bb8ff1b1a41ddd6
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Jul 3 10:38:06 2012 -0700

    cgroup: cgroup_rm_files() was calling simple_unlink() with the wrong inode
    
    While refactoring cgroup file removal path, 05ef1d7c4a "cgroup:
    introduce struct cfent" incorrectly changed the @dir argument of
    simple_unlink() to the inode of the file being deleted instead of that
    of the containing directory.
    
    The effect of this bug is minor - ctime and mtime of the parent
    weren't properly updated on file deletion.
    
    Fix it by using @cgrp->dentry->d_inode instead.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Reported-by: Al Viro <viro@ZenIV.linux.org.uk>
    Acked-by: Li Zefan <lizefan@huawei.com>
    Cc: stable@vger.kernel.org

commit c0fa93efa2042ab903b727559b5b35e5537cb929
Author: Tejun Heo <tj@kernel.org>
Date:   Sat Jul 7 16:08:18 2012 -0700

    cgroup: fix cgroup hierarchy umount race
    
    48ddbe1946 "cgroup: make css->refcnt clearing on cgroup removal
    optional" allowed a css to linger after the associated cgroup is
    removed.  As a css holds a reference on the cgroup's dentry, it means
    that cgroup dentries may linger for a while.
    
    Destroying a superblock which has dentries with positive refcnts is a
    critical bug and triggers BUG() in vfs code.  As each cgroup dentry
    holds an s_active reference, any lingering cgroup has both its dentry
    and the superblock pinned and thus preventing premature release of
    superblock.
    
    Unfortunately, after 48ddbe1946, there's a small window while
    releasing a cgroup which is directly under the root of the hierarchy.
    When a cgroup directory is released, vfs layer first deletes the
    corresponding dentry and then invokes dput() on the parent, which may
    recurse further, so when a cgroup directly below root cgroup is
    released, the cgroup is first destroyed - which releases the s_active
    it was holding - and then the dentry for the root cgroup is dput().
    
    This creates a window where the root dentry's refcnt isn't zero but
    superblock's s_active is.  If umount happens before or during this
    window, vfs will see the root dentry with non-zero refcnt and trigger
    BUG().
    
    Before 48ddbe1946, this problem didn't exist because the last dentry
    reference was guaranteed to be put synchronously from rmdir(2)
    invocation which holds s_active around the whole process.
    
    Fix it by holding an extra superblock->s_active reference across
    dput() from css release, which is the dput() path added by 48ddbe1946
    and the only one which doesn't hold an extra s_active ref across the
    final cgroup dput().
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    LKML-Reference: <4FEEA5CB.8070809@huawei.com>
    Reported-by: shyju pv <shyju.pv@huawei.com>
    Tested-by: shyju pv <shyju.pv@huawei.com>
    Cc: Sasha Levin <levinsasha928@gmail.com>
    Acked-by: Li Zefan <lizefan@huawei.com>

commit d39f72f75184eed54615e28e934ccb9a964c2d23
Author: Tejun Heo <tj@kernel.org>
Date:   Sat Jul 7 15:55:47 2012 -0700

    Revert "cgroup: superblock can't be released with active dentries"
    
    This reverts commit fa980ca87d15bb8a1317853f257a505990f3ffde.  The
    commit was an attempt to fix a race condition where a cgroup hierarchy
    may be unmounted with positive dentry reference on root cgroup.  While
    the commit made the race condition slightly more difficult to trigger,
    the race was still there and could be reliably triggered using a
    different test case.
    
    Revert the incorrect fix.  The next commit will describe the race and
    fix it correctly.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    LKML-Reference: <4FEEA5CB.8070809@huawei.com>
    Reported-by: shyju pv <shyju.pv@huawei.com>
    Cc: Sasha Levin <levinsasha928@gmail.com>
    Acked-by: Li Zefan <lizefan@huawei.com>

commit 902a1bcdd6eadb0255ec9b4782fe893c19620d17
Author: Salman Qazi <sqazi@google.com>
Date:   Thu Jun 14 14:55:30 2012 -0700

    cgroups: Account for CSS_DEACT_BIAS in __css_put
    
    When we fixed the race between atomic_dec and css_refcnt, we missed
    the fact that css_refcnt internally subtracts CSS_DEACT_BIAS to get
    the actual reference count.  This can potentially cause a refcount leak
    if __css_put races with cgroup_clear_css_refs.
    
    Signed-off-by: Salman Qazi <sqazi@google.com>
    Acked-by: Li Zefan <lizefan@huawei.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

commit 4ec7e66199514f96250e4e1d2fb12df9067d62b5
Author: Li Zefan <lizefan@huawei.com>
Date:   Wed Jun 6 19:12:30 2012 -0700

    cgroup: remove hierarchy_mutex
    
    It was introduced for memcg to iterate cgroup hierarchy without
    holding cgroup_mutex, but soon after that it was replaced with
    a lockless way in memcg.
    
    No one used hierarchy_mutex since that, so remove it.
    
    Signed-off-by: Li Zefan <lizefan@huawei.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

commit 8c2380997e8d8e8f69c3bd61a0c092b479b02196
Author: Salman Qazi <sqazi@google.com>
Date:   Wed Jun 6 18:51:35 2012 -0700

    cgroup: make sure that decisions in __css_put are atomic
    
    __css_put is using atomic_dec on the ref count, and then
    looking at the ref count to make decisions.  This is prone
    to races, as someone else may decrement ref count between
    our decrement and our decision.  Instead, we should base our
    decisions on the value that we decremented the ref count to.
    
    (This results in an actual race on Google's kernel which I
    haven't been able to reproduce on the upstream kernel.  Having
    said that, it's still incorrect by inspection).
    
    Signed-off-by: Salman Qazi <sqazi@google.com>
    Acked-by: Li Zefan <lizefan@huawei.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: stable@vger.kernel.org

commit c16f85aaba0a5cfeaf585f4b24f99b5c7805faaf
Author: Tejun Heo <tj@kernel.org>
Date:   Thu May 24 08:24:39 2012 -0700

    cgroup: superblock can't be released with active dentries
    
    48ddbe1946 "cgroup: make css->refcnt clearing on cgroup removal
    optional" allowed a css to linger after the associated cgroup is
    removed.  As a css holds a reference on the cgroup's dentry, it means
    that cgroup dentries may linger for a while.
    
    cgroup_create() does grab an active reference on the superblock to
    prevent it from going away while there are !root cgroups; however, the
    reference is put from cgroup_diput() which is invoked on cgroup
    removal, so cgroup dentries which are removed but persisting due to
    lingering csses already have released their superblock active refs
    allowing superblock to be killed while those dentries are around.
    
    Given the right condition, this makes cgroup_kill_sb() call
    kill_litter_super() with dentries with non-zero d_count leading to
    BUG() in shrink_dcache_for_umount_subtree().
    
    Fix it by adding cgroup_dops->d_release() operation and moving
    deactivate_super() to it.  cgroup_diput() now marks dentry->d_fsdata
    with itself if superblock should be deactivated and cgroup_d_release()
    deactivates the superblock on dentry release.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Reported-by: Sasha Levin <levinsasha928@gmail.com>
    Tested-by: Sasha Levin <levinsasha928@gmail.com>
    LKML-Reference: <CA+1xoqe5hMuxzCRhMy7J0XchDk2ZnuxOHJKikROk1-ReAzcT6g@mail.gmail.com>
    Acked-by: Li Zefan <lizefan@huawei.com>

commit d5538ef442453943fd68b3f0880b66587a3f43f6
Author: Mike Galbraith <mgalbraith@suse.de>
Date:   Sat Apr 21 09:13:46 2012 +0200

    cgroups: disallow attaching kthreadd or PF_THREAD_BOUND threads
    
    Allowing kthreadd to be moved to a non-root group makes no sense, it being
    a global resource, and needlessly leads unsuspecting users toward trouble.
    
    1. An RT workqueue worker thread spawned in a task group with no rt_runtime
    allocated is not schedulable.  Simple user error, but harmful to the box.
    
    2. A worker thread which acquires PF_THREAD_BOUND can never leave a cpuset,
    rendering the cpuset immortal.
    
    Save the user some unexpected trouble, just say no.
    
    Signed-off-by: Mike Galbraith <mgalbraith@suse.de>
    Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Li Zefan <lizefan@huawei.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

commit f7d05c331e2e48b26de4781bb9c6ea6c3339b569
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Apr 10 10:16:36 2012 -0700

    cgroup: remove cgroup_subsys->populate()
    
    With memcg converted, cgroup_subsys->populate() doesn't have any user
    left.  Remove it.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Acked-by: Li Zefan <lizefan@huawei.com>

commit d6a9494658e64a10318ad23a7a85d544244e64a6
Author: Tejun Heo <tj@kernel.org>
Date:   Sun Apr 1 12:09:56 2012 -0700

    cgroup: make css->refcnt clearing on cgroup removal optional
    
    Currently, cgroup removal tries to drain all css references.  If there
    are active css references, the removal logic waits and retries
    ->pre_detroy() until either all refs drop to zero or removal is
    cancelled.
    
    This semantics is unusual and adds non-trivial complexity to cgroup
    core and IMHO is fundamentally misguided in that it couples internal
    implementation details (references to internal data structure) with
    externally visible operation (rmdir).  To userland, this is a behavior
    peculiarity which is unnecessary and difficult to expect (css refs is
    otherwise invisible from userland), and, to policy implementations,
    this is an unnecessary restriction (e.g. blkcg wants to hold css refs
    for caching purposes but can't as that becomes visible as rmdir hang).
    
    Unfortunately, memcg currently depends on ->pre_destroy() retrials and
    cgroup removal vetoing and can't be immmediately switched to the new
    behavior.  This patch introduces the new behavior of not waiting for
    css refs to drain and maintains the old behavior for subsystems which
    have __DEPRECATED_clear_css_refs set.
    
    Once, memcg is updated, we can drop the code paths for the old
    behavior as proposed in the following patch.  Note that the following
    patch is incorrect in that dput work item is in cgroup and may lose
    some of dputs when multiples css's are released back-to-back, and
    __css_put() triggers check_for_release() when refcnt reaches 0 instead
    of 1; however, it shows what part can be removed.
    
      http://thread.gmane.org/gmane.linux.kernel.containers/22559/focus=75251
    
    Note that, in not-too-distant future, cgroup core will start emitting
    warning messages for subsys which require the old behavior, so please
    get moving.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Acked-by: Li Zefan <lizf@cn.fujitsu.com>
    Cc: Vivek Goyal <vgoyal@redhat.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Michal Hocko <mhocko@suse.cz>
    Cc: Balbir Singh <bsingharora@gmail.com>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>

commit fe8d997a90b95edf31c8f8e175566fe4722114ac
Author: Tejun Heo <tj@kernel.org>
Date:   Sun Apr 1 12:09:56 2012 -0700

    cgroup: use negative bias on css->refcnt to block css_tryget()
    
    When a cgroup is about to be removed, cgroup_clear_css_refs() is
    called to check and ensure that there are no active css references.
    
    This is currently achieved by dropping the refcnt to zero iff it has
    only the base ref.  If all css refs could be dropped to zero, ref
    clearing is successful and CSS_REMOVED is set on all css.  If not, the
    base ref is restored.  While css ref is zero w/o CSS_REMOVED set, any
    css_tryget() attempt on it busy loops so that they are atomic
    w.r.t. the whole css ref clearing.
    
    This does work but dropping and re-instating the base ref is somewhat
    hairy and makes it difficult to add more logic to the put path as
    there are two of them - the regular css_put() and the reversible base
    ref clearing.
    
    This patch updates css ref clearing such that blocking new
    css_tryget() and putting the base ref are separate operations.
    CSS_DEACT_BIAS, defined as INT_MIN, is added to css->refcnt and
    css_tryget() busy loops while refcnt is negative.  After all css refs
    are deactivated, if they were all one, ref clearing succeeded and
    CSS_REMOVED is set and the base ref is put using the regular
    css_put(); otherwise, CSS_DEACT_BIAS is subtracted from the refcnts
    and the original postive values are restored.
    
    css_refcnt() accessor which always returns the unbiased positive
    reference counts is added and used to simplify refcnt usages.  While
    at it, relocate and reformat comments in cgroup_has_css_refs().
    
    This separates css->refcnt deactivation and putting the base ref,
    which enables the next patch to make ref clearing optional.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Acked-by: Li Zefan <lizf@cn.fujitsu.com>

commit b330465a62e27640fb18a29a985ec2a4bad2e298
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Jul 22 01:35:30 2015 +0200

    alarm: convert to later alarm system
    mainly taken from @neobuddy89 and @alucard24
    thanks very much to you two! and Alu, cherry-picking was this time a no go, just that you know :)

commit 81aa507b5967c686cd93ae46038b658cfb496336
Author: Pranav Vashi <neobuddy89@gmail.com>
Date:   Fri Feb 20 10:01:06 2015 +0530

    alarmtimer: Fix flags after after upstream merge
    
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 60df62e8d194e9f82dff43398d22bba8701b9280
Author: John Stultz <john.stultz@linaro.org>
Date:   Mon Jul 7 14:06:11 2014 -0700

    alarmtimer: Fix bug where relative alarm timers were treated as absolute
    
    commit 16927776ae757d0d132bdbfabbfe2c498342bd59 upstream.
    
    Sharvil noticed with the posix timer_settime interface, using the
    CLOCK_REALTIME_ALARM or CLOCK_BOOTTIME_ALARM clockid, if the users
    tried to specify a relative time timer, it would incorrectly be
    treated as absolute regardless of the state of the flags argument.
    
    This patch corrects this, properly checking the absolute/relative flag,
    as well as adds further error checking that no invalid flag bits are set.
    
    Reported-by: Sharvil Nanavati <sharvil@google.com>
    Signed-off-by: John Stultz <john.stultz@linaro.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Prarit Bhargava <prarit@redhat.com>
    Cc: Sharvil Nanavati <sharvil@google.com>
    Link: http://lkml.kernel.org/r/1404767171-6902-1-git-send-email-john.stultz@linaro.org
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 57b9c64f96b6260680f9e8dc61deddd838431491
Author: Maria Yu <aiquny@codeaurora.org>
Date:   Thu Jan 9 16:59:21 2014 +0800

    rtc: alarm: have dummy set_power_on_alarm function
    
    When !CONFIG_RTC_CLASS we need to have a dummy set_power_on_alarm
    function. Otherwise, it is easy to have build issues.
    
    Change-Id: I39310241cc2e59f672964c8bc0ee4c3607aedf4e
    Signed-off-by: Maria Yu <aiquny@codeaurora.org>

commit c7ba2359174f05d3b08da02b6adb1f0ae2b1697a
Author: Kumar Gala <galak@codeaurora.org>
Date:   Thu Jan 2 14:15:29 2014 -0600

    rtc: alarm: init power_on_alarm_lock mutex in alarmtimer_rtc_timer_init
    
    Moved mutex_init of power_on_alarm_lock into alarmtimer_rtc_timer_init
    so that if CONFIG_RTC_CLASS is not enabled, we dont try to initialize
    the mutex.  This fixes a build issue when !CONFIG_RTC_CLASS
    
    Change-Id: I270564b94fd54162aa177a2c9767655098662259
    Signed-off-by: Kumar Gala <galak@codeaurora.org>

commit 40c3bd87d4729a68764381aa92f30cac40e589a0
Author: Mohit Aggarwal <maggarwa@codeaurora.org>
Date:   Wed Sep 18 15:26:19 2013 +0530

    rtc: qpnp-rtc: Add NULL pointer checks
    
    In shutdown hook, null pointer check is missing.
    So during probe, if RTC H/w is not enabled, rtc
    driver data will not be initialized. This patch
    adds null pointer checks in shutdown hook.
    
    Change-Id: I1ecdaef8415394c853ce9658ec0da0946f08ad92
    CRs-Fixed: 545637
    Signed-off-by: Mohit Aggarwal <maggarwa@codeaurora.org>

commit ac888f3ab478097ef19cb6375745c493204044be
Author: Ashay Jaiswal <ashayj@codeaurora.org>
Date:   Tue Jun 25 12:52:12 2013 +0530

    rtc: qpnp-rtc: Add module parameter to control power-on alarm
    
    Power-on alarm feature can be enabled/disabled by writing 1/0
    to the module parameter exported by driver.
    
    CRs-Fixed: 432312
    Change-Id: I2a2337adb2ddc186ec312648b11b6a35ea698856
    Signed-off-by: Ashay Jaiswal <ashayj@codeaurora.org>

commit cbe44c4f235890310c3c130351b99784dcc48fe0
Author: Ashay Jaiswal <ashayj@codeaurora.org>
Date:   Mon May 6 16:54:44 2013 +0530

    rtc: qpnp-rtc: Remove enable operation for RTC peripheral
    
    As RTC is already enabled from trustzone removing enable operation
    from RTC driver.
    
    Change-Id: I1891d2f35a9e595d30a1965d4cf20aedaf83d7d5
    Signed-off-by: Ashay Jaiswal <ashayj@codeaurora.org>

commit c7f2bdc69e4fd2184e8d5e23c71c3ea568dcdc99
Author: Matthew Qin <yqin@codeaurora.org>
Date:   Mon Dec 23 20:07:48 2013 +0800

    qpnp-rtc: clear alarm register when rtc irq is disabled
    
    The rtc alarm register should be cleared when the rtc irq is
    disabled
    
    Change-Id: I97a8bf989ff610093240a6b308a297702da6cb89
    Signed-off-by: Xiaocheng Li <lix@codeaurora.org>
    Signed-off-by: Matthew Qin <yqin@codeaurora.org>

commit 0afbdef5aacad4d9d429e955dde2746abf6f2766
Author: Matthew Qin <yqin@codeaurora.org>
Date:   Tue Dec 17 14:10:39 2013 +0800

    rtc: alarm: Add power-on alarm feature
    
    Android does not support powering up of phone through alarm.
    Adding shutdown hook in alarm driver which will set alarm while phone
    is going down so as to power-up the phone after alarm expiration.
    
    Change-Id: Ic2611e33ae9c1f8e83f21efdb93e26ac9f9499de
    Signed-off-by: Matthew Qin <yqin@codeaurora.org>

commit 601b8fa8848445eb4790169e29c08299fefbbe7f
Author: Dan Carpenter <dan.carpenter@oracle.com>
Date:   Tue Jun 4 16:13:25 2013 +0300

    staging: alarm-dev: information leak in alarm_compat_ioctl()
    
    If we pass an invalid clock type then "ts" is never set.  We need to
    check for errors earlier, otherwise we end up passing uninitialized
    stack data to userspace.
    
    Reported-by: John Stultz <john.stultz@linaro.org>
    Signed-off-by: Dan Carpenter <dan.carpenter@oracle.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit bd114dabf69cd2f713de2f1cf12ff661aeac6cfb
Author: Dan Carpenter <dan.carpenter@oracle.com>
Date:   Mon Jun 3 02:02:31 2013 -0700

    staging: alarm-dev: information leak in alarm_ioctl()
    
    Smatch complains that if we pass an invalid clock type then "ts" is
    never set.  We need to check for errors earlier, otherwise we end up
    passing uninitialized stack data to userspace.
    
    Signed-off-by: Dan Carpenter <dan.carpenter@oracle.com>
    Acked-by: John Stultz <john.stultz@linaro.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit c89a368c1b03166cbed3150fed4d4f47ce5c1f68
Author: John Stultz <john.stultz@linaro.org>
Date:   Fri Jan 11 15:46:24 2013 -0800

    staging: alarm-dev: Implement compat_ioctl support
    
    Implement compat_ioctl support for the alarm-dev ioctl.
    
    Cc: Serban Constantinescu <serban.constantinescu@arm.com>
    Cc: Arve Hjnnevg <arve@android.com>
    Cc: Colin Cross <ccross@google.com>
    Cc: Android Kernel Team <kernel-team@android.com>
    Signed-off-by: John Stultz <john.stultz@linaro.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 8115736b7df86cda9e666465d17fb2f4da2c527a
Author: John Stultz <john.stultz@linaro.org>
Date:   Fri Jan 11 15:46:23 2013 -0800

    staging: alarm-dev: Refactor alarm-dev ioctl code in prep for compat_ioctl
    
    Cleanup the Android alarm-dev driver's ioctl code to refactor it
    in preparation for compat_ioctl support.
    
    Cc: Serban Constantinescu <serban.constantinescu@arm.com>
    Cc: Arve Hjnnevg <arve@android.com>
    Cc: Colin Cross <ccross@google.com>
    Cc: Android Kernel Team <kernel-team@android.com>
    Signed-off-by: John Stultz <john.stultz@linaro.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit d717c78d7659ca6ae8e9b179d05aef68b25d1dc0
Author: John Stultz <john.stultz@linaro.org>
Date:   Fri Jan 11 15:46:22 2013 -0800

    staging: alarm-dev: Drop pre Android 1.0 _OLD ioctls
    
    Per Colin's comment:
    "The "support old userspace code" comment for those two ioctls has
    been there since pre-Android 1.0.  Those apis are not exposed to
    Android apps, I don't see any problem deleting them."
    
    Thus this patch removes the ANDROID_ALARM_SET_OLD and
    ANDROID_ALARM_SET_AND_WAIT_OLD ioctl compatability
    logic.
    
    Cc: Serban Constantinescu <serban.constantinescu@arm.com>
    Cc: Arve Hjnnevg <arve@android.com>
    Cc: Colin Cross <ccross@google.com>
    Cc: Android Kernel Team <kernel-team@android.com>
    Signed-off-by: John Stultz <john.stultz@linaro.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 43416d9526165d57f8665b6316f26e7d75c93820
Author: Dae S. Kim <dae@velatum.com>
Date:   Wed Aug 29 17:08:33 2012 +0200

    Staging: android: Alarm driver cleanups
    
    Little cleanups. Enum value ANDROID_ALARM_TYPE_COUNT was treated as
    an alarm type within a switch statement. That condition was unreachable
    though.
    
    Signed-off-by: Dae S. Kim <dae@velatum.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 237d66bb30ad84c4d0b39c7b0ecd8abcc1ed56ff
Author: Devendra Naga <devendra.aaru@gmail.com>
Date:   Sun May 20 00:23:26 2012 +0530

    staging: android: alarm: remove unnecessary goto statement
    
    Signed-off-by: Devendra Naga <devendra.aaru@gmail.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit aad9980f175009513192cf202bc81926c08850f1
Author: Sachin Kamat <sachin.kamat@linaro.org>
Date:   Tue Jun 5 15:05:13 2012 +0530

    staging: Android: Fix NULL pointer related warning in alarm-dev.c file
    
    Fixes the following sparse warning:
    drivers/staging/android/alarm-dev.c:259:35: warning: Using plain integer as NULL pointer
    
    Cc: Brian Swetland <swetland@google.com>
    Signed-off-by: Sachin Kamat <sachin.kamat@linaro.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 95822ef746eeecfa3a2dbd49c270bdde9dbf2b65
Author: Joe Perches <joe@perches.com>
Date:   Sun May 20 14:06:36 2012 -0700

    Staging: android: alarm: Rename pr_alarm to alarm_dbg
    
    Rename a macro to make it explicit it's for debugging.
    
    Use %s: __func__ instead of embedding function names.
    Coalesce formats, align arguments.
    
    Signed-off-by: Joe Perches <joe@perches.com>
    Acked-by: David Rientjes <rientjes@google.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit f7892422ed82aed12153cfea6bdbd3356072fe8b
Author: Xiaocheng Li <lix@codeaurora.org>
Date:   Wed Nov 6 18:58:05 2013 +0800

    alarmtimer: add rtc irq support for alarm
    
    Add the rtc irq support for alarmtimer to wakeup the
    alarm during system suspend.
    
    Change-Id: I41b774ed4e788359321e1c6a564551cc9cd40c8e
    Signed-off-by: Xiaocheng Li <lix@codeaurora.org>

commit e90454c6c7d35f34be19e1e1d12f68903fe1b8fc
Author: Todd Poynor <toddpoynor@google.com>
Date:   Tue May 7 20:43:29 2013 -0700

    alarmtimer: add alarm_expires_remaining
    
    Similar to hrtimer_expires_remaining, return the amount of time
    remaining until alarm expiry.
    
    Change-Id: I8c57512d619ac66bcdaf2d9ccdf0d7f74af2ff66
    Signed-off-by: Todd Poynor <toddpoynor@google.com>

commit 1bddaa0c967bf4ac7e1cd351effe4f28f2252d46
Author: Todd Poynor <toddpoynor@google.com>
Date:   Fri May 10 17:41:06 2013 -0700

    alarmtimer: add alarm_start_relative
    
    Start an alarmtimer with an expires time relative to the current time
    of the associated clock.
    
    Change-Id: Ifb5309a15e0d502bb4d0209ca5510a56ee7fa88c
    Signed-off-by: Todd Poynor <toddpoynor@google.com>

commit 09be7aebe580678ce15d1aa0ef650236c0afe614
Author: Todd Poynor <toddpoynor@google.com>
Date:   Fri May 10 17:10:42 2013 -0700

    alarmtimer: add alarm_forward_now
    
    Similar to hrtimer_forward_now, move the expires time forward to an
    interval from the current time of the associated clock.
    
    Change-Id: I73fed223321167507b6eddcb7a57d235ffcfc1be
    Signed-off-by: Todd Poynor <toddpoynor@google.com>

commit b6c8fee3be8a3b69e7c6af53959a8f6d07be8873
Author: Todd Poynor <toddpoynor@google.com>
Date:   Wed May 8 15:49:18 2013 -0700

    alarmtimer: add alarm_restart
    
    Analogous to hrtimer_restart, restart an alarmtimer after the expires
    time has already been updated (as with alarm_forward).
    
    Change-Id: Ia2613bbb467404cb2c35c11efa772bc56294963a
    Signed-off-by: Todd Poynor <toddpoynor@google.com>

commit eca148e11974995bebb4fdc12d3903e8b6b4c3fe
Author: John Stultz <john.stultz@linaro.org>
Date:   Thu Sep 13 19:25:22 2012 -0400

    alarmtimer: Rename alarmtimer_remove to alarmtimer_dequeue
    
    Now that alarmtimer_remove has been simplified, change
    its name to _dequeue to better match its paired _enqueue
    function.
    
    Cc: Arve Hjnnevg <arve@android.com>
    Cc: Colin Cross <ccross@android.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: John Stultz <john.stultz@linaro.org>

commit 8f6b627d577bc0a366edf082600844fb39e24a94
Author: John Stultz <john.stultz@linaro.org>
Date:   Thu Sep 13 19:12:16 2012 -0400

    alarmtimer: Use hrtimer per-alarm instead of per-base
    
    Arve Hjnnevg reported numerous crashes from the
    "BUG_ON(timer->state != HRTIMER_STATE_CALLBACK)" check
    in __run_hrtimer after it called alarmtimer_fired.
    
    It ends up the alarmtimer code was not properly handling
    possible failures of hrtimer_try_to_cancel, and because
    these faulres occur when the underlying base hrtimer is
    being run, this limits the ability to properly handle
    modifications to any alarmtimers on that base.
    
    Because much of the logic duplicates the hrtimer logic,
    it seems that we might as well have a per-alarmtimer
    hrtimer, and avoid the extra complextity of trying to
    multiplex many alarmtimers off of one hrtimer.
    
    Thus this patch moves the hrtimer to the alarm structure
    and simplifies the management logic.
    
    Changelog:
    v2:
    * Includes a fix for double alarm_start calls found by
      Arve
    
    Cc: Arve Hjnnevg <arve@android.com>
    Cc: Colin Cross <ccross@android.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Reported-by: Arve Hjnnevg <arve@android.com>
    Tested-by: Arve Hjnnevg <arve@android.com>
    Signed-off-by: John Stultz <john.stultz@linaro.org>

commit 18efda885dee3718892c8dca959d4c45d7c5fdfb
Author: Todd Poynor <toddpoynor@google.com>
Date:   Thu Aug 9 00:37:27 2012 -0700

    alarmtimer: Implement minimum alarm interval for allowing suspend
    
    alarmtimer suspend return -EBUSY if the next alarm will fire in less
    than 2 seconds.  This allows one RTC seconds tick to occur subsequent
    to this check before the alarm wakeup time is set, ensuring the wakeup
    time is still in the future (assuming the RTC does not tick one more
    second prior to setting the alarm).
    
    If suspend is rejected due to an imminent alarm, hold a wakeup source
    for 2 seconds to process the alarm prior to reattempting suspend.
    
    If setting the alarm incurs an -ETIME for an alarm set in the past,
    or any other problem setting the alarm, abort suspend and hold a
    wakelock for 1 second while the alarm is allowed to be serviced or
    other hopefully transient conditions preventing the alarm clear up.
    
    Signed-off-by: Todd Poynor <toddpoynor@google.com>
    Signed-off-by: John Stultz <john.stultz@linaro.org>

commit 2071cbc364a219231c8a224eb902b24a3171142e
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sat Mar 24 12:46:23 2012 +0100

    alarmtimer: Don't call rtc_timer_init() when CONFIG_RTC_CLASS=n
    
    rtc_timer_init() is not available when CONFIG_RTC_CLASS=n. Provide a
    proper wrapper in the RTC section of alarmtimer.c
    
    Reported-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: John Stultz <john.stultz@linaro.org>

commit 979f8635f1e1af1beb85b8769504e2a5f8b61c7d
Author: John Stultz <john.stultz@linaro.org>
Date:   Fri Mar 23 15:52:25 2012 -0700

    alarmtimer: Make sure we initialize the rtctimer
    
    jonghwan Choi reported seeing warnings with the alarmtimer
    code at suspend/resume time, and pointed out that the
    rtctimer isn't being properly initialized.
    
    This patch corrects this issue.
    
    Reported-by: jonghwan Choi <jhbird.choi@gmail.com>
    Signed-off-by: John Stultz <john.stultz@linaro.org>

commit 51c02920b9c37d5b35096d54b3cc9441cdc0e991
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Mon Mar 16 12:47:20 2015 +0100

     CHROMIUM: zram: finish fixing 32-bit overflow
    
    Nasty bug.  PAGE_MASK is unsigned, so it extends to 64 bit with zeros.
    The first read in the zram block device is done by mkswap, which
    tries to read at the end of device.  zram_set_disksize is a side
    effect of the first read, so the disk size is now silently changed
    from 6GB to 2GB.  The read fails, but mkswap does not complain either:
    it finds the size of the device by trial and error and continues.
    Swapon detects the mismatch and fails.
    
    BUG=chromium:245703
    TEST=ran before and after change with various printks
    Signed-off-by: Luigi Semenzato <semenzato@chromium.org>
    
    Change-Id: I01750554964756c2759375df1f14d7bb8b859ccf
    Reviewed-on: https://gerrit.chromium.org/gerrit/65752
    Tested-by: Luigi Semenzato <semenzato@chromium.org>
    Reviewed-by: Mike Frysinger <vapier@chromium.org>
    Commit-Queue: Luigi Semenzato <semenzato@chromium.org>

commit 585400544ac08227788f0d76b3bfa0f1f5aa4c14
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Mon Mar 16 12:46:54 2015 +0100

     CHROMIUM: zram: fix 32-bit overflow
    
    The 64-bit division operator is not available in 32-bit kernels,
    so have to use do_div.
    
    BUG=chromium:245703
    TEST=compiled
    
    Change-Id: I44ab763ea9e115cef6212f6524518cf7d9eac8c7
    Signed-off-by: Luigi Semenzato <semenzato@chromium.org>
    Reviewed-on: https://gerrit.chromium.org/gerrit/64695
    Reviewed-by: Doug Anderson <dianders@chromium.org>

commit 86f5300e99afaf7ec9bf7d5a6281460c9a182da8
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Mon Sep 14 14:45:13 2015 +0200

    fix zram

commit 6e6e56183e5669e27fd502077801eb04d31bbe6e
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Mon Sep 14 14:35:25 2015 +0200

    Revert " CHROMIUM: zram: fix 32-bit overflow"
    
    This reverts commit 4e7842448c6b30442c6e618aebf2552d9238fbc1.

commit 9f57d86eba3e9670f3f82777b7d740b3ea88e840
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Mon Sep 14 14:35:14 2015 +0200

    Revert " CHROMIUM: zram: finish fixing 32-bit overflow"
    
    This reverts commit 1700debb8a69f2a58381e3c14772fd18fb9afae7.

commit d2913d16c377ca6b188e72432214e2f0a40ddd0e
Author: Minchan Kim <minchan@kernel.org>
Date:   Fri Jun 8 15:39:25 2012 +0900

    staging: zsmalloc: zsmalloc: use unsigned long instead of void *
    
    We should use unsigned long as handle instead of void * to avoid any
    confusion. Without this, users may just treat zs_malloc return value as
    a pointer and try to deference it.
    
    This patch passed compile test(zram, zcache and ramster) and zram is
    tested on qemu.
    
    changelog
      * from v2
    	- remove hval pointed out by Nitin
    	- based on next-20120607
      * from v1
    	- change zcache's zv_create return value
    	- baesd on next-20120604
    
    Cc: Dan Magenheimer <dan.magenheimer@oracle.com>
    Acked-by: Seth Jennings <sjenning@linux.vnet.ibm.com>
    Acked-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Signed-off-by: Minchan Kim <minchan@kernel.org>
    Acked-by: Nitin Gupta <ngupta@vflare.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 27bc39e7e63187d5b677ea37bd3c11d5b4862215
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Mon Sep 14 14:30:13 2015 +0200

    update TW defconfig

commit 28759d4a863e1cef41f89f2895dcd7546b80a0a5
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Mon Sep 14 14:28:39 2015 +0200

    Revert "staging: zsmalloc: remove unused pool name"
    
    This reverts commit 35dd89e132a3b0c2447efc218eee692d464149de.

commit 9f1897bb43038234acf94fcd3520d021d5c5653f
Author: Amir Goldstein <amir@cellrox.com>
Date:   Mon Aug 4 19:29:32 2014 +0300

    sysrq: Emergency Remount R/O in reverse order
    
    This change fixes a problem where reboot on Android panics the kernel
    almost every time when file systems are mounted over loop devices.
    
    Android reboot command does:
    - sync
    - echo u > /proc/sysrq-trigger
    - syscall_reboot
    
    The problem is with sysrq emergency remount R/O trying to remount-ro
    in wrong order.
    since /data is re-mounted ro before loop devices, loop device
    remount-ro fails to flush the journal and panics the kernel:
    
      EXT4-fs (loop0): Remounting filesystem read-only
      EXT4-fs (loop0): previous I/O error to superblock detected
      loop: Write error at byte offset 0, length 4096.
      Buffer I/O error on device loop0, logical block 0
      lost page write due to I/O error on loop0
      Kernel panic - not syncing: EXT4-fs panic from previous error
    
    The fix is quite simple. In do_emergency_remount(), use
    list_for_each_entry_reverse() on sb list instead of list_for_each_entry().
    It makes a lot of sense to umount the file systems in reverse order in
    which they were added to sb list.
    
    Change-Id: I4370e39b5873bd16ade5d5f9ddb2704beb02a2bb
    Signed-off-by: Amir Goldstein <amir@cellrox.com>
    Acked-by: Oren Laadan <orenl@cellrox.com>

commit e3d37bbdf6128c8a2a1efeeea101e810d7b0748c
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Thu Sep 3 01:33:52 2015 -0500

    jf: f2fs: bring up to date with Jaegeuk's branch
    
    Up to date as of 19b2c30d3cce928010138cae4b9e57c388aa065c
    
    Change-Id: I3db64a656e18147654102ee1e13b7acc2e59d337

commit 1437060e00733cd7dd87361ed8ab2daec8f3d1bf
Author: M1cha <sigmaepsilon92@gmail.com>
Date:   Mon Jun 1 11:48:39 2015 +0200

    ptrace: fix ptrace defect cause by a merge fail
    
    this bug was introduced with I9493f28c30356a10eccb320e0a2d1a141388af9a
    
    Change-Id: Ib4e9866eb56f4370ca8d33761cf0cd4f795db5be
    Signed-off-by: M1cha <sigmaepsilon92@gmail.com>

commit d6b521032e284b33b18aeae0fc40861ccc64cae6
Author: Eric Dumazet <edumazet@google.com>
Date:   Sat May 30 09:16:53 2015 -0700

    udp: fix behavior of wrong checksums
    
    We have two problems in UDP stack related to bogus checksums :
    
    1) We return -EAGAIN to application even if receive queue is not empty.
       This breaks applications using edge trigger epoll()
    
    2) Under UDP flood, we can loop forever without yielding to other
       processes, potentially hanging the host, especially on non SMP.
    
    This patch is an attempt to make things better.
    
    We might in the future add extra support for rt applications
    wanting to better control time spent doing a recv() in a hostile
    environment. For example we could validate checksums before queuing
    packets in socket receive queue.
    
    Change-Id: Ia10d0047442995483b71d111f7eb1b4a51f45c92
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Cc: Willem de Bruijn <willemb@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 63b7269508e7cdf6044d467984de66c5c6676597
Author: Riley Andrews <riandrews@android.com>
Date:   Thu May 28 15:10:14 2015 -0700

    android: drivers: workaround debugfs race in binder
    
    If a /d/binder/proc/[pid] entry is kept open after linux has
    torn down the associated process, binder_proc_show can deference
    an invalid binder_proc that has been stashed in the debugfs
    inode.  Validate that the binder_proc ptr passed into binder_proc_show
    has not been freed by looking for it within the global process list
    whilst the global lock is held. If the ptr is not valid, print nothing.
    
    Bug 19587483
    Change-Id: I4abc6443d96cca6500608976cded5ff3d1697d33
    Signed-off-by: Riley Andrews <riandrews@android.com>

commit d95e62f104de3c9b5b3b0a46796013233284a36d
Author: Vignesh Radhakrishnan <vigneshr@codeaurora.org>
Date:   Fri Dec 19 16:01:07 2014 +0530

    irq_work: register irq_work_cpu_notify in early init
    
    Currently irq_work_cpu_notify is registered using
    device_initcall().
    
    In cases where CPU is hotplugged early (example
    would be thermal engine hotplugging CPU), there
    are chances where irq_work_cpu_notifier has not
    even registered, but CPU is already hotplugged out.
    irq_work uses CPU_DYING notifier to clear out the
    pending irq_work. But since the cpu notifier is not
    even registered that early, pending irq_work
    items are never run since this pending list is
    percpu.
    
    One specific scenario where this is impacting
    the system is, rcu framework using irq_work
    to wakeup and complete cleanup operations. In this
    scenario we notice that RCU operations needs cleanup
    on the hotplugged CPU.
    
    Fix this by registering irq_work_cpu_notify
    in early init.
    
    Signed-off-by: Prasad Sodagudi <psodagud@codeaurora.org>
    Signed-off-by: Vignesh Radhakrishnan <vigneshr@codeaurora.org>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 4c8ef08dea917811f9b528fccf1980f7fa221aae
Author: anish kumar <anish198519851985@gmail.com>
Date:   Sun Feb 3 22:08:23 2013 +0100

    irq_work: Remove return value from the irq_work_queue() function
    
    As no one is using the return value of irq_work_queue(),
    so it is better to just make it void.
    
    Signed-off-by: anish kumar <anish198519851985@gmail.com>
    Acked-by: Steven Rostedt <rostedt@goodmis.org>
    [ Fix stale comments, remove now unnecessary __irq_work_queue() intermediate function ]
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Link: http://lkml.kernel.org/r/1359925703-24304-1-git-send-email-fweisbec@gmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 9a080b9a603543400080374cdc1e1c93e7c3779c
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Fri Oct 19 16:43:41 2012 -0400

    irq_work: Make self-IPIs optable
    
    On irq work initialization, let the user choose to define it
    as "lazy" or not. "Lazy" means that we don't want to send
    an IPI (provided the arch can anyway) when we enqueue this
    work but we rather prefer to wait for the next timer tick
    to execute our work if possible.
    
    This is going to be a benefit for non-urgent enqueuers
    (like printk in the future) that may prefer not to raise
    an IPI storm in case of frequent enqueuing on short periods
    of time.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Acked-by: Steven Rostedt <rostedt@goodmis.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Paul Gortmaker <paul.gortmaker@windriver.com>

commit ebf4824054b00576e31652b2097f5a05d92ddbb2
Author: Steven Rostedt <rostedt@goodmis.org>
Date:   Thu Nov 15 12:52:44 2012 -0500

    irq_work: Warn if there's still work on cpu_down
    
    If we are in nohz and there's still irq_work to be done when the idle
    task is about to go offline, give a nasty warning. Everything should
    have been flushed from the CPU_DYING notifier already. Further attempts
    to enqueue an irq_work are buggy because irqs are disabled by
    __cpu_disable(). The best we can do is to report the issue to the user.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Paul Gortmaker <paul.gortmaker@windriver.com>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>

commit 273932980df943d98e5286a3222c2dbfb71fdc28
Author: Steven Rostedt <rostedt@goodmis.org>
Date:   Thu Nov 15 11:34:21 2012 -0500

    irq_work: Flush work on CPU_DYING
    
    In order not to offline a CPU with pending irq works, flush the
    queue from CPU_DYING. The notifier is called by stop_machine on
    the CPU that is going down. The code will not be called from irq context
    (so things like get_irq_regs() wont work) but I'm not sure what the
    requirements are for irq_work in that regard (Peter?). But irqs are
    disabled and the CPU is about to go offline. Might as well flush the work.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Paul Gortmaker <paul.gortmaker@windriver.com>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>

commit 5ecfd24a22801250a0e726c43dda3db4cbce4169
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Sat Oct 27 15:21:36 2012 +0200

    irq_work: Fix racy check on work pending flag
    
    Work claiming wants to be SMP-safe.
    
    And by the time we try to claim a work, if it is already executing
    concurrently on another CPU, we want to succeed the claiming and queue
    the work again because the other CPU may have missed the data we wanted
    to handle in our work if it's about to complete there.
    
    This scenario is summarized below:
    
            CPU 1                                   CPU 2
            -----                                   -----
            (flags = 0)
            cmpxchg(flags, 0, IRQ_WORK_FLAGS)
            (flags = 3)
            [...]
            xchg(flags, IRQ_WORK_BUSY)
            (flags = 2)
            func()
                                                    if (flags & IRQ_WORK_PENDING)
                                                            (not true)
                                                    cmpxchg(flags, flags, IRQ_WORK_FLAGS)
                                                    (flags = 3)
                                                    [...]
            cmpxchg(flags, IRQ_WORK_BUSY, 0);
            (fail, pending on CPU 2)
    
    This state machine is synchronized using [cmp]xchg() on the flags.
    As such, the early IRQ_WORK_PENDING check in CPU 2 above is racy.
    By the time we check it, we may be dealing with a stale value because
    we aren't using an atomic accessor. As a result, CPU 2 may "see"
    that the work is still pending on another CPU while it may be
    actually completing the work function exection already, leaving
    our data unprocessed.
    
    To fix this, we start by speculating about the value we wish to be
    in the work->flags but we only make any conclusion after the value
    returned by the cmpxchg() call that either claims the work or let
    the current owner handle the pending work for us.
    
    Changelog-heavily-inspired-by: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Acked-by: Steven Rostedt <rostedt@goodmis.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Paul Gortmaker <paul.gortmaker@windriver.com>
    Cc: Anish Kumar <anish198519851985@gmail.com>

commit 0b64c9a5c3fecaee5768849bcb7992ca858a21b9
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Tue Oct 30 13:33:54 2012 +0100

    irq_work: Fix racy IRQ_WORK_BUSY flag setting
    
    The IRQ_WORK_BUSY flag is set right before we execute the
    work. Once this flag value is set, the work enters a
    claimable state again.
    
    So if we have specific data to compute in our work, we ensure it's
    either handled by another CPU or locally by enqueuing the work again.
    This state machine is guanranteed by atomic operations on the flags.
    
    So when we set IRQ_WORK_BUSY without using an xchg-like operation,
    we break this guarantee as in the following summarized scenario:
    
            CPU 1                                   CPU 2
            -----                                   -----
                                                    (flags = 0)
                                                    old_flags = flags;
            (flags = 0)
            cmpxchg(flags, old_flags,
                    old_flags | IRQ_WORK_FLAGS)
            (flags = 3)
            [...]
            flags = IRQ_WORK_BUSY
            (flags = 2)
            func()
                                                    (sees flags = 3)
                                                    cmpxchg(flags, old_flags,
                                                            old_flags | IRQ_WORK_FLAGS)
                                                    (give up)
    
            cmpxchg(flags, 2, 0);
            (flags = 0)
    
    CPU 1 claims a work and executes it, so it sets IRQ_WORK_BUSY and
    the work is again in a claimable state. Now CPU 2 has new data to process
    and try to claim that work but it may see a stale value of the flags
    and think the work is still pending somewhere that will handle our data.
    This is because CPU 1 doesn't set IRQ_WORK_BUSY atomically.
    
    As a result, the data expected to be handle by CPU 2 won't get handled.
    
    To fix this, use xchg() to set IRQ_WORK_BUSY, this way we ensure the CPU 2
    will see the correct value with cmpxchg() using the expected ordering.
    
    Changelog-heavily-inspired-by: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Acked-by: Steven Rostedt <rostedt@goodmis.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Paul Gortmaker <paul.gortmaker@windriver.com>
    Cc: Anish Kumar <anish198519851985@gmail.com>

commit 8972849ee6e5da5485f8ac63f72746276cea4da0
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Wed Nov 7 21:03:07 2012 +0100

    irq_work: Don't stop the tick with pending works
    
    Don't stop the tick if we have pending irq works on the
    queue, otherwise if the arch can't raise self-IPIs, we may not
    find an opportunity to execute the pending works for a while.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Acked-by: Steven Rostedt <rostedt@goodmis.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Paul Gortmaker <paul.gortmaker@windriver.com>

commit 49ffb741f122330566269e62ca460896653156b2
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Thu Oct 11 17:52:56 2012 +0200

    nohz: Add API to check tick state
    
    We need some quick way to check if the CPU has stopped
    its tick. This will be useful to implement the printk tick
    using the irq work subsystem.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Acked-by: Steven Rostedt <rostedt@goodmis.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Paul Gortmaker <paul.gortmaker@windriver.com>

commit eafcf53226a4fc916980f2acf8546b46dec79d94
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Thu Oct 4 01:46:44 2012 +0200

    nohz: Fix one jiffy count too far in idle cputime
    
    When we stop the tick in idle, we save the current jiffies value
    in ts->idle_jiffies. This snapshot is substracted from the later
    value of jiffies when the tick is restarted and the resulting
    delta is accounted as idle cputime. This is how we handle the
    idle cputime accounting without the tick.
    
    But sometimes we need to schedule the next tick to some time in
    the future instead of completely stopping it. In this case, a
    tick may happen before we restart the periodic behaviour and
    from that tick we account one jiffy to idle cputime as usual but
    we also increment the ts->idle_jiffies snapshot by one so that
    when we compute the delta to account, we substract the one jiffy
    we just accounted.
    
    To prepare for stopping the tick outside idle, we introduced a
    check that prevents from fixing up that ts->idle_jiffies if we
    are not running the idle task. But we use idle_cpu() for that
    and this is a problem if we run the tick while another CPU
    remotely enqueues a ttwu to our runqueue:
    
    CPU 0:                            CPU 1:
    
    tick_sched_timer() {              ttwu_queue_remote()
           if (idle_cpu(CPU 0))
               ts->idle_jiffies++;
    }
    
    Here, idle_cpu() notes that &rq->wake_list is not empty and
    hence won't consider the CPU as idle. As a result,
    ts->idle_jiffies won't be incremented. But this is wrong because
    we actually account the current jiffy to idle cputime. And that
    jiffy won't get substracted from the nohz time delta. So in the
    end, this jiffy is accounted twice.
    
    Fix this by changing idle_cpu(smp_processor_id()) with
    is_idle_task(current). This way the jiffy is substracted
    correctly even if a ttwu operation is enqueued on the CPU.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: <stable@vger.kernel.org> # 3.5+
    Link: http://lkml.kernel.org/r/1349308004-3482-1-git-send-email-fweisbec@gmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit f45de180f73e5fec18ffd95bd51d98e210c6ae23
Author: Paul E. McKenney <paul.mckenney@linaro.org>
Date:   Thu Aug 23 08:34:07 2012 -0700

    time: RCU permitted to stop idle entry via softirq
    
    The can_stop_idle_tick() function complains if a softirq vector is
    raised too late in the idle-entry process, presumably in order to
    prevent dangling softirq invocations from being delayed across the
    full idle period, which might be indefinitely long -- and if softirq
    was asserted any later than the call to this function, such a delay
    might well happen.
    
    However, RCU needs to be able to use softirq to stop idle entry in
    order to be able to drain RCU callbacks from the current CPU, which in
    turn enables faster entry into dyntick-idle mode, which in turn reduces
    power consumption.  Because RCU takes this action at a well-defined
    point in the idle-entry path, it is safe for RCU to take this approach.
    
    This commit therefore silences the error message that is sometimes
    produced when the going-idle CPU suddenly finds that it has an RCU_SOFTIRQ
    to process.  The error message will continue to be issued for other
    softirq vectors.
    
    Reported-by: Sedat Dilek <sedat.dilek@gmail.com>
    Signed-off-by: Paul E. McKenney <paul.mckenney@linaro.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Tested-by: Sedat Dilek <sedat.dilek@gmail.com>
    Reviewed-by: Josh Triplett <josh@joshtriplett.org>

commit d5fc4d73e8fb170672631b809084ef699b32aad4
Author: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
Date:   Fri Feb 14 11:29:08 2014 +0530

    CPU hotplug: Provide lockless versions of callback registration functions
    
    The following method of CPU hotplug callback registration is not safe
    due to the possibility of an ABBA deadlock involving the cpu_add_remove_lock
    and the cpu_hotplug.lock.
    
    	get_online_cpus();
    
    	for_each_online_cpu(cpu)
    		init_cpu(cpu);
    
    	register_cpu_notifier(&foobar_cpu_notifier);
    
    	put_online_cpus();
    
    The deadlock is shown below:
    
              CPU 0                                         CPU 1
              -----                                         -----
    
       Acquire cpu_hotplug.lock
       [via get_online_cpus()]
    
                                                  CPU online/offline operation
                                                  takes cpu_add_remove_lock
                                                  [via cpu_maps_update_begin()]
    
       Try to acquire
       cpu_add_remove_lock
       [via register_cpu_notifier()]
    
                                                  CPU online/offline operation
                                                  tries to acquire cpu_hotplug.lock
                                                  [via cpu_hotplug_begin()]
    
                                *** DEADLOCK! ***
    
    The problem here is that callback registration takes the locks in one order
    whereas the CPU hotplug operations take the same locks in the opposite order.
    To avoid this issue and to provide a race-free method to register CPU hotplug
    callbacks (along with initialization of already online CPUs), introduce new
    variants of the callback registration APIs that simply register the callbacks
    without holding the cpu_add_remove_lock during the registration. That way,
    we can avoid the ABBA scenario. However, we will need to hold the
    cpu_add_remove_lock throughout the entire critical section, to protect updates
    to the callback/notifier chain.
    
    This can be achieved by writing the callback registration code as follows:
    
    	cpu_maps_update_begin(); [ or cpu_notifier_register_begin(); see below ]
    
    	for_each_online_cpu(cpu)
    		init_cpu(cpu);
    
    	/* This doesn't take the cpu_add_remove_lock */
    	__register_cpu_notifier(&foobar_cpu_notifier);
    
    	cpu_maps_update_done();  [ or cpu_notifier_register_done(); see below ]
    
    Note that we can't use get_online_cpus() here instead of cpu_maps_update_begin()
    because the cpu_hotplug.lock is dropped during the invocation of CPU_POST_DEAD
    notifiers, and hence get_online_cpus() cannot provide the necessary
    synchronization to protect the callback/notifier chains against concurrent
    reads and writes. On the other hand, since the cpu_add_remove_lock protects
    the entire hotplug operation (including CPU_POST_DEAD), we can use
    cpu_maps_update_begin/done() to guarantee proper synchronization.
    
    Also, since cpu_maps_update_begin/done() is like a super-set of
    get/put_online_cpus(), the former naturally protects the critical sections
    from concurrent hotplug operations.
    
    Since the names cpu_maps_update_begin/done() don't make much sense in CPU
    hotplug callback registration scenarios, we'll introduce new APIs named
    cpu_notifier_register_begin/done() and map them to cpu_maps_update_begin/done().
    
    In summary, introduce the lockless variants of un/register_cpu_notifier() and
    also export the cpu_notifier_register_begin/done() APIs for use by modules.
    This way, we provide a race-free way to register hotplug callbacks as well as
    perform initialization for the CPUs that are already online.
    
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Toshi Kani <toshi.kani@hp.com>
    Cc: "Rafael J. Wysocki" <rjw@rjwysocki.net>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Ingo Molnar <mingo@kernel.org>
    Acked-by: Oleg Nesterov <oleg@redhat.com>
    Signed-off-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Git-commit: 93ae4f978ca7f26d17df915ac7afc919c1dd0353
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Osvaldo Banuelos <osvaldob@codeaurora.org>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit d37d2c60bea0146ea8b7ab67c191e8df4448f938
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Wed Nov 21 16:26:44 2012 +0100

    cputime: Rename thread_group_times to thread_group_cputime_adjusted
    
    We have thread_group_cputime() and thread_group_times(). The naming
    doesn't provide enough information about the difference between
    these two APIs.
    
    To lower the confusion, rename thread_group_times() to
    thread_group_cputime_adjusted(). This name better suggests that
    it's a version of thread_group_cputime() that does some stabilization
    on the raw cputime values. ie here: scale on top of CFS runtime
    stats and bound lower value for monotonicity.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Paul Gortmaker <paul.gortmaker@windriver.com>

commit 00c0a4ace3f4d992b0240d9009d46d303af931e2
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Wed Nov 21 15:55:59 2012 +0100

    cputime: Move thread_group_cputime() to sched code
    
    thread_group_cputime() is a general cputime API that is not only
    used by posix cpu timer. Let's move this helper to sched code.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Paul Gortmaker <paul.gortmaker@windriver.com>

commit 55ee7e280f7c3f69f01037ddf7e1e0eadbc7aae0
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Sat Oct 6 05:23:22 2012 +0200

    cputime: Separate irqtime accounting from generic vtime
    
    vtime_account() doesn't have the same role in
    CONFIG_VIRT_CPU_ACCOUNTING and CONFIG_IRQ_TIME_ACCOUNTING.
    
    In the first case it handles time accounting in any context. In
    the second case it only handles irq time accounting.
    
    So when vtime_account() is called from outside vtime_account_irq_*()
    this call is pointless to CONFIG_IRQ_TIME_ACCOUNTING.
    
    To fix the confusion, change vtime_account() to irqtime_account_irq()
    in CONFIG_IRQ_TIME_ACCOUNTING. This way we ensure future account_vtime()
    calls won't waste useless cycles in the irqtime APIs.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Paul Gortmaker <paul.gortmaker@windriver.com>

commit 599645ec0e675efcc370ae476cc4393f0e01effb
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Sat Oct 6 04:07:19 2012 +0200

    cputime: Specialize irq vtime hooks
    
    With CONFIG_VIRT_CPU_ACCOUNTING, when vtime_account()
    is called in irq entry/exit, we perform a check on the
    context: if we are interrupting the idle task we
    account the pending cputime to idle, otherwise account
    to system time or its sub-areas: tsk->stime, hardirq time,
    softirq time, ...
    
    However this check for idle only concerns the hardirq entry
    and softirq entry:
    
    * Hardirq may directly interrupt the idle task, in which case
    we need to flush the pending CPU time to idle.
    
    * The idle task may be directly interrupted by a softirq if
    it calls local_bh_enable(). There is probably no such call
    in any idle task but we need to cover every case. Ksoftirqd
    is not concerned because the idle time is flushed on context
    switch and softirq in the end of hardirq have the idle time
    already flushed from the hardirq entry.
    
    In the other cases we always account to system/irq time:
    
    * On hardirq exit we account the time to hardirq time.
    * On softirq exit we account the time to softirq time.
    
    To optimize this and avoid the indirect call to vtime_account()
    and the checks it performs, specialize the vtime irq APIs and
    only perform the check on irq entry. Irq exit can directly call
    vtime_account_system().
    
    CONFIG_IRQ_TIME_ACCOUNTING behaviour doesn't change and directly
    maps to its own vtime_account() implementation. One may want
    to take benefits from the new APIs to optimize irq time accounting
    as well in the future.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Paul Gortmaker <paul.gortmaker@windriver.com>

commit 6f6f46c4fd61c735b677a978b51c89a7f99073c4
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Wed Oct 24 18:05:51 2012 +0200

    vtime: Make vtime_account_system() irqsafe
    
    vtime_account_system() currently has only one caller with
    vtime_account() which is irq safe.
    
    Now we are going to call it from other places like kvm where
    irqs are not always disabled by the time we account the cputime.
    
    So let's make it irqsafe. The arch implementation part is now
    prefixed with "__".
    
    vtime_account_idle() arch implementation is prefixed accordingly
    to stay consistent.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Paul Gortmaker <paul.gortmaker@windriver.com>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>

commit f5683bd7bad71fccaa46644f45c55e36bfa3bba3
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Fri Oct 5 23:07:19 2012 +0200

    vtime: Gather vtime declarations to their own header file
    
    These APIs are scattered around and are going to expand a bit.
    Let's create a dedicated header file for sanity.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Paul Gortmaker <paul.gortmaker@windriver.com>

commit a4c2d2290cb39c92f3fee30979703fd3c4c96d64
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue Jul 3 13:53:26 2012 +0200

    sched: Describe CFS load-balancer
    
    Add some scribbles on how and why the load-balancer works..
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/1341316406.23484.64.camel@twins
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 9560eb4191c95dcd8501d06576ab9cd939459bd5
Author: Paul Turner <pjt@google.com>
Date:   Thu Oct 4 13:18:32 2012 +0200

    sched: Introduce temporary FAIR_GROUP_SCHED dependency for load-tracking
    
    While per-entity load-tracking is generally useful, beyond computing shares
    distribution, e.g. runnable based load-balance (in progress), governors,
    power-management, etc.
    
    These facilities are not yet consumers of this data.  This may be trivially
    reverted when the information is required; but avoid paying the overhead for
    calculations we will not use until then.
    
    Signed-off-by: Paul Turner <pjt@google.com>
    Reviewed-by: Ben Segall <bsegall@google.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/20120823141507.422162369@google.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 93506dd6b5e1d6e3cc4bc64e1d3aafba977d4a58
Author: Paul Turner <pjt@google.com>
Date:   Thu Oct 4 13:18:32 2012 +0200

    sched: Make __update_entity_runnable_avg() fast
    
    __update_entity_runnable_avg forms the core of maintaining an entity's runnable
    load average.  In this function we charge the accumulated run-time since last
    update and handle appropriate decay.  In some cases, e.g. a waking task, this
    time interval may be much larger than our period unit.
    
    Fortunately we can exploit some properties of our series to perform decay for a
    blocked update in constant time and account the contribution for a running
    update in essentially-constant* time.
    
    [*]: For any running entity they should be performing updates at the tick which
    gives us a soft limit of 1 jiffy between updates, and we can compute up to a
    32 jiffy update in a single pass.
    
    C program to generate the magic constants in the arrays:
    
      #include <math.h>
      #include <stdio.h>
    
      #define N 32
      #define WMULT_SHIFT 32
    
      const long WMULT_CONST = ((1UL << N) - 1);
      double y;
    
      long runnable_avg_yN_inv[N];
      void calc_mult_inv() {
      	int i;
      	double yn = 0;
    
      	printf("inverses\n");
      	for (i = 0; i < N; i++) {
      		yn = (double)WMULT_CONST * pow(y, i);
      		runnable_avg_yN_inv[i] = yn;
      		printf("%2d: 0x%8lx\n", i, runnable_avg_yN_inv[i]);
      	}
      	printf("\n");
      }
    
      long mult_inv(long c, int n) {
      	return (c * runnable_avg_yN_inv[n]) >>  WMULT_SHIFT;
      }
    
      void calc_yn_sum(int n)
      {
      	int i;
      	double sum = 0, sum_fl = 0, diff = 0;
    
      	/*
      	 * We take the floored sum to ensure the sum of partial sums is never
      	 * larger than the actual sum.
      	 */
      	printf("sum y^n\n");
      	printf("   %8s  %8s %8s\n", "exact", "floor", "error");
      	for (i = 1; i <= n; i++) {
      		sum = (y * sum + y * 1024);
      		sum_fl = floor(y * sum_fl+ y * 1024);
      		printf("%2d: %8.0f  %8.0f %8.0f\n", i, sum, sum_fl,
      			sum_fl - sum);
      	}
      	printf("\n");
      }
    
      void calc_conv(long n) {
      	long old_n;
      	int i = -1;
    
      	printf("convergence (LOAD_AVG_MAX, LOAD_AVG_MAX_N)\n");
      	do {
      		old_n = n;
      		n = mult_inv(n, 1) + 1024;
      		i++;
      	} while (n != old_n);
      	printf("%d> %ld\n", i - 1, n);
      	printf("\n");
      }
    
      void main() {
      	y = pow(0.5, 1/(double)N);
      	calc_mult_inv();
      	calc_conv(1024);
      	calc_yn_sum(N);
      }
    
    [ Compile with -lm ]
    Signed-off-by: Paul Turner <pjt@google.com>
    Reviewed-by: Ben Segall <bsegall@google.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/20120823141507.277808946@google.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 37cfb42c5fcb4405d18154e2198a25c12f4ffd67
Author: Paul Turner <pjt@google.com>
Date:   Thu Oct 4 13:18:31 2012 +0200

    sched: Update_cfs_shares at period edge
    
    Now that our measurement intervals are small (~1ms) we can amortize the posting
    of update_shares() to be about each period overflow.  This is a large cost
    saving for frequently switching tasks.
    
    Signed-off-by: Paul Turner <pjt@google.com>
    Reviewed-by: Ben Segall <bsegall@google.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/20120823141507.200772172@google.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit ff11b630a8fe34c0cf441293b8d96c7b04d68181
Author: Paul Turner <pjt@google.com>
Date:   Thu Oct 4 13:18:31 2012 +0200

    sched: Refactor update_shares_cpu() -> update_blocked_avgs()
    
    Now that running entities maintain their own load-averages the work we must do
    in update_shares() is largely restricted to the periodic decay of blocked
    entities.  This allows us to be a little less pessimistic regarding our
    occupancy on rq->lock and the associated rq->clock updates required.
    
    Signed-off-by: Paul Turner <pjt@google.com>
    Reviewed-by: Ben Segall <bsegall@google.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/20120823141507.133999170@google.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit c6456d3a3aa64e20ab967bb1f129145efe4a1ae6
Author: Paul Turner <pjt@google.com>
Date:   Thu Oct 4 13:18:31 2012 +0200

    sched: Replace update_shares weight distribution with per-entity computation
    
    Now that the machinery in place is in place to compute contributed load in a
    bottom up fashion; replace the shares distribution code within update_shares()
    accordingly.
    
    Signed-off-by: Paul Turner <pjt@google.com>
    Reviewed-by: Ben Segall <bsegall@google.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/20120823141507.061208672@google.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit a94478c48b527918cc139ad4e144afb00b5e1f65
Author: Paul Turner <pjt@google.com>
Date:   Thu Oct 4 13:18:31 2012 +0200

    sched: Maintain runnable averages across throttled periods
    
    With bandwidth control tracked entities may cease execution according to user
    specified bandwidth limits.  Charging this time as either throttled or blocked
    however, is incorrect and would falsely skew in either direction.
    
    What we actually want is for any throttled periods to be "invisible" to
    load-tracking as they are removed from the system for that interval and
    contribute normally otherwise.
    
    Do this by moderating the progression of time to omit any periods in which the
    entity belonged to a throttled hierarchy.
    
    Signed-off-by: Paul Turner <pjt@google.com>
    Reviewed-by: Ben Segall <bsegall@google.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/20120823141506.998912151@google.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 284e6483758a383bf3514ac30e1081943c23f2af
Author: Paul Turner <pjt@google.com>
Date:   Thu Oct 4 13:18:31 2012 +0200

    sched: Normalize tg load contributions against runnable time
    
    Entities of equal weight should receive equitable distribution of cpu time.
    This is challenging in the case of a task_group's shares as execution may be
    occurring on multiple cpus simultaneously.
    
    To handle this we divide up the shares into weights proportionate with the load
    on each cfs_rq.  This does not however, account for the fact that the sum of
    the parts may be less than one cpu and so we need to normalize:
      load(tg) = min(runnable_avg(tg), 1) * tg->shares
    Where runnable_avg is the aggregate time in which the task_group had runnable
    children.
    
    Signed-off-by: Paul Turner <pjt@google.com>
    Reviewed-by: Ben Segall <bsegall@google.com>.
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/20120823141506.930124292@google.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 799f001bd582f9dd8c305806326d1123422fd86f
Author: Paul Turner <pjt@google.com>
Date:   Thu Oct 4 13:18:31 2012 +0200

    sched: Compute load contribution by a group entity
    
    Unlike task entities who have a fixed weight, group entities instead own a
    fraction of their parenting task_group's shares as their contributed weight.
    
    Compute this fraction so that we can correctly account hierarchies and shared
    entity nodes.
    
    Signed-off-by: Paul Turner <pjt@google.com>
    Reviewed-by: Ben Segall <bsegall@google.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/20120823141506.855074415@google.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 2bbce7d07d3ad32d3d4585ecdae2df435358f7e5
Author: Paul Turner <pjt@google.com>
Date:   Thu Oct 4 13:18:30 2012 +0200

    sched: Aggregate total task_group load
    
    Maintain a global running sum of the average load seen on each cfs_rq belonging
    to each task group so that it may be used in calculating an appropriate
    shares:weight distribution.
    
    Signed-off-by: Paul Turner <pjt@google.com>
    Reviewed-by: Ben Segall <bsegall@google.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/20120823141506.792901086@google.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 35e99e23d2fd66d98ca059810f270fbfc73f0d16
Author: Paul Turner <pjt@google.com>
Date:   Thu Oct 4 13:18:30 2012 +0200

    sched: Account for blocked load waking back up
    
    When a running entity blocks we migrate its tracked load to
    cfs_rq->blocked_runnable_avg.  In the sleep case this occurs while holding
    rq->lock and so is a natural transition.  Wake-ups however, are potentially
    asynchronous in the presence of migration and so special care must be taken.
    
    We use an atomic counter to track such migrated load, taking care to match this
    with the previously introduced decay counters so that we don't migrate too much
    load.
    
    Signed-off-by: Paul Turner <pjt@google.com>
    Reviewed-by: Ben Segall <bsegall@google.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/20120823141506.726077467@google.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 1f10b31194750049c27e0fb283e1b2b38931a1bf
Author: Paul Turner <pjt@google.com>
Date:   Thu Oct 4 13:18:30 2012 +0200

    sched: Add an rq migration call-back to sched_class
    
    Since we are now doing bottom up load accumulation we need explicit
    notification when a task has been re-parented so that the old hierarchy can be
    updated.
    
    Adds: migrate_task_rq(struct task_struct *p, int next_cpu)
    
    (The alternative is to do this out of __set_task_cpu, but it was suggested that
    this would be a cleaner encapsulation.)
    
    Signed-off-by: Paul Turner <pjt@google.com>
    Reviewed-by: Ben Segall <bsegall@google.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/20120823141506.660023400@google.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 287e7ab0b4c0e9434c416a139422c76f211806c0
Author: Paul Turner <pjt@google.com>
Date:   Thu Oct 4 13:18:30 2012 +0200

    sched: Maintain the load contribution of blocked entities
    
    We are currently maintaining:
    
      runnable_load(cfs_rq) = \Sum task_load(t)
    
    For all running children t of cfs_rq.  While this can be naturally updated for
    tasks in a runnable state (as they are scheduled); this does not account for
    the load contributed by blocked task entities.
    
    This can be solved by introducing a separate accounting for blocked load:
    
      blocked_load(cfs_rq) = \Sum runnable(b) * weight(b)
    
    Obviously we do not want to iterate over all blocked entities to account for
    their decay, we instead observe that:
    
      runnable_load(t) = \Sum p_i*y^i
    
    and that to account for an additional idle period we only need to compute:
    
      y*runnable_load(t).
    
    This means that we can compute all blocked entities at once by evaluating:
    
      blocked_load(cfs_rq)` = y * blocked_load(cfs_rq)
    
    Finally we maintain a decay counter so that when a sleeping entity re-awakens
    we can determine how much of its load should be removed from the blocked sum.
    
    Signed-off-by: Paul Turner <pjt@google.com>
    Reviewed-by: Ben Segall <bsegall@google.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/20120823141506.585389902@google.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit a9630b55a664e21f1a91d7f1a24f72afde347b01
Author: Paul Turner <pjt@google.com>
Date:   Thu Oct 4 13:18:30 2012 +0200

    sched: Aggregate load contributed by task entities on parenting cfs_rq
    
    For a given task t, we can compute its contribution to load as:
    
      task_load(t) = runnable_avg(t) * weight(t)
    
    On a parenting cfs_rq we can then aggregate:
    
      runnable_load(cfs_rq) = \Sum task_load(t), for all runnable children t
    
    Maintain this bottom up, with task entities adding their contributed load to
    the parenting cfs_rq sum.  When a task entity's load changes we add the same
    delta to the maintained sum.
    
    Signed-off-by: Paul Turner <pjt@google.com>
    Reviewed-by: Ben Segall <bsegall@google.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/20120823141506.514678907@google.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit deeff266395d1758266a60e77b35a8f44b880d6e
Author: Ben Segall <bsegall@google.com>
Date:   Thu Oct 4 12:51:20 2012 +0200

    sched: Maintain per-rq runnable averages
    
    Since runqueues do not have a corresponding sched_entity we instead embed a
    sched_avg structure directly.
    
    Signed-off-by: Ben Segall <bsegall@google.com>
    Reviewed-by: Paul Turner <pjt@google.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/20120823141506.442637130@google.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 86562201a5f487a6eaa99bebdfdeffcd47609657
Author: Paul Turner <pjt@google.com>
Date:   Thu Oct 4 13:18:29 2012 +0200

    sched: Track the runnable average on a per-task entity basis
    
    Instead of tracking averaging the load parented by a cfs_rq, we can track
    entity load directly. With the load for a given cfs_rq then being the sum
    of its children.
    
    To do this we represent the historical contribution to runnable average
    within each trailing 1024us of execution as the coefficients of a
    geometric series.
    
    We can express this for a given task t as:
    
      runnable_sum(t) = \Sum u_i * y^i, runnable_avg_period(t) = \Sum 1024 * y^i
      load(t) = weight_t * runnable_sum(t) / runnable_avg_period(t)
    
    Where: u_i is the usage in the last i`th 1024us period (approximately 1ms)
    ~ms and y is chosen such that y^k = 1/2.  We currently choose k to be 32 which
    roughly translates to about a sched period.
    
    Signed-off-by: Paul Turner <pjt@google.com>
    Reviewed-by: Ben Segall <bsegall@google.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/20120823141506.372695337@google.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    
    Conflicts:
    	kernel/sched/fair.c

commit 26fea0ad806e9c2efb4f428c42465b589fb0cb3c
Author: Ingo Molnar <mingo@elte.hu>
Date:   Sun Oct 14 14:28:50 2012 +0200

    sched: Add WAKEUP_PREEMPTION feature flag, on by default
    
    As per the recent discussion with Mike and Linus, make it easier to
    test with/without this feature. No change in default behavior.
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Link: http://lkml.kernel.org/n/tip-izoxq4haeg4mTognnDbwcevt@git.kernel.org

commit 71ad6ec5cb0a38cf329b946912b0a7189b10e672
Author: Tang Chen <tangchen@cn.fujitsu.com>
Date:   Tue Sep 25 21:12:31 2012 +0800

    sched: Update sched_domains_numa_masks[][] when new cpus are onlined
    
    Once array sched_domains_numa_masks[] []is defined, it is never updated.
    
    When a new cpu on a new node is onlined, the coincident member in
    sched_domains_numa_masks[][] is not initialized, and all the masks are 0.
    As a result, the build_overlap_sched_groups() will initialize a NULL
    sched_group for the new cpu on the new node, which will lead to kernel panic:
    
    [ 3189.403280] Call Trace:
    [ 3189.403286]  [<ffffffff8106c36f>] warn_slowpath_common+0x7f/0xc0
    [ 3189.403289]  [<ffffffff8106c3ca>] warn_slowpath_null+0x1a/0x20
    [ 3189.403292]  [<ffffffff810b1d57>] build_sched_domains+0x467/0x470
    [ 3189.403296]  [<ffffffff810b2067>] partition_sched_domains+0x307/0x510
    [ 3189.403299]  [<ffffffff810b1ea2>] ? partition_sched_domains+0x142/0x510
    [ 3189.403305]  [<ffffffff810fcc93>] cpuset_update_active_cpus+0x83/0x90
    [ 3189.403308]  [<ffffffff810b22a8>] cpuset_cpu_active+0x38/0x70
    [ 3189.403316]  [<ffffffff81674b87>] notifier_call_chain+0x67/0x150
    [ 3189.403320]  [<ffffffff81664647>] ? native_cpu_up+0x18a/0x1b5
    [ 3189.403328]  [<ffffffff810a044e>] __raw_notifier_call_chain+0xe/0x10
    [ 3189.403333]  [<ffffffff81070470>] __cpu_notify+0x20/0x40
    [ 3189.403337]  [<ffffffff8166663e>] _cpu_up+0xe9/0x131
    [ 3189.403340]  [<ffffffff81666761>] cpu_up+0xdb/0xee
    [ 3189.403348]  [<ffffffff8165667c>] store_online+0x9c/0xd0
    [ 3189.403355]  [<ffffffff81437640>] dev_attr_store+0x20/0x30
    [ 3189.403361]  [<ffffffff8124aa63>] sysfs_write_file+0xa3/0x100
    [ 3189.403368]  [<ffffffff811ccbe0>] vfs_write+0xd0/0x1a0
    [ 3189.403371]  [<ffffffff811ccdb4>] sys_write+0x54/0xa0
    [ 3189.403375]  [<ffffffff81679c69>] system_call_fastpath+0x16/0x1b
    [ 3189.403377] ---[ end trace 1e6cf85d0859c941 ]---
    [ 3189.403398] BUG: unable to handle kernel NULL pointer dereference at 0000000000000018
    
    This patch registers a new notifier for cpu hotplug notify chain, and
    updates sched_domains_numa_masks every time a new cpu is onlined or offlined.
    
    Signed-off-by: Tang Chen <tangchen@cn.fujitsu.com>
    Signed-off-by: Wen Congyang <wency@cn.fujitsu.com>
    [ fixed compile warning ]
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/1348578751-16904-3-git-send-email-tangchen@cn.fujitsu.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 46076e7719201e4ddd8a25dfd804daeb5ee0dc9c
Author: Tang Chen <tangchen@cn.fujitsu.com>
Date:   Tue Sep 25 21:12:30 2012 +0800

    sched: Ensure 'sched_domains_numa_levels' is safe to use in other functions
    
    We should temporarily reset 'sched_domains_numa_levels' to 0 after
    it is reset to 'level' in sched_init_numa(). If it fails to allocate
    memory for array sched_domains_numa_masks[][], the array will contain
    less then 'level' members. This could be dangerous when we use it to
    iterate array sched_domains_numa_masks[][] in other functions.
    
    This patch set sched_domains_numa_levels to 0 before initializing
    array sched_domains_numa_masks[][], and reset it to 'level' when
    sched_domains_numa_masks[][] is fully initialized.
    
    Signed-off-by: Tang Chen <tangchen@cn.fujitsu.com>
    Signed-off-by: Wen Congyang <wency@cn.fujitsu.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/1348578751-16904-2-git-send-email-tangchen@cn.fujitsu.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit d6ab463beac04924cbb12795c47a657d23dd3161
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Fri Jun 1 14:22:01 2012 -0400

    sanitize tsk_is_polling()
    
    Make default just return 0.  The current default (checking
    TIF_POLLING_NRFLAG) is taken to architectures that need it;
    ones that don't do polling in their idle threads don't need
    to defined TIF_POLLING_NRFLAG at all.
    
    ia64 defined both TS_POLLING (used by its tsk_is_polling())
    and TIF_POLLING_NRFLAG (not used at all).  Killed the latter...
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>
    
    Conflicts:
    	arch/alpha/include/asm/thread_info.h
    	arch/arm/include/asm/thread_info.h
    	arch/avr32/include/asm/thread_info.h
    	arch/blackfin/include/asm/thread_info.h
    	arch/c6x/include/asm/thread_info.h
    	arch/c6x/kernel/asm-offsets.c
    	arch/cris/include/asm/thread_info.h
    	arch/frv/include/asm/thread_info.h
    	arch/h8300/include/asm/thread_info.h
    	arch/hexagon/include/asm/thread_info.h
    	arch/ia64/include/asm/thread_info.h
    	arch/m32r/include/asm/thread_info.h
    	arch/microblaze/include/asm/thread_info.h
    	arch/mips/include/asm/thread_info.h
    	arch/mn10300/include/asm/thread_info.h
    	arch/openrisc/include/asm/thread_info.h
    	arch/parisc/include/asm/thread_info.h
    	arch/powerpc/include/asm/thread_info.h
    	arch/s390/include/asm/thread_info.h
    	arch/score/include/asm/thread_info.h
    	arch/sh/include/asm/thread_info.h
    	arch/sparc/include/asm/thread_info_32.h
    	arch/sparc/include/asm/thread_info_64.h
    	arch/xtensa/include/asm/thread_info.h
    
    Conflicts:
    	arch/arm/include/asm/thread_info.h

commit c6a9ec6d29205800a41635d25f08a893cfd84b93
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Wed Jul 11 20:26:37 2012 +0200

    rcu: Exit RCU extended QS on user preemption
    
    When exceptions or irq are about to resume userspace, if
    the task needs to be rescheduled, the arch low level code
    calls schedule() directly.
    
    If we call it, it is because we have the TIF_RESCHED flag:
    
    - It can be set after random local calls to set_need_resched()
    (RCU, drm, ...)
    
    - A wake up happened and the CPU needs preemption. This can
      happen in several ways:
    
        * Remotely: the remote waking CPU has set TIF_RESCHED and send the
          wakee an IPI to schedule the new task.
        * Remotely enqueued: the remote waking CPU sends an IPI to the target
          and the wake up is made by the target.
        * Locally: waking CPU == wakee CPU and the wakeup is done locally.
          set_need_resched() is called without IPI.
    
    In the case of local and remotely enqueued wake ups, the tick can
    be restarted when we enqueue the new task and RCU can exit the
    extended quiescent state at the same time. Then by the time we reach
    irq exit path and we call schedule, we are not in RCU user mode.
    
    But if we call schedule() only because something called set_need_resched(),
    RCU may still be in user mode when we reach schedule.
    
    Also if a wake up is done remotely, the CPU might see the TIF_RESCHED
    flag and call schedule while the IPI has not yet happen to restart the
    tick and exit RCU user mode.
    
    We need to manually protect against these corner cases.
    
    Create a new API schedule_user() that calls schedule() inside
    rcu_user_exit()-rcu_user_enter() in order to protect it. Archs
    will need to rely on it now to implement user preemption safely.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Alessio Igor Bogani <abogani@kernel.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Avi Kivity <avi@redhat.com>
    Cc: Chris Metcalf <cmetcalf@tilera.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Geoff Levand <geoff@infradead.org>
    Cc: Gilad Ben Yossef <gilad@benyossef.com>
    Cc: Hakan Akkan <hakanakkan@gmail.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Josh Triplett <josh@joshtriplett.org>
    Cc: Kevin Hilman <khilman@ti.com>
    Cc: Max Krasnyansky <maxk@qualcomm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Stephen Hemminger <shemminger@vyatta.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Sven-Thorsten Dietrich <thebigcorporation@gmail.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Reviewed-by: Josh Triplett <josh@joshtriplett.org>

commit 044867b484eecd9b40d70477ef74460d59efe80e
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Wed Jul 11 20:26:36 2012 +0200

    rcu: Exit RCU extended QS on kernel preemption after irq/exception
    
    When an exception or an irq exits, and we are going to resume into
    interrupted kernel code, the low level architecture code calls
    preempt_schedule_irq() if there is a need to reschedule.
    
    If the interrupt/exception occured between a call to rcu_user_enter()
    (from syscall exit, exception exit, do_notify_resume exit, ...) and
    a real resume to userspace (iret,...), preempt_schedule_irq() can be
    called whereas RCU thinks we are in userspace. But preempt_schedule_irq()
    is going to run kernel code and may be some RCU read side critical
    section. We must exit the userspace extended quiescent state before
    we call it.
    
    To solve this, just call rcu_user_exit() in the beginning of
    preempt_schedule_irq().
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Alessio Igor Bogani <abogani@kernel.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Avi Kivity <avi@redhat.com>
    Cc: Chris Metcalf <cmetcalf@tilera.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Geoff Levand <geoff@infradead.org>
    Cc: Gilad Ben Yossef <gilad@benyossef.com>
    Cc: Hakan Akkan <hakanakkan@gmail.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Josh Triplett <josh@joshtriplett.org>
    Cc: Kevin Hilman <khilman@ti.com>
    Cc: Max Krasnyansky <maxk@qualcomm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Stephen Hemminger <shemminger@vyatta.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Sven-Thorsten Dietrich <thebigcorporation@gmail.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Reviewed-by: Josh Triplett <josh@joshtriplett.org>

commit e1e39bc0c8cd4e4681a5831af63f02192a61e1e0
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Mon Jul 16 15:06:40 2012 -0700

    rcu: Switch task's syscall hooks on context switch
    
    Clear the syscalls hook of a task when it's scheduled out so that if
    the task migrates, it doesn't run the syscall slow path on a CPU
    that might not need it.
    
    Also set the syscalls hook on the next task if needed.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Alessio Igor Bogani <abogani@kernel.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Avi Kivity <avi@redhat.com>
    Cc: Chris Metcalf <cmetcalf@tilera.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Geoff Levand <geoff@infradead.org>
    Cc: Gilad Ben Yossef <gilad@benyossef.com>
    Cc: Hakan Akkan <hakanakkan@gmail.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Josh Triplett <josh@joshtriplett.org>
    Cc: Kevin Hilman <khilman@ti.com>
    Cc: Max Krasnyansky <maxk@qualcomm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Stephen Hemminger <shemminger@vyatta.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Sven-Thorsten Dietrich <thebigcorporation@gmail.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Reviewed-by: Josh Triplett <josh@joshtriplett.org>

commit 4082d97655a1a237b97ea5e7651b257e78730da6
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Wed Jul 11 20:26:32 2012 +0200

    rcu: Ignore userspace extended quiescent state by default
    
    By default we don't want to enter into RCU extended quiescent
    state while in userspace because doing this produces some overhead
    (eg: use of syscall slowpath). Set it off by default and ready to
    run when some feature like adaptive tickless need it.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Alessio Igor Bogani <abogani@kernel.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Avi Kivity <avi@redhat.com>
    Cc: Chris Metcalf <cmetcalf@tilera.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Geoff Levand <geoff@infradead.org>
    Cc: Gilad Ben Yossef <gilad@benyossef.com>
    Cc: Hakan Akkan <hakanakkan@gmail.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Josh Triplett <josh@joshtriplett.org>
    Cc: Kevin Hilman <khilman@ti.com>
    Cc: Max Krasnyansky <maxk@qualcomm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Stephen Hemminger <shemminger@vyatta.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Sven-Thorsten Dietrich <thebigcorporation@gmail.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Reviewed-by: Josh Triplett <josh@joshtriplett.org>

commit 5f79a2666ba5047365fcfbe86e37a3a165608443
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Wed Jul 11 20:26:31 2012 +0200

    rcu: Allow rcu_user_enter()/exit() to nest
    
    Allow calls to rcu_user_enter() even if we are already
    in userspace (as seen by RCU) and allow calls to rcu_user_exit()
    even if we are already in the kernel.
    
    This makes the APIs more flexible to be called from architectures.
    Exception entries for example won't need to know if they come from
    userspace before calling rcu_user_exit().
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Alessio Igor Bogani <abogani@kernel.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Avi Kivity <avi@redhat.com>
    Cc: Chris Metcalf <cmetcalf@tilera.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Geoff Levand <geoff@infradead.org>
    Cc: Gilad Ben Yossef <gilad@benyossef.com>
    Cc: Hakan Akkan <hakanakkan@gmail.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Josh Triplett <josh@joshtriplett.org>
    Cc: Kevin Hilman <khilman@ti.com>
    Cc: Max Krasnyansky <maxk@qualcomm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Stephen Hemminger <shemminger@vyatta.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Sven-Thorsten Dietrich <thebigcorporation@gmail.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Reviewed-by: Josh Triplett <josh@joshtriplett.org>

commit 28f85cc34a05f6101ef039fa6f9483f31a767adb
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Wed Jul 11 20:26:30 2012 +0200

    rcu: Settle config for userspace extended quiescent state
    
    Create a new config option under the RCU menu that put
    CPUs under RCU extended quiescent state (as in dynticks
    idle mode) when they run in userspace. This require
    some contribution from architectures to hook into kernel
    and userspace boundaries.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Alessio Igor Bogani <abogani@kernel.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Avi Kivity <avi@redhat.com>
    Cc: Chris Metcalf <cmetcalf@tilera.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Geoff Levand <geoff@infradead.org>
    Cc: Gilad Ben Yossef <gilad@benyossef.com>
    Cc: Hakan Akkan <hakanakkan@gmail.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Josh Triplett <josh@joshtriplett.org>
    Cc: Kevin Hilman <khilman@ti.com>
    Cc: Max Krasnyansky <maxk@qualcomm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Stephen Hemminger <shemminger@vyatta.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Sven-Thorsten Dietrich <thebigcorporation@gmail.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Reviewed-by: Josh Triplett <josh@joshtriplett.org>

commit 6fbebb6c095737ac6a873bf9a78e8285a0c3d995
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Mon Jun 4 16:42:35 2012 -0700

    rcu: New rcu_user_enter_after_irq() and rcu_user_exit_after_irq() APIs
    
    In some cases, it is necessary to enter or exit userspace-RCU-idle mode
    from an interrupt handler, for example, if some other CPU sends this
    CPU a resched IPI.  In this case, the current CPU would enter the IPI
    handler in userspace-RCU-idle mode, but would need to exit the IPI handler
    after having exited that mode.
    
    To allow this to work, this commit adds two new APIs to TREE_RCU:
    
    - rcu_user_enter_after_irq(). This must be called from an interrupt between
    rcu_irq_enter() and rcu_irq_exit().  After the irq calls rcu_irq_exit(),
    the irq handler will return into an RCU extended quiescent state.
    In theory, this interrupt is never a nested interrupt, but in practice
    it might interrupt softirq, which looks to RCU like a nested interrupt.
    
    - rcu_user_exit_after_irq(). This must be called from a non-nesting
    interrupt, interrupting an RCU extended quiescent state, also
    between rcu_irq_enter() and rcu_irq_exit(). After the irq calls
    rcu_irq_exit(), the irq handler will return in an RCU non-quiescent
    state.
    
    [ Combined with "Allow calls to rcu_exit_user_irq from nesting irqs." ]
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Reviewed-by: Josh Triplett <josh@joshtriplett.org>

commit 110f31d81a61e7cde5791eca5d119e31ea99c8bb
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Thu Jun 28 11:20:21 2012 -0700

    rcu: New rcu_user_enter() and rcu_user_exit() APIs
    
    RCU currently insists that only idle tasks can enter RCU idle mode, which
    prohibits an adaptive tickless kernel (AKA nohz cpusets), which in turn
    would mean that usermode execution would always take scheduling-clock
    interrupts, even when there is only one task runnable on the CPU in
    question.
    
    This commit therefore adds rcu_user_enter() and rcu_user_exit(), which
    allow non-idle tasks to enter RCU idle mode.  These are quite similar
    to rcu_idle_enter() and rcu_idle_exit(), respectively, except that they
    omit the idle-task checks.
    
    [ Updated to use "user" flag rather than separate check functions. ]
    
    [ paulmck: Updated to drop exports of new functions based on Josh's patch
      getting rid of the need for them. ]
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Alessio Igor Bogani <abogani@kernel.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Avi Kivity <avi@redhat.com>
    Cc: Chris Metcalf <cmetcalf@tilera.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Daniel Lezcano <daniel.lezcano@linaro.org>
    Cc: Geoff Levand <geoff@infradead.org>
    Cc: Gilad Ben Yossef <gilad@benyossef.com>
    Cc: Hakan Akkan <hakanakkan@gmail.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Kevin Hilman <khilman@ti.com>
    Cc: Max Krasnyansky <maxk@qualcomm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Stephen Hemminger <shemminger@vyatta.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Sven-Thorsten Dietrich <thebigcorporation@gmail.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Josh Triplett <josh@joshtriplett.org>

commit b1af8e7065dc16c6592f15b76a1bd6cd0297dc8e
Author: Paul E. McKenney <paul.mckenney@linaro.org>
Date:   Fri Aug 3 13:16:15 2012 -0700

    rcu: Disallow callback registry on offline CPUs
    
    Posting a callback after the CPU_DEAD notifier effectively leaks
    that callback unless/until that CPU comes back online.  Silence is
    unhelpful when attempting to track down such leaks, so this commit emits
    a WARN_ON_ONCE() and unconditionally leaks the callback when an offline
    CPU attempts to register a callback.  The rdp->nxttail[RCU_NEXT_TAIL] is
    set to NULL in the CPU_DEAD notifier and restored in the CPU_UP_PREPARE
    notifier, allowing _call_rcu() to determine exactly when posting callbacks
    is illegal.
    
    Signed-off-by: Paul E. McKenney <paul.mckenney@linaro.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Reviewed-by: Josh Triplett <josh@joshtriplett.org>

commit 4626d67557ef654b8d9f8e9bae62464f4e5626a4
Author: Paul E. McKenney <paul.mckenney@linaro.org>
Date:   Thu Aug 2 17:43:50 2012 -0700

    rcu: Remove _rcu_barrier() dependency on __stop_machine()
    
    Currently, _rcu_barrier() relies on preempt_disable() to prevent
    any CPU from going offline, which in turn depends on CPU hotplug's
    use of __stop_machine().
    
    This patch therefore makes _rcu_barrier() use get_online_cpus() to
    block CPU-hotplug operations.  This has the added benefit of removing
    the need for _rcu_barrier() to adopt callbacks:  Because CPU-hotplug
    operations are excluded, there can be no callbacks to adopt.  This
    commit simplifies the code accordingly.
    
    Signed-off-by: Paul E. McKenney <paul.mckenney@linaro.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Reviewed-by: Josh Triplett <josh@joshtriplett.org>

commit 33738b2bf7fc2c136ba22d5f6e53d00c27e64257
Author: Paul E. McKenney <paul.mckenney@linaro.org>
Date:   Mon Aug 13 16:34:12 2012 -0700

    rcu: Remove redundant memory barrier from __call_rcu()
    
    The first memory barrier in __call_rcu() is supposed to order any
    updates done beforehand by the caller against the actual queuing
    of the callback.  However, the second memory barrier (which is intended
    to order incrementing the queue lengths before queuing the callback)
    is also between the caller's updates and the queuing of the callback.
    The second memory barrier can therefore serve both purposes.
    
    This commit therefore removes the first memory barrier.
    
    Signed-off-by: Paul E. McKenney <paul.mckenney@linaro.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Reviewed-by: Josh Triplett <josh@joshtriplett.org>

commit b2daadae9552d822983aa9e7fc40534ac3d718d1
Author: Paul E. McKenney <paul.mckenney@linaro.org>
Date:   Mon Aug 13 11:17:06 2012 -0700

    rcu: Avoid spurious RCU CPU stall warnings
    
    If a given CPU avoids the idle loop but also avoids starting a new
    RCU grace period for a full minute, RCU can issue spurious RCU CPU
    stall warnings.  This commit fixes this issue by adding a check for
    ongoing grace period to avoid these spurious stall warnings.
    
    Reported-by: Becky Bruce <bgillbruce@gmail.com>
    Signed-off-by: Paul E. McKenney <paul.mckenney@linaro.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Reviewed-by: Josh Triplett <josh@joshtriplett.org>

commit 3d8d55260c618bd04c3a3abf76711c88dd9851f7
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Fri Aug 10 16:55:59 2012 -0700

    rcu: Protect rcu_node accesses during CPU stall warnings
    
    The print_other_cpu_stall() function accesses a number of rcu_node
    fields without protection from the ->lock.  In theory, this is not
    a problem because the fields accessed are all integers, but in
    practice the compiler can get nasty.  Therefore, the commit extends
    the existing critical section to cover the entire loop body.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

commit d87cc82f2d33e4d3e24c051e606fb55c5308e2b6
Author: Paul E. McKenney <paul.mckenney@linaro.org>
Date:   Wed Aug 1 14:29:20 2012 -0700

    rcu: Make offline-CPU checking allow for indefinite delays
    
    The rcu_implicit_offline_qs() function implicitly assumed that execution
    would progress predictably when interrupts are disabled, which is of course
    not guaranteed when running on a hypervisor.  Furthermore, this function
    is short, and is called from one place only in a short function.
    
    This commit therefore ensures that the timing is checked before
    checking the condition, which guarantees correct behavior even given
    indefinite delays.  It also inlines rcu_implicit_offline_qs() into
    rcu_implicit_dynticks_qs().
    
    Signed-off-by: Paul E. McKenney <paul.mckenney@linaro.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Reviewed-by: Josh Triplett <josh@joshtriplett.org>

commit 34ba7b37f59998556d16994fb0cf62ba5f6a56a1
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Thu Sep 6 15:38:02 2012 -0700

    rcu: Shrink RCU based on number of CPUs
    
    Currently, rcu_init_geometry() only reshapes RCU's combining trees
    if the leaf fanout is changed at boot time.  This means that by
    default, kernels compiled with (say) NR_CPUS=4096 will keep oversized
    data structures, even when running on systems with (say) four CPUs.
    
    This commit therefore checks to see if the maximum number of CPUs on
    the actual running system (nr_cpu_ids) differs from NR_CPUS, and if so
    reshapes the combining trees accordingly.
    
    Reported-by: Mike Galbraith <efault@gmx.de>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

commit 80a6d4c14a25a100c336a6ba1c8672d35cc4cc7a
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Wed Sep 5 21:43:57 2012 -0700

    rcu: Handle unbalanced rcu_node configurations with few CPUs
    
    If CONFIG_RCU_FANOUT_EXACT=y, if there are not enough CPUs (according
    to nr_cpu_ids) to require more than a single rcu_node structure, but if
    NR_CPUS is larger than would fit into a single rcu_node structure, then
    the current rcu_init_levelspread() code is subject to integer overflow
    in the eight-bit ->levelspread[] array in the rcu_state structure.
    
    In this case, the solution is -not- to increase the size of the
    elements in this array because the values in that array should be
    constrained to the number of bits in an unsigned long.  Instead, this
    commit replaces NR_CPUS with nr_cpu_ids in the rcu_init_levelspread()
    function's initialization of the cprv local variable.  This results in
    all of the arithmetic being consistently based off of the nr_cpu_ids
    value, thus avoiding the overflow, which was caused by the mixing of
    nr_cpu_ids and NR_CPUS.
    
    Reported-by: Mike Galbraith <efault@gmx.de>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

commit 51e1a8bc9283326b92ab4faedf36632e299e1866
Author: Paul E. McKenney <paul.mckenney@linaro.org>
Date:   Tue Aug 21 15:00:05 2012 -0700

    rcu: Simplify quiescent-state detection
    
    The current quiescent-state detection algorithm is needlessly
    complex.  It records the grace-period number corresponding to
    the quiescent state at the time of the quiescent state, which
    works, but it seems better to simply erase any record of previous
    quiescent states at the time that the CPU notices the new grace
    period.  This has the further advantage of removing another piece
    of RCU for which lockless reasoning is required.
    
    Therefore, this commit makes this change.
    
    Signed-off-by: Paul E. McKenney <paul.mckenney@linaro.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Reviewed-by: Josh Triplett <josh@joshtriplett.org>

commit 533f0b7f54d66509a92786abca40c2514391afd6
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Tue Jun 26 14:00:48 2012 -0700

    rcu: Adjust debugfs tracing for kthread-based quiescent-state forcing
    
    Moving quiescent-state forcing into a kthread dispenses with the need
    for the ->n_rp_need_fqs field, so this commit removes it.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Reviewed-by: Josh Triplett <josh@joshtriplett.org>

commit bc33c1bf17c42058ba6fb7333d4166bb207d94cd
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Thu May 17 15:12:45 2012 -0700

    rcu: Fix broken strings in RCU's source code.
    
    Although the C language allows you to break strings across lines, doing
    this makes it hard for people to find the Linux kernel code corresponding
    to a given console message.  This commit therefore fixes broken strings
    throughout RCU's source code.
    
    Suggested-by: Josh Triplett <josh@joshtriplett.org>
    Suggested-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    
    Conflicts:
    	kernel/rcutorture.c

commit 6372e56c59465089375f85df763b68445406acc0
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Tue Jun 12 12:25:39 2012 -0700

    rcu: Use for_each_rcu_flavor() in TREE_RCU tracing
    
    This commit applies the new for_each_rcu_flavor() macro to the
    kernel/rcutree_trace.c file.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

commit fb67f0c0ad62ee77103eec229b35265269d88c3e
Author: Paul E. McKenney <paul.mckenney@linaro.org>
Date:   Fri May 25 11:49:15 2012 -0700

    rcu: Add rcu_barrier() statistics to debugfs tracing
    
    This commit adds an rcubarrier file to RCU's debugfs statistical tracing
    directory, providing diagnostic information on rcu_barrier().
    
    Signed-off-by: Paul E. McKenney <paul.mckenney@linaro.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Reviewed-by: Josh Triplett <josh@joshtriplett.org>

commit 432d788db3fdc74ae29ec514fc7ffbd26f827bd8
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Wed Jul 11 05:23:18 2012 -0700

    rcu: Adjust for unconditional ->completed assignment
    
    Now that the rcu_node structures' ->completed fields are unconditionally
    assigned at grace-period cleanup time, they should already have the
    correct value for the new grace period at grace-period initialization
    time.  This commit therefore inserts a WARN_ON_ONCE() to verify this
    invariant.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Reviewed-by: Josh Triplett <josh@joshtriplett.org>

commit 06e67704fab06e41dc1717ec3de762210b5dfd89
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Sat Jul 7 05:57:03 2012 -0700

    rcu: Add random PROVE_RCU_DELAY to grace-period initialization
    
    Preemption greatly raised the probability of certain types of race
    conditions, so this commit adds an anti-heisenbug to greatly increase
    the collision cross section, also known as the probability of occurrence.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Reviewed-by: Josh Triplett <josh@joshtriplett.org>

commit e194316e82b53775686d44a58d626cb697cbc526
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Sat Jul 7 07:56:57 2012 -0700

    rcu: Fix day-zero grace-period initialization/cleanup race
    
    The current approach to grace-period initialization is vulnerable to
    extremely low-probability races.  These races stem from the fact that
    the old grace period is marked completed on the same traversal through
    the rcu_node structure that is marking the start of the new grace period.
    This means that some rcu_node structures will believe that the old grace
    period is still in effect at the same time that other rcu_node structures
    believe that the new grace period has already started.
    
    These sorts of disagreements can result in too-short grace periods,
    as shown in the following scenario:
    
    1.	CPU 0 completes a grace period, but needs an additional
    	grace period, so starts initializing one, initializing all
    	the non-leaf rcu_node structures and the first leaf rcu_node
    	structure.  Because CPU 0 is both completing the old grace
    	period and starting a new one, it marks the completion of
    	the old grace period and the start of the new grace period
    	in a single traversal of the rcu_node structures.
    
    	Therefore, CPUs corresponding to the first rcu_node structure
    	can become aware that the prior grace period has completed, but
    	CPUs corresponding to the other rcu_node structures will see
    	this same prior grace period as still being in progress.
    
    2.	CPU 1 passes through a quiescent state, and therefore informs
    	the RCU core.  Because its leaf rcu_node structure has already
    	been initialized, this CPU's quiescent state is applied to the
    	new (and only partially initialized) grace period.
    
    3.	CPU 1 enters an RCU read-side critical section and acquires
    	a reference to data item A.  Note that this CPU believes that
    	its critical section started after the beginning of the new
    	grace period, and therefore will not block this new grace period.
    
    4.	CPU 16 exits dyntick-idle mode.  Because it was in dyntick-idle
    	mode, other CPUs informed the RCU core of its extended quiescent
    	state for the past several grace periods.  This means that CPU 16
    	is not yet aware that these past grace periods have ended.  Assume
    	that CPU 16 corresponds to the second leaf rcu_node structure --
    	which has not yet been made aware of the new grace period.
    
    5.	CPU 16 removes data item A from its enclosing data structure
    	and passes it to call_rcu(), which queues a callback in the
    	RCU_NEXT_TAIL segment of the callback queue.
    
    6.	CPU 16 enters the RCU core, possibly because it has taken a
    	scheduling-clock interrupt, or alternatively because it has
    	more than 10,000 callbacks queued.  It notes that the second
    	most recent grace period has completed (recall that because it
    	corresponds to the second as-yet-uninitialized rcu_node structure,
    	it cannot yet become aware that the most recent grace period has
    	completed), and therefore advances its callbacks.  The callback
    	for data item A is therefore in the RCU_NEXT_READY_TAIL segment
    	of the callback queue.
    
    7.	CPU 0 completes initialization of the remaining leaf rcu_node
    	structures for the new grace period, including the structure
    	corresponding to CPU 16.
    
    8.	CPU 16 again enters the RCU core, again, possibly because it has
    	taken a scheduling-clock interrupt, or alternatively because
    	it now has more than 10,000 callbacks queued.	It notes that
    	the most recent grace period has ended, and therefore advances
    	its callbacks.	The callback for data item A is therefore in
    	the RCU_DONE_TAIL segment of the callback queue.
    
    9.	All CPUs other than CPU 1 pass through quiescent states.  Because
    	CPU 1 already passed through its quiescent state, the new grace
    	period completes.  Note that CPU 1 is still in its RCU read-side
    	critical section, still referencing data item A.
    
    10.	Suppose that CPU 2 wais the last CPU to pass through a quiescent
    	state for the new grace period, and suppose further that CPU 2
    	did not have any callbacks queued, therefore not needing an
    	additional grace period.  CPU 2 therefore traverses all of the
    	rcu_node structures, marking the new grace period as completed,
    	but does not initialize a new grace period.
    
    11.	CPU 16 yet again enters the RCU core, yet again possibly because
    	it has taken a scheduling-clock interrupt, or alternatively
    	because it now has more than 10,000 callbacks queued.	It notes
    	that the new grace period has ended, and therefore advances
    	its callbacks.	The callback for data item A is therefore in
    	the RCU_DONE_TAIL segment of the callback queue.  This means
    	that this callback is now considered ready to be invoked.
    
    12.	CPU 16 invokes the callback, freeing data item A while CPU 1
    	is still referencing it.
    
    This scenario represents a day-zero bug for TREE_RCU.  This commit
    therefore ensures that the old grace period is marked completed in
    all leaf rcu_node structures before a new grace period is marked
    started in any of them.
    
    That said, it would have been insanely difficult to force this race to
    happen before the grace-period initialization process was preemptible.
    Therefore, this commit is not a candidate for -stable.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Reviewed-by: Josh Triplett <josh@joshtriplett.org>
    
    Conflicts:
    
    	kernel/rcutree.c

commit 9b4163fc8743f2328ec5f6a47083d02eb3e0308d
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Sun Jul 1 15:42:33 2012 -0700

    rcu: Make rcutree module parameters visible in sysfs
    
    The module parameters blimit, qhimark, and qlomark (and more
    recently, rcu_fanout_leaf) have permission masks of zero, so
    that their values are not visible from sysfs.  This is unnecessary
    and inconvenient to administrators who might like an easy way to
    see what these values are on a running system.  This commit therefore
    sets their permission masks to 0444, allowing them to be read but
    not written.
    
    Reported-by: Rusty Russell <rusty@ozlabs.org>
    Reported-by: Josh Triplett <josh@joshtriplett.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Reviewed-by: Josh Triplett <josh@joshtriplett.org>

commit 6e0d09537dca8f281f636895c9a32173bf239c24
Author: Paul E. McKenney <paul.mckenney@linaro.org>
Date:   Tue Jun 26 20:45:57 2012 -0700

    rcu: Control grace-period duration from sysfs
    
    Although almost everyone is well-served by the defaults, some uses of RCU
    benefit from shorter grace periods, while others benefit more from the
    greater efficiency provided by longer grace periods.  Situations requiring
    a large number of grace periods to elapse (and wireshark startup has
    been called out as an example of this) are helped by lower-latency
    grace periods.  Furthermore, in some embedded applications, people are
    willing to accept a small degradation in update efficiency (due to there
    being more of the shorter grace-period operations) in order to gain the
    lower latency.
    
    In contrast, those few systems with thousands of CPUs need longer grace
    periods because the CPU overhead of a grace period rises roughly
    linearly with the number of CPUs.  Such systems normally do not make
    much use of facilities that require large numbers of grace periods to
    elapse, so this is a good tradeoff.
    
    Therefore, this commit allows the durations to be controlled from sysfs.
    There are two sysfs parameters, one named "jiffies_till_first_fqs" that
    specifies the delay in jiffies from the end of grace-period initialization
    until the first attempt to force quiescent states, and the other named
    "jiffies_till_next_fqs" that specifies the delay (again in jiffies)
    between subsequent attempts to force quiescent states.  They both default
    to three jiffies, which is compatible with the old hard-coded behavior.
    
    At some future time, it may be possible to automatically increase the
    grace-period length with the number of CPUs, but we do not yet have
    sufficient data to do a good job.  Preliminary data indicates that we
    should add an addiitonal jiffy to each of the delays for every 200 CPUs
    in the system, but more experimentation is needed.  For now, the number
    of systems with more than 1,000 CPUs is small enough that this can be
    relegated to boot-time hand tuning.
    
    Signed-off-by: Paul E. McKenney <paul.mckenney@linaro.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Reviewed-by: Josh Triplett <josh@joshtriplett.org>
    
    Conflicts:
    	Documentation/kernel-parameters.txt

commit 86f0f77604d52b006e2a2e64c7a781d68cd06be5
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Tue Jun 26 17:00:35 2012 -0700

    rcu: Prevent force_quiescent_state() memory contention
    
    Large systems running RCU_FAST_NO_HZ kernels see extreme memory
    contention on the rcu_state structure's ->fqslock field.  This
    can be avoided by disabling RCU_FAST_NO_HZ, either at compile time
    or at boot time (via the nohz kernel boot parameter), but large
    systems will no doubt become sensitive to energy consumption.
    This commit therefore uses a combining-tree approach to spread the
    memory contention across new cache lines in the leaf rcu_node structures.
    This can be thought of as a tournament lock that has only a try-lock
    acquisition primitive.
    
    The effect on small systems is minimal, because such systems have
    an rcu_node "tree" consisting of a single node.  In addition, this
    functionality is not used on fastpaths.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Reviewed-by: Josh Triplett <josh@joshtriplett.org>
    
    Conflicts:
    	kernel/rcutree.h

commit a7b67a9055754d76516c9fa99b390ba136f4d9d6
Author: Dimitri Sivanich <sivanich@sgi.com>
Date:   Fri Jun 29 14:17:29 2012 -0700

    rcu: Segregate rcu_state fields to improve cache locality
    
    The fields in the rcu_state structure that are protected by the
    root rcu_node structure's ->lock can share a cache line with the
    fields protected by ->onofflock.  This can result in excessive
    memory contention on large systems, so this commit applies
    ____cacheline_internodealigned_in_smp to the ->onofflock field in
    order to segregate them.
    
    Signed-off-by: Dimitri Sivanich <sivanich@sgi.com>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Tested-by: Dimitri Sivanich <sivanich@sgi.com>
    Reviewed-by: Josh Triplett <josh@joshtriplett.org>

commit ac952720f530fb15a74e8a39aa66afa0c7839c65
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Mon Jun 25 08:41:11 2012 -0700

    rcu: Allow RCU quiescent-state forcing to be preempted
    
    RCU quiescent-state forcing is currently carried out without preemption
    points, which can result in excessive latency spikes on large systems
    (many hundreds or thousands of CPUs).  This patch therefore inserts
    a voluntary preemption point into force_qs_rnp(), which should greatly
    reduce the magnitude of these spikes.
    
    Reported-by: Mike Galbraith <mgalbraith@suse.de>
    Reported-by: Dimitri Sivanich <sivanich@sgi.com>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Reviewed-by: Josh Triplett <josh@joshtriplett.org>

commit f1e99d0a84dbdb926a8b49066fc8ecaed631b99d
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Fri Jun 22 17:06:26 2012 -0700

    rcu: Move quiescent-state forcing into kthread
    
    As the first step towards allowing quiescent-state forcing to be
    preemptible, this commit moves RCU quiescent-state forcing into the
    same kthread that is now used to initialize and clean up after grace
    periods.  This is yet another step towards keeping scheduling
    latency down to a dull roar.
    
    Updated to change from raw_spin_lock_irqsave() to raw_spin_lock_irq()
    and to remove the now-unused rcu_state structure fields as suggested by
    Peter Zijlstra.
    
    Reported-by: Mike Galbraith <mgalbraith@suse.de>
    Reported-by: Dimitri Sivanich <sivanich@sgi.com>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

commit cfdf5e8a53e6e57cff48ac21a26ce7bb8cc86348
Author: Paul E. McKenney <paul.mckenney@linaro.org>
Date:   Thu Jun 21 09:54:10 2012 -0700

    rcu: Prevent offline CPUs from executing RCU core code
    
    Earlier versions of RCU invoked the RCU core from the CPU_DYING notifier
    in order to note a quiescent state for the outgoing CPU.  Because the
    CPU is marked "offline" during the execution of the CPU_DYING notifiers,
    the RCU core had to tolerate being invoked from an offline CPU.  However,
    commit b1420f1c (Make rcu_barrier() less disruptive) left only tracing
    code in the CPU_DYING notifier, so the RCU core need no longer execute
    on offline CPUs.  This commit therefore enforces this restriction.
    
    Signed-off-by: Paul E. McKenney <paul.mckenney@linaro.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Reviewed-by: Josh Triplett <josh@joshtriplett.org>

commit 044dd61f9e9b28968031bb684ff5172d0a2ef214
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Fri Jun 22 11:08:41 2012 -0700

    rcu: Break up rcu_gp_kthread() into subfunctions
    
    Then rcu_gp_kthread() function is too large and furthermore needs to
    have the force_quiescent_state() code pulled in.  This commit therefore
    breaks up rcu_gp_kthread() into rcu_gp_init() and rcu_gp_cleanup().
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Reviewed-by: Josh Triplett <josh@joshtriplett.org>

commit 66b755f6352774f2d22c62b19dfa2ef01a4980d6
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Thu Jun 21 08:19:05 2012 -0700

    rcu: Allow RCU grace-period cleanup to be preempted
    
    RCU grace-period cleanup is currently carried out with interrupts
    disabled, which can result in excessive latency spikes on large systems
    (many hundreds or thousands of CPUs).  This patch therefore makes the
    RCU grace-period cleanup be preemptible, including voluntary preemption
    points, which should eliminate those latency spikes.  Similar spikes from
    forcing of quiescent states will be dealt with similarly by later patches.
    
    Updated to replace uses of spin_lock_irqsave() with spin_lock_irq(), as
    suggested by Peter Zijlstra.
    
    Reported-by: Mike Galbraith <mgalbraith@suse.de>
    Reported-by: Dimitri Sivanich <sivanich@sgi.com>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Reviewed-by: Josh Triplett <josh@joshtriplett.org>

commit 4969fea4de59f4603599d8a07b62487d26cecbd9
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Wed Jun 20 17:07:14 2012 -0700

    rcu: Move RCU grace-period cleanup into kthread
    
    As a first step towards allowing grace-period cleanup to be preemptible,
    this commit moves the RCU grace-period cleanup into the same kthread
    that is now used to initialize grace periods.  This is needed to keep
    scheduling latency down to a dull roar.
    
    [ paulmck: Get rid of stray spin_lock_irqsave() calls. ]
    
    Reported-by: Mike Galbraith <mgalbraith@suse.de>
    Reported-by: Dimitri Sivanich <sivanich@sgi.com>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Reviewed-by: Josh Triplett <josh@joshtriplett.org>

commit e68c3be56fc884ade69a7120cfcda4f68f443e29
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Tue Jun 19 17:18:20 2012 -0700

    rcu: Allow RCU grace-period initialization to be preempted
    
    RCU grace-period initialization is currently carried out with interrupts
    disabled, which can result in 200-microsecond latency spikes on systems
    on which RCU has been configured for 4096 CPUs.  This patch therefore
    makes the RCU grace-period initialization be preemptible, which should
    eliminate those latency spikes.  Similar spikes from grace-period cleanup
    and the forcing of quiescent states will be dealt with similarly by later
    patches.
    
    Reported-by: Mike Galbraith <mgalbraith@suse.de>
    Reported-by: Dimitri Sivanich <sivanich@sgi.com>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Reviewed-by: Josh Triplett <josh@joshtriplett.org>

commit 33aecd0ed6f1c5c0470420e4c872bec5484daad1
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Mon Sep 17 14:32:58 2012 -0700

    rcu: Prevent initialization-time quiescent-state race
    
    The next step in reducing RCU's grace-period initialization latency on
    large systems will make this initialization preemptible.  Unfortunately,
    making the grace-period initialization subject to interrupts (let alone
    preemption) exposes the following race on systems whose rcu_node tree
    contains more than one node:
    
    1.	CPU 31 starts initializing the grace period, including the
        	first leaf rcu_node structures, and is then preempted.
    
    2.	CPU 0 refers to the first leaf rcu_node structure, and notes
        	that a new grace period has started.  It passes through a
        	quiescent state shortly thereafter, and informs the RCU core
        	of this rite of passage.
    
    3.	CPU 0 enters an RCU read-side critical section, acquiring
        	a pointer to an RCU-protected data item.
    
    4.	CPU 31 takes an interrupt whose handler removes the data item
    	referenced by CPU 0 from the data structure, and registers an
    	RCU callback in order to free it.
    
    5.	CPU 31 resumes initializing the grace period, including its
        	own rcu_node structure.  In invokes rcu_start_gp_per_cpu(),
        	which advances all callbacks, including the one registered
        	in #4 above, to be handled by the current grace period.
    
    6.	The remaining CPUs pass through quiescent states and inform
        	the RCU core, but CPU 0 remains in its RCU read-side critical
        	section, still referencing the now-removed data item.
    
    7.	The grace period completes and all the callbacks are invoked,
        	including the one that frees the data item that CPU 0 is still
        	referencing.  Oops!!!
    
    One way to avoid this race is to remove grace-period acceleration from
    rcu_start_gp_per_cpu().  Now, the only reason for this acceleration was
    to allow CPUs bringing RCU out of idle state to have their callbacks
    invoked after only one grace period, rather than the two grace periods
    that would otherwise be required.  But this acceleration does not
    work when RCU grace-period initialization is moved to a kthread because
    the CPU posting the callback is no longer necessarily the CPU that is
    initializing the resulting grace period.
    
    This commit therefore removes this now-pointless (and soon to be dangerous)
    grace-period acceleration, thus avoiding the above race.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

commit 580f3f64732f379d270a886cef2bab9a8f733b13
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Mon Jun 18 18:36:08 2012 -0700

    rcu: Move RCU grace-period initialization into a kthread
    
    As the first step towards allowing grace-period initialization to be
    preemptible, this commit moves the RCU grace-period initialization
    into its own kthread.  This is needed to keep large-system scheduling
    latency at reasonable levels.
    
    Also change raw_spin_lock_irqsave() to raw_spin_lock_irq() as suggested
    by Peter Zijlstra in review comments.
    
    Reported-by: Mike Galbraith <mgalbraith@suse.de>
    Reported-by: Dimitri Sivanich <sivanich@sgi.com>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Reviewed-by: Josh Triplett <josh@joshtriplett.org>

commit bf1ccce57464a04ebfccd4118cb8c5a3c219b316
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Mon Jul 16 10:42:38 2012 +0000

    rcu: Use smp_hotplug_thread facility for RCUs per-CPU kthread
    
    Bring RCU into the new-age CPU-hotplug fold by modifying RCU's per-CPU
    kthread code to use the new smp_hotplug_thread facility.
    
    [ tglx: Adapted it to use callbacks and to the simplified rcu yield ]
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Namhyung Kim <namhyung@kernel.org>
    Link: http://lkml.kernel.org/r/20120716103948.673354828@linutronix.de
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

commit 5835c632e4ce274dab280443173c101aea1bc0ae
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Jul 16 10:42:35 2012 +0000

    rcu: Yield simpler
    
    The rcu_yield() code is amazing. It's there to avoid starvation of the
    system when lots of (boosting) work is to be done.
    
    Now looking at the code it's functionality is:
    
     Make the thread SCHED_OTHER and very nice, i.e. get it out of the way
     Arm a timer with 2 ticks
     schedule()
    
    Now if the system goes idle the rcu task returns, regains SCHED_FIFO
    and plugs on. If the systems stays busy the timer fires and wakes a
    per node kthread which in turn makes the per cpu thread SCHED_FIFO and
    brings it back on the cpu. For the boosting thread the "make it FIFO"
    bit is missing and it just runs some magic boost checks. Now this is a
    lot of code with extra threads and complexity.
    
    It's way simpler to let the tasks when they detect overload schedule
    away for 2 ticks and defer the normal wakeup as long as they are in
    yielded state and the cpu is not idle.
    
    That solves the same problem and the only difference is that when the
    cpu goes idle it's not guaranteed that the thread returns right away,
    but it won't be longer out than two ticks, so no harm is done. If
    that's an issue than it is way simpler just to wake the task from
    idle as RCU has callbacks there anyway.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Namhyung Kim <namhyung@kernel.org>
    Reviewed-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Link: http://lkml.kernel.org/r/20120716103948.131256723@linutronix.de
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

commit 8f8048d3f7b97c4f85fe35ff455c40b8136a8973
Author: Paul E. McKenney <paul.mckenney@linaro.org>
Date:   Thu Jun 28 08:08:25 2012 -0700

    rcu: Fix code-style issues involving "else"
    
    The Linux kernel coding style says that single-statement blocks should
    omit curly braces unless the other leg of the "if" statement has
    multiple statements, in which case the curly braces should be included.
    This commit fixes RCU's violations of this rule.
    
    Signed-off-by: Paul E. McKenney <paul.mckenney@linaro.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Reviewed-by: Josh Triplett <josh@joshtriplett.org>

commit 6caf331001a2f7ae5a5066c787fab193887f2d49
Author: Paul E. McKenney <paul.mckenney@linaro.org>
Date:   Mon Jun 25 12:54:17 2012 -0700

    rcu: Introduce check for callback list/count mismatch
    
    The recent bug that introduced the RCU callback list/count mismatch
    showed the need for a diagnostic to check for this, which this commit
    adds.
    
    Signed-off-by: Paul E. McKenney <paul.mckenney@linaro.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Reviewed-by: Josh Triplett <josh@joshtriplett.org>

commit f024841e07c9ff3b2a89769cc819da92417943ee
Author: Paul E. McKenney <paul.mckenney@linaro.org>
Date:   Wed May 9 15:55:39 2012 -0700

    rcu: Dump only the current CPU's buffers for idle-entry/exit warnings
    
    Problems in RCU idle entry and exit are almost always confined to the
    offending CPU.  This commit therefore switches ftrace_dump() from
    DUMP_ALL to DUMP_ORIG.
    
    Signed-off-by: Paul E. McKenney <paul.mckenney@linaro.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Tested-by: Heiko Carstens <heiko.carstens@de.ibm.com>
    Tested-by: Pascal Chapperon <pascal.chapperon@wanadoo.fr>

commit 7744615718f3d2355e5fefc5ba279ad036cf744b
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Tue Jul 21 22:37:32 2015 +0200

    cpuidle: fix compiling errors

commit 432f4996068924ed1c9dfc8427815c716697ea2a
Author: Paul E. McKenney <paul.mckenney@linaro.org>
Date:   Thu Jun 21 11:26:42 2012 -0700

    rcu: Add check for CPUs going offline with callbacks queued
    
    If a CPU goes offline with callbacks queued, those callbacks might be
    indefinitely postponed, which can result in a system hang.  This commit
    therefore inserts warnings for this condition.
    
    Signed-off-by: Paul E. McKenney <paul.mckenney@linaro.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

commit 809a634ba12c9eb96340162f578240a1e2a0bae9
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon May 7 17:59:48 2012 +0000

    smp: Implement kick_all_cpus_sync()
    
    Will replace the misnomed cpu_idle_wait() function which is copied a
    gazillion times all over arch/*
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/20120507175652.049316594@linutronix.de

commit 08d6b614053b3bb5aaf927db377df20bda50e39b
Author: Daniel Lezcano <daniel.lezcano@linaro.org>
Date:   Tue Jan 15 14:18:04 2013 +0100

    cpuidle: remove the power_specified field in the driver
    
    We realized that the power usage field is never filled and when it
    is filled for tegra, the power_specified flag is not set causing all
    of these values to be reset when the driver is initialized with
    set_power_state().
    
    However, the power_specified flag can be simply removed under the
    assumption that the states are always backward sorted, which is the
    case with the current code.
    
    This change allows the menu governor select function and the
    cpuidle_play_dead() to be simplified.  Moreover, the
    set_power_states() function can removed as it does not make sense
    any more.
    
    Drop the power_specified flag from struct cpuidle_driver and make
    the related changes as described above.
    
    As a consequence, this also fixes the bug where on the dynamic
    C-states system, the power fields are not initialized.
    
    [rjw: Changelog]
    References: https://bugzilla.kernel.org/show_bug.cgi?id=42870
    References: https://bugzilla.kernel.org/show_bug.cgi?id=43349
    References: https://lkml.org/lkml/2012/10/16/518
    Signed-off-by: Daniel Lezcano <daniel.lezcano@linaro.org>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit 7feb6c8104832625d8ec588c65773502790d6399
Author: Sivaram Nair <sivaramn@nvidia.com>
Date:   Tue Dec 18 13:52:50 2012 +0100

    cpuidle: Fix finding state with min power_usage
    
    Since cpuidle_state.power_usage is a signed value, use INT_MAX (instead
    of -1) to init the local copies so that functions that tries to find
    cpuidle states with minimum power usage works correctly even if they use
    non-negative values.
    
    Signed-off-by: Sivaram Nair <sivaramn@nvidia.com>
    Reviewed-by: Rik van Riel <riel@redhat.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit 07f516acfd2b04b7b03f7f8e26765c7fbb9301a0
Author: Daniel Lezcano <daniel.lezcano@linaro.org>
Date:   Wed Oct 31 16:44:48 2012 +0000

    cpuidle: support multiple drivers
    
    With the tegra3 and the big.LITTLE [1] new architectures, several cpus
    with different characteristics (latencies and states) can co-exists on the
    system.
    
    The cpuidle framework has the limitation of handling only identical cpus.
    
    This patch removes this limitation by introducing the multiple driver support
    for cpuidle.
    
    This option is configurable at compile time and should be enabled for the
    architectures mentioned above. So there is no impact for the other platforms
    if the option is disabled. The option defaults to 'n'. Note the multiple drivers
    support is also compatible with the existing drivers, even if just one driver is
    needed, all the cpu will be tied to this driver using an extra small chunk of
    processor memory.
    
    The multiple driver support use a per-cpu driver pointer instead of a global
    variable and the accessor to this variable are done from a cpu context.
    
    In order to keep the compatibility with the existing drivers, the function
    'cpuidle_register_driver' and 'cpuidle_unregister_driver' will register
    the specified driver for all the cpus.
    
    The semantic for the output of /sys/devices/system/cpu/cpuidle/current_driver
    remains the same except the driver name will be related to the current cpu.
    
    The /sys/devices/system/cpu/cpu[0-9]/cpuidle/driver/name files are added
    allowing to read the per cpu driver name.
    
    [1] http://lwn.net/Articles/481055/
    
    Signed-off-by: Daniel Lezcano <daniel.lezcano@linaro.org>
    Acked-by: Peter De Schrijver <pdeschrijver@nvidia.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit b1e33295e5b64ff4ff18fd6519ac22a92ffd20d0
Author: Daniel Lezcano <daniel.lezcano@linaro.org>
Date:   Wed Oct 31 16:44:47 2012 +0000

    cpuidle: prepare the cpuidle core to handle multiple drivers
    
    This patch is a preparation for the multiple cpuidle drivers support.
    
    As the next patch will introduce the multiple drivers with the Kconfig
    option and we want to keep the code clean and understandable, this patch
    defines a set of functions for encapsulating some common parts and splits
    what should be done under a lock from the rest.
    
    [rjw: Modified the subject and changelog slightly.]
    Signed-off-by: Daniel Lezcano <daniel.lezcano@linaro.org>
    Acked-by: Peter De Schrijver <pdeschrijver@nvidia.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit 212751d35e8d884a863ba608d9b07772c58c096e
Author: Daniel Lezcano <daniel.lezcano@linaro.org>
Date:   Wed Oct 31 16:44:46 2012 +0000

    cpuidle: move driver checking within the lock section
    
    The code is racy and the check with cpuidle_curr_driver should be
    done under the lock.
    
    I don't find a path in the different drivers where that could happen
    because the arch specific drivers are written in such way it is not
    possible to register a driver while it is unregistered, except maybe
    in a very improbable case when "intel_idle" and "processor_idle" are
    competing. One could unregister a driver, while the other one is
    registering.
    
    Signed-off-by: Daniel Lezcano <daniel.lezcano@linaro.org>
    Acked-by: Peter De Schrijver <pdeschrijver@nvidia.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit 4eca6aea45571dc813535b83b802e02a57cdc556
Author: Daniel Lezcano <daniel.lezcano@linaro.org>
Date:   Wed Oct 31 16:44:45 2012 +0000

    cpuidle: move driver's refcount to cpuidle
    
    We want to support different cpuidle drivers co-existing together.
    In this case we should move the refcount to the cpuidle_driver
    structure to handle several drivers at a time.
    
    Signed-off-by: Daniel Lezcano <daniel.lezcano@linaro.org>
    Acked-by: Peter De Schrijver <pdeschrijver@nvidia.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit 809ffaa570e207a0892b8d616db7184c00e5fc89
Author: Daniel Lezcano <daniel.lezcano@linaro.org>
Date:   Sat Sep 22 00:38:32 2012 +0200

    cpuidle: rename function name "__cpuidle_register_driver", v2
    
    The function __cpuidle_register_driver name is confusing because it
    suggests, conforming to the coding style of the kernel, it registers
    the driver without taking a lock. Actually, it just fill the different
    power field states with a decresing value if the power has not been
    specified.
    
    Clarify the purpose of the function by changing its name and
    move the condition out of this function.
    
    This patch fix nothing and does not change the behavior of the
    function. It is just for the sake of clarity.
    
    IHMO, reading in the code:
    
    +       if (!drv->power_specified)
    +               set_power_states(drv);
    
    is much more explicit than:
    
    -       __cpuidle_register_driver(drv);
    
    Signed-off-by: Daniel Lezcano <daniel.lezcano@linaro.org>
    Signed-off-by: Rafael J. Wysocki <rjw@sisk.pl>

commit a1ddc425faa5bda1b01817e809d7d5edcce66e1b
Author: Daniel Lezcano <daniel.lezcano@linaro.org>
Date:   Wed Sep 19 21:59:42 2012 +0200

    cpuidle: remove some empty lines
    
    This mindless patch is just about removing some trailing
    carriage returns.
    
    [rjw: Changed the subject.]
    
    Signed-off-by: Daniel Lezcano <daniel.lezcano@linaro.org>
    Signed-off-by: Rafael J. Wysocki <rjw@sisk.pl>

commit cc2f1c3bac30cd873b5661d2c30b2a29fa480753
Author: Rafael J. Wysocki <rjw@sisk.pl>
Date:   Sat Jun 16 15:20:11 2012 +0200

    PM / cpuidle: Add driver reference counter
    
    Add a reference counter for the cpuidle driver, so that it can't
    be unregistered when it is in use.
    
    Signed-off-by: Rafael J. Wysocki <rjw@sisk.pl>

commit 6425a00be627ebafcbd157f44b02ce78e7a4fb40
Author: Youquan Song <youquan.song@intel.com>
Date:   Fri Oct 26 12:26:59 2012 +0200

    cpuidle: Set residency to 0 if target Cstate not enter
    
    When cpuidle governor choose a C-state to enter for idle CPU, but it notice that
    there is tasks request to be executed. So the idle CPU will not really enter
    the target C-state and go to run task.
    
    In this situation, it will use the residency of previous really entered target
    C-states. Obviously, it is not reasonable.
    
    So, this patch fix it by set the target C-state residency to 0.
    
    Signed-off-by: Rik van Riel <riel@redhat.com>
    Signed-off-by: Youquan Song <youquan.song@intel.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit 4b6d05fb17723f7f723101de0f4b49935a35c7f2
Author: Daniel Lezcano <daniel.lezcano@linaro.org>
Date:   Fri Oct 26 12:26:32 2012 +0200

    cpuidle / sysfs: move kobj initialization in the syfs file
    
    Move the kobj initialization and completion in the sysfs.c
    and encapsulate the code more.
    
    Signed-off-by: Daniel Lezcano <daniel.lezcano@linaro.org>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit 5e617be4a9a2fad48609444a0a77791b979e5fc4
Author: Daniel Lezcano <daniel.lezcano@linaro.org>
Date:   Fri Oct 26 12:26:24 2012 +0200

    cpuidle / sysfs: change function parameter
    
    The function needs the cpuidle_device which is initially passed to the
    caller.
    
    The current code gets the struct device from the struct cpuidle_device,
    pass it the cpuidle_add_sysfs function. This function calls
    per_cpu(cpuidle_devices, cpu) to get the cpuidle_device.
    
    This patch pass the cpuidle_device instead and simplify the code.
    
    Signed-off-by: Daniel Lezcano <daniel.lezcano@linaro.org>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit a820608351a4dd5681e2e05e470e092d1e0cbfbe
Author: Colin Cross <ccross@android.com>
Date:   Mon May 7 17:57:41 2012 -0700

    cpuidle: add support for states that affect multiple cpus
    
    On some ARM SMP SoCs (OMAP4460, Tegra 2, and probably more), the
    cpus cannot be independently powered down, either due to
    sequencing restrictions (on Tegra 2, cpu 0 must be the last to
    power down), or due to HW bugs (on OMAP4460, a cpu powering up
    will corrupt the gic state unless the other cpu runs a work
    around).  Each cpu has a power state that it can enter without
    coordinating with the other cpu (usually Wait For Interrupt, or
    WFI), and one or more "coupled" power states that affect blocks
    shared between the cpus (L2 cache, interrupt controller, and
    sometimes the whole SoC).  Entering a coupled power state must
    be tightly controlled on both cpus.
    
    The easiest solution to implementing coupled cpu power states is
    to hotplug all but one cpu whenever possible, usually using a
    cpufreq governor that looks at cpu load to determine when to
    enable the secondary cpus.  This causes problems, as hotplug is an
    expensive operation, so the number of hotplug transitions must be
    minimized, leading to very slow response to loads, often on the
    order of seconds.
    
    This file implements an alternative solution, where each cpu will
    wait in the WFI state until all cpus are ready to enter a coupled
    state, at which point the coupled state function will be called
    on all cpus at approximately the same time.
    
    Once all cpus are ready to enter idle, they are woken by an smp
    cross call.  At this point, there is a chance that one of the
    cpus will find work to do, and choose not to enter idle.  A
    final pass is needed to guarantee that all cpus will call the
    power state enter function at the same time.  During this pass,
    each cpu will increment the ready counter, and continue once the
    ready counter matches the number of online coupled cpus.  If any
    cpu exits idle, the other cpus will decrement their counter and
    retry.
    
    To use coupled cpuidle states, a cpuidle driver must:
    
       Set struct cpuidle_device.coupled_cpus to the mask of all
       coupled cpus, usually the same as cpu_possible_mask if all cpus
       are part of the same cluster.  The coupled_cpus mask must be
       set in the struct cpuidle_device for each cpu.
    
       Set struct cpuidle_device.safe_state to a state that is not a
       coupled state.  This is usually WFI.
    
       Set CPUIDLE_FLAG_COUPLED in struct cpuidle_state.flags for each
       state that affects multiple cpus.
    
       Provide a struct cpuidle_state.enter function for each state
       that affects multiple cpus.  This function is guaranteed to be
       called on all cpus at approximately the same time.  The driver
       should ensure that the cpus all abort together if any cpu tries
       to abort once the function is called.
    
    update1:
    
    cpuidle: coupled: fix count of online cpus
    
    online_count was never incremented on boot, and was also counting
    cpus that were not part of the coupled set.  Fix both issues by
    introducting a new function that counts online coupled cpus, and
    call it from register as well as the hotplug notifier.
    
    update2:
    
    cpuidle: coupled: fix decrementing ready count
    
    cpuidle_coupled_set_not_ready sometimes refuses to decrement the
    ready count in order to prevent a race condition.  This makes it
    unsuitable for use when finished with idle.  Add a new function
    cpuidle_coupled_set_done that decrements both the ready count and
    waiting count, and call it after idle is complete.
    
    Cc: Amit Kucheria <amit.kucheria@linaro.org>
    Cc: Arjan van de Ven <arjan@linux.intel.com>
    Cc: Trinabh Gupta <g.trinabh@gmail.com>
    Cc: Deepthi Dharwar <deepthi@linux.vnet.ibm.com>
    Reviewed-by: Santosh Shilimkar <santosh.shilimkar@ti.com>
    Tested-by: Santosh Shilimkar <santosh.shilimkar@ti.com>
    Reviewed-by: Kevin Hilman <khilman@ti.com>
    Tested-by: Kevin Hilman <khilman@ti.com>
    Signed-off-by: Colin Cross <ccross@android.com>
    Acked-by: Rafael J. Wysocki <rjw@sisk.pl>
    Signed-off-by: Len Brown <len.brown@intel.com>

commit 41d23170f72eaa8597150a1b4c81de42ecd67830
Author: Colin Cross <ccross@android.com>
Date:   Mon May 7 17:57:40 2012 -0700

    cpuidle: fix error handling in __cpuidle_register_device
    
    Fix the error handling in __cpuidle_register_device to include
    the missing list_del.  Move it to a label, which will simplify
    the error handling when coupled states are added.
    
    Reviewed-by: Santosh Shilimkar <santosh.shilimkar@ti.com>
    Tested-by: Santosh Shilimkar <santosh.shilimkar@ti.com>
    Reviewed-by: Kevin Hilman <khilman@ti.com>
    Tested-by: Kevin Hilman <khilman@ti.com>
    Signed-off-by: Colin Cross <ccross@android.com>
    Reviewed-by: Rafael J. Wysocki <rjw@sisk.pl>
    Signed-off-by: Len Brown <len.brown@intel.com>

commit b0aa26eb9375e866c7d1c7bfcc65f9ebbec66133
Author: Colin Cross <ccross@android.com>
Date:   Mon May 7 17:57:39 2012 -0700

    cpuidle: refactor out cpuidle_enter_state
    
    Split the code to enter a state and update the stats into a helper
    function, cpuidle_enter_state, and export it.  This function will
    be called by the coupled state code to handle entering the safe
    state and the final coupled state.
    
    Reviewed-by: Santosh Shilimkar <santosh.shilimkar@ti.com>
    Tested-by: Santosh Shilimkar <santosh.shilimkar@ti.com>
    Reviewed-by: Kevin Hilman <khilman@ti.com>
    Tested-by: Kevin Hilman <khilman@ti.com>
    Signed-off-by: Colin Cross <ccross@android.com>
    Reviewed-by: Rafael J. Wysocki <rjw@sisk.pl>
    Signed-off-by: Len Brown <len.brown@intel.com>

commit e961aa39dff76e12c47bb6559dbefb5a10f9cf50
Author: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
Date:   Fri May 4 14:06:02 2012 -0700

    cpuidle: add checks to avoid NULL pointer dereference
    
    The existing check for dev == NULL in __cpuidle_register_device() is
    rendered useless because dev is dereferenced before the check itself.
    Moreover, correctly speaking, it is the job of the callers of this
    function, i.e., cpuidle_register_device() & cpuidle_enable_device() (which
    also happen to be exported functions) to ensure that
    __cpuidle_register_device() is called with a non-NULL dev.
    
    So add the necessary dev == NULL checks in the two callers and remove the
    (useless) check from __cpuidle_register_device().
    
    Signed-off-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Acked-by: Daniel Lezcano <daniel.lezcano@linaro.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Len Brown <len.brown@intel.com>
    
    Conflicts:
    	drivers/cpuidle/cpuidle.c

commit 2dafd19dff54620a45f38eca3c5dfb37c1b7ec01
Author: Sergey Senozhatsky <sergey.senozhatsky@gmail.com>
Date:   Fri May 4 14:06:02 2012 -0700

    cpuidle: remove unused hrtimer_peek_ahead_timers() call
    
      commit 9a6558371bcd01c2973b7638181db4ccc34eab4f
      Author: Arjan van de Ven <arjan@linux.intel.com>
      Date:   Sun Nov 9 12:45:10 2008 -0800
    
         regression: disable timer peek-ahead for 2.6.28
    
         It's showing up as regressions; disabling it very likely just papers
         over an underlying issue, but time is running out for 2.6.28, lets get
         back to this for 2.6.29
    
     Many years has passed since 2008, so it seems ok to remove whole `#if 0' block.
    
    Signed-off-by: Sergey Senozhatsky <sergey.senozhatsky@gmail.com>
    Cc: Kevin Hilman <khilman@ti.com>
    Cc: Trinabh Gupta <g.trinabh@gmail.com>
    Cc: Deepthi Dharwar <deepthi@linux.vnet.ibm.com>
    Cc: Arjan van de Ven <arjan@infradead.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Len Brown <len.brown@intel.com>

commit 8416b41aa5dce25dc65e7aef8359b11bb3fd24ce
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon May 7 17:59:48 2012 +0000

    cpuidle: Use kick_all_cpus_sync()
    
    kick_all_cpus_sync() is the core implementation of cpu_idle_wait()
    which is copied all over the arch code.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/20120507175652.119842173@linutronix.de

commit 39cd48f2eb0677ef49ea510074a705703408acf3
Author: Dan Carpenter <dan.carpenter@oracle.com>
Date:   Wed May 14 19:08:46 2014 +0300

    PM / hibernate: Fix memory corruption in resumedelay_setup()
    
    In the original code "resume_delay" is an int so on 64 bits, the call to
    kstrtoul() will cause memory corruption.  We may as well fix a style
    issue here as well and make "resume_delay" unsigned int, since that's
    what we pass to ssleep().
    
    Fixes: 317cf7e5e85e (PM / hibernate: convert simple_strtoul to kstrtoul)
    Signed-off-by: Dan Carpenter <dan.carpenter@oracle.com>
    Acked-by: Pavel Machek <pavel@ucw.cz>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit 49660bab8bafb46bd55956b6246acd2314ba3251
Author: Fabian Frederick <fabf@skynet.be>
Date:   Fri May 9 23:32:08 2014 +0200

    PM / hibernate: convert simple_strtoul to kstrtoul
    
    Replace obsolete function.
    
    Signed-off-by: Fabian Frederick <fabf@skynet.be>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit 98cbc19d54c87da7ace7d43a28269c3ce992df51
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Tue Jun 19 11:58:27 2012 -0700

    rcu: Disable preemption in rcu_blocking_is_gp()
    
    It is time to optimize CONFIG_TREE_PREEMPT_RCU's synchronize_rcu()
    for uniprocessor optimization, which means that rcu_blocking_is_gp()
    can no longer rely on RCU read-side critical sections having disabled
    preemption.  This commit therefore disables preemption across
    rcu_blocking_is_gp()'s scan of the cpu_online_mask.
    
    (Updated from previous version to fix embarrassing bug spotted by
    Wu Fengguang.)
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

commit d1492060f85517a996ce82145ab65bfdd4760c2c
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Wed May 30 03:21:48 2012 -0700

    rcu: Split RCU core processing out of __call_rcu()
    
    The __call_rcu() function is a bit overweight, so this commit splits
    it into actual enqueuing of and accounting for the callback (__call_rcu())
    and associated RCU-core processing (__call_rcu_core()).
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Reviewed-by: Josh Triplett <josh@joshtriplett.org>

commit f1d4c836043afcfe111a78df19399cf9af3f2a70
Author: Paul E. McKenney <paul.mckenney@linaro.org>
Date:   Sat May 26 08:56:01 2012 -0700

    rcu: Prevent __call_rcu() from invoking RCU core on offline CPUs
    
    The __call_rcu() function will invoke the RCU core, for example, if
    it detects that the current CPU has too many callbacks.  However, this
    can happen on an offline CPU that is on its way to the idle loop, in
    which case it is an error to invoke the RCU core, and the excess callbacks
    will be adopted in any case.  This commit therefore adds checks to
    __call_rcu() for running on an offline CPU, refraining from invoking
    the RCU core in this case.
    
    Signed-off-by: Paul E. McKenney <paul.mckenney@linaro.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Reviewed-by: Josh Triplett <josh@joshtriplett.org>

commit ff99cb9836953c14e8ef9be5ff006d12912c2553
Author: Paul E. McKenney <paul.mckenney@linaro.org>
Date:   Tue May 22 22:10:24 2012 -0700

    rcu: Make __call_rcu() handle invocation from idle
    
    Although __call_rcu() is handled correctly when called from a momentary
    non-idle period, if it is called on a CPU that RCU believes to be idle
    on RCU_FAST_NO_HZ kernels, the callback might be indefinitely postponed.
    This commit therefore ensures that RCU is aware of the new callback and
    has a chance to force the CPU out of dyntick-idle mode when a new callback
    is posted.
    
    Reported-by: Frederic Weisbecker <fweisbec@gmail.com>
    Signed-off-by: Paul E. McKenney <paul.mckenney@linaro.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Reviewed-by: Josh Triplett <josh@joshtriplett.org>

commit ac647998cb9b50f22867a7c9d90a526ce4e879a5
Author: Paul E. McKenney <paul.mckenney@linaro.org>
Date:   Wed May 9 15:44:42 2012 -0700

    rcu: Add ACCESS_ONCE() to ->qlen accesses
    
    The _rcu_barrier() function accesses other CPUs' rcu_data structure's
    ->qlen field without benefit of locking.  This commit therefore adds
    the required ACCESS_ONCE() wrappers around accesses and updates that
    need it.
    
    ACCESS_ONCE() is not needed when a CPU accesses its own ->qlen, or
    in code that cannot run while _rcu_barrier() is sampling ->qlen fields.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Reviewed-by: Josh Triplett <josh@joshtriplett.org>

commit ec625d20d95a2fe56e5bda1a2a5013f14c18ce92
Author: Paul E. McKenney <paul.mckenney@linaro.org>
Date:   Wed May 9 15:39:56 2012 -0700

    rcu: Consolidate duplicate callback-list initialization
    
    There are a couple of open-coded initializations of the rcu_data
    structure's RCU callback list.  This commit therefore consolidates
    them into a new init_callback_list() function.
    
    Signed-off-by: Paul E. McKenney <paul.mckenney@linaro.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Reviewed-by: Josh Triplett <josh@joshtriplett.org>

commit 81f06cf57555b8d7735fff0b4c3708172acd3c60
Author: Paul E. McKenney <paul.mckenney@linaro.org>
Date:   Wed May 9 08:45:12 2012 -0700

    rcu: Fix detection of abruptly-ending stall
    
    The code that attempts to identify stalls that end just as we detect
    them is broken by both flavors of initialization failure.  This commit
    therefore properly initializes and computes the count of the number
    of reasons why the RCU grace period is stalled.
    
    Signed-off-by: Paul E. McKenney <paul.mckenney@linaro.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Reviewed-by: Josh Triplett <josh@joshtriplett.org>

commit bacff57bb9c3f1a854bcf018b6262f08000f89e4
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Tue Jun 12 13:16:58 2012 -0700

    rcu: RCU_SAVE_DYNTICK code no longer ever dead
    
    Before RCU had unified idle, the RCU_SAVE_DYNTICK leg of the switch
    statement in force_quiescent_state() was dead code for CONFIG_NO_HZ=n
    kernel builds.  With unified idle, the code is never dead.  This commit
    therefore removes the "if" statement designed to make gcc aware of when
    the code was and was not dead.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Reviewed-by: Josh Triplett <josh@joshtriplett.org>

commit 863d021effbe538a4bbc1a09fb3fff6ba1060a69
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Tue Jun 12 11:01:13 2012 -0700

    rcu: Introduce for_each_rcu_flavor() and use it
    
    The arrival of TREE_PREEMPT_RCU some years back included some ugly
    code involving either #ifdef or #ifdef'ed wrapper functions to iterate
    over all non-SRCU flavors of RCU.  This commit therefore introduces
    a for_each_rcu_flavor() iterator over the rcu_state structures for each
    flavor of RCU to clean up a bit of the ugliness.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

commit ede262060dddd13d5598cbfb4812fb6bd3ba6ed4
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Tue Jun 12 09:40:38 2012 -0700

    rcu: Remove unneeded __rcu_process_callbacks() argument
    
    With the advent of __this_cpu_ptr(), it is no longer necessary to pass
    both the rcu_state and rcu_data structures into __rcu_process_callbacks().
    This commit therefore computes the rcu_data pointer from the rcu_state
    pointer within __rcu_process_callbacks() so that callers can pass in
    only the pointer to the rcu_state structure.  This paves the way for
    linking the rcu_state structures together and iterating over them.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Reviewed-by: Josh Triplett <josh@joshtriplett.org>

commit d8d63d4759c7a617a2d7f285fc7f8aae36397a3f
Author: Paul E. McKenney <paul.mckenney@linaro.org>
Date:   Wed May 23 18:47:05 2012 -0700

    rcu: Add tracing for _rcu_barrier()
    
    This commit adds event tracing for _rcu_barrier() execution.  This
    is defined only if RCU_TRACE=y.
    
    Signed-off-by: Paul E. McKenney <paul.mckenney@linaro.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Reviewed-by: Josh Triplett <josh@joshtriplett.org>

commit 7a9d7cc3784c015c2b95c7dc39374ed1ee83d6cf
Author: Paul E. McKenney <paul.mckenney@linaro.org>
Date:   Tue May 29 14:56:46 2012 -0700

    rcu: Increase rcu_barrier() concurrency
    
    The traditional rcu_barrier() implementation has serialized all requests,
    regardless of RCU flavor, and also does not coalesce concurrent requests.
    In the past, this has been good and sufficient.
    
    However, systems are getting larger and use of rcu_barrier() has been
    increasing.  This commit therefore introduces a counter-based scheme
    that allows _rcu_barrier() calls for the same flavor of RCU to take
    advantage of each others' work.
    
    Signed-off-by: Paul E. McKenney <paul.mckenney@linaro.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

commit b9f487b65458184c94e332ffd2a106f7c853a13c
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Fri Jun 15 18:22:05 2012 -0700

    rcu: Remove needless initialization
    
    For global variables, C defaults all fields to zero.  The initialization
    of the rcu_state structure's ->n_force_qs and ->n_force_qs_ngp fields
    is therefore redundant, so this commit removes these initializations.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

commit 8d842088fbd1fda87bd1113bebf6d1151cd6c69a
Author: Paul E. McKenney <paul.mckenney@linaro.org>
Date:   Tue May 29 05:18:53 2012 -0700

    rcu: Move rcu_barrier_mutex to rcu_state structure
    
    In order to allow each RCU flavor to concurrently execute its
    rcu_barrier() function, it is necessary to move the relevant
    state to the rcu_state structure.  This commit therefore moves the
    rcu_barrier_mutex global variable to a new ->barrier_mutex field
    in the rcu_state structure.
    
    Signed-off-by: Paul E. McKenney <paul.mckenney@linaro.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

commit ef33562e85cadd77fd5c2c8d8dcdd45b2412e6c3
Author: Paul E. McKenney <paul.mckenney@linaro.org>
Date:   Tue May 29 03:03:37 2012 -0700

    rcu: Move rcu_barrier_completion to rcu_state structure
    
    In order to allow each RCU flavor to concurrently execute its
    rcu_barrier() function, it is necessary to move the relevant
    state to the rcu_state structure.  This commit therefore moves the
    rcu_barrier_completion global variable to a new ->barrier_completion
    field in the rcu_state structure.
    
    Signed-off-by: Paul E. McKenney <paul.mckenney@linaro.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Reviewed-by: Josh Triplett <josh@joshtriplett.org>

commit 0aee96adbf224accad73ce51d94d66f09d96f17a
Author: Paul E. McKenney <paul.mckenney@linaro.org>
Date:   Tue May 29 00:34:56 2012 -0700

    rcu: Move rcu_barrier_cpu_count to rcu_state structure
    
    In order to allow each RCU flavor to concurrently execute its rcu_barrier()
    function, it is necessary to move the relevant state to the rcu_state
    structure.  This commit therefore moves the rcu_barrier_cpu_count global
    variable to a new ->barrier_cpu_count field in the rcu_state structure.
    
    Signed-off-by: Paul E. McKenney <paul.mckenney@linaro.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Reviewed-by: Josh Triplett <josh@joshtriplett.org>

commit 22131281ddb13222f8c98c440466bdc741b80b64
Author: Paul E. McKenney <paul.mckenney@linaro.org>
Date:   Mon May 28 23:57:46 2012 -0700

    rcu: Move _rcu_barrier()'s rcu_head structures to rcu_data structures
    
    In order for multiple flavors of RCU to each concurrently run one
    rcu_barrier(), each flavor needs its own per-CPU set of rcu_head
    structures.  This commit therefore moves _rcu_barrier()'s set of
    per-CPU rcu_head structures from per-CPU variables to the existing
    per-CPU and per-RCU-flavor rcu_data structures.
    
    Signed-off-by: Paul E. McKenney <paul.mckenney@linaro.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Reviewed-by: Josh Triplett <josh@joshtriplett.org>

commit 366cdf46c9a98cc612f9cdf23be0c4e662f5d78b
Author: Paul E. McKenney <paul.mckenney@linaro.org>
Date:   Mon May 28 23:26:01 2012 -0700

    rcu: Place pointer to call_rcu() in rcu_data structure
    
    This is a preparatory commit for increasing rcu_barrier()'s concurrency.
    It adds a pointer in the rcu_data structure to the corresponding call_rcu()
    function.  This allows a pointer to the rcu_data structure to imply the
    function pointer, which allows _rcu_barrier() state to be placed in the
    rcu_state structure.
    
    Signed-off-by: Paul E. McKenney <paul.mckenney@linaro.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Reviewed-by: Josh Triplett <josh@joshtriplett.org>

commit 4cd30484baafe4ff08f1e3de953c50f04bb4cd68
Author: Paul E. McKenney <paul.mckenney@linaro.org>
Date:   Mon May 28 19:41:41 2012 -0700

    rcu: Prevent excessive line length in RCU_STATE_INITIALIZER()
    
    Upcoming rcu_barrier() concurrency commits will result in line lengths
    greater than 80 characters in the RCU_STATE_INITIALIZER(), so this commit
    shortens the name of the macro's argument to prevent this.
    
    Signed-off-by: Paul E. McKenney <paul.mckenney@linaro.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Reviewed-by: Josh Triplett <josh@joshtriplett.org>

commit d691c5858dfb9ab1ba2c403d9f75f1cf8249ea9a
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Tue May 8 21:00:28 2012 -0700

    rcu: Size rcu_node tree from nr_cpu_ids rather than NR_CPUS
    
    The rcu_node tree array is sized based on compile-time constants,
    including NR_CPUS.  Although this approach has worked well in the past,
    the recent trend by many distros to define NR_CPUS=4096 results in
    excessive grace-period-initialization latencies.
    
    This commit therefore substitutes the run-time computed nr_cpu_ids for
    the compile-time NR_CPUS when building the tree.  This can result in
    much of the compile-time-allocated rcu_node array being unused.  If
    this is a major problem, you are in a specialized situation anyway,
    so you can manually adjust the NR_CPUS, RCU_FANOUT, and RCU_FANOUT_LEAF
    kernel config parameters.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

commit 91c0c830e748ffa07589299f9c8f438e222fa035
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Mon Apr 23 15:52:53 2012 -0700

    rcu: Control RCU_FANOUT_LEAF from boot-time parameter
    
    Although making RCU_FANOUT_LEAF a kernel configuration parameter rather
    than a fixed constant makes it easier for people to decrease cache-miss
    overhead for large systems, it is of little help for people who must
    run a single pre-built kernel binary.
    
    This commit therefore allows the value of RCU_FANOUT_LEAF to be
    increased (but not decreased!) via a boot-time parameter named
    rcutree.rcu_fanout_leaf.
    
    Reported-by: Mike Galbraith <efault@gmx.de>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

commit d47f7096c850ea1c2a096fceab6376e872e9de95
Author: Paul E. McKenney <paul.mckenney@linaro.org>
Date:   Fri May 4 08:31:53 2012 -0700

    rcu: RCU_FAST_NO_HZ detection of callback adoption
    
    In the present implementations of CPU hotplug, the outgoing CPU is
    guaranteed to run its stop-machine process on the way out, which
    will guarantee that RCU_FAST_NO_HZ forces the CPU out of dyntick-idle
    mode.
    
    However, new versions of CPU hotplug might not work this way.  This
    commit therefore removes this design constraint by explicitly notifying
    CPUs when they adopt non-lazy RCU callbacks.
    
    Signed-off-by: Paul E. McKenney <paul.mckenney@linaro.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Tested-by: Heiko Carstens <heiko.carstens@de.ibm.com>
    Tested-by: Pascal Chapperon <pascal.chapperon@wanadoo.fr>

commit 6b7b1e4b01361ae212d987f9c5ac665f72fdecf2
Author: Paul E. McKenney <paul.mckenney@linaro.org>
Date:   Thu Feb 23 13:30:16 2012 -0800

    rcu: Document why rcu_blocking_is_gp() is safe
    
    The rcu_blocking_is_gp() function tests to see if there is only one
    online CPU, and if so, synchronize_sched() and friends become no-ops.
    However, for larger systems, num_online_cpus() scans a large vector,
    and might be preempted while doing so.  While preempted, any number
    of CPUs might come online and go offline, potentially resulting in
    num_online_cpus() returning 1 when there never had only been one
    CPU online.  This could result in a too-short RCU grace period, which
    could in turn result in total failure, except that the only way that
    the grace period is too short is if there is an RCU read-side critical
    section spanning it.  For RCU-sched and RCU-bh (which are the only
    cases using rcu_blocking_is_gp()), RCU read-side critical sections
    have either preemption or bh disabled, which prevents CPUs from going
    offline.  This in turn prevents actual failures from occurring.
    
    This commit therefore adds a large block comment to rcu_blocking_is_gp()
    documenting why it is safe.  This commit also moves rcu_blocking_is_gp()
    into kernel/rcutree.c, which should help prevent unwary developers from
    mistaking it for a generally useful function.
    
    Signed-off-by: Paul E. McKenney <paul.mckenney@linaro.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

commit 93c8fce47a996e8e33b80aa919fa665e4d4c00f9
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Thu Apr 19 12:20:14 2012 -0700

    rcu: Reduce cache-miss initialization latencies for large systems
    
    Commit #0209f649 (rcu: limit rcu_node leaf-level fanout) set an upper
    limit of 16 on the leaf-level fanout for the rcu_node tree.  This was
    needed to reduce lock contention that was induced by the synchronization
    of scheduling-clock interrupts, which was in turn needed to improve
    energy efficiency for moderate-sized lightly loaded servers.
    
    However, reducing the leaf-level fanout means that there are more
    leaf-level rcu_node structures in the tree, which in turn means that
    RCU's grace-period initialization incurs more cache misses.  This is
    not a problem on moderate-sized servers with only a few tens of CPUs,
    but becomes a major source of real-time latency spikes on systems with
    many hundreds of CPUs.  In addition, the workloads running on these large
    systems tend to be CPU-bound, which eliminates the energy-efficiency
    advantages of synchronizing scheduling-clock interrupts.  Therefore,
    these systems need maximal values for the rcu_node leaf-level fanout.
    
    This commit addresses this problem by introducing a new kernel parameter
    named RCU_FANOUT_LEAF that directly controls the leaf-level fanout.
    This parameter defaults to 16 to handle the common case of a moderate
    sized lightly loaded servers, but may be set higher on larger systems.
    
    Reported-by: Mike Galbraith <efault@gmx.de>
    Reported-by: Dimitri Sivanich <sivanich@sgi.com>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

commit 06beae5a7ee0d1d7dd98fa917748f5db025437b4
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Sat Sep 8 16:14:02 2012 +0200

    vtime: Consolidate system/idle context detection
    
    Move the code that finds out to which context we account the
    cputime into generic layer.
    
    Archs that consider the whole time spent in the idle task as idle
    time (ia64, powerpc) can rely on the generic vtime_account()
    and implement vtime_account_system() and vtime_account_idle(),
    letting the generic code to decide when to call which API.
    
    Archs that have their own meaning of idle time, such as s390
    that only considers the time spent in CPU low power mode as idle
    time, can just override vtime_account().
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>

commit c46785f8987499d00e60fc428e7596633bcb47f0
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Sat Sep 8 15:23:11 2012 +0200

    cputime: Use a proper subsystem naming for vtime related APIs
    
    Use a naming based on vtime as a prefix for virtual based
    cputime accounting APIs:
    
    - account_system_vtime() -> vtime_account()
    - account_switch_vtime() -> vtime_task_switch()
    
    It makes it easier to allow for further declension such
    as vtime_account_system(), vtime_account_idle(), ... if we
    want to find out the context we account to from generic code.
    
    This also make it better to know on which subsystem these APIs
    refer to.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>

commit 2f29ba36d434703152dd3e2e1a8556dae3cc9aa8
Author: Alex Shi <alex.shi@intel.com>
Date:   Mon Sep 10 15:10:58 2012 +0800

    sched/nohz: Clean up select_nohz_load_balancer()
    
    There is no load_balancer to be selected now. It just sets the
    state of the nohz tick to stop.
    
    So rename the function, pass the 'cpu' as a parameter and then
    remove the useless call from tick_nohz_restart_sched_tick().
    
    [ s/set_nohz_tick_stopped/nohz_balance_enter_idle/g
      s/clear_nohz_tick_stopped/nohz_balance_exit_idle/g ]
    Signed-off-by: Alex Shi <alex.shi@intel.com>
    Acked-by: Suresh Siddha <suresh.b.siddha@intel.com>
    Cc: Venkatesh Pallipadi <venki@google.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/1347261059-24747-1-git-send-email-alex.shi@intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 7209ef06178a0386991046c6854800efb43797ee
Author: Paul E. McKenney <paul.mckenney@linaro.org>
Date:   Sun Jun 24 10:15:02 2012 -0700

    rcu: Make RCU_FAST_NO_HZ respect nohz= boot parameter
    
    If the nohz= boot parameter disables nohz, then RCU_FAST_NO_HZ needs to
    also disable itself.  This commit therefore checks for tick_nohz_enabled
    being zero, disabling rcu_prepare_for_idle() if so.  This commit assumes
    that tick_nohz_enabled can change at runtime: If this is not the case,
    then a simpler approach suffices.
    
    Signed-off-by: Paul E. McKenney <paul.mckenney@linaro.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

commit ed6e6b957a48772e9de0b5dfef0145ab9effacdb
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Mon Aug 1 01:25:38 2011 +0200

    nohz: Move next idle expiry time record into idle logic area
    
    The next idle expiry time record and idle sleeps tracking are
    statistics that only concern idle.
    
    Since we want the nohz APIs to become usable further idle
    context, let's pull up the handling of these statistics to the
    callers in idle.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Alessio Igor Bogani <abogani@kernel.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Avi Kivity <avi@redhat.com>
    Cc: Chris Metcalf <cmetcalf@tilera.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Daniel Lezcano <daniel.lezcano@linaro.org>
    Cc: Geoff Levand <geoff@infradead.org>
    Cc: Gilad Ben Yossef <gilad@benyossef.com>
    Cc: Hakan Akkan <hakanakkan@gmail.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Kevin Hilman <khilman@ti.com>
    Cc: Max Krasnyansky <maxk@qualcomm.com>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Stephen Hemminger <shemminger@vyatta.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Sven-Thorsten Dietrich <thebigcorporation@gmail.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>

commit d37bdaca52697b8e3356b721586fc8b151ca4e9d
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Mon Aug 1 00:06:10 2011 +0200

    nohz: Move ts->idle_calls incrementation into strict idle logic
    
    Since we want to prepare for making the nohz API to work further
    the idle case, we need to pull ts->idle_calls incrementation up to
    the callers in idle.
    
    To perform this, we split tick_nohz_stop_sched_tick() in two parts:
    a first one that checks if we can really stop the tick for idle,
    and another that actually stops it. Then from the callers in idle,
    we check if we can stop the tick and only then we increment idle_calls
    and finally relay to the nohz API that won't care about these details
    anymore.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Alessio Igor Bogani <abogani@kernel.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Avi Kivity <avi@redhat.com>
    Cc: Chris Metcalf <cmetcalf@tilera.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Daniel Lezcano <daniel.lezcano@linaro.org>
    Cc: Geoff Levand <geoff@infradead.org>
    Cc: Gilad Ben Yossef <gilad@benyossef.com>
    Cc: Hakan Akkan <hakanakkan@gmail.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Kevin Hilman <khilman@ti.com>
    Cc: Max Krasnyansky <maxk@qualcomm.com>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Stephen Hemminger <shemminger@vyatta.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Sven-Thorsten Dietrich <thebigcorporation@gmail.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    
    Conflicts:
    	kernel/time/tick-sched.c

commit 7903288d003a38b02696bbab9e1ee14d1e0e90f0
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Sun Jul 31 17:44:12 2011 +0200

    nohz: Rename ts->idle_tick to ts->last_tick
    
    Now that idle and nohz logics are going to be independant each others,
    ts->idle_tick becomes too much a biased name to describe the field that
    saves the last scheduled tick on top of which we re-calculate the next
    tick to schedule when the timer is restarted.
    
    We want to reuse this even to stop the tick outside idle cases. So let's
    rename it to some more generic name: ts->last_tick.
    
    This changes a bit the timer list stat export so we need to increase its
    version.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Alessio Igor Bogani <abogani@kernel.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Avi Kivity <avi@redhat.com>
    Cc: Chris Metcalf <cmetcalf@tilera.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Daniel Lezcano <daniel.lezcano@linaro.org>
    Cc: Geoff Levand <geoff@infradead.org>
    Cc: Gilad Ben Yossef <gilad@benyossef.com>
    Cc: Hakan Akkan <hakanakkan@gmail.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Kevin Hilman <khilman@ti.com>
    Cc: Max Krasnyansky <maxk@qualcomm.com>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Stephen Hemminger <shemminger@vyatta.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Sven-Thorsten Dietrich <thebigcorporation@gmail.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>

commit f108ff019e0de8c3606213bf9968191420ec2885
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Thu Jul 28 04:00:47 2011 +0200

    nohz: Make nohz API agnostic against idle ticks cputime accounting
    
    When the timer tick fires, it accounts the new jiffy as either part
    of system, user or idle time. This is how we record the cputime
    statistics.
    
    But when the tick is stopped from the idle task, we still need
    to record the number of jiffies spent tickless until we restart
    the tick and fall back to traditional tick-based cputime accounting.
    
    To do this, we take a snapshot of jiffies when the tick is stopped
    and compute the difference against the new value of jiffies when
    the tick is restarted. Then we account this whole difference to
    the idle cputime.
    
    However we are preparing to be able to stop the tick from other places
    than idle. So this idle time accounting needs to be performed from
    the callers of nohz APIs, not from the nohz APIs themselves because
    we now want them to be agnostic against places that stop/restart tick.
    
    Therefore, we pull the tickless idle time accounting out of generic
    nohz helpers up to idle entry/exit callers.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Alessio Igor Bogani <abogani@kernel.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Avi Kivity <avi@redhat.com>
    Cc: Chris Metcalf <cmetcalf@tilera.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Daniel Lezcano <daniel.lezcano@linaro.org>
    Cc: Geoff Levand <geoff@infradead.org>
    Cc: Gilad Ben Yossef <gilad@benyossef.com>
    Cc: Hakan Akkan <hakanakkan@gmail.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Kevin Hilman <khilman@ti.com>
    Cc: Max Krasnyansky <maxk@qualcomm.com>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Stephen Hemminger <shemminger@vyatta.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Sven-Thorsten Dietrich <thebigcorporation@gmail.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    
    Conflicts:
    	kernel/time/tick-sched.c

commit eac2ced59959128238507c512ff86156598e74fc
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Wed Jul 27 17:29:28 2011 +0200

    nohz: Separate idle sleeping time accounting from nohz logic
    
    As we plan to be able to stop the tick outside the idle task, we
    need to prepare for separating nohz logic from idle. As a start,
    this pulls the idle sleeping time accounting out of the tick
    stop/restart API to the callers on idle entry/exit.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Alessio Igor Bogani <abogani@kernel.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Avi Kivity <avi@redhat.com>
    Cc: Chris Metcalf <cmetcalf@tilera.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Daniel Lezcano <daniel.lezcano@linaro.org>
    Cc: Geoff Levand <geoff@infradead.org>
    Cc: Gilad Ben Yossef <gilad@benyossef.com>
    Cc: Hakan Akkan <hakanakkan@gmail.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Kevin Hilman <khilman@ti.com>
    Cc: Max Krasnyansky <maxk@qualcomm.com>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Stephen Hemminger <shemminger@vyatta.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Sven-Thorsten Dietrich <thebigcorporation@gmail.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    
    Conflicts:
    	kernel/time/tick-sched.c

commit 3d5ba16dcda01e217b1caa1649926014a71a35bd
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri May 25 14:08:57 2012 +0200

    tick: Move skew_tick option into the HIGH_RES_TIMER section
    
    commit 5307c95 (tick: Add tick skew boot option) broke the
    !CONFIG_HIGH_RES_TIMERS build.
    
    Move the boot option parsing into the CONFIG_HIGH_RES_TIMERS section.
    
    Reported-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Mike Galbraith <mgalbraith@suse.de>

commit a93c828bb9d608e9a4a341ded6036adc71e86303
Author: Mike Galbraith <mgalbraith@suse.de>
Date:   Tue May 8 12:20:58 2012 +0200

    tick: Add tick skew boot option
    
    Let the user decide whether power consumption or jitter is the
    more important consideration for their machines.
    
    Quoting removal commit af5ab277ded04bd9bc6b048c5a2f0e7d70ef0867:
    
    "Historically, Linux has tried to make the regular timer tick on the
     various CPUs not happen at the same time, to avoid contention on
     xtime_lock.
    
     Nowadays, with the tickless kernel, this contention no longer happens
     since time keeping and updating are done differently. In addition,
     this skew is actually hurting power consumption in a measurable way on
     many-core systems."
    
    Problems:
    
    - Contrary to the above, systems do encounter contention on both
      xtime_lock and RCU structure locks when the tick is synchronized.
    
    - Moderate sized RT systems suffer intolerable jitter due to the tick
      being synchronized.
    
    - SGI reports the same for their large systems.
    
    - Fully utilized systems reap no power saving benefit from skew removal,
      but do suffer from resulting induced lock contention.
    
    - 0209f649 rcu: limit rcu_node leaf-level fanout
      This patch was born to combat lock contention which testing showed
      to have been _induced by_ skew removal.  Skew the tick, contention
      disappeared virtually completely.
    
    Signed-off-by: Mike Galbraith <mgalbraith@suse.de>
    Link: http://lkml.kernel.org/r/1336472458.21924.78.camel@marge.simpson.net
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

commit 35dd89e132a3b0c2447efc218eee692d464149de
Author: Seth Jennings <sjenning@linux.vnet.ibm.com>
Date:   Wed Jan 30 09:36:52 2013 -0600

    staging: zsmalloc: remove unused pool name
    
    zs_create_pool() currently takes a name argument which is
    never used in any useful way.
    
    This patch removes it.
    
    Signed-off-by: Seth Jennings <sjenning@linux.vnet.ibm.com>
    Acked-by: Nitin Gupta <ngupta@vflare.org>
    Acked-by: Rik van Riel <riel@redhat.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 8751231df93ab3aa42cbd8dc9fa3b8995e1c8807
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Thu Sep 6 00:03:50 2012 +0200

    sched: Fix load avg vs. cpu-hotplug
    
    Commit f319da0c68 ("sched: Fix load avg vs cpu-hotplug") was an
    incomplete fix:
    
    In particular, the problem is that at the point it calls
    calc_load_migrate() nr_running := 1 (the stopper thread), so move the
    call to CPU_DEAD where we're sure that nr_running := 0.
    
    Also note that we can call calc_load_migrate() without serialization, we
    know the state of rq is stable since its cpu is dead, and we modify the
    global state using appropriate atomic ops.
    
    Suggested-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/1346882630.2600.59.camel@twins
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit c9327f51e39bcf54841424e3a62299be97697439
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Jun 18 16:34:11 2015 +0200

    sched: fix build errors

commit 653b602ef1e518e3fba129c04426f5b79fed2919
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Wed Sep 12 11:22:00 2012 +0200

    sched: Remove __ARCH_WANT_INTERRUPTS_ON_CTXSW
    
    Now that the last architecture to use this has stopped doing so (ARM,
    thanks Catalin!) we can remove this complexity from the scheduler
    core.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Link: http://lkml.kernel.org/n/tip-g9p2a1w81xxbrze25v9zpzbf@git.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 5456ba1dfa9573ecfc3075ea16263ff3803c9740
Author: Vincent Guittot <vincent.guittot@linaro.org>
Date:   Thu Sep 13 06:11:26 2012 +0200

    sched: Fix nohz_idle_balance()
    
    On tickless systems, one CPU runs load balance for all idle CPUs.
    
    The cpu_load of this CPU is updated before starting the load balance
    of each other idle CPUs. We should instead update the cpu_load of
    the balance_cpu.
    
    Signed-off-by: Vincent Guittot <vincent.guittot@linaro.org>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Venkatesh Pallipadi <venki@google.com>
    Cc: Suresh Siddha <suresh.b.siddha@intel.com>
    Link: http://lkml.kernel.org/r/1347509486-8688-1-git-send-email-vincent.guittot@linaro.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 1d00730ba24639e19ff85d63e1b82cf690520cda
Author: Michael Wang <wangyun@linux.vnet.ibm.com>
Date:   Tue Jul 3 14:34:02 2012 +0800

    sched: Remove useless code in yield_to()
    
    It's impossible to enter the else branch if we have set
    skip_clock_update in task_yield_fair(), as yield_to_task_fair()
     will directly return true after invoke task_yield_fair().
    
    Signed-off-by: Michael Wang <wangyun@linux.vnet.ibm.com>
    Acked-by: Mike Galbraith <efault@gmx.de>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/4FF2925A.9060005@linux.vnet.ibm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 7b0badc49bcd40a788bc89abe1df241d0ad5ee42
Author: Namhyung Kim <namhyung.kim@lge.com>
Date:   Thu Aug 16 13:21:05 2012 +0900

    sched: Remove AFFINE_WAKEUPS feature flag
    
    Commit beac4c7e4a1c ("sched: Remove AFFINE_WAKEUPS feature") removed
    use of the flag but left the definition. Get rid of it.
    
    Signed-off-by: Namhyung Kim <namhyung@kernel.org>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Mike Galbraith <efault@gmx.de>
    Link: http://lkml.kernel.org/r/1345090865-20851-1-git-send-email-namhyung@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 0f749e40c28188982ca3a4d845c0f48232be54a2
Author: Randy Dunlap <rdunlap@xenotime.net>
Date:   Sat Aug 18 17:45:08 2012 -0700

    sched: Fix kernel-doc warnings in kernel/sched/fair.c
    
    Fix two kernel-doc warnings in kernel/sched/fair.c:
    
      Warning(kernel/sched/fair.c:3660): Excess function parameter 'cpus' description in 'update_sg_lb_stats'
      Warning(kernel/sched/fair.c:3806): Excess function parameter 'cpus' description in 'update_sd_lb_stats'
    
    Signed-off-by: Randy Dunlap <rdunlap@xenotime.net>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/50303714.3090204@xenotime.net
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 2b5dfda09176905d00f01487d4962df405950c84
Author: Peter Boonstoppel <pboonstoppel@nvidia.com>
Date:   Thu Aug 9 15:34:47 2012 -0700

    sched: Unthrottle rt runqueues in __disable_runtime()
    
    migrate_tasks() uses _pick_next_task_rt() to get tasks from the
    real-time runqueues to be migrated. When rt_rq is throttled
    _pick_next_task_rt() won't return anything, in which case
    migrate_tasks() can't move all threads over and gets stuck in an
    infinite loop.
    
    Instead unthrottle rt runqueues before migrating tasks.
    
    Additionally: move unthrottle_offline_cfs_rqs() to rq_offline_fair()
    
    Signed-off-by: Peter Boonstoppel <pboonstoppel@nvidia.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Paul Turner <pjt@google.com>
    Link: http://lkml.kernel.org/r/5FBF8E85CA34454794F0F7ECBA79798F379D3648B7@HQMAIL04.nvidia.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit e4c23d455df6cc9ce8d87e9d5fe942413f35bccb
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Mon Aug 20 11:26:57 2012 +0200

    sched: Fix load avg vs cpu-hotplug
    
    Rabik and Paul reported two different issues related to the same few
    lines of code.
    
    Rabik's issue is that the nr_uninterruptible migration code is wrong in
    that he sees artifacts due to this (Rabik please do expand in more
    detail).
    
    Paul's issue is that this code as it stands relies on us using
    stop_machine() for unplug, we all would like to remove this assumption
    so that eventually we can remove this stop_machine() usage altogether.
    
    The only reason we'd have to migrate nr_uninterruptible is so that we
    could use for_each_online_cpu() loops in favour of
    for_each_possible_cpu() loops, however since nr_uninterruptible() is the
    only such loop and its using possible lets not bother at all.
    
    The problem Rabik sees is (probably) caused by the fact that by
    migrating nr_uninterruptible we screw rq->calc_load_active for both rqs
    involved.
    
    So don't bother with fancy migration schemes (meaning we now have to
    keep using for_each_possible_cpu()) and instead fold any nr_active delta
    after we migrate all tasks away to make sure we don't have any skewed
    nr_active accounting.
    
    Reported-by: Rakib Mullick <rakib.mullick@gmail.com>
    Reported-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/1345454817.23018.27.camel@twins
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 048e3fcda9fee33d4053bd01ef6c17cf5432c7b3
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Mon Jun 18 17:54:14 2012 +0200

    cputime: Consolidate vtime handling on context switch
    
    The archs that implement virtual cputime accounting all
    flush the cputime of a task when it gets descheduled
    and sometimes set up some ground initialization for the
    next task to account its cputime.
    
    These archs all put their own hooks in their context
    switch callbacks and handle the off-case themselves.
    
    Consolidate this by creating a new account_switch_vtime()
    callback called in generic code right after a context switch
    and that these archs must implement to flush the prev task
    cputime and initialize the next task cputime related state.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Acked-by: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>

commit fe9f73c4f1fc8e8c0e9749515ca4c31cc478f29d
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Sat Jun 16 15:57:37 2012 +0200

    sched: Move cputime code to its own file
    
    Extract cputime code from the giant sched/core.c and
    put it in its own file. This make it easier to deal with
    this particular area and de-bloat a bit more core.c
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Acked-by: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>

commit d311e73faf2e0d87c9b6815416348a36e39e7fcd
Author: Alex Shi <alex.shi@intel.com>
Date:   Thu Jul 26 08:55:34 2012 +0800

    sched: recover SD_WAKE_AFFINE in select_task_rq_fair and code clean up
    
    Since power saving code was removed from sched now, the implement
    code is out of service in this function, and even pollute other logical.
    like, 'want_sd' never has chance to be set '0', that remove the effect
    of SD_WAKE_AFFINE here.
    
    So, clean up the obsolete code, includes SD_PREFER_LOCAL.
    
    Signed-off-by: Alex Shi <alex.shi@intel.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/5028F431.6000306@intel.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

commit 2f0216f47472dbcbdf319a1c34c4fa15cfdf8957
Author: Michael Wang <wangyun@linux.vnet.ibm.com>
Date:   Mon Aug 6 16:41:59 2012 +0800

    sched: using dst_rq instead of this_rq during load balance
    
    As we already have dst_rq in lb_env, using or changing "this_rq" do not
    make sense.
    
    This patch will replace "this_rq" with dst_rq in load_balance, and we
    don't need to change "this_rq" while process LBF_SOME_PINNED any more.
    
    Signed-off-by: Michael Wang <wangyun@linux.vnet.ibm.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/501F8357.3070102@linux.vnet.ibm.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

commit 63b668750e05209b3e59e147c25ad2f3f62f67de
Author: Pekka Enberg <penberg@kernel.org>
Date:   Sat Aug 4 11:49:47 2012 +0300

    sched: Document schedule() entry points
    
    This patch adds a comment on top of the schedule() function to explain
    to scheduler newbies how the main scheduler function is entered.
    
    Acked-by: Randy Dunlap <rdunlap@xenotime.net>
    Explained-by: Ingo Molnar <mingo@kernel.org>
    Explained-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/1344070187-2420-1-git-send-email-penberg@kernel.org
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

commit b4e331aade47286ae3ed45eab586d32146f138d5
Author: Borislav Petkov <borislav.petkov@amd.com>
Date:   Wed Aug 8 16:16:04 2012 +0200

    sched: Fix __sched_period comment
    
    It should be sched_nr_latency so fix it before it annoys me more.
    
    Signed-off-by: Borislav Petkov <borislav.petkov@amd.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/1344435364-18632-1-git-send-email-bp@amd64.org
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

commit c2f38d02febaaf7051bffe67379314e2864fb9da
Author: Mike Galbraith <efault@gmx.de>
Date:   Tue Aug 7 10:02:38 2012 +0200

    sched,rt: fix isolated CPUs leaving root_task_group indefinitely throttled
    
    Root task group bandwidth replenishment must service all CPUs, regardless of
    where the timer was last started, and regardless of the isolation mechanism,
    lest 'Quoth the Raven, "Nevermore"' become rt scheduling policy.
    
    Signed-off-by: Mike Galbraith <efault@gmx.de>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/1344326558.6968.25.camel@marge.simpson.net
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

commit 08b8787b70fac029a470e3d9eb472cd288ec7d61
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Wed Aug 8 21:46:40 2012 +0200

    sched, cgroup: Reduce rq->lock hold times for large cgroup hierarchies
    
    Peter Portante reported that for large cgroup hierarchies (and or on
    large CPU counts) we get immense lock contention on rq->lock and stuff
    stops working properly.
    
    His workload was a ton of processes, each in their own cgroup,
    everybody idling except for a sporadic wakeup once every so often.
    
    It was found that:
    
      schedule()
        idle_balance()
          load_balance()
            local_irq_save()
            double_rq_lock()
            update_h_load()
              walk_tg_tree(tg_load_down)
                tg_load_down()
    
    Results in an entire cgroup hierarchy walk under rq->lock for every
    new-idle balance and since new-idle balance isn't throttled this
    results in a lot of work while holding the rq->lock.
    
    This patch does two things, it removes the work from under rq->lock
    based on the good principle of race and pray which is widely employed
    in the load-balancer as a whole. And secondly it throttles the
    update_h_load() calculation to max once per jiffy.
    
    I considered excluding update_h_load() for new-idle balance
    all-together, but purely relying on regular balance passes to update
    this data might not work out under some rare circumstances where the
    new-idle busiest isn't the regular busiest for a while (unlikely, but
    a nightmare to debug if someone hits it and suffers).
    
    Cc: pjt@google.com
    Cc: Larry Woodman <lwoodman@redhat.com>
    Cc: Mike Galbraith <efault@gmx.de>
    Reported-by: Peter Portante <pportant@redhat.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/n/tip-aaarrzfpnaam7pqrekofu8a6@git.kernel.org
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

commit 90fbbc2bc87f1f7ba3df823c4d6a8775dd62ae16
Author: Michael Wang <wangyun@linux.vnet.ibm.com>
Date:   Thu Jul 12 16:10:13 2012 +0800

    sched/cleanups: Add load balance cpumask pointer to 'struct lb_env'
    
    With this patch struct ld_env will have a pointer of the load balancing
    cpumask and we don't need to pass a cpumask around anymore.
    
    Signed-off-by: Michael Wang <wangyun@linux.vnet.ibm.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/4FFE8665.3080705@linux.vnet.ibm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit d4a28fdf866e64460c06df15b40dff039a9c06e0
Author: Andrew Vagin <avagin@openvz.org>
Date:   Thu Jul 12 14:14:29 2012 +0400

    sched: Deliver sched_switch events to the current task
    
    Otherwise they can't be filtered for a defined task:
    
      perf record -e sched:sched_switch ./foo
    
    This command doesn't report any events without this patch.
    
    I think it isn't a security concern if someone knows who will
    be executed next - this can already be observed by polling /proc
    state. By default perf is disabled for non-root users in any case.
    
    I need these events for profiling sleep times.  sched_switch is used for
    getting callchains and sched_stat_* is used for getting time periods.
    These events are combined in user space, then it can be analyzed by
    perf tools.
    
    Signed-off-by: Andrew Vagin <avagin@openvz.org>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Arun Sharma <asharma@fb.com>
    Link: http://lkml.kernel.org/r/1342088069-1005148-1-git-send-email-avagin@openvz.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 43bb738bbc7689cb8df1fbbfb75cbec1e1490dde
Author: Ying Xue <ying.xue0@gmail.com>
Date:   Thu Jul 12 15:03:42 2012 +0800

    sched: Fix minor code style issues
    
    Delete redudant spaces between type name and data name or operators.
    
    Signed-off-by: Ying Xue <ying.xue0@gmail.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/1342076622-6606-1-git-send-email-ying.xue0@gmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit f1a5e8dc34045b39b34942feb5284a72d6cd1964
Author: Namhyung Kim <namhyung.kim@lge.com>
Date:   Sat Jul 7 16:49:02 2012 +0900

    sched: Use task_rq_unlock() in __sched_setscheduler()
    
    It seems there's no specific reason to open-code it.  I guess
    commit 0122ec5b02f76 ("sched: Add p->pi_lock to task_rq_lock()")
    simply missed it.  Let's be consistent with others.
    
    Signed-off-by: Namhyung Kim <namhyung@kernel.org>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/1341647342-6742-1-git-send-email-namhyung@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit c206fb9328f61705dbc284c499b7f0b8a83e0f2d
Author: Srivatsa Vaddagiri <vatsa@linux.vnet.ibm.com>
Date:   Tue Jun 19 17:43:15 2012 +0530

    sched: Improve balance_cpu() to consider other cpus in its group as target of (pinned) task
    
    Current load balance scheme requires only one cpu in a
    sched_group (balance_cpu) to look at other peer sched_groups for
    imbalance and pull tasks towards itself from a busy cpu. Tasks
    thus pulled by balance_cpu could later get picked up by cpus
    that are in the same sched_group as that of balance_cpu.
    
    This scheme however fails to pull tasks that are not allowed to
    run on balance_cpu (but are allowed to run on other cpus in its
    sched_group). That can affect fairness and in some worst case
    scenarios cause starvation.
    
    Consider a two core (2 threads/core) system running tasks as
    below:
    
              Core0            Core1
             /     \          /     \
    	C0     C1	 C2     C3
            |      |         |      |
            v      v         v      v
    	F0     T1        F1     [idle]
    			 T2
    
     F0 = SCHED_FIFO task (pinned to C0)
     F1 = SCHED_FIFO task (pinned to C2)
     T1 = SCHED_OTHER task (pinned to C1)
     T2 = SCHED_OTHER task (pinned to C1 and C2)
    
    F1 could become a cpu hog, which will starve T2 unless C1 pulls
    it. Between C0 and C1 however, C0 is required to look for
    imbalance between cores, which will fail to pull T2 towards
    Core0. T2 will starve eternally in this case. The same scenario
    can arise in presence of non-rt tasks as well (say we replace F1
    with high irq load).
    
    We tackle this problem by having balance_cpu move pinned tasks
    to one of its sibling cpus (where they can run). We first check
    if load balance goal can be met by ignoring pinned tasks,
    failing which we retry move_tasks() with a new env->dst_cpu.
    
    This patch modifies load balance semantics on who can move load
    towards a given cpu in a given sched_domain.
    
    Before this patch, a given_cpu or a ilb_cpu acting on behalf of
    an idle given_cpu is responsible for moving load to given_cpu.
    
    With this patch applied, balance_cpu can in addition decide on
    moving some load to a given_cpu.
    
    There is a remote possibility that excess load could get moved
    as a result of this (balance_cpu and given_cpu/ilb_cpu deciding
    *independently* and at *same* time to move some load to a
    given_cpu). However we should see less of such conflicting
    decisions in practice and moreover subsequent load balance
    cycles should correct the excess load moved to given_cpu.
    
    Signed-off-by: Srivatsa Vaddagiri <vatsa@linux.vnet.ibm.com>
    Signed-off-by: Prashanth Nageshappa <prashanth@linux.vnet.ibm.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/4FE06CDB.2060605@linux.vnet.ibm.com
    [ minor edits ]
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    
    Conflicts:
    	kernel/sched/fair.c

commit d150be9774a4e64f07c097bb479fa540ee8f35b3
Author: Prashanth Nageshappa <prashanth@linux.vnet.ibm.com>
Date:   Tue Jun 19 17:52:07 2012 +0530

    sched: Reset loop counters if all tasks are pinned and we need to redo load balance
    
    While load balancing, if all tasks on the source runqueue are pinned,
    we retry after excluding the corresponding source cpu. However, loop counters
    env.loop and env.loop_break are not reset before retrying, which can lead
    to failure in moving the tasks. In this patch we reset env.loop and
    env.loop_break to their inital values before we retry.
    
    Signed-off-by: Prashanth Nageshappa <prashanth@linux.vnet.ibm.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/4FE06EEF.2090709@linux.vnet.ibm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit e843b82e1fae286876a0355fd6e168b52c5ef55d
Author: Prashanth Nageshappa <prashanth@linux.vnet.ibm.com>
Date:   Tue Jun 19 17:47:34 2012 +0530

    sched: Reorder 'struct lb_env' members to reduce its size
    
    Members of 'struct lb_env' are not in appropriate order to reuse compiler
    added padding on 64bit architectures. In this patch we reorder those struct
    members and help reduce the size of the structure from 96 bytes to 80
    bytes on 64 bit architectures.
    
    Suggested-by: Srivatsa Vaddagiri <vatsa@linux.vnet.ibm.com>
    Signed-off-by: Prashanth Nageshappa <prashanth@linux.vnet.ibm.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/4FE06DDE.7000403@linux.vnet.ibm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 73a1ed354589b13596992eedb92c51e5922a4c2d
Author: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
Date:   Thu May 24 19:46:55 2012 +0530

    cpusets, hotplug: Restructure functions that are invoked during hotplug
    
    Separate out the cpuset related handling for CPU/Memory online/offline.
    This also helps us exploit the most obvious and basic level of optimization
    that any notification mechanism (CPU/Mem online/offline) has to offer us:
    "We *know* why we have been invoked. So stop pretending that we are lost,
    and do only the necessary amount of processing!".
    
    And while at it, rename scan_for_empty_cpusets() to
    scan_cpusets_upon_hotplug(), which is more appropriate considering how
    it is restructured.
    
    Signed-off-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/20120524141650.3692.48637.stgit@srivatsabhat.in.ibm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 6b53f04c33b04a938b5d2fb744b54d46149b7c99
Author: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
Date:   Thu May 24 19:46:41 2012 +0530

    cpusets, hotplug: Implement cpuset tree traversal in a helper function
    
    At present, the functions that deal with cpusets during CPU/Mem hotplug
    are quite messy, since a lot of the functionality is mixed up without clear
    separation. And this takes a toll on optimization as well. For example,
    the function cpuset_update_active_cpus() is called on both CPU offline and CPU
    online events; and it invokes scan_for_empty_cpusets(), which makes sense
    only for CPU offline events. And hence, the current code ends up unnecessarily
    traversing the cpuset tree during CPU online also.
    
    As a first step towards cleaning up those functions, encapsulate the cpuset
    tree traversal in a helper function, so as to facilitate upcoming changes.
    
    Signed-off-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/20120524141635.3692.893.stgit@srivatsabhat.in.ibm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit b4c7feae31cb3faac296c3063d080412325ad2d6
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Mon Jul 2 07:08:42 2012 -0700

    Revert "rcu: Move PREEMPT_RCU preemption to switch_to() invocation"
    
    This reverts commit 616c310e83b872024271c915c1b9ab505b9efad9.
    (Move PREEMPT_RCU preemption to switch_to() invocation).
    Testing by Sasha Levin <levinsasha928@gmail.com> showed that this
    can result in deadlock due to invoking the scheduler when one of
    the runqueue locks is held.  Because this commit was simply a
    performance optimization, revert it.
    
    Reported-by: Sasha Levin <levinsasha928@gmail.com>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Tested-by: Sasha Levin <levinsasha928@gmail.com>

commit bab2e3d89af9230922a10ae339021ae2e02045ed
Author: Randy Dunlap <rdunlap@xenotime.net>
Date:   Fri Jun 8 13:18:33 2012 -0700

    sched/fair: fix lots of kernel-doc warnings
    
    Fix lots of new kernel-doc warnings in kernel/sched/fair.c:
    
      Warning(kernel/sched/fair.c:3625): No description found for parameter 'env'
      Warning(kernel/sched/fair.c:3625): Excess function parameter 'sd' description in 'update_sg_lb_stats'
      Warning(kernel/sched/fair.c:3735): No description found for parameter 'env'
      Warning(kernel/sched/fair.c:3735): Excess function parameter 'sd' description in 'update_sd_pick_busiest'
      Warning(kernel/sched/fair.c:3735): Excess function parameter 'this_cpu' description in 'update_sd_pick_busiest'
      .. more warnings
    
    Signed-off-by: Randy Dunlap <rdunlap@xenotime.net>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit ed8b97864032d9d4df4ee68be6d4230d70ec604e
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Thu May 31 21:20:16 2012 +0200

    sched: Validate assumptions in sched_init_numa()
    
    Add some code to validate assumptions we're making and output
    warnings if they are not.
    
    If this trigger we want to know about it.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Alex Shi <lkml.alex@gmail.com>
    Link: http://lkml.kernel.org/n/tip-6uc3wk5s9udxtdl9cnku0vtt@git.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 1b7882f7601cb20e644c99bd47d67fc32b62352a
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Thu May 31 12:05:32 2012 +0200

    sched: Always initialize cpu-power
    
    Often when we run into mis-shapen topologies the balance iteration
    fails to update the cpu power properly and we'll end up in /0 traps.
    
    Always initialize the cpu-power to a semi-sane value so that we can
    at least boot the machine, even if the load-balancer might not
    function correctly.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/n/tip-3lbhyj25sr169ha7z3qht5na@git.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 82a6d3cb960c7382158021bee65e8362203f5dbd
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Thu May 31 14:47:33 2012 +0200

    sched: Fix domain iteration
    
    Weird topologies can lead to asymmetric domain setups. This needs
    further consideration since these setups are typically non-minimal
    too.
    
    For now, make it work by adding an extra mask selecting which CPUs
    are allowed to iterate up.
    
    The topology that triggered it is the one from David Rientjes:
    
    	10 20 20 30
    	20 10 20 20
    	20 20 10 20
    	30 20 20 10
    
    resulting in boxes that wouldn't even boot.
    
    Reported-by: David Rientjes <rientjes@google.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/n/tip-3p86l9cuaqnxz7uxsojmz5rm@git.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 65c1f12c2e70fda0de01a4c0f904cac9215b3138
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Thu May 17 21:19:46 2012 +0200

    sched/rt: Fix lockdep annotation within find_lock_lowest_rq()
    
    Roland Dreier reported spurious, hard to trigger lockdep warnings
    within the scheduler - without any real lockup.
    
    This bit gives us the right clue:
    
    > [89945.640512]  [<ffffffff8103fa1a>] double_lock_balance+0x5a/0x90
    > [89945.640568]  [<ffffffff8104c546>] push_rt_task+0xc6/0x290
    
    if you look at that code you'll find the double_lock_balance() in
    question is the one in find_lock_lowest_rq() [yay for inlining].
    
    Now find_lock_lowest_rq() has a bug.. it fails to use
    double_unlock_balance() in one exit path, if this results in a retry in
    push_rt_task() we'll call double_lock_balance() again, at which point
    we'll run into said lockdep confusion.
    
    Reported-by: Roland Dreier <roland@kernel.org>
    Acked-by: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/1337282386.4281.77.camel@twins
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 8c34a116812471edd99b7016f29b907cea45d6ae
Author: Alex Shi <alex.shi@intel.com>
Date:   Wed Jun 6 14:52:51 2012 +0800

    sched/numa: Load balance between remote nodes
    
    Commit cb83b629b ("sched/numa: Rewrite the CONFIG_NUMA sched
    domain support") removed the NODE sched domain and started checking
    if the node distance in SLIT table is farther than REMOTE_DISTANCE,
    if so, it will lose the load balance chance at exec/fork/wake_affine
    points.
    
    But actually, even the node distance is farther than REMOTE_DISTANCE.
    
    Modern CPUs also has QPI like connections, which ensures that memory
    access is not too slow between nodes. So the above change in behavior
    on NUMA machine causes a performance regression on various benchmarks:
    hackbench, tbench, netperf, oltp, etc.
    
    This patch will recover the scheduler behavior to old mode on all my
    Intel platforms: NHM EP/EX, WSM EP, SNB EP/EP4S, and thus fixes the
    perfromance regressions. (all of them just have 2 kinds distance, 10, 21)
    
    Signed-off-by: Alex Shi <alex.shi@intel.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/1338965571-9812-1-git-send-email-alex.shi@intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 0e8d9b122a95ba77d91cda804de5a2325db70c66
Author: Kamalesh Babulal <kamalesh@linux.vnet.ibm.com>
Date:   Wed May 23 14:44:11 2012 +0530

    sched: Remove NULL assignment of dattr_cur
    
    Remove explicit NULL assignment of static pointer
    dattr_cur from init_sched_domains().
    
    Signed-off-by: Kamalesh Babulal <kamalesh@linux.vnet.ibm.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/20120523091411.GG5005@linux.vnet.ibm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 0adbf00c0e111767b30f92c3937c70009af3fb86
Author: alucard24 <dmbaoh2@gmail.com>
Date:   Mon Jun 8 20:37:01 2015 +0200

    Alucard, darkness and nightmare cpu govs: use max load for calculating target cpu frequency!

commit 625500dc34a63506242712a66cd178ab2e70e87a
Author: Hiroshi Shimamoto <h-shimamoto@ct.jp.nec.com>
Date:   Fri May 25 15:42:47 2012 +0900

    sched: Remove the last NULL entry from sched_feat_names
    
    No need to have the last NULL entry.
    
    Signed-off-by: Hiroshi Shimamoto <h-shimamoto@ct.jp.nec.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/4FBF29E7.5020805@ct.jp.nec.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit fd70b33b92973e70a03ae9407e1ca129b4e3edda
Author: Hiroshi Shimamoto <h-shimamoto@ct.jp.nec.com>
Date:   Fri May 25 15:41:54 2012 +0900

    sched: Make sched_feat_names const
    
    The strings sched_feat_names are never changed.
    
    Signed-off-by: Hiroshi Shimamoto <h-shimamoto@ct.jp.nec.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/4FBF29B2.9030904@ct.jp.nec.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 99d874930c5c1946ba0ad00285436b7135645fb3
Author: Colin Cross <ccross@android.com>
Date:   Wed May 16 21:34:23 2012 -0700

    sched/rt: Fix SCHED_RR across cgroups
    
    task_tick_rt() has an optimization to only reschedule SCHED_RR tasks
    if they were the only element on their rq.  However, with cgroups
    a SCHED_RR task could be the only element on its per-cgroup rq but
    still be competing with other SCHED_RR tasks in its parent's
    cgroup.  In this case, the SCHED_RR task in the child cgroup would
    never yield at the end of its timeslice.  If the child cgroup
    rt_runtime_us was the same as the parent cgroup rt_runtime_us,
    the task in the parent cgroup would starve completely.
    
    Modify task_tick_rt() to check that the task is the only task on its
    rq, and that the each of the scheduling entities of its ancestors
    is also the only entity on its rq.
    
    Signed-off-by: Colin Cross <ccross@android.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/1337229266-15798-1-git-send-email-ccross@android.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit b640a5528788af8883501fc7b4a217c7e4edadd2
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Mon Apr 23 12:11:21 2012 +0200

    sched: Move nr_cpus_allowed out of 'struct sched_rt_entity'
    
    Since nr_cpus_allowed is used outside of sched/rt.c and wants to be
    used outside of there more, move it to a more natural site.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/n/tip-kr61f02y9brwzkh6x53pdptm@git.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 34ce73a9484d33ce072094de7c9fd78dc4ccd44d
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Wed May 23 18:00:43 2012 +0200

    sched: Fix SD_OVERLAP
    
    SD_OVERLAP exists to allow overlapping groups, overlapping groups
    appear in NUMA topologies that aren't fully connected.
    
    The typical result of not fully connected NUMA is that each cpu (or
    rather node) will have different spans for a particular distance.
    However due to how sched domains are traversed -- only the first cpu
    in the mask goes one level up -- the next level only cares about the
    spans of the cpus that went up.
    
    Due to this two things were observed to be broken:
    
     - build_overlap_sched_groups() -- since its possible the cpu we're
       building the groups for exists in multiple (or all) groups, the
       selection criteria of the first group didn't ensure there was a cpu
       for which is was true that cpumask_first(span) == cpu. Thus load-
       balancing would terminate.
    
     - update_group_power() -- assumed that the cpu span of the first
       group of the domain was covered by all groups of the child domain.
       The above explains why this isn't true, so deal with it.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: David Rientjes <rientjes@google.com>
    Link: http://lkml.kernel.org/r/1337788843.9783.14.camel@laptop
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit ab269a56dc62dc30803c8c5ee0e68dc0fe02d630
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Fri May 25 09:26:43 2012 +0200

    sched: Don't try allocating memory from offline nodes
    
    Allocators don't appreciate it when you try and allocate memory from
    offline nodes.
    
    Reported-and-tested-by: Tony Luck <tony.luck@intel.com>
    Reported-and-tested-by: Anton Blanchard <anton@samba.org>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/n/tip-epfc1io9whb7o22bcujf31vn@git.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit d81b5f51475b1a08ec132d7276c9c9b199775267
Author: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
Date:   Sun Apr 20 23:43:01 2014 +0200

    PM / suspend: Make cpuidle work in the "freeze" state
    
    The "freeze" system sleep state introduced by commit 7e73c5ae6e79
    (PM: Introduce suspend state PM_SUSPEND_FREEZE) requires cpuidle
    to be functional when freeze_enter() is executed to work correctly
    (that is, to be able to save any more energy than runtime idle),
    but that is impossible after commit 8651f97bd951d (PM / cpuidle:
    System resume hang fix with cpuidle) which caused cpuidle to be
    paused in dpm_suspend_noirq() and resumed in dpm_resume_noirq().
    
    To avoid that problem, add cpuidle_resume() and cpuidle_pause()
    to the beginning and the end of freeze_enter(), respectively.
    
    Reported-by: Zhang Rui <rui.zhang@intel.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Reviewed-by: Preeti U Murthy <preeti@linux.vnet.ibm.com>

commit ee1320b452b43daf229bd2e48d299c929143c12e
Author: Jiri Olsa <jolsa@redhat.com>
Date:   Wed May 23 13:13:02 2012 +0200

    Revert "sched, perf: Use a single callback into the scheduler"
    
    This reverts commit cb04ff9ac424 ("sched, perf: Use a single
    callback into the scheduler").
    
    Before this change was introduced, the process switch worked
    like this (wrt. to perf event schedule):
    
         schedule (prev, next)
           - schedule out all perf events for prev
           - switch to next
           - schedule in all perf events for current (next)
    
    After the commit, the process switch looks like:
    
         schedule (prev, next)
           - schedule out all perf events for prev
           - schedule in all perf events for (next)
           - switch to next
    
    The problem is, that after we schedule perf events in, the pmu
    is enabled and we can receive events even before we make the
    switch to next - so "current" still being prev process (event
    SAMPLE data are filled based on the value of the "current"
    process).
    
    Thats exactly what we see for test__PERF_RECORD test. We receive
    SAMPLES with PID of the process that our tracee is scheduled
    from.
    
    Discussed with Peter Zijlstra:
    
     > Bah!, yeah I guess reverting is the right thing for now. Sad
     > though.
     >
     > So by having the two hooks we have a black-spot between them
     > where we receive no events at all, this black-spot covers the
     > hand-over of current and we thus don't receive the 'wrong'
     > events.
     >
     > I rather liked we could do away with both that black-spot and
     > clean up the code a little, but apparently people rely on it.
    
    Signed-off-by: Jiri Olsa <jolsa@redhat.com>
    Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: acme@redhat.com
    Cc: paulus@samba.org
    Cc: cjashfor@linux.vnet.ibm.com
    Cc: fweisbec@gmail.com
    Cc: eranian@google.com
    Link: http://lkml.kernel.org/r/20120523111302.GC1638@m.brq.redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 68d97454c8195a4bd422699b310a98af751a5882
Author: Konstantin Khlebnikov <khlebnikov@openvz.org>
Date:   Thu May 10 16:20:04 2012 +0400

    sched: Taint kernel with TAINT_WARN after sleep-in-atomic bug
    
    Usually sleep-in-atomic bugs are followed by dozens other warnings.
    This patch should help to figure out original source of problem.
    
    Signed-off-by: Konstantin Khlebnikov <khlebnikov@openvz.org>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/20120510122004.4873.12726.stgit@zurg
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit a56ebf07c931e57a54b3eae14f1c94a9dbac60c4
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Mon Jan 9 11:28:35 2012 +0100

    sched: Remove stale power aware scheduling remnants and dysfunctional knobs
    
    It's been broken forever (i.e. it's not scheduling in a power
    aware fashion), as reported by Suresh and others sending
    patches, and nobody cares enough to fix it properly ...
    so remove it to make space free for something better.
    
    There's various problems with the code as it stands today, first
    and foremost the user interface which is bound to topology
    levels and has multiple values per level. This results in a
    state explosion which the administrator or distro needs to
    master and almost nobody does.
    
    Furthermore large configuration state spaces aren't good, it
    means the thing doesn't just work right because it's either
    under so many impossibe to meet constraints, or even if
    there's an achievable state workloads have to be aware of
    it precisely and can never meet it for dynamic workloads.
    
    So pushing this kind of decision to user-space was a bad idea
    even with a single knob - it's exponentially worse with knobs
    on every node of the topology.
    
    There is a proposal to replace the user interface with a single
    3 state knob:
    
     sched_balance_policy := { performance, power, auto }
    
    where 'auto' would be the preferred default which looks at things
    like Battery/AC mode and possible cpufreq state or whatever the hw
    exposes to show us power use expectations - but there's been no
    progress on it in the past many months.
    
    Aside from that, the actual implementation of the various knobs
    is known to be broken. There have been sporadic attempts at
    fixing things but these always stop short of reaching a mergable
    state.
    
    Therefore this wholesale removal with the hopes of spurring
    people who care to come forward once again and work on a
    coherent replacement.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Suresh Siddha <suresh.b.siddha@intel.com>
    Cc: Arjan van de Ven <arjan@linux.intel.com>
    Cc: Vincent Guittot <vincent.guittot@linaro.org>
    Cc: Vaidyanathan Srinivasan <svaidy@linux.vnet.ibm.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Link: http://lkml.kernel.org/r/1326104915.2442.53.camel@twins
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 7273be1ef7651ea12fcc84a3b89cc53c10ff2d45
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Mon May 14 14:34:00 2012 +0200

    sched/debug: Fix printing large integers on 32-bit platforms
    
    Some numbers like nr_running and nr_uninterruptible are fundamentally
    unsigned since its impossible to have a negative amount of tasks, yet
    we still print them as signed to easily recognise the underflow
    condition.
    
    rq->nr_uninterruptible has 'special' accounting and can in fact very
    easily become negative on a per-cpu basis.
    
    It was noted that since the P() macro assumes things are long long and
    the promotion of unsigned 'int/long' to long long on 32bit doesn't
    sign extend we print silly large numbers instead of the easier to read
    signed numbers.
    
    Therefore extend the P() macro to not require the sign extention.
    
    Reported-by: Diwakar Tundlam <dtundlam@nvidia.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/n/tip-gk5tm8t2n4ix2vkpns42uqqp@git.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit fcb7cf2e887f819d39ee1b3780634064e448659d
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Fri May 11 00:22:12 2012 +0200

    sched/fair: Improve the ->group_imb logic
    
    Group imbalance is meant to deal with situations where affinity masks
    and sched domains don't align well, such as 3 cpus from one group and
    6 from another. In this case the domain based balancer will want to
    put an equal amount of tasks on each side even though they don't have
    equal cpus.
    
    Currently group_imb is set whenever two cpus of a group have a weight
    difference of at least one avg task and the heaviest cpu has at least
    two tasks. A group with imbalance set will always be picked as busiest
    and a balance pass will be forced.
    
    The problem is that even if there are no affinity masks this stuff can
    trigger and cause weird balancing decisions, eg. the observed
    behaviour was that of 6 cpus, 5 had 2 and 1 had 3 tasks, due to the
    difference of 1 avg load (they all had the same weight) and nr_running
    being >1 the group_imbalance logic triggered and did the weird thing
    of pulling more load instead of trying to move the 1 excess task to
    the other domain of 6 cpus that had 5 cpu with 2 tasks and 1 cpu with
    1 task.
    
    Curb the group_imbalance stuff by making the nr_running condition
    weaker by also tracking the min_nr_running and using the difference in
    nr_running over the set instead of the absolute max nr_running.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/n/tip-9s7dedozxo8kjsb9kqlrukkf@git.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit c3c0a9668185b14eb49f07139584d6d613adf82a
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Fri May 11 00:26:27 2012 +0200

    sched/numa: Don't scale the imbalance
    
    It's far too easy to get ridiculously large imbalance pct when you
    scale it like that. Use a fixed 125% for now.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/n/tip-zsriaft1dv7hhboyrpvqjy6s@git.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 77184246ff2dd86d118ea0349c2920f6300a93a1
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Fri May 11 00:12:02 2012 +0200

    sched/fair: Revert sched-domain iteration breakage
    
    Patches c22402a2f ("sched/fair: Let minimally loaded cpu balance the
    group") and 0ce90475 ("sched/fair: Add some serialization to the
    sched_domain load-balance walk") are horribly broken so revert them.
    
    The problem is that while it sounds good to have the minimally loaded
    cpu do the pulling of more load, the way we walk the domains there is
    absolutely no guarantee this cpu will actually get to the domain. In
    fact its very likely it wont. Therefore the higher up the tree we get,
    the less likely it is we'll balance at all.
    
    The first of mask always walks up, while sucky in that it accumulates
    load on the first cpu and needs extra passes to spread it out at least
    guarantees a cpu gets up that far and load-balancing happens at all.
    
    Since its now always the first and idle cpus should always be able to
    balance so they get a task as fast as possible we can also do away
    with the added serialization.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/n/tip-rpuhs5s56aiv1aw7khv9zkw6@git.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 6ac8e1faca3f279f8818708e0860b180a6c0b7d8
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Fri May 11 00:56:20 2012 +0200

    sched/numa: Fix the new NUMA topology bits
    
    There's no need to convert a node number to a node number by
    pretending its a cpu number..
    
    Reported-by: Yinghai Lu <yinghai@kernel.org>
    Reported-and-Tested-by: Greg Pearson <greg.pearson@hp.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/n/tip-0sqhrht34phowgclj12dgk8h@git.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 0c64b5d7e19534db4b56a10ff1d845459a6e8929
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Tue May 8 18:56:04 2012 +0200

    sched, perf: Use a single callback into the scheduler
    
    We can easily use a single callback for both sched-in and sched-out. This
    reduces the code footprint in the scheduler path as well as removes
    the PMU black spot otherwise present between the out and in callback.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/n/tip-o56ajxp1edwqg6x9d31wb805@git.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 60f1fb438c12b5a2ace5cacb4fb1e8f43d7d115c
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Tue Apr 17 15:49:36 2012 +0200

    sched/numa: Rewrite the CONFIG_NUMA sched domain support
    
    The current code groups up to 16 nodes in a level and then puts an
    ALLNODES domain spanning the entire tree on top of that. This doesn't
    reflect the numa topology and esp for the smaller not-fully-connected
    machines out there today this might make a difference.
    
    Therefore, build a proper numa topology based on node_distance().
    
    Since there's no fixed numa layers anymore, the static SD_NODE_INIT
    and SD_ALLNODES_INIT aren't usable anymore, the new code tries to
    construct something similar and scales some values either on the
    number of cpus in the domain and/or the node_distance() ratio.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Anton Blanchard <anton@samba.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Chris Metcalf <cmetcalf@tilera.com>
    Cc: David Howells <dhowells@redhat.com>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Ivan Kokshaysky <ink@jurassic.park.msu.ru>
    Cc: linux-alpha@vger.kernel.org
    Cc: linux-ia64@vger.kernel.org
    Cc: linux-kernel@vger.kernel.org
    Cc: linux-mips@linux-mips.org
    Cc: linuxppc-dev@lists.ozlabs.org
    Cc: linux-sh@vger.kernel.org
    Cc: Matt Turner <mattst88@gmail.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Paul Mundt <lethal@linux-sh.org>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: Richard Henderson <rth@twiddle.net>
    Cc: sparclinux@vger.kernel.org
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: x86@kernel.org
    Cc: Dimitri Sivanich <sivanich@sgi.com>
    Cc: Greg Pearson <greg.pearson@hp.com>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: bob.picco@oracle.com
    Cc: chris.mason@oracle.com
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Link: http://lkml.kernel.org/n/tip-r74n3n8hhuc2ynbrnp3vt954@git.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    
    Conflicts:
    	arch/ia64/include/asm/topology.h
    	arch/mips/include/asm/mach-ip27/topology.h
    	arch/powerpc/include/asm/topology.h
    	arch/sh/include/asm/topology.h
    	arch/sparc/include/asm/topology_64.h
    	arch/tile/include/asm/topology.h
    	arch/x86/include/asm/topology.h
    	kernel/sched/core.c

commit 006f89f31683d1f8fc02c064efdd8ed57150a267
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Wed May 2 14:20:37 2012 +0200

    sched/fair: Propagate 'struct lb_env' usage into find_busiest_group
    
    More function argument passing reduction.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/n/tip-v66ivjfqdiqdso01lqgqx6qf@git.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit c900eb5aa1bc5fd1ba4551395bbfc85c4ca8375e
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Wed Apr 25 00:30:36 2012 +0200

    sched/fair: Add some serialization to the sched_domain load-balance walk
    
    Since the sched_domain walk is completely unserialized (!SD_SERIALIZE)
    it is possible that multiple cpus in the group get elected to do the
    next level. Avoid this by adding some serialization.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/n/tip-vqh9ai6s0ewmeakjz80w4qz6@git.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 8d30691216743d886bdb78bd82a47bb9e1137610
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Fri Apr 20 16:57:22 2012 +0200

    sched/fair: Let minimally loaded cpu balance the group
    
    Currently we let the leftmost (or first idle) cpu ascend the
    sched_domain tree and perform load-balancing. The result is that the
    busiest cpu in the group might be performing this function and pull
    more load to itself. The next load balance pass will then try to
    equalize this again.
    
    Change this to pick the least loaded cpu to perform higher domain
    balancing.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/n/tip-v8zlrmgmkne3bkcy9dej1fvm@git.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 6b7c675c10fbca8791b16c576e145452f7003f1f
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Thu Apr 26 13:12:27 2012 +0200

    sched: Change rq->nr_running to unsigned int
    
    Since there's a PID space limit of 30bits (see
    futex.h:FUTEX_TID_MASK) and allocating that many tasks (assuming a
    lower bound of 2 pages per task) would still take 8T of memory it
    seems reasonable to say that unsigned int is sufficient for
    rq->nr_running.
    
    When we do get anywhere near that amount of tasks I suspect other
    things would go funny, load-balancer load computations would really
    need to be hoisted to 128bit etc.
    
    So save a few bytes and convert rq->nr_running and friends to
    unsigned int.
    
    Suggested-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/n/tip-y3tvyszjdmbibade5bw8zl81@git.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 3a7c782a908ccfeb802041430f8a00eb5f1074a3
Author: Hiroshi Shimamoto <h-shimamoto@ct.jp.nec.com>
Date:   Mon Apr 2 17:00:44 2012 +0900

    sched: Update documentation and comments
    
    Change sched_*.c to sched/*.c in documentation and comments.
    
    Signed-off-by: Hiroshi Shimamoto <h-shimamoto@ct.jp.nec.com>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/4F795CAC.9080206@ct.jp.nec.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit a92c88382ccaef13b97af63d926b3ce66ea9f6ce
Author: Paul E. McKenney <paul.mckenney@linaro.org>
Date:   Tue Mar 27 16:02:08 2012 -0700

    rcu: Move PREEMPT_RCU preemption to switch_to() invocation
    
    Currently, PREEMPT_RCU readers are enqueued upon entry to the scheduler.
    This is inefficient because enqueuing is required only if there is a
    context switch, and entry to the scheduler does not guarantee a context
    switch.
    
    The commit therefore moves the enqueuing to immediately precede the
    call to switch_to() from the scheduler.
    
    Signed-off-by: Paul E. McKenney <paul.mckenney@linaro.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Tested-by: Linus Torvalds <torvalds@linux-foundation.org>

commit 79c42ecef2448777d0523e81023c2bd05372311b
Author: John Stultz <john.stultz@linaro.org>
Date:   Thu Sep 6 23:19:06 2012 +0200

    PM / wakeup: Use irqsave/irqrestore for events_lock
    
    Jon Medhurst (Tixy) recently noticed a problem with the
    events_lock usage. One of the Android patches that uses
    wakeup_sources calls wakeup_source_add() with irqs disabled.
    However, the event_lock usage in wakeup_source_add() uses
    spin_lock_irq()/spin_unlock_irq(), which reenables interrupts.
    This results in lockdep warnings.
    
    The fix is to use spin_lock_irqsave()/spin_lock_irqrestore()
    instead for the events_lock.
    
    References: https://bugs.launchpad.net/linaro-landing-team-arm/+bug/1037565
    Reported-and-debugged-by: Jon Medhurst (Tixy) <tixy@linaro.org>
    Signed-off-by: John Stultz <john.stultz@linaro.org>
    Signed-off-by: Rafael J. Wysocki <rjw@sisk.pl>

commit a0aa6f47e72e2d0d7b3f1c465cbaa1d75d4d0bf1
Author: Shuah Khan <shuah.kh@samsung.com>
Date:   Wed May 8 01:14:32 2013 +0200

    PM: Avoid calling kfree() under spinlock in dev_pm_put_subsys_data()
    
    Fix dev_pm_put_subsys_data() so that it doesn't call kfree() under
    a spinlock and make it return 1 whenever it leaves NULL
    power.subsys_data (regardless of the reason).
    
    Signed-off-by: Shuah Khan <shuah.kh@samsung.com>
    Reviewed-by: Pavel Machek <pavel@ucw.cz>
    Acked-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit bb3bf3df93e3f3c9592d4b1c387c8ff5e3830768
Author: Rafael J. Wysocki <rjw@sisk.pl>
Date:   Tue Aug 7 13:50:14 2012 +0200

    PM: Make dev_pm_get_subsys_data() always return 0 on success
    
    Commits 1d5fcfec22 (PM / Domains: Add device domain data reference
    counter) and 62d4490294 (PM / Domains: Allow device callbacks to be
    added at any time) added checks for the return value of
    dev_pm_get_subsys_data(), but those checks were incorrect, because
    that function returned 1 on success in some cases.
    
    Since all of the existing users of dev_pm_get_subsys_data() don't use
    the positive value returned by it on success, change its definition
    so that it always returns 0 when successful.
    
    Reported-by: Heiko Stbner <heiko@sntech.de>
    Reported-by: Tushar Behera <tushar.behera@linaro.org>
    Signed-off-by: Rafael J. Wysocki <rjw@sisk.pl>

commit 80795b98b90505b2c2054d840599b7164e2ff6c6
Author: Murali Karicheri <m-karicheri2@ti.com>
Date:   Tue Oct 23 01:18:40 2012 +0200

    base: power - use clk_prepare_enable and clk_prepare_disable
    
    When PM runtime is enabled in DaVinci and the machine migrates to
    common clk framework, the clk_enable() gets called without
    clk_prepare(). This patch is to fix this issue so that PM run
    time can inter work with common clk framework.
    
    Signed-off-by: Murali Karicheri <m-karicheri2@ti.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit 105bdf439bdb14a6eeb0f6ca81bdba894881b6be
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Fri Jun 12 01:03:29 2015 +0200

    rcutree: also remove old definition of atomic

commit 4b01f649cf4ebeaa124bd214eb47fe4bd82cad7d
Author: Cong Wang <xiyou.wangcong@gmail.com>
Date:   Tue May 29 15:06:43 2012 -0700

    mm: move readahead syscall to mm/readahead.c
    
    It is better to define readahead(2) in mm/readahead.c than in
    mm/filemap.c.
    
    Signed-off-by: Cong Wang <xiyou.wangcong@gmail.com>
    Cc: Fengguang Wu <fengguang.wu@intel.com>
    Cc: Al Viro <viro@zeniv.linux.org.uk>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit 1ccbc3e210b72bc769f19207da3692fcbfed509e
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Mon Sep 14 13:37:40 2015 +0200

    convert more smps

commit 03a2f7c20515934b7d2dc1c0d632ca143ab578e9
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Fri Jul 3 18:02:11 2015 +0200

    more convertion of smp_mb__

commit 2b90ab75d6be1b87acedc0f3921cb56893aa0f5e
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Fri Jul 3 18:02:11 2015 +0200

    more convertion of smp_mb__
    
    Conflicts:
    	fs/ext4/resize.c

commit cdc6e0b443f54a426c82e5f761eceef6afa55cb4
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Tue Jul 21 14:40:29 2015 +0200

     mm: pass readahead info down to the i/o scheduler
    
    Some i/o schedulers (i.e. row-iosched, cfq-iosched) deploy an idling
    algorithm in order to be better synced with the readahead algorithm.
    Idling is a prediction algorithm for incoming read requests.
    
    In this patch we mark pages which are part of a readahead window, by
    setting a newly introduced flag. With this flag, the i/o scheduler can
    identify a request which is associated with a readahead page. This
    enables the i/o scheduler's idling mechanism to be en-sync with the
    readahead mechanism and, in turn, can increase read throughput.
    
    Change-Id: I0654f23315b6d19d71bcc9cc029c6b281a44b196
    Signed-off-by: Lee Susman <lsusman@codeaurora.org>

commit a5960d0f11afcae767ab9a8c35576191f0ab3d8c
Author: Rasmus Villemoes <linux@rasmusvillemoes.dk>
Date:   Fri Feb 13 14:36:44 2015 -0800

    lib/string.c: improve strrchr()
    
    Instead of potentially passing over the string twice in case c is not
    found, just keep track of the last occurrence.  According to
    bloat-o-meter, this also cuts the generated code by a third (54 vs 36
    bytes).  Oh, and we get rid of those 7-space indented lines.
    
    Signed-off-by: Rasmus Villemoes <linux@rasmusvillemoes.dk>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit f170bd9ca8bfbd0fb9ae8db234086f8f63c901d6
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Jun 11 23:45:57 2015 +0200

    add the befor deleted um files

commit 1784d15a63cc296db76a6affa312270223923dc7
Author: Thomas Graf <tgraf@suug.ch>
Date:   Fri Jan 2 23:00:19 2015 +0100

    spinlock: Add spin_lock_bh_nested()
    
    Signed-off-by: Thomas Graf <tgraf@suug.ch>
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit a57c762510658648c38872cd6c0b17c994e4eb11
Author: Bart Van Assche <bvanassche@acm.org>
Date:   Fri Aug 8 12:35:36 2014 +0200

    locking/spinlocks: Always evaluate the second argument of spin_lock_nested()
    
    Evaluating a macro argument only if certain configuration options
    have been selected is confusing and error-prone. Hence always
    evaluate the second argument of spin_lock_nested().
    
    An intentional side effect of this patch is that it avoids that
    the following warning is reported for netif_addr_lock_nested()
    when building with CONFIG_DEBUG_LOCK_ALLOC=n and with W=1:
    
      include/linux/netdevice.h: In function 'netif_addr_lock_nested':
      include/linux/netdevice.h:2865:6: warning: variable 'subclass' set but not used [-Wunused-but-set-variable]
        int subclass = SINGLE_DEPTH_NESTING;
            ^
    
    Signed-off-by: Bart Van Assche <bvanassche@acm.org>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Cc: David Rientjes <rientjes@google.com>
    Cc: David S. Miller <davem@davemloft.net>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Link: http://lkml.kernel.org/r/53E4A7F8.1040700@acm.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 88d208416ff0d4f3dfc9ed8e340df1e5bfc0b15e
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Mon Aug 12 18:14:00 2013 +0200

    sched: fix the theoretical signal_wake_up() vs schedule() race
    
    This is only theoretical, but after try_to_wake_up(p) was changed
    to check p->state under p->pi_lock the code like
    
    	__set_current_state(TASK_INTERRUPTIBLE);
    	schedule();
    
    can miss a signal. This is the special case of wait-for-condition,
    it relies on try_to_wake_up/schedule interaction and thus it does
    not need mb() between __set_current_state() and if(signal_pending).
    
    However, this __set_current_state() can move into the critical
    section protected by rq->lock, now that try_to_wake_up() takes
    another lock we need to ensure that it can't be reordered with
    "if (signal_pending(current))" check inside that section.
    
    The patch is actually one-liner, it simply adds smp_wmb() before
    spin_lock_irq(rq->lock). This is what try_to_wake_up() already
    does by the same reason.
    
    We turn this wmb() into the new helper, smp_mb__before_spinlock(),
    for better documentation and to allow the architectures to change
    the default implementation.
    
    While at it, kill smp_mb__after_lock(), it has no callers.
    
    Perhaps we can also add smp_mb__before/after_spinunlock() for
    prepare_to_wait().
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit bed6e3df737ffb469bbb4ad154282dae0b7146b8
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Wed Dec 11 13:59:08 2013 -0800

    locking: Add an smp_mb__after_unlock_lock() for UNLOCK+BLOCK barrier
    
    The Linux kernel has traditionally required that an UNLOCK+LOCK
    pair act as a full memory barrier when either (1) that
    UNLOCK+LOCK pair was executed by the same CPU or task, or (2)
    the same lock variable was used for the UNLOCK and LOCK.  It now
    seems likely that very few places in the kernel rely on this
    full-memory-barrier semantic, and with the advent of queued
    locks, providing this semantic either requires complex
    reasoning, or for some architectures, added overhead.
    
    This commit therefore adds a smp_mb__after_unlock_lock(), which
    may be placed after a LOCK primitive to restore the
    full-memory-barrier semantic. All definitions are currently
    no-ops, but will be upgraded for some architectures when queued
    locks arrive.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Reviewed-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: <linux-arch@vger.kernel.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Link: http://lkml.kernel.org/r/1386799151-2219-5-git-send-email-paulmck@linux.vnet.ibm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit e4c120967d9ae2803fb4f84f6a79cef2f70917cb
Author: Will Deacon <will.deacon@arm.com>
Date:   Wed Dec 12 19:13:42 2012 +0000

    ARM: opcodes: add missing include of linux/linkage.h
    
    opcodes.h wants to declare an asmlinkage function, so we need to include
    linux/linkage.h
    
    Acked-by: Arnd Bergmann <arnd@arndb.de>
    Acked-by: Dave Martin <dave.martin@linaro.org>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

commit bf08d255128f16f18dbbac8084edf4a9c2232c86
Author: Dave Martin <dave.martin@linaro.org>
Date:   Mon Sep 3 13:49:25 2012 +0100

    ARM: 7511/1: opcodes: Opcode definitions for the Virtualization Extensions
    
    For now, this patch just adds a definition for the HVC instruction.
    More can be added here later, as needed.
    
    Now that we have a real example of how to use the opcode injection
    macros properly, this patch also adds a cross-reference from the
    explanation in opcodes.h (since without an example, figuring out
    how to use the macros is not that easy).
    
    Signed-off-by: Dave Martin <dave.martin@linaro.org>
    Acked-by: Nicolas Pitre <nico@linaro.org>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

commit 0476987972646fe392eb851dec1ceacf410a85b5
Author: Dave Martin <dave.martin@linaro.org>
Date:   Mon Sep 3 13:49:24 2012 +0100

    ARM: 7510/1: opcodes: Add helpers for emitting custom opcodes
    
    This patch adds some __inst_() macros for injecting custom opcodes
    in assembler (both inline and in .S files).  They should make it
    easier and cleaner to get things right in little-/big-
    endian/ARM/Thumb-2 kernels without a lot of #ifdefs.
    
    This pure-preprocessor approach is preferred over the alternative
    method of wedging extra assembler directives into the assembler
    input using top-level asm() blocks, since there is no way to
    guarantee that the compiler won't reorder those with respect to
    each other or with respect to non-toplevel asm() blocks, unless
    -fno-toplevel-reorder is passed (which is in itself somewhat
    undesirable because it defeats some potential optimisations).
    
    Currently <asm/unified.h> _does_ silently rely on the compiler not
    reordering at the top level, but it seems better to avoid adding
    extra code which depends on this if the same result can be achieved
    in another way.
    
    Signed-off-by: Dave Martin <dave.martin@linaro.org>
    Acked-by: Nicolas Pitre <nico@linaro.org>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

commit b41be5fc2c09209d37bcb70c6fe22c970fe6ae09
Author: Dave Martin <dave.martin@linaro.org>
Date:   Mon Sep 3 13:49:23 2012 +0100

    ARM: 7509/1: opcodes: Make opcode byteswapping macros assembly-compatible
    
    Most of the existing macros don't work with assembler, due to the
    use of type casts and C functions from <linux/swab.h>.
    
    This patch abstracts out those operations and provides simple
    explicit versions for use in assembly code.
    
    __opcode_is_thumb32() and __opcode_is_thumb16() are also converted
    to do bitmask-based testing to avoid confusion if these are used in
    assembly code (the assembler typically treats all arithmetic values
    as signed).
    
    These changes avoid the need for the compiler to pre-evaluate
    constant expressions used to generate opcodes.  By ensuring that
    the forms of these expressions can be evaluated directly by the
    assembler, we can just stringify the expressions directly into the
    asm during the preprocessing pass.  The alternative approach
    (passing the evaluated expression via an inline asm "i" constraint)
    gets painful because the contents of the asm and the constraints
    must be kept in sync.  This makes the resulting macros awkward to
    use.
    
    Retaining the C forms of the macros allows more efficient code to
    be generated when opcodes are generated programmatically at run-
    time, but there is no way to embed run-time-generated opcodes in
    asm() blocks.
    
    Signed-off-by: Dave Martin <dave.martin@linaro.org>
    Acked-by: Nicolas Pitre <nico@linaro.org>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

commit e399197c4fe7ff43de8a393be829004166c9c640
Author: Dave Martin <dave.martin@linaro.org>
Date:   Mon Sep 3 13:49:22 2012 +0100

    ARM: 7508/1: opcodes: Don't define the thumb32 byteswapping macros for BE32
    
    The existing __mem_to_opcode_thumb32() is incorrect for BE32
    platforms.  However, these don't support Thumb-2 kernels, so this
    option is not so relevant for those platforms anyway.
    
    This operation is complicated by the lack of unaligned memory
    access support prior to ARMv6.
    
    Rather than provide a "working" macro which will probably won't get
    used (or worse, will get misused), this patch removes the macro for
    BE32 kernels.  People manipulating Thumb opcodes prior to ARMv6
    should almost certainly be splitting these operations into
    halfwords anyway, using __opcode_thumb32_{first,second,compose}()
    and the 16-bit opcode transformations.
    
    Signed-off-by: Dave Martin <dave.martin@linaro.org>
    Acked-by: Nicolas Pitre <nico@linaro.org>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

commit fc09448c5ed1c4ec1e4d5a7d2f2d7be9ad597e15
Author: Ben Dooks <ben.dooks@codethink.co.uk>
Date:   Thu Jul 25 14:38:03 2013 +0100

    ARM: Correct BUG() assembly to ensure it is endian-agnostic
    
    Currently BUG() uses .word or .hword to create the necessary illegal
    instructions. However if we are building BE8 then these get swapped
    by the linker into different illegal instructions in the text. This
    means that the BUG() macro does not get trapped properly.
    
    Change to using <asm/opcodes.h> to provide the necessary ARM instruction
    building as we cannot rely on gcc/gas having the `.inst` instructions
    which where added to try and resolve this issue (reported by Dave Martin
    <Dave.Martin@arm.com>).
    
    Signed-off-by: Ben Dooks <ben.dooks@codethink.co.uk>
    Reviewed-by: Dave Martin <Dave.Martin@arm.com>

commit 634f6a4a7108e27adc0e2b33e63657a4b2f758c9
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Wed Mar 12 17:11:00 2014 +0100

    arch,arm: Convert smp_mb__*()
    
    ARM uses ll/sc primitives that do not imply barriers for all regular
    atomic ops, therefore smp_mb__{before,after} need be a full barrier.
    
    Since ARM doesn't use asm-generic/barrier.h include the required
    definitions in its asm/barrier.h
    
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Acked-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Link: http://lkml.kernel.org/n/tip-yijo7sglsl7uusbp13upcuvo@git.kernel.org
    Cc: Albin Tonnerre <albin.tonnerre@arm.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Chen Gang <gang.chen@asianux.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Nicolas Pitre <nico@linaro.org>
    Cc: Russell King <linux@arm.linux.org.uk>
    Cc: Victor Kamensky <victor.kamensky@linaro.org>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: linux-arm-kernel@lists.infradead.org
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 8e4f59efd3bd47bb6ea92a29a876256656a21898
Author: Lars Ellenberg <lars.ellenberg@linbit.com>
Date:   Mon Feb 21 13:20:55 2011 +0100

    drbd: use clear_bit_unlock() where appropriate
    
    Some open-coded clear_bit(); smp_mb__after_clear_bit();
    should in fact have been smp_mb__before_clear_bit(); clear_bit();
    
    Instead, use clear_bit_unlock() to annotate the intention,
    and have it do the right thing.
    
    Signed-off-by: Philipp Reisner <philipp.reisner@linbit.com>
    Signed-off-by: Lars Ellenberg <lars.ellenberg@linbit.com>

commit 9b1d6c198e393cd0e5513321eb347f692faa6e7a
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Thu Feb 6 18:16:07 2014 +0100

    arch: Prepare for smp_mb__{before,after}_atomic()
    
    Since the smp_mb__{before,after}*() ops are fundamentally dependent on
    how an arch can implement atomics it doesn't make sense to have 3
    variants of them. They must all be the same.
    
    Furthermore, the 3 variants suggest they're only valid for those 3
    atomic ops, while we have many more where they could be applied.
    
    So move away from
    smp_mb__{before,after}_{atomic,clear}_{dec,inc,bit}() and reduce the
    interface to just the two: smp_mb__{before,after}_atomic().
    
    This patch prepares the way by introducing default implementations in
    asm-generic/barrier.h that default to a full barrier and providing
    __deprecated inlines for the previous 6 barriers if they're not
    provided by the arch.
    
    This should allow for a mostly painless transition (lots of deprecated
    warns in the interim).
    
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Acked-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Link: http://lkml.kernel.org/n/tip-wr59327qdyi9mbzn6x937s4e@git.kernel.org
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: "Chen, Gong" <gong.chen@linux.intel.com>
    Cc: John Sullivan <jsrhbz@kanargh.force9.co.uk>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mauro Carvalho Chehab <m.chehab@samsung.com>
    Cc: Srinivas Pandruvada <srinivas.pandruvada@linux.intel.com>
    Cc: "Theodore Ts'o" <tytso@mit.edu>
    Cc: linux-arch@vger.kernel.org
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 556f740c0a38eb8d838fe1dda57946851f01eb33
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Wed Nov 6 14:57:36 2013 +0100

    arch: Introduce smp_load_acquire(), smp_store_release()
    
    A number of situations currently require the heavyweight smp_mb(),
    even though there is no need to order prior stores against later
    loads.  Many architectures have much cheaper ways to handle these
    situations, but the Linux kernel currently has no portable way
    to make use of them.
    
    This commit therefore supplies smp_load_acquire() and
    smp_store_release() to remedy this situation.  The new
    smp_load_acquire() primitive orders the specified load against
    any subsequent reads or writes, while the new smp_store_release()
    primitive orders the specifed store against any prior reads or
    writes.  These primitives allow array-based circular FIFOs to be
    implemented without an smp_mb(), and also allow a theoretical
    hole in rcu_assign_pointer() to be closed at no additional
    expense on most architectures.
    
    In addition, the RCU experience transitioning from explicit
    smp_read_barrier_depends() and smp_wmb() to rcu_dereference()
    and rcu_assign_pointer(), respectively resulted in substantial
    improvements in readability.  It therefore seems likely that
    replacing other explicit barriers with smp_load_acquire() and
    smp_store_release() will provide similar benefits.  It appears
    that roughly half of the explicit barriers in core kernel code
    might be so replaced.
    
    [Changelog by PaulMck]
    
    Reviewed-by: "Paul E. McKenney" <paulmck@linux.vnet.ibm.com>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Mathieu Desnoyers <mathieu.desnoyers@polymtl.ca>
    Cc: Michael Ellerman <michael@ellerman.id.au>
    Cc: Michael Neuling <mikey@neuling.org>
    Cc: Russell King <linux@arm.linux.org.uk>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Victor Kaplansky <VICTORK@il.ibm.com>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Link: http://lkml.kernel.org/r/20131213150640.908486364@infradead.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    
    Conflicts:
    	arch/arm64/include/asm/barrier.h
    	arch/ia64/include/asm/barrier.h
    	arch/metag/include/asm/barrier.h
    	arch/mips/include/asm/barrier.h
    	arch/powerpc/include/asm/barrier.h
    	arch/s390/include/asm/barrier.h
    	arch/sparc/include/asm/barrier_64.h
    	arch/x86/include/asm/barrier.h
    	include/linux/compiler.h

commit d5d0409cb00a03b64d5c17e104cb95fd7a675039
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Wed Nov 6 14:57:36 2013 +0100

    arch: Clean up asm/barrier.h implementations using asm-generic/barrier.h
    
    We're going to be adding a few new barrier primitives, and in order to
    avoid endless duplication make more agressive use of
    asm-generic/barrier.h.
    
    Change the asm-generic/barrier.h such that it allows partial barrier
    definitions and fills out the rest with defaults.
    
    There are a few architectures (m32r, m68k) that could probably
    do away with their barrier.h file entirely but are kept for now due to
    their unconventional nop() implementation.
    
    Suggested-by: Geert Uytterhoeven <geert@linux-m68k.org>
    Reviewed-by: "Paul E. McKenney" <paulmck@linux.vnet.ibm.com>
    Reviewed-by: Mathieu Desnoyers <mathieu.desnoyers@polymtl.ca>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Cc: Michael Ellerman <michael@ellerman.id.au>
    Cc: Michael Neuling <mikey@neuling.org>
    Cc: Russell King <linux@arm.linux.org.uk>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Victor Kaplansky <VICTORK@il.ibm.com>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Link: http://lkml.kernel.org/r/20131213150640.846368594@infradead.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 47e60b620c520d8f44c902520c6c823404fef613
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Mon Mar 17 18:06:10 2014 +0100

    arch: Mass conversion of smp_mb__*()
    
    Mostly scripted conversion of the smp_mb__* barriers.
    
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Acked-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Link: http://lkml.kernel.org/n/tip-55dhyhocezdw1dg7u19hmh1u@git.kernel.org
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: linux-arch@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    
    Conflicts:
    	drivers/cpuidle/coupled.c
    	drivers/gpu/drm/drm_irq.c
    	drivers/gpu/drm/i915/i915_irq.c
    	drivers/md/bcache/closure.h
    	drivers/media/usb/dvb-usb-v2/dvb_usb_core.c
    	drivers/net/ethernet/broadcom/bnx2x/bnx2x_cmn.c
    	drivers/net/ethernet/broadcom/bnx2x/bnx2x_main.c
    	drivers/net/ethernet/broadcom/bnx2x/bnx2x_sriov.c
    	drivers/net/ethernet/brocade/bna/bnad.c
    	drivers/net/ethernet/freescale/gianfar.c
    	drivers/net/ethernet/intel/i40e/i40e_main.c
    	drivers/net/ethernet/intel/ixgbe/ixgbe_main.c
    	drivers/net/ethernet/intel/ixgbevf/ixgbevf_main.c
    	drivers/net/wireless/ti/wlcore/main.c
    	drivers/target/target_core_alua.c
    	drivers/target/target_core_iblock.c
    	drivers/target/target_core_pr.c
    	drivers/target/target_core_transport.c
    	drivers/tty/n_tty.c
    	drivers/tty/serial/mxs-auart.c
    	drivers/usb/gadget/tcm_usb_gadget.c
    	drivers/vhost/scsi.c
    	fs/btrfs/btrfs_inode.h
    	fs/btrfs/inode.c
    	fs/btrfs/ioctl.c
    	fs/gfs2/glock.c
    	fs/nfs/inode.c
    	fs/nfs/nfs4filelayoutdev.c
    	fs/nfs/pagelist.c
    	fs/nfs/pnfs.c
    	include/linux/sched.h
    	include/net/ip_vs.h
    	kernel/futex.c
    	kernel/rcu/tree.c
    	kernel/rcu/tree_plugin.h
    	kernel/sched/wait.c
    	net/atm/pppoatm.c
    	net/bluetooth/hci_event.c
    	net/ipv4/tcp_output.c
    	net/sunrpc/backchannel_rqst.c
    	net/sunrpc/xprt.c
    
    Conflicts:
    	drivers/md/bcache/bcache.h

commit 9d43f449d878ed460a8d37a8028a336f384f0e1c
Author: Josh Triplett <josh@joshtriplett.org>
Date:   Mon Apr 7 15:39:13 2014 -0700

    bug: Make BUG() always stop the machine
    
    When !CONFIG_BUG and !HAVE_ARCH_BUG, define the generic BUG() as an
    infinite loop rather than a no-op.  This avoids undefined behavior if
    execution ever actually reaches BUG(), and avoids warnings about code
    after BUG() (such as on non-void functions calling BUG() and then not
    returning).
    
    bloat-o-meter results:
    
      add/remove: 0/0 grow/shrink: 43/10 up/down: 235/-98 (137)
      function                             old     new   delta
      umount_collect                       119     138     +19
      notify_change                        306     324     +18
      xstate_enable_boot_cpu               252     269     +17
      kunmap                                54      70     +16
      balloon_page_dequeue                 112     126     +14
      mm_take_all_locks                    223     233     +10
      list_lru_walk_node                   143     152      +9
      vma_adjust                          1059    1067      +8
      pcpu_setup_first_chunk              1130    1138      +8
      mm_drop_all_locks                    143     151      +8
      ns_capable                            55      62      +7
      anon_transport_class_unregister        8      15      +7
      srcu_init_notifier_head               35      41      +6
      shrink_dcache_for_umount             174     180      +6
      kunmap_high                           99     105      +6
      end_page_writeback                    43      49      +6
      do_exit                             1339    1345      +6
      __kfifo_dma_out_prepare_r             86      92      +6
      __kfifo_dma_in_prepare_r              90      96      +6
      fixup_user_fault                     120     125      +5
      repair_env_string                     73      77      +4
      read_cache_pages_invalidate_page      56      60      +4
      isolate_lru_pages.isra               142     146      +4
      do_notify_parent_cldstop             255     259      +4
      cpu_init                             370     374      +4
      utimes_common                        270     272      +2
      tasklet_hi_action                     91      93      +2
      tasklet_action                        91      93      +2
      set_pte_vaddr                         46      48      +2
      find_get_pages_tag                   202     204      +2
      early_iounmap                        185     187      +2
      __native_set_fixmap                   36      38      +2
      __get_user_pages                     822     824      +2
      __early_ioremap                      299     301      +2
      yield_task_stop                        1       2      +1
      tick_resume                           37      38      +1
      switched_to_stop                       1       2      +1
      switched_to_idle                       1       2      +1
      prio_changed_stop                      1       2      +1
      prio_changed_idle                      1       2      +1
      pm_qos_power_read                    111     112      +1
      arch_cpu_idle_dead                     1       2      +1
      __insert_vmap_area                   140     141      +1
      sys_renameat                         614     612      -2
      mm_fault_error                       297     295      -2
      SyS_renameat                         614     612      -2
      sys_linkat                           416     413      -3
      SyS_linkat                           416     413      -3
      chmod_common                         129     122      -7
      proc_cap_handler                     240     225     -15
      __schedule                           849     831     -18
      sys_madvise                         1077    1054     -23
      SyS_madvise                         1077    1054     -23
    
    Signed-off-by: Josh Triplett <josh@joshtriplett.org>
    Reported-by: Arnd Bergmann <arnd@arndb.de>
    Acked-by: Arnd Bergmann <arnd@arndb.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit c875d6fe7194cc1ca98ed6230546a6212fc3cdd6
Author: Josh Triplett <josh@joshtriplett.org>
Date:   Mon Apr 7 15:39:12 2014 -0700

    bug: when !CONFIG_BUG, make WARN call no_printk to check format and args
    
    The stub version of WARN for !CONFIG_BUG completely ignored its format
    string and subsequent arguments; make it check them instead, using
    no_printk.
    
    Signed-off-by: Josh Triplett <josh@joshtriplett.org>
    Reported-by: Arnd Bergmann <arnd@arndb.de>
    Acked-by: Arnd Bergmann <arnd@arndb.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit 911f9003a5e68771e10e87b4ff457a189e3249a1
Author: Josh Triplett <josh@joshtriplett.org>
Date:   Mon Apr 7 15:39:11 2014 -0700

    include/asm-generic/bug.h: style fix: s/while(0)/while (0)/
    
    Signed-off-by: Josh Triplett <josh@joshtriplett.org>
    Reported-by: Randy Dunlap <rdunlap@infradead.org>
    Acked-by: Arnd Bergmann <arnd@arndb.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit fd45b21841b1d1dae2dacaa83836126fafaf9cff
Author: Josh Triplett <josh@joshtriplett.org>
Date:   Mon Apr 7 15:39:10 2014 -0700

    bug: when !CONFIG_BUG, simplify WARN_ON_ONCE and family
    
    When !CONFIG_BUG, WARN_ON and family become simple passthroughs of their
    condition argument; however, WARN_ON_ONCE and family still have conditions
    and a boolean to detect one-time invocation, even though the warning
    they'd emit doesn't exist.  Make the existing definitions conditional on
    CONFIG_BUG, and add definitions for !CONFIG_BUG that map to the
    passthrough versions of WARN and WARN_ON.
    
    This saves 4.4k on a minimized configuration (smaller than allnoconfig),
    and 20.6k with defconfig plus CONFIG_BUG=n.
    
    Signed-off-by: Josh Triplett <josh@joshtriplett.org>
    Acked-by: Arnd Bergmann <arnd@arndb.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit 8d6bb5ed0dd609b7884bed17129d47f6ee19b813
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Wed Jun 27 09:24:13 2012 +0400

    trim task_work: get rid of hlist
    
    layout based on Oleg's suggestion; single-linked list,
    task->task_works points to the last element, forward pointer
    from said last element points to head.  I'd still prefer
    much more regular scheme with two pointers in task_work,
    but...
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

commit ebc6a952c8e98dce7911aefb03629962583587dc
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Fri May 11 10:59:07 2012 +1000

    task_work_add: generic process-context callbacks
    
    Provide a simple mechanism that allows running code in the (nonatomic)
    context of the arbitrary task.
    
    The caller does task_work_add(task, task_work) and this task executes
    task_work->func() either from do_notify_resume() or from do_exit().  The
    callback can rely on PF_EXITING to detect the latter case.
    
    "struct task_work" can be embedded in another struct, still it has "void
    *data" to handle the most common/simple case.
    
    This allows us to kill the ->replacement_session_keyring hack, and
    potentially this can have more users.
    
    Performance-wise, this adds 2 "unlikely(!hlist_empty())" checks into
    tracehook_notify_resume() and do_exit().  But at the same time we can
    remove the "replacement_session_keyring != NULL" checks from
    arch/*/signal.c and exit_creds().
    
    Note: task_work_add/task_work_run abuses ->pi_lock.  This is only because
    this lock is already used by lookup_pi_state() to synchronize with
    do_exit() setting PF_EXITING.  Fortunately the scope of this lock in
    task_work.c is really tiny, and the code is unlikely anyway.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Acked-by: David Howells <dhowells@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Richard Kuo <rkuo@codeaurora.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Alexander Gordeev <agordeev@redhat.com>
    Cc: Chris Zankel <chris@zankel.net>
    Cc: David Smith <dsmith@redhat.com>
    Cc: "Frank Ch. Eigler" <fche@redhat.com>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Larry Woodman <lwoodman@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

commit 17ff3f10b55c22c4e5e96258818952328c90f5cc
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Wed May 23 14:44:37 2012 -0400

    move key_repace_session_keyring() into tracehook_notify_resume()
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>
    
    Conflicts:
    	arch/alpha/kernel/signal.c
    	arch/avr32/kernel/signal.c
    	arch/blackfin/kernel/signal.c
    	arch/c6x/kernel/signal.c
    	arch/cris/kernel/ptrace.c
    	arch/frv/kernel/signal.c
    	arch/h8300/kernel/signal.c
    	arch/hexagon/kernel/signal.c
    	arch/ia64/kernel/process.c
    	arch/m32r/kernel/signal.c
    	arch/m68k/kernel/signal.c
    	arch/microblaze/kernel/signal.c
    	arch/mips/kernel/signal.c
    	arch/mn10300/kernel/signal.c
    	arch/openrisc/kernel/signal.c
    	arch/parisc/kernel/signal.c
    	arch/powerpc/kernel/signal.c
    	arch/s390/kernel/signal.c
    	arch/score/kernel/signal.c
    	arch/sh/kernel/signal_32.c
    	arch/sh/kernel/signal_64.c
    	arch/sparc/kernel/signal_32.c
    	arch/sparc/kernel/signal_64.c
    	arch/tile/kernel/process.c
    	arch/um/kernel/process.c
    	arch/unicore32/kernel/signal.c
    	arch/x86/kernel/signal.c
    	arch/xtensa/kernel/signal.c

commit 2604cc57454d56e66b272728468d7d53af851b1c
Author: Jan Kara <jack@suse.cz>
Date:   Tue Jun 12 16:20:29 2012 +0200

    mm: Make default vm_ops provide ->page_mkwrite handler
    
    Make default vm_ops provide ->page_mkwrite handler. Currently it only updates
    file's modification times and gets locked page but later it will also handle
    filesystem freezing.
    
    BugLink: https://bugs.launchpad.net/bugs/897421
    Tested-by: Kamal Mostafa <kamal@canonical.com>
    Tested-by: Peter M. Petrakis <peter.petrakis@canonical.com>
    Tested-by: Dann Frazier <dann.frazier@canonical.com>
    Tested-by: Massimo Morana <massimo.morana@canonical.com>
    Signed-off-by: Jan Kara <jack@suse.cz>
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

commit 701888b7907c7eb6d6b39525bc9a26d935bca42f
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun May 6 13:54:06 2012 -0700

    vm: remove 'nr_accounted' calculations from the unmap_vmas() interfaces
    
    The VM accounting makes no sense at this level, and half of the callers
    didn't ever actually use the end result.  The only time we want to
    unaccount the memory is when we actually remove the vma, so do the
    accounting at that point instead.
    
    This simplifies the interfaces (no need to pass down that silly page
    counter to functions that really don't care), and also makes it much
    more obvious what is actually going on: we do vm_[un]acct_memory() when
    adding or removing the vma, not on random page walking.
    
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit 278a0f4882bb938a68ce71f241296a2d7d6eeb48
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun May 6 13:43:15 2012 -0700

    vm: simplify unmap_vmas() calling convention
    
    None of the callers want to pass in 'zap_details', and it doesn't even
    make sense for the case of actually unmapping vma's.  So remove the
    argument, and clean up the interface.
    
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit a79df814bc80f459e2ff0cc64394460a5cb3bc55
Author: Andy Shevchenko <andriy.shevchenko@linux.intel.com>
Date:   Mon Dec 17 16:01:18 2012 -0800

    string: introduce helper to get base file name from given path
    
    There are several places in the kernel that use functionality like
    basename(3) with the exception: in case of '/foo/bar/' we expect to get an
    empty string.  Let's do it common helper for them.
    
    Signed-off-by: Andy Shevchenko <andriy.shevchenko@linux.intel.com>
    Cc: Jason Baron <jbaron@redhat.com>
    Cc: YAMANE Toshiaki <yamanetoshi@gmail.com>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit b0c121fbadf7b894b8c0db5cba8ebe3eda826559
Author: Akinobu Mita <akinobu.mita@gmail.com>
Date:   Mon Jul 30 14:40:55 2012 -0700

    string: introduce memweight()
    
    memweight() is the function that counts the total number of bits set in
    memory area.  Unlike bitmap_weight(), memweight() takes pointer and size
    in bytes to specify a memory area which does not need to be aligned to
    long-word boundary.
    
    [akpm@linux-foundation.org: rename `w' to `ret']
    Signed-off-by: Akinobu Mita <akinobu.mita@gmail.com>
    Cc: Anders Larsen <al@alarsen.net>
    Cc: Alasdair Kergon <agk@redhat.com>
    Cc: Laurent Pinchart <laurent.pinchart@ideasonboard.com>
    Cc: Mark Fasheh <mfasheh@suse.com>
    Cc: Joel Becker <jlbec@evilplan.org>
    Cc: Jan Kara <jack@suse.cz>
    Cc: Andreas Dilger <adilger.kernel@dilger.ca>
    Cc: "Theodore Ts'o" <tytso@mit.edu>
    Cc: Matthew Wilcox <matthew@wil.cx>
    Cc: Mauro Carvalho Chehab <mchehab@infradead.org>
    Cc: Tony Luck <tony.luck@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit 0939e992ae385fe8a28af5f8e7585c7ecf37b821
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Jul 2 13:39:53 2015 +0200

    host: msm_sdcc reduce spam logging

commit f85067688f72a260d5ceeefe89daeaea1e10e0ef
Author: Dorimanx <yuri@bynet.co.il>
Date:   Thu Jul 2 03:08:34 2015 +0300

    MMC: Stop CMD52 CRC ERROR SPAM IN LOG
    
    from drivers/mmc/core/core.c
            /*
             * sdio_reset sends CMD52 to reset card.  Since we do not know
             * if the card is being re-initialized, just send it.  CMD52
             * should be ignored by SD/eMMC cards.
             */

commit 3f8239d3d98e2f698165736912185d97ba39cd19
Author: Markus Trippelsdorf <markus@trippelsdorf.de>
Date:   Fri Oct 5 14:57:17 2012 +0200

    tty: Fix bogus "callbacks suppressed" messages
    
    On the current git tree one sees messages such as:
     tty_init_dev: 24 callbacks suppressed
     tty_init_dev: 3 callbacks suppressed
    
    To fix this we need to look at condition before calling __ratelimit in
    the WARN_RATELIMIT macro. While at it remove the superfluous
    __WARN_RATELIMIT macros.
    
    Original patch is from Joe Perches and Jiri Slaby.
    
    Signed-off-by: Markus Trippelsdorf <markus@trippelsdorf.de>
    Acked-and-tested-by: Borislav Petkov <borislav.petkov@amd.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 52fc1a669cd803c650f29c3624a2dba038937f2b
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Mon Jun 25 17:15:31 2012 +0900

    bug.h: Fix up CONFIG_BUG=n implicit function declarations.
    
    Commit 2603efa31a03 ("bug.h: Fix up powerpc build regression") corrected
    the powerpc build case and extended the __ASSEMBLY__ guards, but it also
    got caught in pre-processor hell accidentally matching the else case of
    CONFIG_BUG resulting in the BUG disabled case tripping up on
    -Werror=implicit-function-declaration.
    
    It's not possible to __ASSEMBLY__ guard the entire file as architecture
    code needs to get at the BUGFLAG_WARNING definition in the GENERIC_BUG
    case, but the rest of the CONFIG_BUG=y/n case needs to be guarded.
    
    Rather than littering endless __ASSEMBLY__ checks in each of the if/else
    cases we just move the BUGFLAG definitions up under their own
    GENERIC_BUG test and then shove everything else under one big
    __ASSEMBLY__ guard.
    
    Build tested on all of x86 CONFIG_BUG=y, CONFIG_BUG=n, powerpc (due to
    it's dependence on BUGFLAG definitions in assembly code), and sh (due to
    not bringing in linux/kernel.h to satisfy the taint flag definitions used
    by the generic bug code).
    
    Hopefully that's the end of the corner cases and I can abstain from ever
    having to touch this infernal header ever again.
    
    Reported-by: Fengguang Wu <fengguang.wu@intel.com>
    Tested-by: Fengguang Wu <wfg@linux.intel.com>
    Acked-by: Randy Dunlap <rdunlap@xenotime.net>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit ddd8fb398634f1220cd6af7670ddeb0d58a7c12d
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Mon Jun 18 13:54:17 2012 +0900

    bug.h: Fix up powerpc build regression.
    
    The asm-generic/bug.h __ASSEMBLY__ guarding is completely bogus, which
    tripped up the powerpc build when the kernel.h include was added:
    
    	In file included from include/asm-generic/bug.h:5:0,
    			 from arch/powerpc/include/asm/bug.h:127,
    			 from arch/powerpc/kernel/head_64.S:31:
    	include/linux/kernel.h:44:0: warning: "ALIGN" redefined [enabled by default]
    	include/linux/linkage.h:57:0: note: this is the location of the previous definition
    	include/linux/sysinfo.h: Assembler messages:
    	include/linux/sysinfo.h:7: Error: Unrecognized opcode: `struct'
    	include/linux/sysinfo.h:8: Error: Unrecognized opcode: `__kernel_long_t'
    
    Moving the __ASSEMBLY__ guard up and stashing the kernel.h include under
    it fixes this up, as well as covering the case the original fix was
    attempting to handle.
    
    Tested-by: Stephen Rothwell <sfr@canb.auug.org.au>
    Acked-by: Arnd Bergmann <arnd@arndb.de>
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit dee256486372d61fad1fc8df27d24de6cfc02a4e
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Mon Jun 11 14:29:58 2012 +0900

    bug.h: need linux/kernel.h for TAINT_WARN.
    
    asm-generic/bug.h uses taint flags that are only defined in
    linux/kernel.h, resulting in build failures on platforms that
    don't include linux/kernel.h some other way:
    
            arch/sh/include/asm/thread_info.h:172:2: error: 'TAINT_WARN' undeclared (first use in this function)
    
    Caused by commit edd63a2763bd ("set_restore_sigmask() is never called
    without SIGPENDING (and never should be)").
    
    Reported-by: Stephen Rothwell <sfr@canb.auug.org.au>
    Cc: Al Viro <viro@zeniv.linux.org.uk>
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

commit 4d6946d80e293618d3fceacc968dc50f2471ede3
Author: P J P <ppandit@redhat.com>
Date:   Tue Nov 12 15:10:22 2013 -0800

    initramfs: read CONFIG_RD_ variables for initramfs compression
    
    When expert configuration option(CONFIG_EXPERT) is enabled, menuconfig
    offers a choice of compression algorithm to compress initial ramfs image;
    This choice is stored into CONFIG_RD_* variables.  But usr/Makefile uses
    earlier INITRAMFS_COMPRESSION_* macros to build initial ramfs file.  Since
    none of them is defined, resulting 'initramfs_data.cpio' file remains
    un-compressed.
    
    This patch updates the Makefile to use CONFIG_RD_* variables and adds
    support for LZ4 compression algorithm.  Also updates the
    'gen_initramfs_list.sh' script to check whether a selected compression
    command is accessible or not.  And fall-back to default gzip(1)
    compression when it is not.
    
    Signed-off-by: P J P <prasad@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 917ed4213a10311aa3eac89d87e4e3a8798a9058
Author: Arve Hjnnevg <arve@android.com>
Date:   Fri Nov 30 17:05:40 2012 -0800

    ARM: decompressor: Flush tlb before swiching domain 0 to client mode
    
    If the bootloader used a page table that is incompatible with domain 0
    in client mode, and boots with the mmu on, then swithing domain 0 to
    client mode causes a fault if we don't flush the tlb after updating
    the page table pointer.
    
    v2: Add ISB before loading dacr.
    
    Signed-off-by: Arve Hjnnevg <arve@android.com>

commit 86025a12127500ac6c13b243c42de8b3ef5dd302
Author: Kyungsik Lee <kyungsik.lee@lge.com>
Date:   Mon Jul 8 16:01:48 2013 -0700

    arm: add support for LZ4-compressed kernel
    
    Integrates the LZ4 decompression code to the arm pre-boot code.
    
    Signed-off-by: Kyungsik Lee <kyungsik.lee@lge.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Russell King <rmk@arm.linux.org.uk>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Florian Fainelli <florian@openwrt.org>
    Cc: Yann Collet <yann.collet.73@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>
    
    Conflicts:
    	arch/arm/Kconfig
    	arch/arm/boot/compressed/.gitignore
    	arch/arm/boot/compressed/Makefile
    	arch/x86/Kconfig
    	arch/x86/boot/compressed/Makefile
    	arch/x86/boot/compressed/misc.c

commit 2fece1fb20dbcd6d73bfcb8cc78e95a251ad76f1
Author: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
Date:   Thu Jul 3 16:06:57 2014 -0700

    lz4: add overrun checks to lz4_uncompress_unknownoutputsize()
    
    Jan points out that I forgot to make the needed fixes to the
    lz4_uncompress_unknownoutputsize() function to mirror the changes done
    in lz4_decompress() with regards to potential pointer overflows.
    
    The only in-kernel user of this function is the zram code, which only
    takes data from a valid compressed buffer that it made itself, so it's
    not a big issue.  But due to external kernel modules using this
    function, it's better to be safe here.
    
    Reported-by: Jan Beulich <JBeulich@suse.com>
    Cc: "Don A. Bailey" <donb@securitymouse.com>
    Cc: stable <stable@vger.kernel.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 64197b96a3c4e3f27d62187568500bf0558566e1
Author: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
Date:   Tue Jun 24 16:59:01 2014 -0400

    lz4: fix another possible overrun
    
    There is one other possible overrun in the lz4 code as implemented by
    Linux at this point in time (which differs from the upstream lz4
    codebase, but will get synced at in a future kernel release.)  As
    pointed out by Don, we also need to check the overflow in the data
    itself.
    
    While we are at it, replace the odd error return value with just a
    "simple" -1 value as the return value is never used for anything other
    than a basic "did this work or not" check.
    
    Reported-by: "Don A. Bailey" <donb@securitymouse.com>
    Reported-by: Willy Tarreau <w@1wt.eu>
    Cc: stable <stable@vger.kernel.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit dea06341a81358eadd353b4912d1fe137cedcc69
Author: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
Date:   Fri Jun 20 22:01:41 2014 -0700

    lz4: ensure length does not wrap
    
    Given some pathologically compressed data, lz4 could possibly decide to
    wrap a few internal variables, causing unknown things to happen.  Catch
    this before the wrapping happens and abort the decompression.
    
    Reported-by: "Don A. Bailey" <donb@securitymouse.com>
    Cc: stable <stable@vger.kernel.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 404c1d30451684e111bd148eb2998d2d52269908
Author: Sergey Senozhatsky <sergey.senozhatsky@gmail.com>
Date:   Wed Sep 11 14:26:32 2013 -0700

    lz4: fix compression/decompression signedness mismatch
    
    LZ4 compression and decompression functions require different in
    signedness input/output parameters: unsigned char for compression and
    signed char for decompression.
    
    Change decompression API to require "(const) unsigned char *".
    
    Signed-off-by: Sergey Senozhatsky <sergey.senozhatsky@gmail.com>
    Cc: Kyungsik Lee <kyungsik.lee@lge.com>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Yann Collet <yann.collet.73@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 8e9194b9aa8de9dff6cedd9fae46cabcba66e94f
Author: Richard Laager <rlaager@wiktel.com>
Date:   Thu Aug 22 16:35:47 2013 -0700

    lib/lz4: correct the LZ4 license
    
    The LZ4 code is listed as using the "BSD 2-Clause License".
    
    Signed-off-by: Richard Laager <rlaager@wiktel.com>
    Acked-by: Kyungsik Lee <kyungsik.lee@lge.com>
    Cc: Chanho Min <chanho.min@lge.com>
    Cc: Richard Yao <ryao@gentoo.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    [ The 2-clause BSD can be just converted into GPL, but that's rude and
      pointless, so don't do it   - Linus ]
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 978ad26efd5742cb2470ba0cb6603920ddd3025c
Author: Chanho Min <chanho.min@lge.com>
Date:   Mon Jul 8 16:01:49 2013 -0700

    lib: add lz4 compressor module
    
    This patchset is for supporting LZ4 compression and the crypto API using
    it.
    
    As shown below, the size of data is a little bit bigger but compressing
    speed is faster under the enabled unaligned memory access.  We can use
    lz4 de/compression through crypto API as well.  Also, It will be useful
    for another potential user of lz4 compression.
    
    lz4 Compression Benchmark:
    Compiler: ARM gcc 4.6.4
    ARMv7, 1 GHz based board
       Kernel: linux 3.4
       Uncompressed data Size: 101 MB
             Compressed Size  compression Speed
       LZO   72.1MB		  32.1MB/s, 33.0MB/s(UA)
       LZ4   75.1MB		  30.4MB/s, 35.9MB/s(UA)
       LZ4HC 59.8MB		   2.4MB/s,  2.5MB/s(UA)
    - UA: Unaligned memory Access support
    - Latest patch set for LZO applied
    
    This patch:
    
    Add support for LZ4 compression in the Linux Kernel.  LZ4 Compression APIs
    for kernel are based on LZ4 implementation by Yann Collet and were changed
    for kernel coding style.
    
    LZ4 homepage : http://fastcompression.blogspot.com/p/lz4.html
    LZ4 source repository : http://code.google.com/p/lz4/
    svn revision : r90
    
    Two APIs are added:
    
    lz4_compress() support basic lz4 compression whereas lz4hc_compress()
    support high compression or CPU performance get lower but compression
    ratio get higher.  Also, we require the pre-allocated working memory with
    the defined size and destination buffer must be allocated with the size of
    lz4_compressbound.
    
    [akpm@linux-foundation.org: make lz4_compresshcctx() static]
    Signed-off-by: Chanho Min <chanho.min@lge.com>
    Cc: "Darrick J. Wong" <djwong@us.ibm.com>
    Cc: Bob Pearson <rpearson@systemfabricworks.com>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: Herbert Xu <herbert@gondor.hengli.com.au>
    Cc: Yann Collet <yann.collet.73@gmail.com>
    Cc: Kyungsik Lee <kyungsik.lee@lge.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit ca279908f49e170e2b123a6ffba7379b86423048
Author: Kyungsik Lee <kyungsik.lee@lge.com>
Date:   Mon Jul 8 16:01:46 2013 -0700

    lib: add support for LZ4-compressed kernel
    
    Add support for extracting LZ4-compressed kernel images, as well as
    LZ4-compressed ramdisk images in the kernel boot process.
    
    Signed-off-by: Kyungsik Lee <kyungsik.lee@lge.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Russell King <rmk@arm.linux.org.uk>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Florian Fainelli <florian@openwrt.org>
    Cc: Yann Collet <yann.collet.73@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit a2624552d7da16a7c10adf9fe6690a9342c8ef46
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sat Sep 12 02:05:56 2015 +0200

    update some md files to get the kernel compiling

commit c3f4130aab2be43c0c339d0c03118e4c328a4872
Author: Kent Overstreet <koverstreet@google.com>
Date:   Thu Sep 6 15:34:55 2012 -0700

    block: Generalized bio pool freeing
    
    With the old code, when you allocate a bio from a bio pool you have to
    implement your own destructor that knows how to find the bio pool the
    bio was originally allocated from.
    
    This adds a new field to struct bio (bi_pool) and changes
    bio_alloc_bioset() to use it. This makes various bio destructors
    unnecessary, so they're then deleted.
    
    v6: Explain the temporary if statement in bio_put
    
    Signed-off-by: Kent Overstreet <koverstreet@google.com>
    CC: Jens Axboe <axboe@kernel.dk>
    CC: NeilBrown <neilb@suse.de>
    CC: Alasdair Kergon <agk@redhat.com>
    CC: Nicholas Bellinger <nab@linux-iscsi.org>
    CC: Lars Ellenberg <lars.ellenberg@linbit.com>
    Acked-by: Tejun Heo <tj@kernel.org>
    Acked-by: Nicholas Bellinger <nab@linux-iscsi.org>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>
    
    Conflicts:
    	drivers/block/drbd/drbd_main.c
    	drivers/md/dm-crypt.c
    	drivers/target/target_core_iblock.c
    	fs/bio.c
    	include/linux/blk_types.h

commit 09ce73daaad8e354a25830d1ce66e26d514c4b23
Author: Stefan Hajnoczi <stefanha@linux.vnet.ibm.com>
Date:   Mon Apr 30 16:35:13 2012 +0100

    target: Handle GET_EVENT_STATUS_NOTIFICATION passthrough
    
    The SCSI MMC GET_EVENT_STATUS_NOTIFICATION command can be used to find
    out about media change, among other things.  This patch adds it to the
    command sequencer so that PSCSI CD-ROM passthrough works with modern
    Linux guests that issue this command.
    
    Tested-by: Cong Meng <mengcong@cn.ibm.com>
    Signed-off-by: Stefan Hajnoczi <stefanha@linux.vnet.ibm.com>
    Signed-off-by: Nicholas Bellinger <nab@linux-iscsi.org>

commit 06ff7826a4100e1ae46276cdb55ab81c543c7968
Author: Dan Carpenter <dan.carpenter@oracle.com>
Date:   Tue Apr 17 09:33:59 2012 +0300

    target/iscsi: cleanup some allocation style issues
    
    We can use kcalloc() here instead of kzalloc().  It's better style and
    it has overflow checking built in.
    
    Also -ENOMEM is the correct error code for allocation errors.  -1 means
    -EPERM.  None of the callers preserve the error codes so it doesn't
    matter except as a cleanup.
    
    Signed-off-by: Dan Carpenter <dan.carpenter@oracle.com>
    Signed-off-by: Nicholas Bellinger <nab@linux-iscsi.org>

commit d67fa02a07669804496d4a090d63328f8a26f7fd
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sat Sep 12 00:44:20 2015 +0200

    port /block from 3.10 to 3.4

commit f1e9a13bfae8f788cc31ca557b59d1928c8d44d4
Author: Mikulas Patocka <mpatocka@redhat.com>
Date:   Fri Jul 27 15:08:03 2012 +0100

    dm: introduce split_discard_requests
    
    This patch introduces a new variable split_discard_requests. It can be
    set by targets so that discard requests are split on max_io_len
    boundaries.
    
    When split_discard_requests is not set, discard requests are only split on
    boundaries between targets, as was the case before this patch.
    
    Signed-off-by: Mikulas Patocka <mpatocka@redhat.com>
    Signed-off-by: Alasdair G Kergon <agk@redhat.com>

commit 55d139cb39f7e27d09679c5c141b999ed6fb2c31
Author: Mike Snitzer <snitzer@redhat.com>
Date:   Fri Jul 27 15:08:00 2012 +0100

    dm: support non power of two target max_io_len
    
    Remove the restriction that limits a target's specified maximum incoming
    I/O size to be a power of 2.
    
    Rename this setting from 'split_io' to the less-ambiguous 'max_io_len'.
    Change it from sector_t to uint32_t, which is plenty big enough, and
    introduce a wrapper function dm_set_target_max_io_len() to set it.
    Use sector_div() to process it now that it is not necessarily a power of 2.
    
    Signed-off-by: Mike Snitzer <snitzer@redhat.com>
    Signed-off-by: Alasdair G Kergon <agk@redhat.com>

commit cc11858d2609011a0e45883a40b1d700bcb8f1ae
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Sun Mar 31 13:43:23 2013 -0400

    new helper: single_open_size()
    
    Same as single_open(), but preallocates the buffer of given size.
    Doesn't make any sense for sizes up to PAGE_SIZE and doesn't make
    sense if output of show() exceeds PAGE_SIZE only rarely - seq_read()
    will take care of growing the buffer and redoing show().  If you
    _know_ that it will be large, it might make more sense to look into
    saner iterator, rather than go with single-shot one.  If that's
    impossible, single_open_size() might be for you.
    
    Again, don't use that without a good reason; occasionally that's really
    the best way to go, but very often there are better solutions.
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

commit ced651388f2d7df69e13aa848415c95e104c5741
Author: Kyungsik Lee <kyungsik.lee@lge.com>
Date:   Mon Jul 8 16:01:45 2013 -0700

    decompressor: add LZ4 decompressor module
    
    Add support for LZ4 decompression in the Linux Kernel.  LZ4 Decompression
    APIs for kernel are based on LZ4 implementation by Yann Collet.
    
    Benchmark Results(PATCH v3)
    Compiler: Linaro ARM gcc 4.6.2
    
    1. ARMv7, 1.5GHz based board
       Kernel: linux 3.4
       Uncompressed Kernel Size: 14MB
            Compressed Size  Decompression Speed
       LZO  6.7MB            20.1MB/s, 25.2MB/s(UA)
       LZ4  7.3MB            29.1MB/s, 45.6MB/s(UA)
    
    2. ARMv7, 1.7GHz based board
       Kernel: linux 3.7
       Uncompressed Kernel Size: 14MB
            Compressed Size  Decompression Speed
       LZO  6.0MB            34.1MB/s, 52.2MB/s(UA)
       LZ4  6.5MB            86.7MB/s
    - UA: Unaligned memory Access support
    - Latest patch set for LZO applied
    
    This patch set is for adding support for LZ4-compressed Kernel.  LZ4 is a
    very fast lossless compression algorithm and it also features an extremely
    fast decoder [1].
    
    But we have five of decompressors already and one question which does
    arise, however, is that of where do we stop adding new ones?  This issue
    had been discussed and came to the conclusion [2].
    
    Russell King said that we should have:
    
     - one decompressor which is the fastest
     - one decompressor for the highest compression ratio
     - one popular decompressor (eg conventional gzip)
    
    If we have a replacement one for one of these, then it should do exactly
    that: replace it.
    
    The benchmark shows that an 8% increase in image size vs a 66% increase
    in decompression speed compared to LZO(which has been known as the
    fastest decompressor in the Kernel).  Therefore the "fast but may not be
    small" compression title has clearly been taken by LZ4 [3].
    
    [1] http://code.google.com/p/lz4/
    [2] http://thread.gmane.org/gmane.linux.kbuild.devel/9157
    [3] http://thread.gmane.org/gmane.linux.kbuild.devel/9347
    
    LZ4 homepage: http://fastcompression.blogspot.com/p/lz4.html
    LZ4 source repository: http://code.google.com/p/lz4/
    
    Signed-off-by: Kyungsik Lee <kyungsik.lee@lge.com>
    Signed-off-by: Yann Collet <yann.collet.73@gmail.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Russell King <rmk@arm.linux.org.uk>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Florian Fainelli <florian@openwrt.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 31d448fa37fa4f5b1430b92454af3a8c4d2a98b4
Author: Alasdair G Kergon <agk@redhat.com>
Date:   Fri May 10 14:37:17 2013 +0100

    dm: document iterate_devices
    
    Document iterate_devices in device-mapper.h.
    
    Signed-off-by: Alasdair G Kergon <agk@redhat.com>

commit 2d730841d1c7866d3d656310f2f60cadbc3f06f9
Author: H Hartley Sweeten <hartleys@visionengravers.com>
Date:   Thu May 31 16:26:10 2012 -0700

    init: disable sparse checking of the mount.o source files
    
    The init/mount.o source files produce a number of sparse warnings of the
    type:
    
    warning: incorrect type in argument 1 (different address spaces)
       expected char [noderef] <asn:1>*dev_name
       got char *name
    
    This is due to the syscalls expecting some of the arguments to be user
    pointers but they are being passed as kernel pointers.  This is harmless
    but adds a lot of noise to a sparse build.
    
    To limit the noise just disable the sparse checking in the relevant source
    files, but still display a warning so that the user knows this has been
    done.
    
    Since the sparse checking has been disabled we can also remove the __user
    __force casts that are scattered thru the source.
    
    Signed-off-by: H Hartley Sweeten <hsweeten@visionengravers.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit e9c2ede064be58c744082c2c7dcefc496fc747ed
Author: Ulf Hansson <ulf.hansson@linaro.org>
Date:   Fri Apr 12 09:41:30 2013 +0000

    PM / Runtime: Asyncronous idle|suspend parent devices at removal
    
    For irq safe devices return the runtime reference for the parent
    by using the asyncronous runtime PM API. Thus we don't have to
    wait for it to become idle|suspended. Instead we can move on and
    handle the next device in queue.
    
    Signed-off-by: Ulf Hansson <ulf.hansson@linaro.org>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit 6ef0f779735cc6d78a23ac5937ec7a270550ff9f
Author: Ming Lei <ming.lei@canonical.com>
Date:   Fri Feb 22 16:34:19 2013 -0800

    pm / runtime: force memory allocation with no I/O during Runtime PM callbcack
    
    Apply the introduced memalloc_noio_save() and memalloc_noio_restore() to
    force memory allocation with no I/O during runtime_resume/runtime_suspend
    callback on device with the flag of 'memalloc_noio' set.
    
    Signed-off-by: Ming Lei <ming.lei@canonical.com>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Eric Dumazet <eric.dumazet@gmail.com>
    Cc: David Decotigny <david.decotigny@google.com>
    Cc: Tom Herbert <therbert@google.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Jens Axboe <axboe@kernel.dk>
    Cc: Minchan Kim <minchan@kernel.org>
    Cc: Alan Stern <stern@rowland.harvard.edu>
    Cc: Oliver Neukum <oneukum@suse.de>
    Cc: Jiri Kosina <jiri.kosina@suse.com>
    Cc: Mel Gorman <mel@csn.ul.ie>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Michal Hocko <mhocko@suse.cz>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: "Rafael J. Wysocki" <rjw@sisk.pl>
    Cc: Greg KH <greg@kroah.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit 0cab59fa775c485804214735aec081099c210905
Author: Rafael J. Wysocki <rjw@sisk.pl>
Date:   Wed Aug 15 21:32:04 2012 +0200

    PM / Runtime: Check device PM QoS setting before "no callbacks" check
    
    If __dev_pm_qos_read_value(dev) returns a negative value,
    rpm_suspend() should return -EPERM for dev even if its
    power.no_callbacks flag is set.  For this to happen, the device's
    power.no_callbacks flag has to be checked after the PM QoS check,
    so move the PM QoS check to rpm_check_suspend_allowed() (this will
    make it cover idle notifications as well as runtime suspend too).
    
    Signed-off-by: Rafael J. Wysocki <rjw@sisk.pl>
    Acked-by: Alan Stern <stern@rowland.harvard.edu>
    Cc: stable@vger.kernel.org

commit d546eb65d089f20a81c03f5c165dccb29153c39f
Author: Kevin Hilman <khilman@ti.com>
Date:   Fri Sep 21 22:47:34 2012 +0000

    PM / Runtime: let rpm_resume() succeed if RPM_ACTIVE, even when disabled, v2
    
    There are several drivers where the return value of
    pm_runtime_get_sync() is used to decide whether or not it is safe to
    access hardware and that don't provide .suspend() callbacks for system
    suspend (but may use late/noirq callbacks.)  If such a driver happens
    to call pm_runtime_get_sync() during system suspend, after the core
    has disabled runtime PM, it will get the error code and will decide
    that the hardware should not be accessed, although this may be a wrong
    conclusion, depending on the state of the device when runtime PM was
    disabled.
    
    Drivers might work around this problem by using a test like:
    
       ret = pm_runtime_get_sync(dev);
       if (!ret || (ret == -EACCES && driver_private_data(dev)->suspended)) {
          /* access hardware */
       }
    
    where driver_private_data(dev)->suspended is a flag set by the
    driver's .suspend() method (that would have to be added for this
    purpose).  However, that potentially would need to be done by multiple
    drivers which means quite a lot of duplicated code and bloat.
    
    To avoid that we can use the observation that the core sets
    dev->power.is_suspended before disabling runtime PM and use that
    instead of the driver's private flag.  Still, potentially many drivers
    would need to repeat that same check in quite a few places, so it's
    better to let the core do it.
    
    Then we can be a bit smarter and check whether or not runtime PM was
    disabled by the core only (disable_depth == 1) or by someone else in
    addition to the core (disable_depth > 1).  In the former case
    rpm_resume() can return 1 if the runtime PM status is RPM_ACTIVE,
    because it means the device was active when the core disabled runtime
    PM.  In the latter case it should still return -EACCES, because it
    isn't clear why runtime PM has been disabled.
    
    Tested on AM3730/Beagle-xM where a wakeup IRQ firing during the late
    suspend phase triggers runtime PM activity in the I2C driver since the
    wakeup IRQ is on an I2C-connected PMIC.
    
    [rjw: Modified whitespace to follow the file's convention.]
    
    Signed-off-by: Kevin Hilman <khilman@ti.com>
    Signed-off-by: Rafael J. Wysocki <rjw@sisk.pl>

commit 1d8b2331532f0a8f469d899cb55711bc12a8efd8
Author: Ming Lei <ming.lei@canonical.com>
Date:   Fri Feb 22 16:34:11 2013 -0800

    pm / runtime: introduce pm_runtime_set_memalloc_noio()
    
    Introduce the flag memalloc_noio in 'struct dev_pm_info' to help PM core
    to teach mm not allocating memory with GFP_KERNEL flag for avoiding
    probable deadlock.
    
    As explained in the comment, any GFP_KERNEL allocation inside
    runtime_resume() or runtime_suspend() on any one of device in the path
    from one block or network device to the root device in the device tree
    may cause deadlock, the introduced pm_runtime_set_memalloc_noio() sets
    or clears the flag on device in the path recursively.
    
    Signed-off-by: Ming Lei <ming.lei@canonical.com>
    Cc: Minchan Kim <minchan@kernel.org>
    Cc: Alan Stern <stern@rowland.harvard.edu>
    Cc: Oliver Neukum <oneukum@suse.de>
    Cc: Jiri Kosina <jiri.kosina@suse.com>
    Cc: Mel Gorman <mel@csn.ul.ie>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Michal Hocko <mhocko@suse.cz>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: "Rafael J. Wysocki" <rjw@sisk.pl>
    Cc: Greg KH <greg@kroah.com>
    Cc: Jens Axboe <axboe@kernel.dk>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Eric Dumazet <eric.dumazet@gmail.com>
    Cc: David Decotigny <david.decotigny@google.com>
    Cc: Tom Herbert <therbert@google.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit ba1fb45392c7fb83f2a1a403fc610f71488dfe62
Author: Laxman Dewangan <ldewangan@nvidia.com>
Date:   Wed Jun 20 14:14:05 2012 +0530

    gpio: fix bits conflict for gpio flags
    
    The bit 2 and 3 in GPIO flag are allocated for the
    flag OPEN_DRAIN/OPEN_SOURCE. These bits are reused
    for the flag EXPORT/EXPORT_CHANGEABLE and so creating
    conflict.
    Fix this conflict by assigning bit 4 and 5 for the
    flag EXPORT/EXPORT_CHANGEABLE.
    
    Signed-off-by: Laxman Dewangan <ldewangan@nvidia.com>
    Signed-off-by: Linus Walleij <linus.walleij@linaro.org>

commit 0f1b3da54b02ae9690cb7d2a742993292757313f
Author: Mark Brown <broonie@opensource.wolfsonmicro.com>
Date:   Wed Apr 4 16:14:48 2012 +0100

    gpiolib: Add !CONFIG_GPIOLIB definitions of devm_ functions
    
    Currently the managed gpio_request() and gpio_free() are not stubbed out
    for configurations not using gpiolib - do that to aid use in drivers.
    
    Signed-off-by: Mark Brown <broonie@opensource.wolfsonmicro.com>
    Signed-off-by: Grant Likely <grant.likely@secretlab.ca>

commit e9a487db2220a9236f6c295e1402bcf7e32a9e3f
Author: Wolfram Sang <w.sang@pengutronix.de>
Date:   Tue Dec 13 18:34:01 2011 +0100

    gpio: add flags to export GPIOs when requesting
    
    Introduce new flags to automatically export GPIOs when using the convenience
    functions gpio_request_one() or gpio_request_array(). This eases support for
    custom boards where lots of GPIOs need to be exported for customer
    applications.
    
    Signed-off-by: Wolfram Sang <w.sang@pengutronix.de>
    Signed-off-by: Grant Likely <grant.likely@secretlab.ca>

commit a810b0a7bbafd99680e8e59f72d84c74e281ec86
Author: Mark Brown <broonie@opensource.wolfsonmicro.com>
Date:   Wed May 2 12:46:46 2012 +0100

    gpiolib: Implement devm_gpio_request_one()
    
    Allow drivers to use the modern request and configure idiom together
    with devres.
    
    As with plain gpio_request() and gpio_request_one() we can't implement
    the old school version in terms of _one() as this would force the
    explicit selection of a direction in gpio_request() which could break
    systems if we pick the wrong one.  Implementing devm_gpio_request_one()
    in terms of devm_gpio_request() would needlessly complicate things or
    lead to duplication from the unmanaged version depending on how it's
    done.
    
    Signed-off-by: Mark Brown <broonie@opensource.wolfsonmicro.com>
    Acked-by: Linus Walleij <linus.walleij@linaro.org>
    Signed-off-by: Grant Likely <grant.likely@secretlab.ca>

commit 214315843042a58bca3ce6f70d21c185f6c76e84
Author: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
Date:   Fri Nov 2 01:40:09 2012 +0100

    ACPI / PM: Move routines for adding/removing device wakeup notifiers
    
    ACPI routines for adding and removing device wakeup notifiers are
    currently defined in a PCI-specific file, but they will be necessary
    for non-PCI devices too, so move them to a separate file under
    drivers/acpi and rename them to indicate their ACPI origins.
    
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    
    Conflicts:
    	include/acpi/acpi_bus.h

commit 5a767ecf0fcb9051c400cd8526f46fe05537850d
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Mon Jun 8 15:58:13 2015 +0200

    dr: fix top layer handling
    
    Most functions in idr fail to deal with the high bits when the idr
    tree grows to the maximum height.
    
    * idr_get_empty_slot() stops growing idr tree once the depth reaches
      MAX_IDR_LEVEL - 1, which is one depth shallower than necessary to
      cover the whole range.  The function doesn't even notice that it
      didn't grow the tree enough and ends up allocating the wrong ID
      given sufficiently high @starting_id.
    
      For example, on 64 bit, if the starting id is 0x7fffff01,
      idr_get_empty_slot() will grow the tree 5 layer deep, which only
      covers the 30 bits and then proceed to allocate as if the bit 30
      wasn't specified.  It ends up allocating 0x3fffff01 without the bit
      30 but still returns 0x7fffff01.
    
    * __idr_remove_all() will not remove anything if the tree is fully
      grown.
    
    * idr_find() can't find anything if the tree is fully grown.
    
    * idr_for_each() and idr_get_next() can't iterate anything if the tree
      is fully grown.
    
    Fix it by introducing idr_max() which returns the maximum possible ID
    given the depth of tree and replacing the id limit checks in all
    affected places.
    
    As the idr_layer pointer array pa[] needs to be 1 larger than the
    maximum depth, enlarge pa[] arrays by one.
    
    While this plugs the discovered issues, the whole code base is
    horrible and in desparate need of rewrite.  It's fragile like hell,
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: <stable@vger.kernel.org>
    
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit ea4e053c94fd2e81ce5a37f394f3214eb65c6a35
Author: Tejun Heo <tj@kernel.org>
Date:   Wed Feb 27 17:03:55 2013 -0800

    idr: implement idr_preload[_end]() and idr_alloc()
    
    The current idr interface is very cumbersome.
    
    * For all allocations, two function calls - idr_pre_get() and
      idr_get_new*() - should be made.
    
    * idr_pre_get() doesn't guarantee that the following idr_get_new*()
      will not fail from memory shortage.  If idr_get_new*() returns
      -EAGAIN, the caller is expected to retry pre_get and allocation.
    
    * idr_get_new*() can't enforce upper limit.  Upper limit can only be
      enforced by allocating and then freeing if above limit.
    
    * idr_layer buffer is unnecessarily per-idr.  Each idr ends up keeping
      around MAX_IDR_FREE idr_layers.  The memory consumed per idr is
      under two pages but it makes it difficult to make idr_layer larger.
    
    This patch implements the following new set of allocation functions.
    
    * idr_preload[_end]() - Similar to radix preload but doesn't fail.
      The first idr_alloc() inside preload section can be treated as if it
      were called with @gfp_mask used for idr_preload().
    
    * idr_alloc() - Allocate an ID w/ lower and upper limits.  Takes
      @gfp_flags and can be used w/o preloading.  When used inside
      preloaded section, the allocation mask of preloading can be assumed.
    
    If idr_alloc() can be called from a context which allows sufficiently
    relaxed @gfp_mask, it can be used by itself.  If, for example,
    idr_alloc() is called inside spinlock protected region, preloading can
    be used like the following.
    
    	idr_preload(GFP_KERNEL);
    	spin_lock(lock);
    
    	id = idr_alloc(idr, ptr, start, end, GFP_NOWAIT);
    
    	spin_unlock(lock);
    	idr_preload_end();
    	if (id < 0)
    		error;
    
    which is much simpler and less error-prone than idr_pre_get and
    idr_get_new*() loop.
    
    The new interface uses per-pcu idr_layer buffer and thus the number of
    idr's in the system doesn't affect the amount of memory used for
    preloading.
    
    idr_layer_alloc() is introduced to handle idr_layer allocations for
    both old and new ID allocation paths.  This is a bit hairy now but the
    new interface is expected to replace the old and the internal
    implementation eventually will become simpler.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit c6eff894996575a074353ac565b2a2a81d5cd9f4
Author: Tejun Heo <tj@kernel.org>
Date:   Wed Feb 27 17:03:52 2013 -0800

    idr: relocate idr_for_each_entry() and reorganize id[r|a]_get_new()
    
    * Move idr_for_each_entry() definition next to other idr related
      definitions.
    
    * Make id[r|a]_get_new() inline wrappers of id[r|a]_get_new_above().
    
    This changes the implementation of idr_get_new() but the new
    implementation is trivial.  This patch doesn't introduce any
    functional change.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    
    Conflicts:
    	include/linux/idr.h

commit 909c922ee558b0c566efda992d8055e847eda4d6
Author: Tejun Heo <tj@kernel.org>
Date:   Wed Feb 27 17:03:54 2013 -0800

    idr: refactor idr_get_new_above()
    
    Move slot filling to idr_fill_slot() from idr_get_new_above_int() and
    make idr_get_new_above() directly call it.  idr_get_new_above_int() is
    no longer needed and removed.
    
    This will be used to implement a new ID allocation interface.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit 092e76d6a832b4a3ef9ee8c5bcf5461d1394b90c
Author: Tejun Heo <tj@kernel.org>
Date:   Wed Feb 27 17:03:53 2013 -0800

    idr: remove _idr_rc_to_errno() hack
    
    idr uses -1, IDR_NEED_TO_GROW and IDR_NOMORE_SPACE to communicate
    exception conditions internally.  The return value is later translated
    to errno values using _idr_rc_to_errno().
    
    This is confusing.  Drop the custom ones and consistently use -EAGAIN
    for "tree needs to grow", -ENOMEM for "need more memory" and -ENOSPC for
    "ran out of ID space".
    
    Due to the weird memory preloading mechanism, [ra]_get_new*() return
    -EAGAIN on memory shortage, so we need to substitute -ENOMEM w/
    -EAGAIN on those interface functions.  They'll eventually be cleaned
    up and the translations will go away.
    
    This patch doesn't introduce any functional changes.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit 51a6e82a2c7ff98b35a3ff7688e078a18535dede
Author: Tejun Heo <tj@kernel.org>
Date:   Wed Feb 27 17:03:35 2013 -0800

    idr: make idr_destroy() imply idr_remove_all()
    
    idr is silly in quite a few ways, one of which is how it's supposed to
    be destroyed - idr_destroy() doesn't release IDs and doesn't even whine
    if the idr isn't empty.  If the caller forgets idr_remove_all(), it
    simply leaks memory.
    
    Even ida gets this wrong and leaks memory on destruction.  There is
    absoltely no reason not to call idr_remove_all() from idr_destroy().
    Nobody is abusing idr_destroy() for shrinking free layer buffer and
    continues to use idr after idr_destroy(), so it's safe to do remove_all
    from destroy.
    
    In the whole kernel, there is only one place where idr_remove_all() is
    legitimiately used without following idr_destroy() while there are quite
    a few places where the caller forgets either idr_remove_all() or
    idr_destroy() leaking memory.
    
    This patch makes idr_destroy() call idr_destroy_all() and updates the
    function description accordingly.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit 601027a195f7dcb616571eb0188ba4442a5b9c55
Author: Tejun Heo <tj@kernel.org>
Date:   Wed Feb 27 17:03:34 2013 -0800

    idr: fix a subtle bug in idr_get_next()
    
    The iteration logic of idr_get_next() is borrowed mostly verbatim from
    idr_for_each().  It walks down the tree looking for the slot matching
    the current ID.  If the matching slot is not found, the ID is
    incremented by the distance of single slot at the given level and
    repeats.
    
    The implementation assumes that during the whole iteration id is aligned
    to the layer boundaries of the level closest to the leaf, which is true
    for all iterations starting from zero or an existing element and thus is
    fine for idr_for_each().
    
    However, idr_get_next() may be given any point and if the starting id
    hits in the middle of a non-existent layer, increment to the next layer
    will end up skipping the same offset into it.  For example, an IDR with
    IDs filled between [64, 127] would look like the following.
    
              [  0  64 ... ]
           /----/   |
           |        |
          NULL    [ 64 ... 127 ]
    
    If idr_get_next() is called with 63 as the starting point, it will try
    to follow down the pointer from 0.  As it is NULL, it will then try to
    proceed to the next slot in the same level by adding the slot distance
    at that level which is 64 - making the next try 127.  It goes around the
    loop and finds and returns 127 skipping [64, 126].
    
    Note that this bug also triggers in idr_for_each_entry() loop which
    deletes during iteration as deletions can make layers go away leaving
    the iteration with unaligned ID into missing layers.
    
    Fix it by ensuring proceeding to the next slot doesn't carry over the
    unaligned offset - ie.  use round_up(id + 1, slot_distance) instead of
    id += slot_distance.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Reported-by: David Teigland <teigland@redhat.com>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit d3f59d02a42d2014d4aa71bf277d81c31658f374
Author: Fengguang Wu <fengguang.wu@intel.com>
Date:   Thu Oct 4 17:13:15 2012 -0700

    idr: rename MAX_LEVEL to MAX_IDR_LEVEL
    
    To avoid name conflicts:
    
      drivers/video/riva/fbdev.c:281:9: sparse: preprocessor token MAX_LEVEL redefined
    
    While at it, also make the other names more consistent and add
    parentheses.
    
    [akpm@linux-foundation.org: repair fallout]
    [sfr@canb.auug.org.au: IB/mlx4: fix for MAX_ID_MASK to MAX_IDR_MASK name change]
    Signed-off-by: Fengguang Wu <fengguang.wu@intel.com>
    Cc: Bernd Petrovitsch <bernd@petrovitsch.priv.at>
    Cc: walter harms <wharms@bfs.de>
    Cc: Glauber Costa <glommer@parallels.com>
    Signed-off-by: Stephen Rothwell <sfr@canb.auug.org.au>
    Cc: Roland Dreier <roland@purestorage.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit 85b834b885a72ab823cf9717aa0b49e6a3e7a19f
Author: Marek Szyprowski <m.szyprowski@samsung.com>
Date:   Tue Dec 4 15:01:01 2012 +0100

    regulators: add regulator_can_change_voltage() function
    
    Introduce a regulator_can_change_voltage() function for the subsytems or
    drivers which might check if applying voltage change is possible and use
    special workaround code when the driver is used with fixed regulators or
    regulators with disabled ability to change the voltage.
    
    Signed-off-by: Marek Szyprowski <m.szyprowski@samsung.com>
    Signed-off-by: Mark Brown <broonie@opensource.wolfsonmicro.com>

commit 159ec130465416b8ea923d715d08979134623827
Author: Kent Overstreet <koverstreet@google.com>
Date:   Thu Sep 6 15:34:58 2012 -0700

    block: Add bio_reset()
    
    Reusing bios is something that's been highly frowned upon in the past,
    but driver code keeps doing it anyways. If it's going to happen anyways,
    we should provide a generic method.
    
    This'll help with getting rid of bi_destructor - drivers/block/pktcdvd.c
    was open coding it, by doing a bio_init() and resetting bi_destructor.
    
    This required reordering struct bio, but the block layer is not yet
    nearly fast enough for any cacheline effects to matter here.
    
    v5: Add a define BIO_RESET_BITS, to be very explicit about what parts of
    bio->bi_flags are saved.
    v6: Further commenting verbosity, per Tejun
    v9: Add a function comment
    
    Signed-off-by: Kent Overstreet <koverstreet@google.com>
    CC: Jens Axboe <axboe@kernel.dk>
    Acked-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>
    
    Conflicts:
    	include/linux/blk_types.h

commit a1a039b2508f3544c95c7ba62d45b2b0bd01eb49
Author: Tejun Heo <tj@kernel.org>
Date:   Mon Mar 19 15:10:59 2012 -0700

    block: remove ioc_*_changed()
    
    After the previous patch to cfq, there's no ioc_get_changed() user
    left.  This patch yanks out ioc_{ioprio|cgroup|get}_changed() and all
    related stuff.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Vivek Goyal <vgoyal@redhat.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>
    
    Conflicts:
    	block/blk-cgroup.c

commit 1beb10de927990270d59882d00a67d515dd7e646
Author: Tejun Heo <tj@kernel.org>
Date:   Mon Mar 5 13:15:27 2012 -0800

    block: implement bio_associate_current()
    
    IO scheduling and cgroup are tied to the issuing task via io_context
    and cgroup of %current.  Unfortunately, there are cases where IOs need
    to be routed via a different task which makes scheduling and cgroup
    limit enforcement applied completely incorrectly.
    
    For example, all bios delayed by blk-throttle end up being issued by a
    delayed work item and get assigned the io_context of the worker task
    which happens to serve the work item and dumped to the default block
    cgroup.  This is double confusing as bios which aren't delayed end up
    in the correct cgroup and makes using blk-throttle and cfq propio
    together impossible.
    
    Any code which punts IO issuing to another task is affected which is
    getting more and more common (e.g. btrfs).  As both io_context and
    cgroup are firmly tied to task including userland visible APIs to
    manipulate them, it makes a lot of sense to match up tasks to bios.
    
    This patch implements bio_associate_current() which associates the
    specified bio with %current.  The bio will record the associated ioc
    and blkcg at that point and block layer will use the recorded ones
    regardless of which task actually ends up issuing the bio.  bio
    release puts the associated ioc and blkcg.
    
    It grabs and remembers ioc and blkcg instead of the task itself
    because task may already be dead by the time the bio is issued making
    ioc and blkcg inaccessible and those are all block layer cares about.
    
    elevator_set_req_fn() is updated such that the bio elvdata is being
    allocated for is available to the elevator.
    
    This doesn't update block cgroup policies yet.  Further patches will
    implement the support.
    
    -v2: #ifdef CONFIG_BLK_CGROUP added around bio->bi_ioc dereference in
         rq_ioc() to fix build breakage.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Vivek Goyal <vgoyal@redhat.com>
    Cc: Kent Overstreet <koverstreet@google.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>
    
    Conflicts:
    	block/blk-core.c

commit 4a2943d2529e4277b2ae77891de9ca9cf753ec27
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Fri Sep 11 14:51:36 2015 +0200

    cpufreq: update to latest develop version thanks to @ZaneZam

commit 587a9de21648a9c4800ea3b26579f19e94ec8214
Author: Tejun Heo <tj@kernel.org>
Date:   Sun Apr 1 12:09:55 2012 -0700

    cgroup: convert all non-memcg controllers to the new cftype interface
    
    Convert debug, freezer, cpuset, cpu_cgroup, cpuacct, net_prio, blkio,
    net_cls and device controllers to use the new cftype based interface.
    Termination entry is added to cftype arrays and populate callbacks are
    replaced with cgroup_subsys->base_cftypes initializations.
    
    This is functionally identical transformation.  There shouldn't be any
    visible behavior change.
    
    memcg is rather special and will be converted separately.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Acked-by: Li Zefan <lizf@cn.fujitsu.com>
    Cc: Paul Menage <paul@paulmenage.org>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Vivek Goyal <vgoyal@redhat.com>
    
    Conflicts:
    	block/blk-cgroup.c

commit ce4455bcfff737cd784a6613c240b9976154fdaa
Author: Tejun Heo <tj@kernel.org>
Date:   Sun Apr 1 12:09:55 2012 -0700

    cgroup: relocate cftype and cgroup_subsys definitions in controllers
    
    blk-cgroup, netprio_cgroup, cls_cgroup and tcp_memcontrol
    unnecessarily define cftype array and cgroup_subsys structures at the
    top of the file, which is unconventional and necessiates forward
    declaration of methods.
    
    This patch relocates those below the definitions of the methods and
    removes the forward declarations.  Note that forward declaration of
    tcp_files[] is added in tcp_memcontrol.c for tcp_init_cgroup().  This
    will be removed soon by another patch.
    
    This patch doesn't introduce any functional change.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Acked-by: Li Zefan <lizf@cn.fujitsu.com>
    
    Conflicts:
    	block/blk-cgroup.c
    
    Conflicts:
    	block/blk-cgroup.c

commit 488b9de60d07baec2f03dc31a8cb4fb9b52f0a0b
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Fri Sep 11 14:43:58 2015 +0200

    cpufreq: update zzmoove governor thanks to @andip71

commit 6e45ca754892ab451a24e3f5b4c624b69d0ae604
Author: Lai Jiangshan <laijs@cn.fujitsu.com>
Date:   Wed Dec 12 13:51:21 2012 -0800

    mm: node_states: introduce N_MEMORY
    
    We have N_NORMAL_MEMORY for standing for the nodes that have normal memory
    with zone_type <= ZONE_NORMAL.
    
    And we have N_HIGH_MEMORY for standing for the nodes that have normal or
    high memory.
    
    But we don't have any word to stand for the nodes that have *any* memory.
    
    And we have N_CPU but without N_MEMORY.
    
    Current code reuse the N_HIGH_MEMORY for this purpose because any node
    which has memory must have high memory or normal memory currently.
    
    A)	But this reusing is bad for *readability*. Because the name
    	N_HIGH_MEMORY just stands for high or normal:
    
    A.example 1)
    	mem_cgroup_nr_lru_pages():
    		for_each_node_state(nid, N_HIGH_MEMORY)
    
    	The user will be confused(why this function just counts for high or
    	normal memory node? does it counts for ZONE_MOVABLE's lru pages?)
    	until someone else tell them N_HIGH_MEMORY is reused to stand for
    	nodes that have any memory.
    
    A.cont) If we introduce N_MEMORY, we can reduce this confusing
    	AND make the code more clearly:
    
    A.example 2) mm/page_cgroup.c use N_HIGH_MEMORY twice:
    
    	One is in page_cgroup_init(void):
    		for_each_node_state(nid, N_HIGH_MEMORY) {
    
    	It means if the node have memory, we will allocate page_cgroup map for
    	the node. We should use N_MEMORY instead here to gaim more clearly.
    
    	The second using is in alloc_page_cgroup():
    		if (node_state(nid, N_HIGH_MEMORY))
    			addr = vzalloc_node(size, nid);
    
    	It means if the node has high or normal memory that can be allocated
    	from kernel. We should keep N_HIGH_MEMORY here, and it will be better
    	if the "any memory" semantic of N_HIGH_MEMORY is removed.
    
    B)	This reusing is out-dated if we introduce MOVABLE-dedicated node.
    	The MOVABLE-dedicated node should not appear in
    	node_stats[N_HIGH_MEMORY] nor node_stats[N_NORMAL_MEMORY],
    	because MOVABLE-dedicated node has no high or normal memory.
    
    	In x86_64, N_HIGH_MEMORY=N_NORMAL_MEMORY, if a MOVABLE-dedicated node
    	is in node_stats[N_HIGH_MEMORY], it is also means it is in
    	node_stats[N_NORMAL_MEMORY], it causes SLUB wrong.
    
    	The slub uses
    		for_each_node_state(nid, N_NORMAL_MEMORY)
    	and creates kmem_cache_node for MOVABLE-dedicated node and cause problem.
    
    In one word, we need a N_MEMORY.  We just intrude it as an alias to
    N_HIGH_MEMORY and fix all im-proper usages of N_HIGH_MEMORY in late
    patches.
    
    Signed-off-by: Lai Jiangshan <laijs@cn.fujitsu.com>
    Acked-by: Christoph Lameter <cl@linux.com>
    Acked-by: Hillf Danton <dhillf@gmail.com>
    Signed-off-by: Wen Congyang <wency@cn.fujitsu.com>
    Cc: Lin Feng <linfeng@cn.fujitsu.com>
    Cc: David Rientjes <rientjes@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit dcba69a4a2cd63d689e703e963a3421bc72b83f3
Author: Lai Jiangshan <laijs@cn.fujitsu.com>
Date:   Wed Dec 12 13:51:24 2012 -0800

    cpuset: use N_MEMORY instead N_HIGH_MEMORY
    
    N_HIGH_MEMORY stands for the nodes that has normal or high memory.
    N_MEMORY stands for the nodes that has any memory.
    
    The code here need to handle with the nodes which have memory, we should
    use N_MEMORY instead.
    
    Signed-off-by: Lai Jiangshan <laijs@cn.fujitsu.com>
    Acked-by: Hillf Danton <dhillf@gmail.com>
    Signed-off-by: Wen Congyang <wency@cn.fujitsu.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Lin Feng <linfeng@cn.fujitsu.com>
    Cc: David Rientjes <rientjes@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    
    Conflicts:
    	kernel/cpuset.c

commit 5c97240214978fa2095b1c843edc509e76d9ef22
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun Jun 7 22:20:15 2015 +0200

    main: fix the fault from my previous updates

commit 639755dc35cc063248f716ad6b1fe94b82c96244
Author: Steven Rostedt <rostedt@goodmis.org>
Date:   Wed May 1 13:35:51 2013 -0400

    init: Do not warn on non-zero initcall return
    
    Commit f91eb62f71b3 ("init: scream bloody murder if interrupts are
    enabled too early") added three new warnings.  The first two seemed
    reasonable, but the third included a warning when an initcall returned
    non-zero.  Although, the third WARN() does include an imbalanced preempt
    disabled, or irqs disable, it shouldn't warn if it only had an initcall
    that just returns non-zero.
    
    In fact, according to Linus, it shouldn't print at all.  As it only
    prints with initcall_debug set, and that already shows enough
    information to fix things.
    
    Link: http://lkml.kernel.org/r/CA+55aFzaBC5SFi7=F2mfm+KWY5qTsBmOqgbbs8E+LUS8JK-sBg@mail.gmail.com
    
    Suggested-by: Linus Torvalds <torvalds@linux-foundation.org>
    Reported-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit ad7fd0ff5473b8070d1f7b9dd50818215584b53a
Author: Andrew Morton <akpm@linux-foundation.org>
Date:   Mon Apr 29 16:18:20 2013 -0700

    init/main.c: convert to pr_foo()
    
    Also enables cleanup of some 80-col trickery.
    
    Cc: Richard Weinberger <richard@nod.at>
    Cc: Uwe Kleine-Knig <u.kleine-koenig@pengutronix.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    
    Conflicts:
    	init/main.c

commit 3435005b00143cb80a46df79d79fadec11732247
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun Jun 7 22:04:36 2015 +0200

    init: raise log level
    
    If the kernel was booted with the "quiet" boot option we have currently no
    chance to see why an initrd fails.  Change KERN_WARNING to KERN_ERR to see
    what is going on.
    
    Signed-off-by: Richard Weinberger <richard@nod.at>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Jim Cromie <jim.cromie@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit 427a6745a4fdcc39706ec11860a507d88f183f06
Author: Steven Rostedt <rostedt@goodmis.org>
Date:   Mon Apr 29 16:18:18 2013 -0700

    init: scream bloody murder if interrupts are enabled too early
    
    As I was testing a lot of my code recently, and having several
    "successes", I accidentally noticed in the dmesg this little line:
    
      start_kernel(): bug: interrupts were enabled *very* early, fixing it
    
    Sure enough, one of my patches two commits ago enabled interrupts early.
    The sad part here is that I never noticed it, and I ran several tests with
    ktest too, and ktest did not notice this line.
    
    What ktest looks for (and so does many other automated testing scripts) is
    a back trace produced by a WARN_ON() or BUG().  As a back trace was never
    produced, my buggy patch could have slipped into linux-next, or even
    worse, mainline.
    
    Adding a WARN(!irqs_disabled()) makes this bug a little more obvious:
    
      PID hash table entries: 4096 (order: 3, 32768 bytes)
      __ex_table already sorted, skipping sort
      Checking aperture...
      No AGP bridge found
      Calgary: detecting Calgary via BIOS EBDA area
      Calgary: Unable to locate Rio Grande table in EBDA - bailing!
      Memory: 2003252k/2054848k available (4857k kernel code, 460k absent, 51136k reserved, 6210k data, 1096k init)
      ------------[ cut here ]------------
      WARNING: at /home/rostedt/work/git/linux-trace.git/init/main.c:543 start_kernel+0x21e/0x415()
      Hardware name: To Be Filled By O.E.M.
      Interrupts were enabled *very* early, fixing it
      Modules linked in:
      Pid: 0, comm: swapper/0 Not tainted 3.8.0-test+ #286
      Call Trace:
        warn_slowpath_common+0x83/0x9b
        warn_slowpath_fmt+0x46/0x48
        start_kernel+0x21e/0x415
        x86_64_start_reservations+0x10e/0x112
        x86_64_start_kernel+0x102/0x111
      ---[ end trace 007d8b0491b4f5d8 ]---
      Preemptible hierarchical RCU implementation.
       RCU restricting CPUs from NR_CPUS=8 to nr_cpu_ids=4.
      NR_IRQS:4352 nr_irqs:712 16
      Console: colour VGA+ 80x25
      console [ttyS0] enabled, bootconsole disabled
    
    Do you see it?
    
    The original version of this patch just slapped a WARN_ON() in there and
    kept the printk().  Ard van Breemen suggested using the WARN() interface,
    which makes the code a bit cleaner.
    
    Also, while examining other warnings in init/main.c, I found two other
    locations that deserve a bloody murder scream if their conditions are hit,
    and updated them accordingly.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>
    Cc: Ard van Breemen <ard@telegraafnet.nl>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit b3d57adaf91c371a58dd8e2ef96a9c7b17de21e7
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Mar 21 22:49:34 2013 +0100

    idle: Provide a generic entry point for the idle code
    
    For now this calls cpu_idle(), but in the long run we want to move the
    cpu bringup code to the core and therefor we add a state argument.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Paul McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Reviewed-by: Cc: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Cc: Magnus Damm <magnus.damm@gmail.com>
    Link: http://lkml.kernel.org/r/20130321215233.583190032@linutronix.de
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

commit 13f13d6885ae8dac96616841966afa18a7f70f33
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Mar 5 15:14:05 2013 +0100

    tick: Call tick_init late
    
    To convert the clockevents code to cpumask_var_t we need to move the
    init call after the allocator setup.
    
    Clockevents are earliest registered from time_init() as they need
    interrupts being set up, so this is safe.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/20130306111537.304379448@linutronix.de
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    
    Conflicts:
    	init/main.c

commit 66508ae571665c654c759cae5ad3a525293e579d
Author: Vineet Gupta <Vineet.Gupta1@synopsys.com>
Date:   Fri Dec 21 12:25:44 2012 +0530

    Ensure that kernel_init_freeable() is not inlined into non __init code
    
    Commit d6b2123802d "make sure that we always have a return path from
    kernel_execve()" reshuffled kernel_init()/init_post() to ensure that
    kernel_execve() has a caller to return to.
    
    It removed __init annotation for kernel_init() and introduced/calls a
    new routine kernel_init_freeable(). Latter however is inlined by any
    reasonable compiler (ARC gcc 4.4 in this case), causing slight code
    bloat.
    
    This patch forces kernel_init_freeable() as noinline reducing the .text
    
    bloat-o-meter vmlinux vmlinux_new
    add/remove: 1/0 grow/shrink: 0/1 up/down: 374/-334 (40)
    function                        old     new   delta
    kernel_init_freeable              -     374    +374 (.init.text)
    kernel_init                     628     294    -334 (.text)
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

commit 1070f5c1b059936902989915b5b1cf8a2ac2dc8d
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Wed Oct 10 19:57:26 2012 -0400

    make sure that we always have a return path from kernel_execve()
    
    The only place where kernel_execve() is called without a way to
    return to the caller of kernel_thread() callback is kernel_post().
    Reorganize kernel_init()/kernel_post() - instead of the former
    calling the latter in the end (and getting freed by it), have the
    latter *begin* with calling the former (and turn the latter into
    kernel_thread() callback, of course).
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>
    
    Conflicts:
    	init/main.c

commit 67eb0cb55434ba5c92bda7283c90e0f30b354786
Author: Jiang Liu <jiang.liu@huawei.com>
Date:   Tue Jul 31 16:43:28 2012 -0700

    mm/hotplug: correctly setup fallback zonelists when creating new pgdat
    
    When hotadd_new_pgdat() is called to create new pgdat for a new node, a
    fallback zonelist should be created for the new node.  There's code to try
    to achieve that in hotadd_new_pgdat() as below:
    
    	/*
    	 * The node we allocated has no zone fallback lists. For avoiding
    	 * to access not-initialized zonelist, build here.
    	 */
    	mutex_lock(&zonelists_mutex);
    	build_all_zonelists(pgdat, NULL);
    	mutex_unlock(&zonelists_mutex);
    
    But it doesn't work as expected.  When hotadd_new_pgdat() is called, the
    new node is still in offline state because node_set_online(nid) hasn't
    been called yet.  And build_all_zonelists() only builds zonelists for
    online nodes as:
    
            for_each_online_node(nid) {
                    pg_data_t *pgdat = NODE_DATA(nid);
    
                    build_zonelists(pgdat);
                    build_zonelist_cache(pgdat);
            }
    
    Though we hope to create zonelist for the new pgdat, but it doesn't.  So
    add a new parameter "pgdat" the build_all_zonelists() to build pgdat for
    the new pgdat too.
    
    Signed-off-by: Jiang Liu <liuj97@gmail.com>
    Signed-off-by: Xishi Qiu <qiuxishi@huawei.com>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Michal Hocko <mhocko@suse.cz>
    Cc: Minchan Kim <minchan@kernel.org>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Yinghai Lu <yinghai@kernel.org>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Keping Chen <chenkeping@huawei.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    
    Conflicts:
    	mm/memory_hotplug.c

commit 37c592bd152f0ad0e7cb3e72b424032b69b6fa4f
Author: Jim Cromie <jim.cromie@gmail.com>
Date:   Fri Apr 27 14:30:34 2012 -0600

    params: add 3rd arg to option handler callback signature
    
    Add a 3rd arg, named "doing", to unknown-options callbacks invoked
    from parse_args(). The arg is passed as:
    
      "Booting kernel" from start_kernel(),
      initcall_level_names[i] from do_initcall_level(),
      mod->name from load_module(), via parse_args(), parse_one()
    
    parse_args() already has the "name" parameter, which is renamed to
    "doing" to better reflect current uses 1,2 above.  parse_args() passes
    it to an altered parse_one(), which now passes it down into the
    unknown option handler callbacks.
    
    The mod->name will be needed to handle dyndbg for loadable modules,
    since params passed by modprobe are not qualified (they do not have a
    "$modname." prefix), and by the time the unknown-param callback is
    called, the module name is not otherwise available.
    
    Minor tweaks:
    
    Add param-name to parse_one's pr_debug(), current message doesnt
    identify the param being handled, add it.
    
    Add a pr_info to print current level and level_name of the initcall,
    and number of registered initcalls at that level.  This adds 7 lines
    to dmesg output, like:
    
       initlevel:6=device, 172 registered initcalls
    
    Drop "parameters" from initcall_level_names[], its unhelpful in the
    pr_info() added above.  This array is passed into parse_args() by
    do_initcall_level().
    
    CC: Rusty Russell <rusty@rustcorp.com.au>
    Signed-off-by: Jim Cromie <jim.cromie@gmail.com>
    Acked-by: Jason Baron <jbaron@redhat.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    
    Conflicts:
    	init/main.c

commit 06fff33fcb468f827011d8ef6d25b08776a71601
Author: Lai Jiangshan <laijs@cn.fujitsu.com>
Date:   Wed Dec 12 13:51:40 2012 -0800

    init: use N_MEMORY instead N_HIGH_MEMORY
    
    N_HIGH_MEMORY stands for the nodes that has normal or high memory.
    N_MEMORY stands for the nodes that has any memory.
    
    The code here need to handle with the nodes which have memory, we should
    use N_MEMORY instead.
    
    Signed-off-by: Lai Jiangshan <laijs@cn.fujitsu.com>
    Signed-off-by: Wen Congyang <wency@cn.fujitsu.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Hillf Danton <dhillf@gmail.com>
    Cc: Lin Feng <linfeng@cn.fujitsu.com>
    Cc: David Rientjes <rientjes@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit 645fececd60d3cb05c3abc3f096e84652703c17c
Author: Tejun Heo <tj@kernel.org>
Date:   Sun Apr 1 12:09:56 2012 -0700

    cgroup: implement cgroup_rm_cftypes()
    
    Implement cgroup_rm_cftypes() which removes an array of cftypes from a
    subsystem.  It can be called whether the target subsys is attached or
    not.  cgroup core will remove the specified file from all existing
    cgroups.
    
    This will be used to improve sub-subsys modularity and will be helpful
    for unified hierarchy.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Acked-by: Li Zefan <lizf@cn.fujitsu.com>

commit 9109b213acf4a75c524fd50b1f30f9f3aaac5c4a
Author: Tejun Heo <tj@kernel.org>
Date:   Sun Apr 1 12:09:56 2012 -0700

    cgroup: introduce struct cfent
    
    This patch adds cfent (cgroup file entry) which is the association
    between a cgroup and a file.  This is in-cgroup representation of
    files under a cgroup directory.  This simplifies walking walking
    cgroup files and thus cgroup_clear_directory(), which is now
    implemented in two parts - cgroup_rm_file() and a loop around it.
    
    cgroup_rm_file() will be used to implement cftype removal and cfent is
    scheduled to serve cgroup specific per-file data (e.g. for sysfs-like
    "sever" semantics).
    
    v2: - cfe was freed from cgroup_rm_file() which led to use-after-free
          if the file had openers at the time of removal.  Moved to
          cgroup_diput().
    
        - cgroup_clear_directory() triggered WARN_ON_ONCE() if d_subdirs
          wasn't empty after removing all files.  This triggered
          spuriously if some files were open during directory clearing.
          Removed.
    
    v3: - In cgroup_diput(), WARN_ONCE(!list_empty(&cfe->node)) could be
          spuriously triggered for root cgroups because they don't go
          through cgroup_clear_directory() on unmount.  Don't trigger WARN
          for root cgroups.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Acked-by: Li Zefan <lizf@cn.fujitsu.com>
    Cc: Glauber Costa <glommer@parallels.com>

commit 2945793fdc4e9112a305e6f83ef0c280d8d5752e
Author: Tejun Heo <tj@kernel.org>
Date:   Sun Apr 1 12:09:55 2012 -0700

    cgroup: relocate __d_cgrp() and __d_cft()
    
    Move the two macros upwards as they'll be used earlier in the file.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Acked-by: Li Zefan <lizf@cn.fujitsu.com>

commit 205fcf59a913150f757cf00d0c1f2c2015bc9be2
Author: Tejun Heo <tj@kernel.org>
Date:   Sun Apr 1 12:09:55 2012 -0700

    cgroup: remove cgroup_add_file[s]()
    
    No controller is using cgroup_add_files[s]().  Unexport them, and
    convert cgroup_add_files() to handle NULL entry terminated array
    instead of taking count explicitly and continue creation on failure
    for internal use.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Acked-by: Li Zefan <lizf@cn.fujitsu.com>

commit 680b32e77987d1bbe8740da4131545f71ba0fcf0
Author: Tejun Heo <tj@kernel.org>
Date:   Sun Apr 1 12:09:55 2012 -0700

    cgroup: merge cft_release_agent cftype array into the base files array
    
    Now that cftype can express whether a file should only be on root,
    cft_release_agent can be merged into the base files cftypes array.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Acked-by: Li Zefan <lizf@cn.fujitsu.com>

commit b58e2a78a9531427bac4e81922437b9640aec60d
Author: Tejun Heo <tj@kernel.org>
Date:   Sun Apr 1 12:09:54 2012 -0700

    cgroup: move cgroup_clear_directory() call out of cgroup_populate_dir()
    
    cgroup_populate_dir() currently clears all files and then repopulate
    the directory; however, the clearing part is only useful when it's
    called from cgroup_remount().  Relocate the invocation to
    cgroup_remount().
    
    This is to prepare for further cgroup file handling updates.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Acked-by: Li Zefan <lizf@cn.fujitsu.com>

commit 1a837d635bbcce7985cb988df265bb7ae42b4946
Author: Tejun Heo <tj@kernel.org>
Date:   Sun Apr 1 12:09:54 2012 -0700

    cgroup: deprecate remount option changes
    
    This patch marks the following features for deprecation.
    
    * Rebinding subsys by remount: Never reached useful state - only works
      on empty hierarchies.
    
    * release_agent update by remount: release_agent itself will be
      replaced with conventional fsnotify notification.
    
    v2: Lennart pointed out that "name=" is necessary for mounts w/o any
        controller attached.  Drop "name=" deprecation.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Acked-by: Li Zefan <lizf@cn.fujitsu.com>
    Cc: Lennart Poettering <mzxreary@0pointer.de>

commit 77d1387719e4c14137b2bea3f2b2b561f1e1cb1d
Author: Tejun Heo <tj@kernel.org>
Date:   Sun Apr 1 12:09:55 2012 -0700

    cgroup: implement cgroup_add_cftypes() and friends
    
    Currently, cgroup directories are populated by subsys->populate()
    callback explicitly creating files on each cgroup creation.  This
    level of flexibility isn't needed or desirable.  It provides largely
    unused flexibility which call for abuses while severely limiting what
    the core layer can do through the lack of structure and conventions.
    
    Per each cgroup file type, the only distinction that cgroup users is
    making is whether a cgroup is root or not, which can easily be
    expressed with flags.
    
    This patch introduces cgroup_add_cftypes().  These deal with cftypes
    instead of individual files - controllers indicate that certain types
    of files exist for certain subsystem.  Newly added CFTYPE_*_ON_ROOT
    flags indicate whether a cftype should be excluded or created only on
    the root cgroup.
    
    cgroup_add_cftypes() can be called any time whether the target
    subsystem is currently attached or not.  cgroup core will create files
    on the existing cgroups as necessary.
    
    Also, cgroup_subsys->base_cftypes is added to ease registration of the
    base files for the subsystem.  If non-NULL on subsys init, the cftypes
    pointed to by ->base_cftypes are automatically registered on subsys
    init / load.
    
    Further patches will convert the existing users and remove the file
    based interface.  Note that this interface allows dynamic addition of
    files to an active controller.  This will be used for sub-controller
    modularity and unified hierarchy in the longer term.
    
    This patch implements the new mechanism but doesn't apply it to any
    user.
    
    v2: replaced DECLARE_CGROUP_CFTYPES[_COND]() with
        cgroup_subsys->base_cftypes, which works better for cgroup_subsys
        which is loaded as module.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Acked-by: Li Zefan <lizf@cn.fujitsu.com>

commit 8b73b7f6444d76c6c377802b3fc3b26368947c96
Author: Tejun Heo <tj@kernel.org>
Date:   Sun Apr 1 12:09:54 2012 -0700

    cgroup: build list of all cgroups under a given cgroupfs_root
    
    Build a list of all cgroups anchored at cgroupfs_root->allcg_list and
    going through cgroup->allcg_node.  The list is protected by
    cgroup_mutex and will be used to improve cgroup file handling.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Acked-by: Li Zefan <lizf@cn.fujitsu.com>

commit 8275df2d633dd79c6079ff5727fcc9f2fe65feea
Author: Mikhail Gruzdev <michail.gruzdev@gmail.com>
Date:   Thu Feb 21 16:43:10 2013 -0800

    printk: add pr_devel_once and pr_devel_ratelimited
    
    Standardize pr_devel logging macros family by adding pr_devel_once and
    pr_devel_ratelimited.
    
    Signed-off-by: Mikhail Gruzdev <michail.gruzdev@gmail.com>
    Acked-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit 92dab772bcdc2b0cb8430fef78337ee3c23b3944
Author: Joe Perches <joe@perches.com>
Date:   Mon Jul 30 14:40:11 2012 -0700

    printk: add kern_levels.h to make KERN_<LEVEL> available for asm use
    
    Separate the printk.h file into 2 pieces so the definitions can be used in
    asm files.
    
    Signed-off-by: Joe Perches <joe@perches.com>
    Cc: Kay Sievers <kay.sievers@vrfy.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit b5a23f375374a3e385c0e419c0338a9cb9f96664
Author: Tejun Heo <tj@kernel.org>
Date:   Mon Mar 5 13:14:58 2012 -0800

    block: implement blk_queue_bypass_start/end()
    
    Rename and extend elv_queisce_start/end() to
    blk_queue_bypass_start/end() which are exported and supports nesting
    via @q->bypass_depth.  Also add blk_queue_bypass() to test bypass
    state.
    
    This will be further extended and used for blkio_group management.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Vivek Goyal <vgoyal@redhat.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

commit 941ed9bacae86d0e39f6caec224976d4b42a5163
Author: Tejun Heo <tj@kernel.org>
Date:   Mon Mar 5 13:14:57 2012 -0800

    elevator: make elevator_init_fn() return 0/-errno
    
    elevator_ops->elevator_init_fn() has a weird return value.  It returns
    a void * which the caller should assign to q->elevator->elevator_data
    and %NULL return denotes init failure.
    
    Update such that it returns integer 0/-errno and sets elevator_data
    directly as necessary.
    
    This makes the interface more conventional and eases further cleanup.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Vivek Goyal <vgoyal@redhat.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

commit b8cac13c76be2cccc76b2a6146769769f72f55c8
Author: Tejun Heo <tj@kernel.org>
Date:   Mon Mar 5 13:14:56 2012 -0800

    elevator: clear auxiliary data earlier during elevator switch
    
    Elevator switch tries hard to keep as much as context until new
    elevator is ready so that it can revert to the original state if
    initializing the new elevator fails for some reason.  Unfortunately,
    with more auxiliary contexts to manage, this makes elevator init and
    exit paths too complex and fragile.
    
    This patch makes elevator_switch() unregister the current elevator and
    flush icq's before start initializing the new one.  As we still keep
    the old elevator itself, the only difference is that we lose icq's on
    rare occassions of switching failure, which isn't critical at all.
    
    Note that this makes explicit elevator parameter to
    elevator_init_queue() and __elv_register_queue() unnecessary as they
    always can use the current elevator.
    
    This patch enables block cgroup cleanups.
    
    -v2: blk_add_trace_msg() prints elevator name from @new_e instead of
         @e->type as the local variable no longer exists.  This caused
         build failure on CONFIG_BLK_DEV_IO_TRACE.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Vivek Goyal <vgoyal@redhat.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

commit 67122de75c5df509355a63dc7c6c8075f1f0149e
Author: Tejun Heo <tj@kernel.org>
Date:   Mon Mar 5 13:14:55 2012 -0800

    cfq: don't register propio policy if !CONFIG_CFQ_GROUP_IOSCHED
    
    cfq has been registering zeroed blkio_poilcy_cfq if CFQ_GROUP_IOSCHED
    is disabled.  This fortunately doesn't collide with blk-throtl as
    BLKIO_POLICY_PROP is zero but is unnecessary and risky.  Just don't
    register it if not enabled.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Acked-by: Vivek Goyal <vgoyal@redhat.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

commit 8c297e900e979c7e617e686f775929f74273775c
Author: andip71 <andreasp@gmx.de>
Date:   Mon Feb 23 07:17:41 2015 +0100

    fs/dyn_sync_cntrl: dynamic sync control 2.0 for msm8974
    
    While screen is on, file sync is suspended.
    When screen is off, a file sync is called to flush all outstanding writes
    and restore file sync operation as normal.
    
    By andip71 (aka Lord Boeffla)
    
    Credits for original implementation and concept to Faux123

commit b9e8a9f271c7e330cd1dcbc07d34ba316d8a5669
Author: Tejun Heo <tj@kernel.org>
Date:   Mon Mar 5 13:14:54 2012 -0800

    blkcg: make CONFIG_BLK_CGROUP bool
    
    Block cgroup core can be built as module; however, it isn't too useful
    as blk-throttle can only be built-in and cfq-iosched is usually the
    default built-in scheduler.  Scheduled blkcg cleanup requires calling
    into blkcg from block core.  To simplify that, disallow building blkcg
    as module by making CONFIG_BLK_CGROUP bool.
    
    If building blkcg core as module really matters, which I doubt, we can
    revisit it after blkcg API cleanup.
    
    -v2: Vivek pointed out that IOSCHED_CFQ was incorrectly updated to
         depend on BLK_CGROUP.  Fixed.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Vivek Goyal <vgoyal@redhat.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

commit 2f746d81c9a047d9d39b77a5191c378c473e3fc6
Author: Dipen Parmar <dipenp@codeaurora.org>
Date:   Tue Oct 8 17:12:55 2013 +0530

    msm: sps: enable 4K resource group support for SPS
    
    Add support for BAM register offsets which are updated
    to 4K resource groups.
    
    Change-Id: I3ed07d9d3a58ba81430b4c357976454972b796b5
    Signed-off-by: Dipen Parmar <dipenp@codeaurora.org>

commit 4b374b4f3242c0dad60b602df66ba40fea5ffecf
Author: Yan He <yanhe@codeaurora.org>
Date:   Thu Jan 23 00:30:14 2014 -0800

    msm: sps: add handling of late EOT
    
    The EOT signal can come later after the descriptor has been closed
    by BAM HW. Add the support to handle this late EOT at the pipe
    level.
    
    Change-Id: Ibd2e467cf77e792888bb3748e8b31924911e2651
    Signed-off-by: Yan He <yanhe@codeaurora.org>

commit 7c42eb06cf80c13881a62b64cf78bba9777808cb
Author: Yan He <yanhe@codeaurora.org>
Date:   Wed Jan 22 16:48:57 2014 -0800

    msm: sps: use a new flag for disconnecting pipe
    
    Use a new flag to indicate if BAM client has decided to disconnect
    a pipe so that we can avoid BAM IRQ handling for a disconnected
    pipe.
    
    Change-Id: I1d1e467cf7366f0b00b32674117d381432611e26
    Signed-off-by: Yan He <yanhe@codeaurora.org>

commit e6f216395193ae61b49aa4a48c5f873dda86a3b0
Author: Dipen Parmar <dipenp@codeaurora.org>
Date:   Sat Dec 28 21:05:20 2013 +0530

    msm: sps: Enable the clocks when BAMDMA available
    
    The dfab and bamdma clocks are used only by bamdma.
    Move the clocks api under bamdma option.
    
    Update local device node data with platform device
    node data from probe to avoid clock failures.
    
    Change-Id: I97bd107eed400a989f0638c958f6361330bc6091
    Signed-off-by: Dipen Parmar <dipenp@codeaurora.org>

commit 2ae343228b8275d594d64d01860e72c702a03ade
Author: Dipen Parmar <dipenp@codeaurora.org>
Date:   Wed Dec 11 17:22:44 2013 +0530

    msm: sps: Add integer wrap check for debugfs buffer
    
    When converting debugfs buffer size to kilobytes value
    there is a possible integer overflow and it may result
    into negative value.
    
    Fix the issue by adding integer wrap check for buffer
    size just before conversion.
    
    CRs-fixed: 564177
    Change-Id: If4615561ea6a1c58e8be8c1b72f7881c068d8520
    Signed-off-by: Dipen Parmar <dipenp@codeaurora.org>

commit 43416e138c2a3a12fb9252fa426b911a493af92f
Author: Dipen Parmar <dipenp@codeaurora.org>
Date:   Thu Nov 28 00:52:42 2013 +0530

    msm: sps: Fix error case handling in probe function
    
    Fix possible NULL pointer dereference in the probe
    function by adding proper error case handling for
    clk errors.
    
    CRs-fixed: 583991
    Change-Id: Ief54cc28487e5559a893fa2f4c57148e76bd1acb
    Signed-off-by: Dipen Parmar <dipenp@codeaurora.org>

commit f99b3ed788e80795adbe941ff788594f89ca8ac1
Author: Dipen Parmar <dipenp@codeaurora.org>
Date:   Mon Oct 21 20:10:03 2013 +0530

    msm: sps: Add probe deferral support in sps driver
    
    Add probe deferral support in sps driver.
    
    Change-Id: If65d2ebbcdece256fdfa000097ac25874c9a5000
    Signed-off-by: Dipen Parmar <dipenp@codeaurora.org>

commit 5168f84f080fe2a9924fffdfac38d871352ccb65
Author: Yan He <yanhe@codeaurora.org>
Date:   Tue Sep 3 15:20:33 2013 -0700

    msm: sps: update the checking of free slots
    
    There is a new BAM use case where the client driver does not use
    either interrupt or polling for a pipe. Thus, add a new way to
    check the free slots in descriptor FIFO for this case.
    
    Change-Id: I133b938718d0c3654309e65ac0670fe1725a937f
    Signed-off-by: Yan He <yanhe@codeaurora.org>

commit 9c88e4aad6b4d8d01c9a6f457216b93af60a7121
Author: Yan He <yanhe@codeaurora.org>
Date:   Fri Jul 26 09:51:54 2013 -0700

    msm: sps: add OCIMEM for pipe memory
    
    BAM uses pipe memory for BAM-to-BAM connnections. OCIMEM is one of
    memory types for pipe memory. Add the support for OCIMEM in this
    change.
    
    Change-Id: Ifc6fe537060d2c4120a03222cb419f463c7b4ab0
    Signed-off-by: Yan He <yanhe@codeaurora.org>

commit 4d005cb9ee87b844b294a0c98593c5b0e8fdf45c
Author: Yan He <yanhe@codeaurora.org>
Date:   Thu Jul 18 10:28:23 2013 -0700

    msm: sps: free the interrupt for satellite mode
    
    When BAM is registered in satellite mode on apps side, we do not
    need to disable BAM HW when apps side does not use that BAM any
    more. But we still should free the interrupt. This is addressed
    in this change.
    
    Change-Id: I955ea1a341036efa82f0ecc0f45cad5ec41651ae
    Signed-off-by: Yan He <yanhe@codeaurora.org>

commit 4c0ebbe6189227572aad54c2c7b7bc55055ea50a
Author: Dipen Parmar <dipenp@codeaurora.org>
Date:   Tue Jul 9 16:17:51 2013 +0530

    msm: sps: enable SPS for APQ8084
    
    Add the support for new BAM HW features and LPAE.
    
    1. Replace data types of physical addresses of descriptor
       and data with phys_addr_t.
    2. Replace sps_transfer_one() function u32 addr argument
       to phys_addr_t addr.
    3. Add new MSB registers address support for LPAE.
    4. Add support for two new PIPE level ERROR IRQs which
       can be enable or disable by client.
    5. Add support to read the BAM PIPEs attributed to
       specific EE.
    6. Add the option for BAM Clients to specify whether
       enable or disable some HW features of their BAMs.
    
    Change-Id: If41e3d5e7e8333d7076d924dbae06caa126a1694
    Signed-off-by: Dipen Parmar <dipenp@codeaurora.org>

commit 95b837cba93246c9c55c2866ad7f2a6c1605edb4
Author: Yan He <yanhe@codeaurora.org>
Date:   Mon May 20 13:26:20 2013 -0700

    msm: sps: turn off BAM DMA clock after initial config
    
    For power saving, SPS driver will turn off BAM DMA clock after the
    initial configuration of BAM DMA. The clients of BAM DMA will vote
    for the clock when BAM DMA is used.
    
    Change-Id: I388a21cff3c619ef49705a064d7d5ef760fa9ef8
    Signed-off-by: Yan He <yanhe@codeaurora.org>

commit 242b3a3b46c255073dc0606e1ac5555b24798de6
Author: Yan He <yanhe@codeaurora.org>
Date:   Mon May 20 13:17:29 2013 -0700

    msm: sps: enable BAM DMA client to control BAM DMA clock
    
    For power saving reason, SPS driver will turn off the clock of
    BAM DMA after the initial configuration. A new API is provided
    to BAM DMA clients to vote for or relinquish BAM DMA clock when
    they start/stop using BAM DMA.
    
    Change-Id: I03f8a6cfa19c1722ca4e92e0785ca309fdd10ba2
    Signed-off-by: Yan He <yanhe@codeaurora.org>

commit 9efd00139042fae1b880c30f68eefaa40db4dd93
Author: Yan He <yanhe@codeaurora.org>
Date:   Fri May 17 13:14:25 2013 -0700

    msm: sps: remove the address checking for descriptors
    
    0x0 is a valid physical address for drivers to use. Thus, remove
    the checking of physical address for descriptors.
    
    Change-Id: I19b8e215a4538ec0a9b7def505f9577e204c7730
    Signed-off-by: Yan He <yanhe@codeaurora.org>

commit a290644cc9a798a8e154233e40e1a8f2a158f1d1
Author: Yan He <yanhe@codeaurora.org>
Date:   Tue Feb 26 17:42:19 2013 -0800

    msm: sps: improve BAM inactivity timer function
    
    1> Add the support for BAM global inactivity timer which is a new feature
    in B family.
    2> There is an update that changes the time unit of the inactivity timer
    counter from 0.1ms to 0.125ms. Thus, we adjust the parameter in SW driver
    for this update.
    3> Inactivity timer timeout interrupt will become a new type of BAM global
    interrupts, and it is a normal interrupt different from other BAM global
    error interrupts. Thus, change the log level for callback message from INFO
    to DEBUG.
    4> Change timer result argument of inactivity timer API to an optional one
    for client drivers.
    
    Change-Id: If223ef311f0393f2f2243f6b8a862fe760ff5214
    Signed-off-by: Yan He <yanhe@codeaurora.org>

commit cc8818139ab92176353623300de4b6d29bb4ffa6
Author: Yan He <yanhe@codeaurora.org>
Date:   Wed Apr 10 14:34:51 2013 -0700

    msm: sps: Add \n to the end of each log message
    
    All the log messages need a \n so that they can be posted
    properly in the kernel log output.
    
    Change-Id: I91c12351bb1c92841373aa539c93870619605258
    Signed-off-by: Yan He <yanhe@codeaurora.org>
    
    Conflicts:
    	drivers/platform/msm/sps/sps_bam.c

commit ff4b1dce9854b0e27e2e7b562112de95b831c22c
Author: Yan He <yanhe@codeaurora.org>
Date:   Tue Mar 5 12:04:59 2013 -0800

    msm: sps: add option to constrain logging for BAM HW enable event
    
    When a BAM device is enabled, we have log to show the presence of BAM HW.
    This logging works fine for most BAMs. However, A2 BAM may be enabled and
    disabled frequently when A2 power collapse is enabled. And thus we may have
    frequent logging output for A2 BAM. Add the logging option in BAM property
    structure so that we could constrain the logging output for A2 BAM.
    
    Change-Id: I14f32f02d32cdf9c6fd4d16f6517ee1305a3ae13
    Signed-off-by: Yan He <yanhe@codeaurora.org>

commit fe9573b62395662be3cee94f92915a5954c6b1a2
Author: Yan He <yanhe@codeaurora.org>
Date:   Mon Apr 8 13:00:56 2013 -0700

    msm: sps: adapt to clock API change
    
    With the change in clock driver, clk_get() can return -EPROBE_DEFER even
    when the clock is actually present in HW. Thus, we need to return
    -EPROBE_DEFER in SPS driver probe function if clk_get() fails.
    
    Change-Id: Ia1b9f18425ca5432f4a56419adf142f2674ce64a
    Signed-off-by: Yan He <yanhe@codeaurora.org>

commit bd58214b11ec2ad3b661b093095f6b2a90d1fdca
Author: Yan He <yanhe@codeaurora.org>
Date:   Thu Apr 4 09:33:23 2013 -0700

    msm: sps: provide an option not to disable a pipe during disconnection
    
    IPA HW will treat a producer pipe as a consumer pipe when that pipe is
    disable. Thus, provide an option for IPA driver not to disable the
    producer pipes when disconnect these pipes.
    
    Change-Id: I8ec53b63a2d2f2908153bb3a4d935fbb70f72a48
    Signed-off-by: Yan He <yanhe@codeaurora.org>

commit b61827e7cba36623576335f2d1e6cd0fe6988e4a
Author: Yan He <yanhe@codeaurora.org>
Date:   Tue Mar 12 16:15:13 2013 -0700

    msm: sps: improve debugging feature in SPS driver
    
    1> Remove the dump of TRUST registers since they are protected by TZ;
    2> Output IRQ registers only for the EE of apps side;
    3> Add the names of registers and some register fields in the dump to
       facilitate the analysis;
    4> Output the flags of descriptors;
    5> Output the BAM physical address when client drivers call SPS debugging
       API;
    6> Add tags to the beginning and end of each BAM dump in order to
       facilitate the post-processing of the dump;
    7> Add an option to save all the BAM logs only to an internal memory
       enabled by debugfs, which can reduce the system overhead to output the
       logs to Linux kernel log;
    8> Output the virtual address of a BAM together with the physical address
       of a BAM when a BAM HW is enabled, which helps the analysis of dumps
       which are based on virtual address of a BAM;
    9> Increase the size range of partially dumped descriptor FIFO.
    
    Change-Id: I98f53ddc92c1b7dfc1f9088778f82130dd4e337a
    Signed-off-by: Yan He <yanhe@codeaurora.org>

commit 9c93057dd6091a0eca23639026a4e5c1ece529f7
Author: Yan He <yanhe@codeaurora.org>
Date:   Mon Mar 11 18:02:36 2013 -0700

    msm: sps: remove world-writable permission of debugfs nodes
    
    There is no need for the sps debugfs nodes to be world-writable.
    Change them to be read-only for others.
    
    CRs-Fixed: 454805
    Change-Id: I8594a4a74ab63ce48fe3b7da3ef8852ff0623afe
    Signed-off-by: Yan He <yanhe@codeaurora.org>

commit ae0b449425a85289cd167faafc0f8c7c6dd0062c
Author: Yan He <yanhe@codeaurora.org>
Date:   Mon Mar 4 16:49:41 2013 -0800

    msm: sps: enable a pipe only when it is disabled
    
    BAM HW may publish the number of available bytes to the peripheral multiple
    times if we enable the same pipe for multiple times. And some peripherials
    may overestimate the available bytes by summing the sizes. Thus, enable a
    pipe only when the pipe is disabled currently.
    
    Change-Id: I8697ba5003eaed1efbfecc10161cc6b178c43d0b
    Signed-off-by: Yan He <yanhe@codeaurora.org>

commit 1059e2bc4213fd8bf0a000a9e98f9cffb3afb52b
Author: Yan He <yanhe@codeaurora.org>
Date:   Fri Feb 8 15:52:20 2013 -0800

    msm: sps: read pipe number from the exact reg field
    
    The reserved fields in regular BAMs have new meanings in NDP-BAM
    and BAM-Lite on B family. Read the exact reg field for pipe number
    instead of reading the whole register.
    
    Change-Id: I25dca6ed3c8beab3c2e943120867c759dd5e4095
    Signed-off-by: Yan He <yanhe@codeaurora.org>

commit 4a9b592e3c1899ebb115c7edea686355cc6ec584
Author: Yan He <yanhe@codeaurora.org>
Date:   Thu Dec 20 16:04:24 2012 -0800

    msm: sps: add the option for BAM HW configuration
    
    Add the option for BAM Clients to specify whether enable or disable
    some HW features of their BAMs.
    
    Change-Id: I98eed08ce32bc6ded93892b5b131f25f1e708817
    Signed-off-by: Yan He <yanhe@codeaurora.org>

commit f50d24ad862133cfeba8e0318c1cd22b9f88692e
Author: Yan He <yanhe@codeaurora.org>
Date:   Fri Nov 9 21:49:27 2012 -0800

    msm: sps: make device type independent from BAM DMA
    
    BAM DMA does not exist on some chipsets. Get device type for BAMs
    from device tree even if BAM DMA does not exist.
    
    Change-Id: Id736cf47b0a3e80520ad9d73f143ff66855e7038
    Signed-off-by: Yan He <yanhe@codeaurora.org>

commit 4fd82e2cebde10612de7938d7af90e84e3a7445c
Author: Yan He <yanhe@codeaurora.org>
Date:   Wed Nov 7 14:57:29 2012 -0800

    msm: sps: do not disable channels in BAM DMA
    
    The channels are turned on by TZ now. So do not disable them
    in the init of BAM DMA on apps.
    
    Change-Id: I696d26533c7e2590127d482199c72b564282d855
    Signed-off-by: Yan He <yanhe@codeaurora.org>

commit 1c4b3e551e0e81195db44ed8d9251fd81cbbbf51
Author: Yan He <yanhe@codeaurora.org>
Date:   Mon Oct 8 11:44:55 2012 -0700

    msm: sps: Allow EE0 for SPS satellite mode
    
    The Execution Environment (EE) allocation allows the satellite mode
    to use EE0 now. Remove the former restriction in the code.
    
    Change-Id: I59f315c23fcdf114edddfaff9322e2055ad4fa5c
    Signed-off-by: Yan He <yanhe@codeaurora.org>

commit 2559d5882e6304cddf004117ceeb40476f20a51a
Author: Yan He <yanhe@codeaurora.org>
Date:   Wed Sep 19 11:51:38 2012 -0700

    msm: sps: disable cache miss feature in NDP-BAM by default
    
    NDP-BAM has a new feature of cache miss error response. However,
    peripherals may not support it. Thus, disable it by default.
    
    Signed-off-by: Yan He <yanhe@codeaurora.org>

commit 155c13929571981eaf613fdc0f41bd352d26efbf
Author: Yan He <yanhe@codeaurora.org>
Date:   Thu Dec 6 13:27:43 2012 -0800

    msm: sps: show flow control recommendation
    
    When the client of SPS driver pushes data into the BAM faster
    than BAM's consumption speed, the descriptor FIFO of that pipe
    will be full and the transfer will return failure. The client
    can avoid this situation by monitoring the number of pending
    transfers. However, this is up to the implementation of error
    handling of the client. Add this debugging message so that SPS
    driver will show flow control recommendation to the client
    for one time when the descriptor FIFO is ever full.
    
    Change-Id: I1627fae7588eb9ae0cd8ba4a4809bfca282bb1e0
    Signed-off-by: Yan He <yanhe@codeaurora.org>

commit 695b69ae8462a2e688dfdfe2adeb39d46165b90c
Author: Jeff Hugo <jhugo@codeaurora.org>
Date:   Tue Apr 9 13:50:09 2013 -0600

    msm: bam_dmux: reduce kernel log chatter
    
    Every time a uplink wakeup sequence occurs, the underlying SPS driver
    logs some BAM information to the kernel log.  The message is identical
    for each occurence, and the information is only relevant at system boot.
    To reduce the log chatter after system boot, make use of the constrained
    logging SPS feature.
    
    CRs-Fixed: 473102
    Change-Id: I1454722d387317ad1b74f5ae09bb55f6a9a9d941
    Signed-off-by: Jeffrey Hugo <jhugo@codeaurora.org>

commit 11651b86fdac2b020bf16752d593181656658351
Author: Paul Reioux <reioux@gmail.com>
Date:   Thu May 23 11:51:15 2013 -0500

    drivers/sensorhub/ssp_sensorhub.c: reduce dmesg log spam
    
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit 50f590d4424ee1914e1b1498aef10e026f9321da
Author: Vikram Mulukutla <markivx@codeaurora.org>
Date:   Wed May 15 19:57:18 2013 -0700

    msm: clock: Add a flag to disable rate caching
    
    Certain clocks can have the rate of their parent
    changed without clk_set_rate being called on them.
    Caching rates for these clocks is incorrect, since
    their rates may have changed since the last
    clk_set_rate.
    
    Change-Id: I1eb5dded6093f5ccd2b43a4d89eca033db54b73f
    Signed-off-by: Vikram Mulukutla <markivx@codeaurora.org>

commit 1e3e846a9be5ef335b96ca6ce7a4ab587440d159
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Fri Jun 5 00:55:32 2015 +0200

     msm: clock: change vdd_corner to 1D array
    
    Previously a 2D array was used for vdd_corner.  Since the dimensions
    are known, 1D array saves one level of pointers and works equally well.
    It's also easier to dynamically construct a 1D array.
    
    Change-Id: Ic873ca40b383ff12363becd74537f58fcd2fdbf7
    Signed-off-by: Junjie Wu <junjiew@codeaurora.org>

commit 627c8cc30b99328ab1992d9268de208162063b46
Author: Patrick Daly <pdaly@codeaurora.org>
Date:   Tue Apr 16 17:18:28 2013 -0700

    msm: clock: Add support for current to vdd_class
    
    The regulator_set_optimum_mode() API is used by acpuclock-krait. As part
    of the effort to rewrite this code using standard clock APIs, broaden
    these standard APIs to allow configuring both regulator voltage and
    current.
    
    Change-Id: Ic6dfa5c0c18d1c3eb531727ee79e4f5be84e8a97
    Signed-off-by: Patrick Daly <pdaly@codeaurora.org>

commit 1e6bb7f7fbbbd954abb0bf57638a677706f98ec1
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Fri Jun 5 00:48:56 2015 +0200

     msm: clock: Allow dynamic allocation for vdd_class parameters
    
    Once clocks move to device tree, parameters for the vdd class will be
    parsed and assigned into the struct. Therefore, elements of the struct
    cannot be const.
    
    Change-Id: Ie2f33d64921bca24035ec8f8ea1ab4d1ac241a60
    Signed-off-by: Patrick Daly <pdaly@codeaurora.org>

commit 70c2defef802f553cf002d54dc47bb580cc917ad
Author: Patrick Daly <pdaly@codeaurora.org>
Date:   Tue Feb 5 11:49:07 2013 -0800

    msm: clock: Move regulator API calls into clock framework
    
    All B-family clock drivers can use standard regulator APIs. Move these
    calls into the shared clock.c file.
    
    Maintain backwards compatibility with older targets by continuing to
    allow usage of target specific non-standard calls.
    
    Change-Id: I538f3118ee135a4266b82977b71c92d3dfa2e8f7
    Signed-off-by: Patrick Daly <pdaly@codeaurora.org>

commit b71304a0f0ae46130e94a202bbd6e16c9300e10a
Author: Patrick Daly <pdaly@codeaurora.org>
Date:   Tue Mar 26 13:12:53 2013 -0700

    msm: clock: Move voting for regulators to clock framework
    
    Some clocks, and the regulators they require, may be enabled by the
    bootloader. The clock driver detects these clocks, and attempts to
    configure its data structures in a consistent manner. This procedure
    involves voting for regulators.
    
    Regulators may support different voltage levels. The first vote by the
    clock driver will override whatever level was set by the bootloaders to
    account for all the clocks that are on. Therefore, temporarily vote for
    the highest supported voltage level until all clocks have voted for their
    required voltage level. Then remove the temporary high vote.
    
    Change-Id: Ibdb97eba374ec139768ad0b23cb1163dec9aa3aa
    Signed-off-by: Patrick Daly <pdaly@codeaurora.org>

commit 3aea9f9ea4a2c0190c9924abb5b09ea68a5adf88
Author: Saravana Kannan <skannan@codeaurora.org>
Date:   Mon Mar 18 21:15:18 2013 -0700

    msm: clock: Refactor clk_set_rate()
    
    The refactor makes the code more readable and maintainable. Also, it avoids
    sending out a clk_set_rate() trace point when rate change does not occur
    due to an invalid argument.
    
    Change-Id: I848301a2fc5ddc4080e75daaaf53163db2588736
    Signed-off-by: Saravana Kannan <skannan@codeaurora.org>

commit e898dfd9c4b75d905ec4ee634420f4c3b51d67f2
Author: Saravana Kannan <skannan@codeaurora.org>
Date:   Wed Mar 20 22:19:10 2013 -0700

    msm: clock: Add helper functions to use when changing clock parent
    
    Any clock op (such as set_rate and set_parent) that changes the parent of a
    clock needs to transfer the prepare and enable requirements of the clock
    from the old parent to the new parent. Provide a generic pair of APIs,
    __clk_pre_reparent() and __clk_post_reparent(), that take care of this.
    
    The pair of APIs require the prepare lock to be held the entire duration
    between and including the calls to the two APIs. Since __clk_pre_reparent
    grabs a spin lock and __clk_post_reparent releases the spin lock, only
    atomic operations can be performed between calls to the two APIs.
    
    Change-Id: I1dc3905655652b2c58841f0b4e65ea6d2ab5020e
    Signed-off-by: Saravana Kannan <skannan@codeaurora.org>

commit b9f0361ee4f4fc44bc4bf616416c6b11f12c106b
Author: Dorimanx <yuri@bynet.co.il>
Date:   Tue Jun 2 15:28:47 2015 +0300

    HOTPLUG: all max 2 cores to work in suspend/standby.

commit 50355b7df333a3f6c994b4207df433ecb98f167c
Author: Dorimanx <yuri@bynet.co.il>
Date:   Thu May 7 03:09:12 2015 +0300

    CPU HOTPLUGS: i have noticed that when hotplug suspend, all cores ONLINE!
    
    i see that all cores set to online when hotplug go to suspend.
    not good! waste of power.
    
    so force all 3 cores to sleep. only one core should be online.

commit da9bd928863810ebbe5d557c88cf1fb72bc9578b
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Jun 4 22:24:19 2015 +0200

    msm-hotplug: enable 1 core as min core

commit ae1f16d7f6c4ce4845fdf2dc781518a6b544019a
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Jun 4 21:38:43 2015 +0200

    cpugov: add bluactive and optimax governor

commit f0bf6a265f808bc220af46d10647a00fe2edf51a
Author: Saravana Kannan <skannan@codeaurora.org>
Date:   Mon Mar 18 20:08:28 2013 -0700

    msm: clock: Fix clk_set_parent()
    
    - Update the clk->parent field when the operation succeeds.
    - Grab the prepare lock when changing the parent. This is necessary to
      avoid race conditions between clk_set_parent() and clk_prepare(),
      clk_unprepare() and clk_set_rate(). Luckily, as of now, only the measure
      clock supports clk_set_parent(). So, this is not a real issue in a
      production device. Each set_parent op still needs to deal with
      grabbing/releasing the enable lock at the right time.
    - If a clock doesn't support clk_set_parent() return an error instead of
      silently failing.
    
    Change-Id: I5fdd6a7bb44bb924dc9aabd7119a03520617e3d1
    Signed-off-by: Saravana Kannan <skannan@codeaurora.org>

commit 20c2431ec58c1bc1fb0a14e2ce0583e6edb641a0
Author: Saravana Kannan <skannan@codeaurora.org>
Date:   Mon Jan 21 17:58:35 2013 -0800

    msm: clock: Improve clock handoff code.
    
    The current clock handoff code hands off the child clock before handing off
    the parent clock. That is technically incorrect since the state and rate of
    a clock can't be determined without knowing the state and rate of the
    source/parent clock. But this works alright for now since the handoff code
    forcefully enables the necessary parent and also assumes that if a clock
    has multiple parents, the rates of all those parents are fixed. As we
    implement more clocks, this assumption no longer holds true.
    
    The current handoff code also causes the prepare and enable ops to be
    called for an already enabled clock. The hardware read/writes caused by
    this might not be harmful in the case of most clock hardware designs, but
    is not always the case. Eg: PLLs, I2C clocks, etc.
    
    To address these issues, rewrite the clock handoff code so that the parent
    clock is identified first, it's handed off and then the child clock is
    handed off. Also, when an already enabled clock is handed off, just
    directly update the software state of the clock and don't call the ops to
    update the hardware. To make sure the parent clock's reference counts are
    updated correctly, call clk_prepare_enable() on the parent clock.
    
    This change also has the nice effect of avoiding any "clock stuck off/on"
    warnings during boot that are caused by the boot code configuring the
    clocks incorrectly (parent off, child on). This is because we don't
    actually call the prepare/enable ops and also make sure the parent clocks
    is on before calling the handoff code for the child clock.
    
    Change-Id: I385a2afaf62a4138d53048f675822e079be2fcca
    Signed-off-by: Saravana Kannan <skannan@codeaurora.org>

commit 6029fbbf6935ec41902209652c8ad18ea7d1acd3
Author: Saravana Kannan <skannan@codeaurora.org>
Date:   Thu Oct 18 20:51:13 2012 -0700

    msm: clock: Move parent field into top level struct clk
    
    Maintaining a parent field inside each clock type leads to pointless
    repetition of code. Almost all clocks have parents, so move the parent
    field to top level struct clk, have it updated whenever the parent changes
    for a clock and remove the get_parent ops implementation.
    
    Change-Id: Id897a55d496446a1047322d40d8df041dfea5840
    Signed-off-by: Saravana Kannan <skannan@codeaurora.org>

commit 934c3eeeccafc0cbc63b5066b0ccb52463777873
Author: Matt Wagantall <mattw@codeaurora.org>
Date:   Thu Oct 18 12:54:15 2012 -0700

    msm: clock: Fix measurement debugfs node creation
    
    A recent change introduced a bug that caused clk_get to be called
    on the 'measure' clock even before the measure clock had been
    registered. Fix this by calling clock_debug_init() after registering
    the first clock table, at the end of msm_clock_init().
    
    Change-Id: I4b506fe55e0269d0f1229a9836058a6ffb79a520
    Signed-off-by: Matt Wagantall <mattw@codeaurora.org>

commit c32bfd444bcd5db5158119ed1e27046f4dd500b9
Author: Matt Wagantall <mattw@codeaurora.org>
Date:   Wed Sep 26 17:28:54 2012 -0700

    msm: clock: Setup sibling lists in msm_clock_register()
    
    msm_clock_register() should initialize each clocks' list of
    children, as is already done in msm_clock_init(). This allows
    the clock-voter.c code to work with clocks registered through
    the msm_clock_register() API. Since no clocks are registered
    through that API yet, this change should have no immediate
    functional effect.
    
    Change-Id: I257999da82615e99da2cf76fdda48f43e975107e
    Signed-off-by: Matt Wagantall <mattw@codeaurora.org>

commit 8985049681f02958f08eac18581ad9dd655b86c8
Author: Deepak Katragadda <dkatraga@codeaurora.org>
Date:   Fri Jun 21 14:36:18 2013 -0700

    msm: clock-rpm: Remove the factor field from struct rpm_clk
    
    The factor field was introduced for QDSS clocks to set a rate of
    0, 1, 2. The rate for other RPM clocks is set in Hz. The factor
    would determine the divisor applied to a rate before sending it
    over to the RPM. In order to remove this extra complexity (factor
    field), the QDSS driver is made to set rates of 0, 1000, 2000.
    
    Change-Id: I722c32e7c66e16b62d4cf31eb865a80b4a7f3922
    Signed-off-by: Deepak Katragadda <dkatraga@codeaurora.org>

commit 68c3d5efa4072bfd6e2d11e224bb93ab8a616698
Author: Tianyi Gou <tgou@codeaurora.org>
Date:   Wed Mar 27 15:15:29 2013 -0700

    msm: clock-rpm: Remove last_set_khz/last_set_sleep_khz from rpm clk
    
    Since last_set_khz and last_set_sleep_khz can be computed from the rate
    value stored in the struct clk, remove them from rpm clk struct to avoid
    unnecessary duplication.
    
    By removing the duplication of data, this also fixes the issue caused by
    clk.rate not matching last_set_khz/last_set_sleep_khz after a handoff.
    The mismatch that occurs without this patch causes the call to
    clk_set_rate() to return early without sending a new request to RPM when
    the request rate is 1 * factor.
    
    Change-Id: Ia47a8cea80441e51112715f14410d23e8c8fa00a
    Signed-off-by: Tianyi Gou <tgou@codeaurora.org>

commit 5f6d9af00b038a25412c5b6f62dfe771583079b9
Author: Vikram Mulukutla <markivx@codeaurora.org>
Date:   Wed Feb 20 15:10:40 2013 -0800

    msm: clock-rpm: Allow set_rate on RPM branch clocks
    
    Drivers may want to explicitly set a rate on fixed
    rate RPM branch clocks. Allow this if the rate that
    is passed in matches the rate of the RPM branch clock
    exactly.
    
    CRs-Fixed: 452047
    Change-Id: I4775b9c1e14a15c74cfd082d679534305818460c
    Signed-off-by: Vikram Mulukutla <markivx@codeaurora.org>

commit bbb8e5edcf822d25ba9aa9010a26b2f53d931376
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Fri Sep 11 14:16:53 2015 +0200

    Revert " Asynchronous Fsync: initial extraction of Async Fsync from HTC"
    
    This reverts commit b9ac93f72f5b5f57924ee39a831704a38aca0635.

commit eab411f81df9313a9e106d9d6dac0d84b9ccb431
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Fri Sep 11 14:15:05 2015 +0200

    Revert "fs/dyn_sync_cntrl: dynamic sync control v1.5 (faux123)"
    
    This reverts commit e04040a020e100998d7d5478866c01d85dc0dac1.

commit 23de2796734db979f485e9950a03ff47d5752980
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Fri Sep 11 14:13:37 2015 +0200

    make it possible to build zram, enable zram, "downgrade" interactive

commit 5c21fdd47e55db2957b875b5c66f4a853a8a5102
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Fri Sep 11 13:54:51 2015 +0200

    fix compiling errors

commit e6ba4017586eb758656ffe5ca2b7da97f182b793
Author: Jeff Hugo <jhugo@codeaurora.org>
Date:   Mon Oct 22 11:34:28 2012 -0600

    msm: bam_dmux: configure satellite mode via devicetree
    
    Add support to allow satellite mode configuration of the BAM hardware
    via device tree.  Satellite mode is only used on specific targets where
    shared access of the BAM is possible.
    
    Change-Id: I6ac4e2ad7827b5651034651d18cf716324462d0f
    Signed-off-by: Jeffrey Hugo <jhugo@codeaurora.org>

commit a43b7d9e1b86f5e0c0f74791630f6bf2033d49fc
Author: Jeff Hugo <jhugo@codeaurora.org>
Date:   Thu Sep 27 12:36:45 2012 -0600

    msm: bam_dmux: remove A2_DEFAULT_DESCRIPTORS
    
    The A2_DEFAULT_DESCRIPTORS macro is unused and has no apparent purpose.
    Remove it to clean out dead code.
    
    Change-Id: I593179ed4f0ccb777ed4178a9e6c138b04da61de
    Signed-off-by: Jeffrey Hugo <jhugo@codeaurora.org>

commit 2a135c7ea1b9ccb57afd58569af6025dd4364e58
Author: Saravana Kannan <skannan@codeaurora.org>
Date:   Thu Oct 11 22:40:12 2012 -0700

    msm: clock-rpm: Set "last set khz" field to non-zero value during handoff
    
    When a RPM (RPM SMD) clock is handed off, the last set KHz field is not
    updated to a non-zero value. This means that despite the clock ref count
    being non-zero, a vote will never be sent to RPM till the first non-zero
    clk_set_rate() call is made on the clock.
    
    If this call doesn't happen before late init, it's possible that the RPM
    will turn off the clock when the application processor is using it. In a
    real world scenario, we always get a non-zero clk_set_rate() call before
    late init. So, this doesn't cause any real issues. Just fixing it for
    completeness sake.
    
    Change-Id: I48f43976e8a64da5060c16fd27b6b77d537b118c
    Signed-off-by: Saravana Kannan <skannan@codeaurora.org>

commit d14906627fe11ddc26b885c482fc0ea6a1a5f6ac
Author: Dorimanx <yuri@bynet.co.il>
Date:   Tue Jun 2 13:41:32 2015 +0300

    CPUFREQ: set min sample time to 50 for interactive base govs.

commit ed6069706ac95c15a66531bd03e66a89c92149d5
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu May 28 01:01:10 2015 +0200

    interactive: small update

commit 4ac375f34b07cf63f419486149ba6812351989f0
Author: Dorimanx <yuri@bynet.co.il>
Date:   Thu May 21 13:08:45 2015 +0300

    USB KEYBOARD/MOUSE: corrected and cleaned ON/OFF trigger. + rename.

commit cb66083df309841563c8ef177ab3066d0d293e2d
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Jun 3 02:13:04 2015 +0200

    usb gadget try another stuff

commit f47372df513e455ebfc5acb755d5c3686470bbcd
Author: dcd <dcd1182@gmail.com>
Date:   Tue May 26 01:46:46 2015 -0500

    jf: f2fs: bring up to date with Jaegeuk's branch
    
    Up to date as of: 10027551ccf5459cc771c31ac8bc8e5cc8db45f8 4/16/15
    
    Change-Id: Iaff30802e39eea53593f663fac9d0cfd95c66ab4

commit ea8ad856c3daf5f506eb4884dac91f7c611ef313
Author: Anji Jonnala <anjir@codeaurora.org>
Date:   Tue Apr 16 13:19:00 2013 +0530

    msm: rpm: Initialize the uninitialized variables
    
    sel_mask_ack array will be used to validate the ack received
    from rpm. Initialize the sel_mask_ack array to NULL before using it
    to make sure that we are comparing the right value read from rpm.
    
    Change-Id: I3dc20bc1594fabf250b98a08461f22482aaa19bf
    Signed-off-by: Anji Jonnala <anjir@codeaurora.org>

commit b05b747b1f1e63f06a8acdbccef1ab779976b0b7
Author: Arun Kumar Neelakantam <aneela@codeaurora.org>
Date:   Thu Jan 8 10:41:19 2015 +0530

    soc: qcom: bam_dmux: Reset SSR flags in bam_init
    
    The modem crash during its first boot prior to the A2 initializing
    break the BAM DMUX assumption that the modem will be able to
    successfully boot the first time, so the SSR handling within
    BAM DMUX goes into wrong state.
    
    Reset the SSR flags in bam_init to handle the modem crash prior to A2
    initialization case.
    
    Signed-off-by: Arun Kumar Neelakantam <aneela@codeaurora.org>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 945efb36ef00a5e6f957d4cf4cce399aed11f00c
Author: Junjie Wu <junjiew@codeaurora.org>
Date:   Thu Jun 20 20:49:00 2013 -0700

    msm: acpuclock: Fix acpuclk_data null pointer dereference
    
    If acpuclk_data is null, dereferencing it in acpuclk_get_switch_time
    causes a kernel panic.  Add pointer check to fix that.
    
    Change-Id: I69b92671c8cf26cb45ec8f1193c147be2ebe4c16
    Signed-off-by: Junjie Wu <junjiew@codeaurora.org>

commit b91272f62800c1e0b10cb576828d4a9fa01e0af7
Author: Vikram Mulukutla <markivx@codeaurora.org>
Date:   Mon Mar 11 12:10:45 2013 -0700

    msm: acpuclock: Add a null check for acpuclk_data
    
    If acpuclock fails to probe before a client calls acpuclk_get_rate,
    a null dereference will occur. Fix this by adding the appropriate
    checks.
    
    Change-Id: If873a64711e9cc807b1734996f7c9358bb08161a
    Signed-off-by: Vikram Mulukutla <markivx@codeaurora.org>

commit 085d69a4f91025f8fd226d372ae3e41a8787ba8e
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Fri Sep 11 13:40:54 2015 +0200

    Enable hotplugging for msm mpdecision

commit b357e50da79468cb721175bde597ab944fba498b
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Sep 9 02:38:27 2015 +0200

    new toolchain 4.9.4

commit 9eafd907c64217164973ed00f5899c136eb00151
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Sep 9 02:44:26 2015 +0200

    Linux 3.4.108

commit 69172bfffe1ae95b2df892f8774021a241d67e9b
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sat Aug 1 17:29:04 2015 +0200

    source drop update from I9505XXUHOF2
    
    Signed-off-by: Tkkg1994 <luca.grifo@outlook.com>

commit 5d618caba538ccfaba2fce92a8343eb92c42689c
Author: Viresh Kumar <viresh.kumar@linaro.org>
Date:   Wed Oct 31 01:28:15 2012 +0100

    cpufreq: Avoid calling cpufreq driver's target() routine if target_freq == policy->cur
    
    Avoid calling cpufreq driver's target() routine if new frequency is same as
    policies current frequency.
    
    Signed-off-by: Viresh Kumar <viresh.kumar@linaro.org>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit 724a8a33724674441ad01b52bbb4e869f343c697
Author: Viresh Kumar <viresh.kumar@linaro.org>
Date:   Fri Oct 26 00:51:32 2012 +0200

    cpufreq: Fix sparse warning by making local function static
    
    cpufreq_disabled() is a local function, so should be marked static.
    
    Signed-off-by: Viresh Kumar <viresh.kumar@linaro.org>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit 40c57d18d0f627b83bd1cffbf36c9ff81a432999
Author: Viresh Kumar <viresh.kumar@linaro.org>
Date:   Tue Oct 23 01:29:03 2012 +0200

    cpufreq: Improve debug prints
    
    With debug options on, it is difficult to locate cpufreq core's debug prints.
    Fix this by prefixing debug prints with KBUILD_MODNAME.
    
    Signed-off-by: Viresh Kumar <viresh.kumar@linaro.org>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit 1500cb1eea9822c9392f8a3023a09d7ed67dd992
Author: viresh kumar <viresh.kumar@linaro.org>
Date:   Tue Oct 23 01:23:43 2012 +0200

    cpufreq / core: Fix printing of governor and driver name
    
    Arrays for governer and driver name are of size CPUFREQ_NAME_LEN or 16.
    i.e. 15 bytes for name and 1 for trailing '\0'.
    
    When cpufreq driver print these names (for sysfs), it includes '\n' or ' ' in
    the fmt string and still passes length as CPUFREQ_NAME_LEN. If the driver or
    governor names are using all 15 fields allocated to them, then the trailing '\n'
    or ' ' will never be printed. And so commands like:
    
    root@linaro-developer# cat /sys/devices/system/cpu/cpu0/cpufreq/scaling_driver
    
    will print something like:
    
    cpufreq_foodrvroot@linaro-developer#
    
    Fix this by increasing print length by one character.
    
    Signed-off-by: Viresh Kumar <viresh.kumar@linaro.org>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit c91849f32c5564367f4937bd18786b78df39ecdd
Author: viresh kumar <viresh.kumar@linaro.org>
Date:   Tue Oct 23 01:23:33 2012 +0200

    cpufreq / core: Fix typo in comment describing show_bios_limit()
    
    show_bios_limit is mistakenly written as show_scaling_driver in a comment
    describing purpose of show_bios_limit() routine.
    
    Fix it.
    
    Signed-off-by: Viresh Kumar <viresh.kumar@linaro.org>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit c27111042711b56739591040666eaf4e5190c674
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed May 27 20:59:33 2015 +0200

    msm_limiter: mixed changes from neobuddy, fib_rules: fix for new helper file

commit 2fa5cc7a509091cfcab3adf41d76396a703cf240
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed May 27 17:59:13 2015 +0200

    cpufreq: remove hardlimit, it only caused issues on this device

commit 73356d9565e39b43f71d425e4e6030a4553760b0
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Mon Feb 2 15:05:36 2015 +0100

    locking/futex: Check PF_KTHREAD rather than !p->mm to filter out kthreads
    
    attach_to_pi_owner() checks p->mm to prevent attaching to kthreads and
    this looks doubly wrong:
    
    1. It should actually check PF_KTHREAD, kthread can do use_mm().
    
    2. If this task is not kthread and it is actually the lock owner we can
       wrongly return -EPERM instead of -ESRCH or retry-if-EAGAIN.
    
       And note that this wrong EPERM is the likely case unless the exiting
       task is (auto)reaped quickly, we check ->mm before PF_EXITING.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Darren Hart <darren@dvhart.com>
    Cc: Davidlohr Bueso <dave@stgolabs.net>
    Cc: Jerome Marchand <jmarchan@redhat.com>
    Cc: Larry Woodman <lwoodman@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mateusz Guzik <mguzik@redhat.com>
    Link: http://lkml.kernel.org/r/20150202140536.GA26406@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit fe8d48d0a6947888c78462f54409563182fa6579
Author: Michael Kerrisk <mtk.manpages@gmail.com>
Date:   Fri Jan 16 20:28:06 2015 +0100

    futex: Fix argument handling in futex_lock_pi() calls
    
    This patch fixes two separate buglets in calls to futex_lock_pi():
    
      * Eliminate unused 'detect' argument
      * Change unused 'timeout' argument of FUTEX_TRYLOCK_PI to NULL
    
    The 'detect' argument of futex_lock_pi() seems never to have been
    used (when it was included with the initial PI mutex implementation
    in Linux 2.6.18, all checks against its value were disabled by
    ANDing against 0 (i.e., if (detect... && 0)), and with
    commit 778e9a9c3e7193ea9f434f382947155ffb59c755, any mention of
    this argument in futex_lock_pi() went way altogether. Its presence
    now serves only to confuse readers of the code, by giving the
    impression that the futex() FUTEX_LOCK_PI operation actually does
    use the 'val' argument. This patch removes the argument.
    
    The futex_lock_pi() call that corresponds to FUTEX_TRYLOCK_PI includes
    'timeout' as one of its arguments. This misleads the reader into thinking
    that the FUTEX_TRYLOCK_PI operation does employ timeouts for some sensible
    purpose; but it does not.  Indeed, it cannot, because the checks at the
    start of sys_futex() exclude FUTEX_TRYLOCK_PI from the set of operations
    that do copy_from_user() on the timeout argument. So, in the
    FUTEX_TRYLOCK_PI futex_lock_pi() call it would be simplest to change
    'timeout' to 'NULL'. This patch does that.
    
    Signed-off-by: Michael Kerrisk <mtk.manpages@gmail.com>
    Reviewed-by: Darren Hart <darren@dvhart.com>
    Link: http://lkml.kernel.org/r/54B96646.8010200@gmail.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 6bea828e6ae7fbe950ad12742f7e9e2c9ef9732f
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Thu Oct 31 18:18:19 2013 +0100

    locking: Move the rtmutex code to kernel/locking/
    
    Suggested-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/n/tip-p9ijt8div0hwldexwfm4nlhj@git.kernel.org
    [ Fixed build failure in kernel/rcu/tree_plugin.h. ]
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit e8c276b9d73275e3ccae6ab861a3a6a7679fa84d
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Thu Oct 31 18:16:43 2013 +0100

    locking: Move the semaphore core to kernel/locking/
    
    Suggested-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/n/tip-vmw5sf6vzmua1z6nx1cg69h2@git.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit e8e66570e2b24710ba6782216854705caced67b6
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Thu Oct 31 18:15:36 2013 +0100

    locking: Move the spinlock code to kernel/locking/
    
    Suggested-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/n/tip-b81ol0z3mon45m51o131yc9j@git.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 989a9652ea6118d5f6929c94d6a0afc825bfd15d
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Thu Oct 31 18:14:17 2013 +0100

    locking: Move the lockdep code to kernel/locking/
    
    Suggested-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/n/tip-wl7s3tta5isufzfguc23et06@git.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 5f77debe4d84228820c137464a45f6dfa5154c9a
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Thu Oct 31 18:11:53 2013 +0100

    locking: Move the mutex code to kernel/locking/
    
    Suggested-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/n/tip-1ditvncg30dgbpvrz2bxfmke@git.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>
    
    Conflicts:
    	kernel/Makefile

commit 569130aa48643c2ba2821b9a93ce4ee29addf531
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Jun 3 12:27:08 2014 +0000

    futex: Make lookup_pi_state more robust
    
    commit 54a217887a7b658e2650c3feff22756ab80c7339 upstream.
    
    The current implementation of lookup_pi_state has ambigous handling of
    the TID value 0 in the user space futex.  We can get into the kernel
    even if the TID value is 0, because either there is a stale waiters bit
    or the owner died bit is set or we are called from the requeue_pi path
    or from user space just for fun.
    
    The current code avoids an explicit sanity check for pid = 0 in case
    that kernel internal state (waiters) are found for the user space
    address.  This can lead to state leakage and worse under some
    circumstances.
    
    Handle the cases explicit:
    
           Waiter | pi_state | pi->owner | uTID      | uODIED | ?
    
      [1]  NULL   | ---      | ---       | 0         | 0/1    | Valid
      [2]  NULL   | ---      | ---       | >0        | 0/1    | Valid
    
      [3]  Found  | NULL     | --        | Any       | 0/1    | Invalid
    
      [4]  Found  | Found    | NULL      | 0         | 1      | Valid
      [5]  Found  | Found    | NULL      | >0        | 1      | Invalid
    
      [6]  Found  | Found    | task      | 0         | 1      | Valid
    
      [7]  Found  | Found    | NULL      | Any       | 0      | Invalid
    
      [8]  Found  | Found    | task      | ==taskTID | 0/1    | Valid
      [9]  Found  | Found    | task      | 0         | 0      | Invalid
      [10] Found  | Found    | task      | !=taskTID | 0/1    | Invalid
    
     [1] Indicates that the kernel can acquire the futex atomically. We
         came came here due to a stale FUTEX_WAITERS/FUTEX_OWNER_DIED bit.
    
     [2] Valid, if TID does not belong to a kernel thread. If no matching
         thread is found then it indicates that the owner TID has died.
    
     [3] Invalid. The waiter is queued on a non PI futex
    
     [4] Valid state after exit_robust_list(), which sets the user space
         value to FUTEX_WAITERS | FUTEX_OWNER_DIED.
    
     [5] The user space value got manipulated between exit_robust_list()
         and exit_pi_state_list()
    
     [6] Valid state after exit_pi_state_list() which sets the new owner in
         the pi_state but cannot access the user space value.
    
     [7] pi_state->owner can only be NULL when the OWNER_DIED bit is set.
    
     [8] Owner and user space value match
    
     [9] There is no transient state which sets the user space TID to 0
         except exit_robust_list(), but this is indicated by the
         FUTEX_OWNER_DIED bit. See [4]
    
    [10] There is no transient state which leaves owner and user space
         TID out of sync.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Will Drewry <wad@chromium.org>
    Cc: Darren Hart <dvhart@linux.intel.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit c392a2ef8e0918a515b5bdbd05bcc4cb68a32305
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Jun 3 12:27:07 2014 +0000

    futex: Always cleanup owner tid in unlock_pi
    
    commit 13fbca4c6ecd96ec1a1cfa2e4f2ce191fe928a5e upstream.
    
    If the owner died bit is set at futex_unlock_pi, we currently do not
    cleanup the user space futex.  So the owner TID of the current owner
    (the unlocker) persists.  That's observable inconsistant state,
    especially when the ownership of the pi state got transferred.
    
    Clean it up unconditionally.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Will Drewry <wad@chromium.org>
    Cc: Darren Hart <dvhart@linux.intel.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 6ec5b36f9817791f12e2998c7ce6dd2f258fec5e
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Jun 3 12:27:06 2014 +0000

    futex: Validate atomic acquisition in futex_lock_pi_atomic()
    
    commit b3eaa9fc5cd0a4d74b18f6b8dc617aeaf1873270 upstream.
    
    We need to protect the atomic acquisition in the kernel against rogue
    user space which sets the user space futex to 0, so the kernel side
    acquisition succeeds while there is existing state in the kernel
    associated to the real owner.
    
    Verify whether the futex has waiters associated with kernel state.  If
    it has, return -EINVAL.  The state is corrupted already, so no point in
    cleaning it up.  Subsequent calls will fail as well.  Not our problem.
    
    [ tglx: Use futex_top_waiter() and explain why we do not need to try
      	restoring the already corrupted user space state. ]
    
    Signed-off-by: Darren Hart <dvhart@linux.intel.com>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Will Drewry <wad@chromium.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit bc63ba3bb11b4f58ebe7dcaee591226e537ec8a7
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon May 12 20:45:34 2014 +0000

    futex: Add another early deadlock detection check
    
    commit 866293ee54227584ffcb4a42f69c1f365974ba7f upstream.
    
    Dave Jones trinity syscall fuzzer exposed an issue in the deadlock
    detection code of rtmutex:
      http://lkml.kernel.org/r/20140429151655.GA14277@redhat.com
    
    That underlying issue has been fixed with a patch to the rtmutex code,
    but the futex code must not call into rtmutex in that case because
        - it can detect that issue early
        - it avoids a different and more complex fixup for backing out
    
    If the user space variable got manipulated to 0x80000000 which means
    no lock holder, but the waiters bit set and an active pi_state in the
    kernel is found we can figure out the recursive locking issue by
    looking at the pi_state owner. If that is the current task, then we
    can safely return -EDEADLK.
    
    The check should have been added in commit 59fa62451 (futex: Handle
    futex_pi OWNER_DIED take over correctly) already, but I did not see
    the above issue caused by user space manipulation back then.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Dave Jones <davej@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Darren Hart <darren@dvhart.com>
    Cc: Davidlohr Bueso <davidlohr@hp.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Clark Williams <williams@redhat.com>
    Cc: Paul McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Lai Jiangshan <laijs@cn.fujitsu.com>
    Cc: Roland McGrath <roland@hack.frob.com>
    Cc: Carlos ODonell <carlos@redhat.com>
    Cc: Jakub Jelinek <jakub@redhat.com>
    Cc: Michael Kerrisk <mtk.manpages@gmail.com>
    Cc: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Link: http://lkml.kernel.org/r/20140512201701.097349971@linutronix.de
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit e2291f406c2a09350f8383acc23095b310c1e500
Author: Pranav Vashi <neobuddy89@gmail.com>
Date:   Tue Feb 17 21:27:33 2015 +0530

    Revert "futex: Validate atomic acquisition in futex_lock_pi_atomic()"
    
    This reverts commit a0df636e6d247a6ab2f088a1f11b6a20a28d1d99.

commit 4279a71b841817509a4291af5792dece95ea7465
Author: Pranav Vashi <neobuddy89@gmail.com>
Date:   Tue Feb 17 21:27:19 2015 +0530

    Revert "futex: Always cleanup owner tid in unlock_pi"
    
    This reverts commit 40ac207719e6c3b9c343ec166776bb78fcec3675.

commit 27e9fb66c271202fe61674e3e5d6b22c45b6ba0d
Author: Pranav Vashi <neobuddy89@gmail.com>
Date:   Tue Feb 17 21:26:57 2015 +0530

    Revert "futex: Make lookup_pi_state more robust"
    
    This reverts commit e2540d0a07e1c3b8852e00c5b9572bbc11a362ee.

commit 2330a8d1a58153734a9e986878889e62af8b58d7
Author: Colin Cross <ccross@android.com>
Date:   Mon May 6 23:50:18 2013 +0000

    futex: use freezable blocking call
    
    Avoid waking up every thread sleeping in a futex_wait call during
    suspend and resume by calling a freezable blocking call.  Previous
    patches modified the freezer to avoid sending wakeups to threads
    that are blocked in freezable blocking calls.
    
    This call was selected to be converted to a freezable call because
    it doesn't hold any locks or release any resources when interrupted
    that might be needed by another freezing task or a kernel driver
    during suspend, and is a common site where idle userspace tasks are
    blocked.
    
    Change-Id: I9ccab9c2d201adb66c85432801cdcf43fc91e94f
    Acked-by: Tejun Heo <tj@kernel.org>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Darren Hart <dvhart@linux.intel.com>
    Signed-off-by: Colin Cross <ccross@android.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit 439f4cadfe024ecc76fd1126da04d636d45e5d43
Author: Randy Dunlap <rdunlap@infradead.org>
Date:   Tue Mar 5 10:00:24 2013 -0800

    futex: fix kernel-doc notation and spello
    
    Fix kernel-doc warning in futex.c and convert 'Returns' to the new Return:
    kernel-doc notation format.
    
      Warning(kernel/futex.c:2286): Excess function parameter 'clockrt' description in 'futex_wait_requeue_pi'
    
    Fix one spello.
    
    Signed-off-by: Randy Dunlap <rdunlap@infradead.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit 1dbeb953686e301a3f45489ddce1aeb5fc235367
Author: Lai Jiangshan <laijs@cn.fujitsu.com>
Date:   Thu Jul 31 11:30:17 2014 +0800

    smpboot: Add missing get_online_cpus() in smpboot_register_percpu_thread()
    
    commit 4bee96860a65c3a62d332edac331b3cf936ba3ad upstream.
    
    The following race exists in the smpboot percpu threads management:
    
    CPU0	      	   	     CPU1
    cpu_up(2)
      get_online_cpus();
      smpboot_create_threads(2);
    			     smpboot_register_percpu_thread();
    			     for_each_online_cpu();
    			       __smpboot_create_thread();
      __cpu_up(2);
    
    This results in a missing per cpu thread for the newly onlined cpu2 and
    in a NULL pointer dereference on a consecutive offline of that cpu.
    
    Proctect smpboot_register_percpu_thread() with get_online_cpus() to
    prevent that.
    
    [ tglx: Massaged changelog and removed the change in
            smpboot_unregister_percpu_thread() because that's an
            optimization and therefor not stable material. ]
    
    Signed-off-by: Lai Jiangshan <laijs@cn.fujitsu.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Cc: David Rientjes <rientjes@google.com>
    Link: http://lkml.kernel.org/r/1406777421-12830-1-git-send-email-laijs@cn.fujitsu.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 2c1cc6b7c562dac0c278e5918f1d963529aced73
Author: Subbaraman Narayanamurthy <subbaram@codeaurora.org>
Date:   Fri Jun 20 12:45:54 2014 -0700

    kthread: Fix the race condition when kthread is parked
    
    While stressing the CPU hotplug path, sometimes we hit a problem
    as shown below.
    
    [57056.416774] ------------[ cut here ]------------
    [57056.489232] ksoftirqd/1 (14): undefined instruction: pc=c01931e8
    [57056.489245] Code: e594a000 eb085236 e15a0000 0a000000 (e7f001f2)
    [57056.489259] ------------[ cut here ]------------
    [57056.492840] kernel BUG at kernel/kernel/smpboot.c:134!
    [57056.513236] Internal error: Oops - BUG: 0 [#1] PREEMPT SMP ARM
    [57056.519055] Modules linked in: wlan(O) mhi(O)
    [57056.523394] CPU: 0 PID: 14 Comm: ksoftirqd/1 Tainted: G        W  O 3.10.0-g3677c61-00008-g180c060 #1
    [57056.532595] task: f0c8b000 ti: f0e78000 task.ti: f0e78000
    [57056.537991] PC is at smpboot_thread_fn+0x124/0x218
    [57056.542750] LR is at smpboot_thread_fn+0x11c/0x218
    [57056.547528] pc : [<c01931e8>]    lr : [<c01931e0>]    psr: 200f0013
    [57056.547528] sp : f0e79f30  ip : 00000000  fp : 00000000
    [57056.558983] r10: 00000001  r9 : 00000000  r8 : f0e78000
    [57056.564192] r7 : 00000001  r6 : c1195758  r5 : f0e78000  r4 : f0e5fd00
    [57056.570701] r3 : 00000001  r2 : f0e79f20  r1 : 00000000  r0 : 00000000
    
    This issue was always seen in the context of "ksoftirqd". It seems to
    be happening because of a potential race condition in __kthread_parkme
    where just after completing the parked completion, before the
    ksoftirqd task has been scheduled again, it can go into running state.
    
    Fix this by waiting for the task state to parked after waiting the
    parked completion.
    
    CRs-Fixed: 659674
    Change-Id: If3f0e9b706eeb5d30d5a32f84378d35bb03fe794
    Signed-off-by: Subbaraman Narayanamurthy <subbaram@codeaurora.org>

commit 16f30d083d3d4579062d48983125edaaa7728f8c
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Apr 30 15:27:21 2013 -0700

    kthread: implement probe_kthread_data()
    
    One of the problems that arise when converting dedicated custom threadpool
    to workqueue is that the shared worker pool used by workqueue anonimizes
    each worker making it more difficult to identify what the worker was doing
    on which target from the output of sysrq-t or debug dump from oops, BUG()
    and friends.
    
    For example, after writeback is converted to use workqueue instead of
    priviate thread pool, there's no easy to tell which backing device a
    writeback work item was working on at the time of task dump, which,
    according to our writeback brethren, is important in tracking down issues
    with a lot of mounted file systems on a lot of different devices.
    
    This patchset implements a way for a work function to mark its execution
    instance so that task dump of the worker task includes information to
    indicate what the work item was doing.
    
    An example WARN dump would look like the following.
    
     WARNING: at fs/fs-writeback.c:1015 bdi_writeback_workfn+0x2b4/0x3c0()
     Modules linked in:
     CPU: 0 Pid: 28 Comm: kworker/u18:0 Not tainted 3.9.0-rc1-work+ #24
     Hardware name: empty empty/S3992, BIOS 080011  10/26/2007
     Workqueue: writeback bdi_writeback_workfn (flush-8:16)
      ffffffff820a3a98 ffff88015b927cb8 ffffffff81c61855 ffff88015b927cf8
      ffffffff8108f500 0000000000000000 ffff88007a171948 ffff88007a1716b0
      ffff88015b49df00 ffff88015b8d3940 0000000000000000 ffff88015b927d08
     Call Trace:
      [<ffffffff81c61855>] dump_stack+0x19/0x1b
      [<ffffffff8108f500>] warn_slowpath_common+0x70/0xa0
      ...
    
    This patch:
    
    Implement probe_kthread_data() which returns kthread_data if accessible.
    The function is equivalent to kthread_data() except that the specified
    @task may not be a kthread or its vfork_done is already cleared rendering
    struct kthread inaccessible.  In the former case, probe_kthread_data() may
    return any value.  In the latter, NULL.
    
    This will be used to safely print debug information without affecting
    synchronization in the normal paths.  Workqueue debug info printing on
    dump_stack() and friends will make use of it.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Acked-by: Jan Kara <jack@suse.cz>
    Cc: Dave Chinner <david@fromorbit.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Jens Axboe <axboe@kernel.dk>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit 5b7f97ebbfdc998449c0e49c404a502afc455a8f
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Mon Apr 29 15:05:12 2013 -0700

    kthread: kill task_get_live_kthread()
    
    task_get_live_kthread() looks confusing and unneeded.  It does
    get_task_struct() but only kthread_stop() needs this, it can be called
    even if the calller doesn't have a reference when we know that this
    kthread can't exit until we do kthread_stop().
    
    kthread_park() and kthread_unpark() do not need get_task_struct(), the
    callers already have the reference.  And it can not help if we can race
    with the exiting kthread anyway, kthread_park() can hang forever in this
    case.
    
    Change kthread_park() and kthread_unpark() to use to_live_kthread(),
    change kthread_stop() to do get_task_struct() by hand and remove
    task_get_live_kthread().
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Namhyung Kim <namhyung@kernel.org>
    Cc: "Paul E. McKenney" <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: "Srivatsa S. Bhat" <srivatsa.bhat@linux.vnet.ibm.com>
    Cc: Tejun Heo <tj@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit 180c791d872c9964bee989bd55abe61cfbb32447
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Mon Apr 29 15:05:01 2013 -0700

    kthread: introduce to_live_kthread()
    
    "k->vfork_done != NULL" with a barrier() after to_kthread(k) in
    task_get_live_kthread(k) looks unclear, and sub-optimal because we load
    ->vfork_done twice.
    
    All we need is to ensure that we do not return to_kthread(NULL).  Add a
    new trivial helper which loads/checks ->vfork_done once, this also looks
    more understandable.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Namhyung Kim <namhyung@kernel.org>
    Cc: "Paul E. McKenney" <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: "Srivatsa S. Bhat" <srivatsa.bhat@linux.vnet.ibm.com>
    Cc: Tejun Heo <tj@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit 34d8d6ef0cde80d445257723f0a7ecb35a97c5f1
Author: alucard24 <dmbaoh2@gmail.com>
Date:   Tue May 26 21:04:16 2015 +0200

    darkness, nightmare cpu govs: small optimization!

commit 4163091975c131bc9ac67f95cdf539e401e87360
Author: alucard24 <dmbaoh2@gmail.com>
Date:   Sat May 23 14:42:06 2015 +0200

    CONFIG_QUICK_WAKEUP: Implemented module call in the proper way, then enabled it! It was my mistake :)

commit bd4afa60e7f3d4e72de796743daa230e24273444
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Mon Nov 14 14:29:51 2011 -0800

    userns: Add kuid_t and kgid_t and associated infrastructure in uidgid.h
    
    Start distinguishing between internal kernel uids and gids and
    values that userspace can use.  This is done by introducing two
    new types: kuid_t and kgid_t.  These types and their associated
    functions are infrastructure are declared in the new header
    uidgid.h.
    
    Ultimately there will be a different implementation of the mapping
    functions for use with user namespaces.  But to keep it simple
    we introduce the mapping functions first to separate the meat
    from the mechanical code conversions.
    
    Export overflowuid and overflowgid so we can use from_kuid_munged
    and from_kgid_munged in modular code.
    
    Acked-by: Serge Hallyn <serge.hallyn@canonical.com>
    Signed-off-by: Eric W. Biederman <ebiederm@xmission.com>

commit c642343df747acb7a0996f25de4a0aea301a72dc
Author: Vincent Stehl <vincent.stehle@laposte.net>
Date:   Fri Apr 4 08:43:18 2014 +0200

    topology: Fix compilation warning when not in SMP
    
    commit 53974e06603977f348ed978d75c426b0532daa67 upstream.
    
    The topology_##name() macro does not use its argument when CONFIG_SMP is not
    set, as it ultimately calls the cpu_data() macro.
    
    So we avoid maintaining a possibly unused `cpu' variable, to avoid the
    following compilation warning:
    
      drivers/base/topology.c: In function show_physical_package_id:
      drivers/base/topology.c:103:118: warning: unused variable cpu [-Wunused-variable]
       define_id_show_func(physical_package_id);
    
      drivers/base/topology.c: In function show_core_id:
      drivers/base/topology.c:106:106: warning: unused variable cpu [-Wunused-variable]
       define_id_show_func(core_id);
    
    This can be seen with e.g. x86 defconfig and CONFIG_SMP not set.
    
    Signed-off-by: Vincent Stehl <vincent.stehle@laposte.net>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit b6f48370c9d465e75d6120e75cc416160eec32a8
Author: Benson Leung <bleung@chromium.org>
Date:   Wed Sep 25 03:05:11 2013 +0000

    driver core : Fix use after free of dev->parent in device_shutdown
    
    The put_device(dev) at the bottom of the loop of device_shutdown
    may result in the dev being cleaned up. In device_create_release,
    the dev is kfreed.
    
    However, device_shutdown attempts to use the dev pointer again after
    put_device by referring to dev->parent.
    
    Copy the parent pointer instead to avoid this condition.
    
    This bug was found on Chromium OS's chromeos-3.8, which is based on v3.8.11.
    See bug report : https://code.google.com/p/chromium/issues/detail?id=297842
    This can easily be reproduced when shutting down with
    hidraw devices that report battery condition.
    Two examples are the HP Bluetooth Mouse X4000b and the Apple Magic Mouse.
    For example, with the magic mouse :
    The dev in question is "hidraw0"
    dev->parent is "magicmouse"
    
    In the course of the shutdown for this device, the input event cleanup calls
    a put on hidraw0, decrementing its reference count.
    When we finally get to put_device(dev) in device_shutdown, kobject_cleanup
    is called and device_create_release does kfree(dev).
    dev->parent is no longer valid, and we may crash in
    put_device(dev->parent).
    
    This change should be applied on any kernel with this change :
    d1c6c030fcec6f860d9bb6c632a3ebe62e28440b
    
    Change-Id: Ib8c7dbce155558aa1087349130d5d1b58c15540f
    Cc: stable@vger.kernel.org
    Signed-off-by: Benson Leung <bleung@chromium.org>
    Reviewed-by: Ming Lei <ming.lei@canonical.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Git-commit: f123db8e9d6c84c863cb3c44d17e61995dc984fb
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Osvaldo Banuelos <osvaldob@codeaurora.org>
    (cherry picked from commit 29b25ef11ea7ed0913705bf3d6ece80853c7161d)

commit b37f9e3a7dbcef0603b245b722528ff78db07080
Author: Ming Lei <ming.lei@canonical.com>
Date:   Fri Jun 22 18:01:40 2012 +0800

    driver core: fix shutdown races with probe/remove(v3)
    
    Firstly, .shutdown callback may touch a uninitialized hardware
    if dev->driver is set and .probe is not completed.
    
    Secondly, device_shutdown() may dereference a null pointer to cause
    oops when dev->driver is cleared after it has been checked in
    device_shutdown().
    
    So just hold device lock and its parent lock(if it has) to
    fix the races.
    
    Cc: Alan Stern <stern@rowland.harvard.edu>
    Signed-off-by: Ming Lei <ming.lei@canonical.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 09c570be19d161b1adbb35a84f4b5b3b47329238
Author: dyoung@redhat.com <dyoung@redhat.com>
Date:   Thu May 16 14:31:30 2013 +0800

    driver core: print sysfs attribute name when warning about bogus permissions
    
    Make it obvious to see what attribute is using bogus permissions.
    
    Signed-off-by: Dave Young <dyoung@redhat.com>
    Acked-by: Felipe Balbi <balbi@ti.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit ec4b1f2c308203df68de35f4ce781c8743bca12c
Author: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
Date:   Thu Apr 11 11:43:29 2013 -0700

    driver core: handle user namespaces properly with the uid/gid devtmpfs change
    
    Now that devtmpfs is caring about uid/gid, we need to use the correct
    internal types so users who have USER_NS enabled will have things work
    properly for them.
    
    Thanks to Eric for pointing this out, and the patch review.
    
    Reported-by: Eric W. Biederman <ebiederm@xmission.com>
    Cc: Kay Sievers <kay@vrfy.org>
    Cc: Ming Lei <ming.lei@canonical.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit c05e2477353fae4df5564114f8833a13538a611a
Author: Kay Sievers <kay@vrfy.org>
Date:   Sat Apr 6 09:56:00 2013 -0700

    driver core: add uid and gid to devtmpfs
    
    Some drivers want to tell userspace what uid and gid should be used for
    their device nodes, so allow that information to percolate through the
    driver core to userspace in order to make this happen.  This means that
    some systems (i.e.  Android and friends) will not need to even run a
    udev-like daemon for their device node manager and can just rely in
    devtmpfs fully, reducing their footprint even more.
    
    Signed-off-by: Kay Sievers <kay@vrfy.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    	#

commit aaf9a96e40988cca974f21c347bef16156186117
Author: Felipe Balbi <balbi@ti.com>
Date:   Wed Feb 20 10:31:42 2013 +0200

    base: core: WARN() about bogus permissions on device attributes
    
    Whenever a struct device_attribute is registered
    with mismatched permissions - read permission without
    a show routine or write permission without store
    routine - we will issue a big warning so we catch
    those early enough.
    
    Signed-off-by: Felipe Balbi <balbi@ti.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 68842872e4d37f823d571a74aa7dad96caa363b5
Author: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
Date:   Fri May 10 09:14:04 2013 -0700

    driver core: export subsys_virtual_register
    
    Modules want to call this function, so it needs to be exported.
    
    Reported-by: Daniel Mack <zonque@gmail.com>
    Cc: Kay Sievers <kay@vrfy.org>
    Cc: Tejun Heo <tj@kernel.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit f25fdcb27d41f94cf7b80a3264372722671dd51d
Author: Michal Hocko <mhocko@suse.cz>
Date:   Tue Mar 12 17:21:19 2013 +0100

    device: separate all subsys mutexes
    
    ca22e56d (driver-core: implement 'sysdev' functionality for regular
    devices and buses) has introduced bus_register macro with a static
    key to distinguish different subsys mutex classes.
    
    This however doesn't work for different subsys which use a common
    registering function. One example is subsys_system_register (and
    mce_device and cpu_device).
    
    In the end this leads to the following lockdep splat:
    [  207.271924] ======================================================
    [  207.271932] [ INFO: possible circular locking dependency detected ]
    [  207.271942] 3.9.0-rc1-0.7-default+ #34 Not tainted
    [  207.271948] -------------------------------------------------------
    [  207.271957] bash/10493 is trying to acquire lock:
    [  207.271963]  (subsys mutex){+.+.+.}, at: [<ffffffff8134af27>] bus_remove_device+0x37/0x1c0
    [  207.271987]
    [  207.271987] but task is already holding lock:
    [  207.271995]  (cpu_hotplug.lock){+.+.+.}, at: [<ffffffff81046ccf>] cpu_hotplug_begin+0x2f/0x60
    [  207.272012]
    [  207.272012] which lock already depends on the new lock.
    [  207.272012]
    [  207.272023]
    [  207.272023] the existing dependency chain (in reverse order) is:
    [  207.272033]
    [  207.272033] -> #4 (cpu_hotplug.lock){+.+.+.}:
    [  207.272044]        [<ffffffff810ae329>] lock_acquire+0xe9/0x120
    [  207.272056]        [<ffffffff814ad807>] mutex_lock_nested+0x37/0x360
    [  207.272069]        [<ffffffff81046ba9>] get_online_cpus+0x29/0x40
    [  207.272082]        [<ffffffff81185210>] drain_all_stock+0x30/0x150
    [  207.272094]        [<ffffffff811853da>] mem_cgroup_reclaim+0xaa/0xe0
    [  207.272104]        [<ffffffff8118775e>] __mem_cgroup_try_charge+0x51e/0xcf0
    [  207.272114]        [<ffffffff81188486>] mem_cgroup_charge_common+0x36/0x60
    [  207.272125]        [<ffffffff811884da>] mem_cgroup_newpage_charge+0x2a/0x30
    [  207.272135]        [<ffffffff81150531>] do_wp_page+0x231/0x830
    [  207.272147]        [<ffffffff8115151e>] handle_pte_fault+0x19e/0x8d0
    [  207.272157]        [<ffffffff81151da8>] handle_mm_fault+0x158/0x1e0
    [  207.272166]        [<ffffffff814b6153>] do_page_fault+0x2a3/0x4e0
    [  207.272178]        [<ffffffff814b2578>] page_fault+0x28/0x30
    [  207.272189]
    [  207.272189] -> #3 (&mm->mmap_sem){++++++}:
    [  207.272199]        [<ffffffff810ae329>] lock_acquire+0xe9/0x120
    [  207.272208]        [<ffffffff8114c5ad>] might_fault+0x6d/0x90
    [  207.272218]        [<ffffffff811a11e3>] filldir64+0xb3/0x120
    [  207.272229]        [<ffffffffa013fc19>] call_filldir+0x89/0x130 [ext3]
    [  207.272248]        [<ffffffffa0140377>] ext3_readdir+0x6b7/0x7e0 [ext3]
    [  207.272263]        [<ffffffff811a1519>] vfs_readdir+0xa9/0xc0
    [  207.272273]        [<ffffffff811a15cb>] sys_getdents64+0x9b/0x110
    [  207.272284]        [<ffffffff814bb599>] system_call_fastpath+0x16/0x1b
    [  207.272296]
    [  207.272296] -> #2 (&type->i_mutex_dir_key#3){+.+.+.}:
    [  207.272309]        [<ffffffff810ae329>] lock_acquire+0xe9/0x120
    [  207.272319]        [<ffffffff814ad807>] mutex_lock_nested+0x37/0x360
    [  207.272329]        [<ffffffff8119c254>] link_path_walk+0x6f4/0x9a0
    [  207.272339]        [<ffffffff8119e7fa>] path_openat+0xba/0x470
    [  207.272349]        [<ffffffff8119ecf8>] do_filp_open+0x48/0xa0
    [  207.272358]        [<ffffffff8118d81c>] file_open_name+0xdc/0x110
    [  207.272369]        [<ffffffff8118d885>] filp_open+0x35/0x40
    [  207.272378]        [<ffffffff8135c76e>] _request_firmware+0x52e/0xb20
    [  207.272389]        [<ffffffff8135cdd6>] request_firmware+0x16/0x20
    [  207.272399]        [<ffffffffa03bdb91>] request_microcode_fw+0x61/0xd0 [microcode]
    [  207.272416]        [<ffffffffa03bd554>] microcode_init_cpu+0x104/0x150 [microcode]
    [  207.272431]        [<ffffffffa03bd61c>] mc_device_add+0x7c/0xb0 [microcode]
    [  207.272444]        [<ffffffff8134a419>] subsys_interface_register+0xc9/0x100
    [  207.272457]        [<ffffffffa04fc0f4>] 0xffffffffa04fc0f4
    [  207.272472]        [<ffffffff81000202>] do_one_initcall+0x42/0x180
    [  207.272485]        [<ffffffff810bbeff>] load_module+0x19df/0x1b70
    [  207.272499]        [<ffffffff810bc376>] sys_init_module+0xe6/0x130
    [  207.272511]        [<ffffffff814bb599>] system_call_fastpath+0x16/0x1b
    [  207.272523]
    [  207.272523] -> #1 (umhelper_sem){++++.+}:
    [  207.272537]        [<ffffffff810ae329>] lock_acquire+0xe9/0x120
    [  207.272548]        [<ffffffff814ae9c4>] down_read+0x34/0x50
    [  207.272559]        [<ffffffff81062bff>] usermodehelper_read_trylock+0x4f/0x100
    [  207.272575]        [<ffffffff8135c7dd>] _request_firmware+0x59d/0xb20
    [  207.272587]        [<ffffffff8135cdd6>] request_firmware+0x16/0x20
    [  207.272599]        [<ffffffffa03bdb91>] request_microcode_fw+0x61/0xd0 [microcode]
    [  207.272613]        [<ffffffffa03bd554>] microcode_init_cpu+0x104/0x150 [microcode]
    [  207.272627]        [<ffffffffa03bd61c>] mc_device_add+0x7c/0xb0 [microcode]
    [  207.272641]        [<ffffffff8134a419>] subsys_interface_register+0xc9/0x100
    [  207.272654]        [<ffffffffa04fc0f4>] 0xffffffffa04fc0f4
    [  207.272666]        [<ffffffff81000202>] do_one_initcall+0x42/0x180
    [  207.272678]        [<ffffffff810bbeff>] load_module+0x19df/0x1b70
    [  207.272690]        [<ffffffff810bc376>] sys_init_module+0xe6/0x130
    [  207.272702]        [<ffffffff814bb599>] system_call_fastpath+0x16/0x1b
    [  207.272715]
    [  207.272715] -> #0 (subsys mutex){+.+.+.}:
    [  207.272729]        [<ffffffff810ae002>] __lock_acquire+0x13b2/0x15f0
    [  207.272740]        [<ffffffff810ae329>] lock_acquire+0xe9/0x120
    [  207.272751]        [<ffffffff814ad807>] mutex_lock_nested+0x37/0x360
    [  207.272763]        [<ffffffff8134af27>] bus_remove_device+0x37/0x1c0
    [  207.272775]        [<ffffffff81349114>] device_del+0x134/0x1f0
    [  207.272786]        [<ffffffff813491f2>] device_unregister+0x22/0x60
    [  207.272798]        [<ffffffff814a24ea>] mce_cpu_callback+0x15e/0x1ad
    [  207.272812]        [<ffffffff814b6402>] notifier_call_chain+0x72/0x130
    [  207.272824]        [<ffffffff81073d6e>] __raw_notifier_call_chain+0xe/0x10
    [  207.272839]        [<ffffffff81498f76>] _cpu_down+0x1d6/0x350
    [  207.272851]        [<ffffffff81499130>] cpu_down+0x40/0x60
    [  207.272862]        [<ffffffff8149cc55>] store_online+0x75/0xe0
    [  207.272874]        [<ffffffff813474a0>] dev_attr_store+0x20/0x30
    [  207.272886]        [<ffffffff812090d9>] sysfs_write_file+0xd9/0x150
    [  207.272900]        [<ffffffff8118e10b>] vfs_write+0xcb/0x130
    [  207.272911]        [<ffffffff8118e924>] sys_write+0x64/0xa0
    [  207.272923]        [<ffffffff814bb599>] system_call_fastpath+0x16/0x1b
    [  207.272936]
    [  207.272936] other info that might help us debug this:
    [  207.272936]
    [  207.272952] Chain exists of:
    [  207.272952]   subsys mutex --> &mm->mmap_sem --> cpu_hotplug.lock
    [  207.272952]
    [  207.272973]  Possible unsafe locking scenario:
    [  207.272973]
    [  207.272984]        CPU0                    CPU1
    [  207.272992]        ----                    ----
    [  207.273000]   lock(cpu_hotplug.lock);
    [  207.273009]                                lock(&mm->mmap_sem);
    [  207.273020]                                lock(cpu_hotplug.lock);
    [  207.273031]   lock(subsys mutex);
    [  207.273040]
    [  207.273040]  *** DEADLOCK ***
    [  207.273040]
    [  207.273055] 5 locks held by bash/10493:
    [  207.273062]  #0:  (&buffer->mutex){+.+.+.}, at: [<ffffffff81209049>] sysfs_write_file+0x49/0x150
    [  207.273080]  #1:  (s_active#150){.+.+.+}, at: [<ffffffff812090c2>] sysfs_write_file+0xc2/0x150
    [  207.273099]  #2:  (x86_cpu_hotplug_driver_mutex){+.+.+.}, at: [<ffffffff81027557>] cpu_hotplug_driver_lock+0x17/0x20
    [  207.273121]  #3:  (cpu_add_remove_lock){+.+.+.}, at: [<ffffffff8149911c>] cpu_down+0x2c/0x60
    [  207.273140]  #4:  (cpu_hotplug.lock){+.+.+.}, at: [<ffffffff81046ccf>] cpu_hotplug_begin+0x2f/0x60
    [  207.273158]
    [  207.273158] stack backtrace:
    [  207.273170] Pid: 10493, comm: bash Not tainted 3.9.0-rc1-0.7-default+ #34
    [  207.273180] Call Trace:
    [  207.273192]  [<ffffffff810ab373>] print_circular_bug+0x223/0x310
    [  207.273204]  [<ffffffff810ae002>] __lock_acquire+0x13b2/0x15f0
    [  207.273216]  [<ffffffff812086b0>] ? sysfs_hash_and_remove+0x60/0xc0
    [  207.273227]  [<ffffffff810ae329>] lock_acquire+0xe9/0x120
    [  207.273239]  [<ffffffff8134af27>] ? bus_remove_device+0x37/0x1c0
    [  207.273251]  [<ffffffff814ad807>] mutex_lock_nested+0x37/0x360
    [  207.273263]  [<ffffffff8134af27>] ? bus_remove_device+0x37/0x1c0
    [  207.273274]  [<ffffffff812086b0>] ? sysfs_hash_and_remove+0x60/0xc0
    [  207.273286]  [<ffffffff8134af27>] bus_remove_device+0x37/0x1c0
    [  207.273298]  [<ffffffff81349114>] device_del+0x134/0x1f0
    [  207.273309]  [<ffffffff813491f2>] device_unregister+0x22/0x60
    [  207.273321]  [<ffffffff814a24ea>] mce_cpu_callback+0x15e/0x1ad
    [  207.273332]  [<ffffffff814b6402>] notifier_call_chain+0x72/0x130
    [  207.273344]  [<ffffffff81073d6e>] __raw_notifier_call_chain+0xe/0x10
    [  207.273356]  [<ffffffff81498f76>] _cpu_down+0x1d6/0x350
    [  207.273368]  [<ffffffff81027557>] ? cpu_hotplug_driver_lock+0x17/0x20
    [  207.273380]  [<ffffffff81499130>] cpu_down+0x40/0x60
    [  207.273391]  [<ffffffff8149cc55>] store_online+0x75/0xe0
    [  207.273402]  [<ffffffff813474a0>] dev_attr_store+0x20/0x30
    [  207.273413]  [<ffffffff812090d9>] sysfs_write_file+0xd9/0x150
    [  207.273425]  [<ffffffff8118e10b>] vfs_write+0xcb/0x130
    [  207.273436]  [<ffffffff8118e924>] sys_write+0x64/0xa0
    [  207.273447]  [<ffffffff814bb599>] system_call_fastpath+0x16/0x1b
    
    Which reports a false possitive deadlock because it sees:
    1) load_module -> subsys_interface_register -> mc_deveice_add (*) -> subsys->p->mutex -> link_path_walk -> lookup_slow -> i_mutex
    2) sys_write -> _cpu_down -> cpu_hotplug_begin -> cpu_hotplug.lock -> mce_cpu_callback -> mce_device_remove(**) -> device_unregister -> bus_remove_device -> subsys mutex
    3) vfs_readdir -> i_mutex -> filldir64 -> might_fault -> might_lock_read(mmap_sem) -> page_fault -> mmap_sem -> drain_all_stock -> cpu_hotplug.lock
    
    but
    1) takes cpu_subsys subsys (*) but 2) takes mce_device subsys (**) so
    the deadlock is not possible AFAICS.
    
    The fix is quite simple. We can pull the key inside bus_type structure
    because they are defined per device so the pointer will be unique as
    well. bus_register doesn't need to be a macro anymore so change it
    to the inline. We could get rid of __bus_register as there is no other
    caller but maybe somebody will want to use a different key so keep it
    around for now.
    
    Reported-by: Li Zefan <lizefan@huawei.com>
    Signed-off-by: Michal Hocko <mhocko@suse.cz>
    Signed-off-by: Jiri Kosina <jkosina@suse.cz>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 63b6d64576c1b53466274235bafb82b1dd7ebbb4
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Mar 12 11:30:05 2013 -0700

    driver/base: implement subsys_virtual_register()
    
    Kay tells me the most appropriate place to expose workqueues to
    userland would be /sys/devices/virtual/workqueues/WQ_NAME which is
    symlinked to /sys/bus/workqueue/devices/WQ_NAME and that we're lacking
    a way to do that outside of driver core as virtual_device_parent()
    isn't exported and there's no inteface to conveniently create a
    virtual subsystem.
    
    This patch implements subsys_virtual_register() by factoring out
    subsys_register() from subsys_system_register() and using it with
    virtual_device_parent() as the origin directory.  It's identical to
    subsys_system_register() other than the origin directory but we aren't
    gonna restrict the device names which should be used under it.
    
    This will be used to expose workqueue attributes to userland.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Acked-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: Kay Sievers <kay.sievers@vrfy.org>

commit 07cc88bae2ef66a67a04eb6e5f798655841e3b28
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Mar 12 11:30:04 2013 -0700

    cpumask: implement cpumask_parse()
    
    We have cpulist_parse() but not cpumask_parse().  Implement it using
    bitmap_parse().
    
    bitmap_parse() is weird in that it takes @len for a string in
    kernel-memory which also is inconsistent with bitmap_parselist().
    Make cpumask_parse() calculate the length and don't expose the
    inconsistency to cpumask users.  Maybe we can fix up bitmap_parse()
    later.
    
    This will be used to expose workqueue cpumask knobs to userland via
    sysfs.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Rusty Russell <rusty@rustcorp.com.au>

commit 4c4e82e5d701eaec6d513b56ef13f8f8d28bef25
Author: Alex Shi <alex.shi@intel.com>
Date:   Fri Jul 27 09:29:42 2012 +0930

    cpumask: cpulist_parse() comments correction
    
    As introduced in Rusty's commit 29c0177e6a4, the function has no
    parameter @len, so need to remove it from comments to avoid kernel-doc
    warning:
    
    alexs@debian:~/linux-next$ scripts/kernel-doc -man
    include/linux/cpumask.h | split-man.pl /tmp/man
    ....
    Warning(include/linux/cpumask.h:602): Excess function parameter 'len'
    description in 'cpulist_parse'
    
    and correct the function name in comments to cpulist_parse.
    
    Signed-off-by: Alex Shi <alex.shi@intel.com>
    Signed-off-by: Rusty Russell <rusty@rustcorp.com.au>

commit 74de4eaf41fdbc29ba70e3a1a782b60106a39c00
Author: Alex Shi <alex.shi@intel.com>
Date:   Mon May 28 22:23:51 2012 +0800

    cpumask: add a few comments of cpumask functions
    
    Current few cpumask functions' purposes are not quite clear. Stupid
    user like myself needs to dig into details for clear function
    purpose and return value.
    Add few explanation for them is helpful.
    
    Thanks for Srivatsa's comments and correction!
    
    Signed-off-by: Alex Shi <alex.shi@intel.com>
    Acked-by: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Signed-off-by: Rusty Russell <rusty@rustcorp.com.au>

commit 54da2182e14bcad14f08a72d38f3c4846d33362a
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Mar 12 11:29:59 2013 -0700

    workqueue: add wokrqueue_struct->maydays list to replace mayday cpu iterators
    
    Similar to how pool_workqueue iteration used to be, raising and
    servicing mayday requests is based on CPU numbers.  It's hairy because
    cpumask_t may not be able to handle WORK_CPU_UNBOUND and cpumasks are
    assumed to be always set on UP.  This is ugly and can't handle
    multiple unbound pools to be added for unbound workqueues w/ custom
    attributes.
    
    Add workqueue_struct->maydays.  When a pool_workqueue needs rescuing,
    it gets chained on the list through pool_workqueue->mayday_node and
    rescuer_thread() consumes the list until it's empty.
    
    This patch doesn't introduce any visible behavior changes.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Reviewed-by: Lai Jiangshan <laijs@cn.fujitsu.com>

commit 8b02929a7d3ca59b9439a64172440b5a83a0cfa3
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Mar 12 11:29:58 2013 -0700

    workqueue: restructure pool / pool_workqueue iterations in freeze/thaw functions
    
    The three freeze/thaw related functions - freeze_workqueues_begin(),
    freeze_workqueues_busy() and thaw_workqueues() - need to iterate
    through all pool_workqueues of all freezable workqueues.  They did it
    by first iterating pools and then visiting all pwqs (pool_workqueues)
    of all workqueues and process it if its pwq->pool matches the current
    pool.  This is rather backwards and done this way partly because
    workqueue didn't have fitting iteration helpers and partly to avoid
    the number of lock operations on pool->lock.
    
    Workqueue now has fitting iterators and the locking operation overhead
    isn't anything to worry about - those locks are unlikely to be
    contended and the same CPU visiting the same set of locks multiple
    times isn't expensive.
    
    Restructure the three functions such that the flow better matches the
    logical steps and pwq iteration is done using for_each_pwq() inside
    workqueue iteration.
    
    * freeze_workqueues_begin(): Setting of FREEZING is moved into a
      separate for_each_pool() iteration.  pwq iteration for clearing
      max_active is updated as described above.
    
    * freeze_workqueues_busy(): pwq iteration updated as described above.
    
    * thaw_workqueues(): The single for_each_wq_cpu() iteration is broken
      into three discrete steps - clearing FREEZING, restoring max_active,
      and kicking workers.  The first and last steps use for_each_pool()
      and the second step uses pwq iteration described above.
    
    This makes the code easier to understand and removes the use of
    for_each_wq_cpu() for walking pwqs, which can't support multiple
    unbound pwqs which will be needed to implement unbound workqueues with
    custom attributes.
    
    This patch doesn't introduce any visible behavior changes.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Reviewed-by: Lai Jiangshan <laijs@cn.fujitsu.com>

commit 2c3b1bfcff9cc06dd4a6934e9118ad11bfc8a714
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Mar 12 11:29:58 2013 -0700

    workqueue: introduce for_each_pool()
    
    With the scheduled unbound pools with custom attributes, there will be
    multiple unbound pools, so it wouldn't be able to use
    for_each_wq_cpu() + for_each_std_worker_pool() to iterate through all
    pools.
    
    Introduce for_each_pool() which iterates through all pools using
    worker_pool_idr and use it instead of for_each_wq_cpu() +
    for_each_std_worker_pool() combination in freeze_workqueues_begin().
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Reviewed-by: Lai Jiangshan <laijs@cn.fujitsu.com>

commit a3aaf8d6f2f4fb1b8d44f1018445102f6060a26c
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Mar 12 11:29:58 2013 -0700

    workqueue: replace for_each_pwq_cpu() with for_each_pwq()
    
    Introduce for_each_pwq() which iterates all pool_workqueues of a
    workqueue using the recently added workqueue->pwqs list and replace
    for_each_pwq_cpu() usages with it.
    
    This is primarily to remove the single unbound CPU assumption from pwq
    iteration for the scheduled unbound pools with custom attributes
    support which would introduce multiple unbound pwqs per workqueue;
    however, it also simplifies iterator users.
    
    Note that pwq->pool initialization is moved to alloc_and_link_pwqs()
    as that now is the only place which is explicitly handling the two pwq
    types.
    
    This patch doesn't introduce any visible behavior changes.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Reviewed-by: Lai Jiangshan <laijs@cn.fujitsu.com>

commit 6c694a69ff295cd6c3bd8fea2ff71174700d6fc8
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Mar 12 11:29:57 2013 -0700

    workqueue: add workqueue_struct->pwqs list
    
    Add workqueue_struct->pwqs list and chain all pool_workqueues
    belonging to a workqueue there.  This will be used to implement
    generic pool_workqueue iteration and handle multiple pool_workqueues
    for the scheduled unbound pools with custom attributes.
    
    This patch doesn't introduce any visible behavior changes.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Reviewed-by: Lai Jiangshan <laijs@cn.fujitsu.com>

commit 035acdd057365814f0263aa2f0859209694568ae
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Mar 12 11:29:57 2013 -0700

    workqueue: introduce kmem_cache for pool_workqueues
    
    pool_workqueues need to be aligned to 1 << WORK_STRUCT_FLAG_BITS as
    the lower bits of work->data are used for flags when they're pointing
    to pool_workqueues.
    
    Due to historical reasons, unbound pool_workqueues are allocated using
    kzalloc() with sufficient buffer area for alignment and aligned
    manually.  The original pointer is stored at the end which free_pwqs()
    retrieves when freeing it.
    
    There's no reason for this hackery anymore.  Set alignment of struct
    pool_workqueue to 1 << WORK_STRUCT_FLAG_BITS, add kmem_cache for
    pool_workqueues with proper alignment and replace the hacky alloc and
    free implementation with plain kmem_cache_zalloc/free().
    
    In case WORK_STRUCT_FLAG_BITS gets shrunk too much and makes fields of
    pool_workqueues misaligned, trigger WARN if the alignment of struct
    pool_workqueue becomes smaller than that of long long.
    
    Note that assertion on IS_ALIGNED() is removed from alloc_pwqs().  We
    already have another one in pwq init loop in __alloc_workqueue_key().
    
    This patch doesn't introduce any visible behavior changes.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Reviewed-by: Lai Jiangshan <laijs@cn.fujitsu.com>

commit 75c1f170b966bd9cf6ac1cad6b0b51fec6edb70f
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Mar 12 11:29:57 2013 -0700

    workqueue: make workqueue_lock irq-safe
    
    workqueue_lock will be used to synchronize areas which require
    irq-safety and there isn't much benefit in keeping it not irq-safe.
    Make it irq-safe.
    
    This patch doesn't introduce any visible behavior changes.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Reviewed-by: Lai Jiangshan <laijs@cn.fujitsu.com>

commit b81c5fc34d139d55ea4bd320c626587b4a05e872
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Mar 12 11:29:57 2013 -0700

    workqueue: make sanity checks less punshing using WARN_ON[_ONCE]()s
    
    Workqueue has been using mostly BUG_ON()s for sanity checks, which
    fail unnecessarily harshly when the assertion doesn't hold.  Most
    assertions can converted to be less drastic such that things can limp
    along instead of dying completely.  Convert BUG_ON()s to
    WARN_ON[_ONCE]()s with softer failure behaviors - e.g. if assertion
    check fails in destroy_worker(), trigger WARN and silently ignore
    destruction request.
    
    Most conversions are trivial.  Note that sanity checks in
    destroy_workqueue() are moved above removal from workqueues list so
    that it can bail out without side-effects if assertion checks fail.
    
    This patch doesn't introduce any visible behavior changes during
    normal operation.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Reviewed-by: Lai Jiangshan <laijs@cn.fujitsu.com>

commit b848f19e246674c3667926ee3f03edbe6e6e20f8
Author: Lai Jiangshan <laijs@cn.fujitsu.com>
Date:   Fri Mar 8 15:18:28 2013 -0800

    workqueue: fix possible pool stall bug in wq_unbind_fn()
    
    Since multiple pools per cpu have been introduced, wq_unbind_fn() has
    a subtle bug which may theoretically stall work item processing.  The
    problem is two-fold.
    
    * wq_unbind_fn() depends on the worker executing wq_unbind_fn() itself
      to start unbound chain execution, which works fine when there was
      only single pool.  With multiple pools, only the pool which is
      running wq_unbind_fn() - the highpri one - is guaranteed to have
      such kick-off.  The other pool could stall when its busy workers
      block.
    
    * The current code is setting WORKER_UNBIND / POOL_DISASSOCIATED of
      the two pools in succession without initiating work execution
      inbetween.  Because setting the flags requires grabbing assoc_mutex
      which is held while new workers are created, this could lead to
      stalls if a pool's manager is waiting for the previous pool's work
      items to release memory.  This is almost purely theoretical tho.
    
    Update wq_unbind_fn() such that it sets WORKER_UNBIND /
    POOL_DISASSOCIATED, goes over schedule() and explicitly kicks off
    execution for a pool and then moves on to the next one.
    
    tj: Updated comments and description.
    
    Signed-off-by: Lai Jiangshan <laijs@cn.fujitsu.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: stable@vger.kernel.org

commit 8f7bc966f322e7578f8e06e67f4a2c452e7a6b1f
Author: Lai Jiangshan <laijs@cn.fujitsu.com>
Date:   Tue Feb 19 12:17:02 2013 -0800

    workqueue: better define synchronization rule around rescuer->pool updates
    
    Rescuers visit different worker_pools to process work items from pools
    under pressure.  Currently, rescuer->pool is updated outside any
    locking and when an outsider looks at a rescuer, there's no way to
    tell when and whether rescuer->pool is gonna change.  While this
    doesn't currently cause any problem, it is nasty.
    
    With recent worker_maybe_bind_and_lock() changes, we can move
    rescuer->pool updates inside pool locks such that if rescuer->pool
    equals a locked pool, it's guaranteed to stay that way until the pool
    is unlocked.
    
    Move rescuer->pool inside pool->lock.
    
    This patch doesn't introduce any visible behavior difference.
    
    tj: Updated the description.
    
    Signed-off-by: Lai Jiangshan <laijs@cn.fujitsu.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

commit 40dba60ca1b2346b3b0329edf870cca05da6f009
Author: Lai Jiangshan <laijs@cn.fujitsu.com>
Date:   Tue Feb 19 12:17:02 2013 -0800

    workqueue: change argument of worker_maybe_bind_and_lock() to @pool
    
    worker_maybe_bind_and_lock() currently takes @worker but only cares
    about @worker->pool.  This patch updates worker_maybe_bind_and_lock()
    to take @pool instead of @worker.  This will be used to better define
    synchronization rules regarding rescuer->pool updates.
    
    This doesn't introduce any functional change.
    
    tj: Updated the comments and description.
    
    Signed-off-by: Lai Jiangshan <laijs@cn.fujitsu.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

commit a820652fd995eb48367e3cfb9f72ab548fca1393
Author: Lai Jiangshan <laijs@cn.fujitsu.com>
Date:   Tue Feb 19 12:17:02 2013 -0800

    workqueue: use %current instead of worker->task in worker_maybe_bind_and_lock()
    
    worker_maybe_bind_and_lock() uses both @worker->task and @current at
    the same time.  As worker_maybe_bind_and_lock() can only be called by
    the current worker task, they are always the same.
    
    Update worker_maybe_bind_and_lock() to use %current consistently.
    
    This doesn't introduce any functional change.
    
    tj: Massaged the description.
    
    Signed-off-by: Lai Jiangshan <laijs@cn.fujitsu.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

commit 73b2bb080d933683ae75a62c104f7aa410190653
Author: Konstantin Khlebnikov <khlebnikov@openvz.org>
Date:   Thu Jan 24 16:36:31 2013 +0400

    workqueue: un-GPL function delayed_work_timer_fn()
    
    commit d8e794dfd51c368ed3f686b7f4172830b60ae47b ("workqueue: set
    delayed_work->timer function on initialization") exports function
    delayed_work_timer_fn() only for GPL modules. This makes delayed-works
    unusable for non-GPL modules, because initialization macro now requires
    GPL symbol. For example schedule_delayed_work() available for non-GPL.
    
    Signed-off-by: Konstantin Khlebnikov <khlebnikov@openvz.org>
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: stable@vger.kernel.org # 3.7

commit f7608e5375fb8cb2502d2b4aa0559997685b6591
Merge: c48d23d 3676fe2
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sat May 23 21:51:14 2015 +0200

    Merge branch 'master' of https://github.com/Tkkg1994/Hulk-Kernel-V2

commit c48d23d96848ab4a36cb32e68e1a4fee25ace2f9
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sat May 23 21:48:23 2015 +0200

    async: fix ifdef

commit 34409166efc0cb06cd4d2f9397211826a340c138
Author: Tejun Heo <tj@kernel.org>
Date:   Fri Jan 18 14:05:56 2013 -0800

    workqueue: implement current_is_async()
    
    This function queries whether %current is an async worker executing an
    async item.  This will be used to implement warning on synchronous
    request_module() from async workers.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    
    Conflicts:
    	include/linux/async.h

commit c7e2b1eb7431f63f837115357e5a728b1a28bc00
Author: Sasha Levin <levinsasha928@gmail.com>
Date:   Tue Oct 30 14:45:57 2012 -0400

    hashtable: introduce a small and naive hashtable
    
    This hashtable implementation is using hlist buckets to provide a simple
    hashtable to prevent it from getting reimplemented all over the kernel.
    
    Signed-off-by: Sasha Levin <levinsasha928@gmail.com>
    [ Merging this now, so that subsystems can start applying Sasha's
      patches that use this   - Linus ]
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit 1b8a17caf560fc2539ee43bf676f48d43c1eb862
Author: Tejun Heo <tj@kernel.org>
Date:   Wed Feb 13 19:29:12 2013 -0800

    workqueue: rename cpu_workqueue to pool_workqueue
    
    workqueue has moved away from global_cwqs to worker_pools and with the
    scheduled custom worker pools, wforkqueues will be associated with
    pools which don't have anything to do with CPUs.  The workqueue code
    went through significant amount of changes recently and mass renaming
    isn't likely to hurt much additionally.  Let's replace 'cpu' with
    'pool' so that it reflects the current design.
    
    * s/struct cpu_workqueue_struct/struct pool_workqueue/
    * s/cpu_wq/pool_wq/
    * s/cwq/pwq/
    
    This patch is purely cosmetic.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    
    Conflicts:
    	kernel/workqueue.c

commit 2284f8b4fa09d827b7a5e60f15b2c62fe5711ea7
Author: Tejun Heo <tj@kernel.org>
Date:   Wed Feb 13 19:29:10 2013 -0800

    workqueue: reimplement is_chained_work() using current_wq_worker()
    
    is_chained_work() was added before current_wq_worker() and implemented
    its own ham-fisted way of finding out whether %current is a workqueue
    worker - it iterates through all possible workers.
    
    Drop the custom implementation and reimplement using
    current_wq_worker().
    
    Signed-off-by: Tejun Heo <tj@kernel.org>

commit 3629d6e2f0197ce6757049ad0a8b73d2d04c2fe6
Author: Tejun Heo <tj@kernel.org>
Date:   Wed Feb 13 19:29:07 2013 -0800

    workqueue: fix is_chained_work() regression
    
    c9e7cf273f ("workqueue: move busy_hash from global_cwq to
    worker_pool") incorrectly converted is_chained_work() to use
    get_gcwq() inside for_each_gcwq_cpu() while removing get_gcwq().
    
    As cwq might not exist for all possible workqueue CPUs, @cwq can be
    NULL and the following cwq deferences can lead to oops.
    
    Fix it by using for_each_cwq_cpu() instead, which is the better one to
    use anyway as we only need to check pools that the wq is associated
    with.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>

commit d99af57faed3d80fbdb40d8689ee150c8acb3475
Author: Lai Jiangshan <laijs@cn.fujitsu.com>
Date:   Thu Feb 7 13:14:20 2013 -0800

    workqueue: pick cwq instead of pool in __queue_work()
    
    Currently, __queue_work() chooses the pool to queue a work item to and
    then determines cwq from the target wq and the chosen pool.  This is a
    bit backwards in that we can determine cwq first and simply use
    cwq->pool.  This way, we can skip get_std_worker_pool() in queueing
    path which will be a hurdle when implementing custom worker pools.
    
    Update __queue_work() such that it chooses the target cwq and then use
    cwq->pool instead of the other way around.  While at it, add missing
    {} in an if statement.
    
    This patch doesn't introduce any functional changes.
    
    tj: The original patch had two get_cwq() calls - the first to
        determine the pool by doing get_cwq(cpu, wq)->pool and the second
        to determine the matching cwq from get_cwq(pool->cpu, wq).
        Updated the function such that it chooses cwq instead of pool and
        removed the second call.  Rewrote the description.
    
    Signed-off-by: Lai Jiangshan <laijs@cn.fujitsu.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

commit 1d9554513396fccb250d2c2e1c912196edc7bbec
Author: Lai Jiangshan <laijs@cn.fujitsu.com>
Date:   Thu Feb 7 13:14:20 2013 -0800

    workqueue: make get_work_pool_id() cheaper
    
    get_work_pool_id() currently first obtains pool using get_work_pool()
    and then return pool->id.  For an off-queue work item, this involves
    obtaining pool ID from worker->data, performing idr_find() to find the
    matching pool and then returning its pool->id which of course is the
    same as the one which went into idr_find().
    
    Just open code WORK_STRUCT_CWQ case and directly return pool ID from
    work->data.
    
    tj: The original patch dropped on-queue work item handling and renamed
        the function to offq_work_pool_id().  There isn't much benefit in
        doing so.  Handling it only requires a single if() and we need at
        least BUG_ON(), which is also a branch, even if we drop on-queue
        handling.  Open code WORK_STRUCT_CWQ case and keep the function in
        line with get_work_pool().  Rewrote the description.
    
    Signed-off-by: Lai Jiangshan <laijs@cn.fujitsu.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

commit d47136b0a1af495223788db79496bdd5875c19eb
Author: Tejun Heo <tj@kernel.org>
Date:   Thu Jan 24 11:39:44 2013 -0800

    workqueue: move nr_running into worker_pool
    
    As nr_running is likely to be accessed from other CPUs during
    try_to_wake_up(), it was kept outside worker_pool; however, while less
    frequent, other fields in worker_pool are accessed from other CPUs
    for, e.g., non-reentrancy check.  Also, with recent pool related
    changes, accessing nr_running matching the worker_pool isn't as simple
    as it used to be.
    
    Move nr_running inside worker_pool.  Keep it aligned to cacheline and
    define CPU pools using DEFINE_PER_CPU_SHARED_ALIGNED().  This should
    give at least the same cacheline behavior.
    
    get_pool_nr_running() is replaced with direct pool->nr_running
    accesses.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Joonsoo Kim <js1304@gmail.com>

commit 57b1c2e4ddd9e7238f24ff6681447b10ea0fcfcd
Author: Tejun Heo <tj@kernel.org>
Date:   Wed Feb 6 18:04:53 2013 -0800

    workqueue: cosmetic update in try_to_grab_pending()
    
    With the recent is-work-queued-here test simplification, the nested
    if() in try_to_grab_pending() can be collapsed.  Collapse it.
    
    This patch is purely cosmetic.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Lai Jiangshan <laijs@cn.fujitsu.com>
    
    Conflicts:
    	kernel/workqueue.c

commit 9ed2a6ae4c222b66ce6ea0827ee91c789a5256da
Author: Lai Jiangshan <laijs@cn.fujitsu.com>
Date:   Wed Feb 6 18:04:53 2013 -0800

    workqueue: simplify is-work-item-queued-here test
    
    Currently, determining whether a work item is queued on a locked pool
    involves somewhat convoluted memory barrier dancing.  It goes like the
    following.
    
    * When a work item is queued on a pool, work->data is updated before
      work->entry is linked to the pending list with a wmb() inbetween.
    
    * When trying to determine whether a work item is currently queued on
      a pool pointed to by work->data, it locks the pool and looks at
      work->entry.  If work->entry is linked, we then do rmb() and then
      check whether work->data points to the current pool.
    
    This works because, work->data can only point to a pool if it
    currently is or were on the pool and,
    
    * If it currently is on the pool, the tests would obviously succeed.
    
    * It it left the pool, its work->entry was cleared under pool->lock,
      so if we're seeing non-empty work->entry, it has to be from the work
      item being linked on another pool.  Because work->data is updated
      before work->entry is linked with wmb() inbetween, work->data update
      from another pool is guaranteed to be visible if we do rmb() after
      seeing non-empty work->entry.  So, we either see empty work->entry
      or we see updated work->data pointin to another pool.
    
    While this works, it's convoluted, to put it mildly.  With recent
    updates, it's now guaranteed that work->data points to cwq only while
    the work item is queued and that updating work->data to point to cwq
    or back to pool is done under pool->lock, so we can simply test
    whether work->data points to cwq which is associated with the
    currently locked pool instead of the convoluted memory barrier
    dancing.
    
    This patch replaces the memory barrier based "are you still here,
    really?" test with much simpler "does work->data points to me?" test -
    if work->data points to a cwq which is associated with the currently
    locked pool, the work item is guaranteed to be queued on the pool as
    work->data can start and stop pointing to such cwq only under
    pool->lock and the start and stop coincide with queue and dequeue.
    
    tj: Rewrote the comments and description.
    
    Signed-off-by: Lai Jiangshan <laijs@cn.fujitsu.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

commit 0d420466ce148d279a8353ee848a11f1c0f252d2
Author: Lai Jiangshan <laijs@cn.fujitsu.com>
Date:   Wed Feb 6 18:04:53 2013 -0800

    workqueue: make work->data point to pool after try_to_grab_pending()
    
    We plan to use work->data pointing to cwq as the synchronization
    invariant when determining whether a given work item is on a locked
    pool or not, which requires work->data pointing to cwq only while the
    work item is queued on the associated pool.
    
    With delayed_work updated not to overload work->data for target
    workqueue recording, the only case where we still have off-queue
    work->data pointing to cwq is try_to_grab_pending() which doesn't
    update work->data after stealing a queued work item.  There's no
    reason for try_to_grab_pending() to not update work->data to point to
    the pool instead of cwq, like the normal execution does.
    
    This patch adds set_work_pool_and_keep_pending() which makes
    work->data point to pool instead of cwq but keeps the pending bit
    unlike set_work_pool_and_clear_pending() (surprise!).
    
    After this patch, it's guaranteed that only queued work items point to
    cwqs.
    
    This patch doesn't introduce any visible behavior change.
    
    tj: Renamed the new helper function to match
        set_work_pool_and_clear_pending() and rewrote the description.
    
    Signed-off-by: Lai Jiangshan <laijs@cn.fujitsu.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

commit 4354200c0a22af75d94683e33dbb80c1f496e28f
Author: Lai Jiangshan <laijs@cn.fujitsu.com>
Date:   Wed Feb 6 18:04:53 2013 -0800

    workqueue: add delayed_work->wq to simplify reentrancy handling
    
    To avoid executing the same work item from multiple CPUs concurrently,
    a work_struct records the last pool it was on in its ->data so that,
    on the next queueing, the pool can be queried to determine whether the
    work item is still executing or not.
    
    A delayed_work goes through timer before actually being queued on the
    target workqueue and the timer needs to know the target workqueue and
    CPU.  This is currently achieved by modifying delayed_work->work.data
    such that it points to the cwq which points to the target workqueue
    and the last CPU the work item was on.  __queue_delayed_work()
    extracts the last CPU from delayed_work->work.data and then combines
    it with the target workqueue to create new work.data.
    
    The only thing this rather ugly hack achieves is encoding the target
    workqueue into delayed_work->work.data without using a separate field,
    which could be a trade off one can make; unfortunately, this entangles
    work->data management between regular workqueue and delayed_work code
    by setting cwq pointer before the work item is actually queued and
    becomes a hindrance for further improvements of work->data handling.
    
    This can be easily made sane by adding a target workqueue field to
    delayed_work.  While delayed_work is used widely in the kernel and
    this does make it a bit larger (<5%), I think this is the right
    trade-off especially given the prospect of much saner handling of
    work->data which currently involves quite tricky memory barrier
    dancing, and don't expect to see any measureable effect.
    
    Add delayed_work->wq and drop the delayed_work->work.data overloading.
    
    tj: Rewrote the description.
    
    Signed-off-by: Lai Jiangshan <laijs@cn.fujitsu.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>
    
    Conflicts:
    	kernel/workqueue.c

commit cd68372706df42e79ea17d8676c55f783668b611
Author: Lai Jiangshan <laijs@cn.fujitsu.com>
Date:   Wed Feb 6 18:04:53 2013 -0800

    workqueue: make work_busy() test WORK_STRUCT_PENDING first
    
    Currently, work_busy() first tests whether the work has a pool
    associated with it and if not, considers it idle.  This works fine
    even for delayed_work.work queued on timer, as __queue_delayed_work()
    sets cwq on delayed_work.work - a queued delayed_work always has its
    cwq and thus pool associated with it.
    
    However, we're about to update delayed_work queueing and this won't
    hold.  Update work_busy() such that it tests WORK_STRUCT_PENDING
    before the associated pool.  This doesn't make any noticeable behavior
    difference now.
    
    With work_pending() test moved, the function read a lot better with
    "if (!pool)" test flipped to positive.  Flip it.
    
    While at it, lose the comment about now non-existent reentrant
    workqueues.
    
    tj: Reorganized the function and rewrote the description.
    
    Signed-off-by: Lai Jiangshan <laijs@cn.fujitsu.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

commit 8034e09a1fa119ee7d5d9d9665b64c327a91e977
Author: Lai Jiangshan <laijs@cn.fujitsu.com>
Date:   Wed Feb 6 18:04:53 2013 -0800

    workqueue: replace WORK_CPU_NONE/LAST with WORK_CPU_END
    
    Now that workqueue has moved away from gcwqs, workqueue no longer has
    the need to have a CPU identifier indicating "no cpu associated" - we
    now use WORK_OFFQ_POOL_NONE instead - and most uses of WORK_CPU_NONE
    are gone.
    
    The only left usage is as the end marker for for_each_*wq*()
    iterators, where the name WORK_CPU_NONE is confusing w/o actual
    WORK_CPU_NONE usages.  Similarly, WORK_CPU_LAST which equals
    WORK_CPU_NONE no longer makes sense.
    
    Replace both WORK_CPU_NONE and LAST with WORK_CPU_END.  This patch
    doesn't introduce any functional difference.
    
    tj: s/WORK_CPU_LAST/WORK_CPU_END/ and rewrote the description.
    
    Signed-off-by: Lai Jiangshan <laijs@cn.fujitsu.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

commit 4ba397dc1ca6114c6398d6b2ed63070348bac155
Author: Tejun Heo <tj@kernel.org>
Date:   Thu Jan 24 11:01:34 2013 -0800

    workqueue: post global_cwq removal cleanups
    
    Remove remaining references to gcwq.
    
    * __next_gcwq_cpu() steals __next_wq_cpu() name.  The original
      __next_wq_cpu() became __next_cwq_cpu().
    
    * s/for_each_gcwq_cpu/for_each_wq_cpu/
      s/for_each_online_gcwq_cpu/for_each_online_wq_cpu/
    
    * s/gcwq_mayday_timeout/pool_mayday_timeout/
    
    * s/gcwq_unbind_fn/wq_unbind_fn/
    
    * Drop references to gcwq in comments.
    
    This patch doesn't introduce any functional changes.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Reviewed-by: Lai Jiangshan <laijs@cn.fujitsu.com>

commit 7de40468d63092367bbc7658f267d7a6a11adbb8
Author: Tejun Heo <tj@kernel.org>
Date:   Thu Jan 24 11:01:34 2013 -0800

    workqueue: rename nr_running variables
    
    Rename per-cpu and unbound nr_running variables such that they match
    the pool variables.
    
    This patch doesn't introduce any functional changes.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Reviewed-by: Lai Jiangshan <laijs@cn.fujitsu.com>

commit cd6152ef3bb01cd4b38094561944c5a91cfa7a31
Author: Tejun Heo <tj@kernel.org>
Date:   Thu Jan 24 11:01:34 2013 -0800

    workqueue: remove global_cwq
    
    global_cwq is now nothing but a container for per-cpu standard
    worker_pools.  Declare the worker pools directly as
    cpu/unbound_std_worker_pools[] and remove global_cwq.
    
    * ____cacheline_aligned_in_smp moved from global_cwq to worker_pool.
      This probably would have made sense even before this change as we
      want each pool to be aligned.
    
    * get_gcwq() is replaced with std_worker_pools() which returns the
      pointer to the standard pool array for a given CPU.
    
    * __alloc_workqueue_key() updated to use get_std_worker_pool() instead
      of open-coding pool determination.
    
    This is part of an effort to remove global_cwq and make worker_pool
    the top level abstraction, which in turn will help implementing worker
    pools with user-specified attributes.
    
    v2: Joonsoo pointed out that it'd better to align struct worker_pool
        rather than the array so that every pool is aligned.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Reviewed-by: Lai Jiangshan <laijs@cn.fujitsu.com>
    Cc: Joonsoo Kim <js1304@gmail.com>

commit ec7744d381ee4c8bb5bb7041ce7426ed9f4dec18
Author: Tejun Heo <tj@kernel.org>
Date:   Thu Jan 24 11:01:34 2013 -0800

    workqueue: remove worker_pool->gcwq
    
    The only remaining user of pool->gcwq is std_worker_pool_pri().
    Reimplement it using get_gcwq() and remove worker_pool->gcwq.
    
    This is part of an effort to remove global_cwq and make worker_pool
    the top level abstraction, which in turn will help implementing worker
    pools with user-specified attributes.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Reviewed-by: Lai Jiangshan <laijs@cn.fujitsu.com>

commit 09c46b189a68bc3cdf8631cf588f0a8d5eeb2126
Author: Tejun Heo <tj@kernel.org>
Date:   Thu Jan 24 11:01:34 2013 -0800

    workqueue: replace for_each_worker_pool() with for_each_std_worker_pool()
    
    for_each_std_worker_pool() takes @cpu instead of @gcwq.
    
    This is part of an effort to remove global_cwq and make worker_pool
    the top level abstraction, which in turn will help implementing worker
    pools with user-specified attributes.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Reviewed-by: Lai Jiangshan <laijs@cn.fujitsu.com>

commit 0fef9f6326680cfe950fc3a9dec024b9204dacc9
Author: Tejun Heo <tj@kernel.org>
Date:   Thu Jan 24 11:01:33 2013 -0800

    workqueue: make freezing/thawing per-pool
    
    Instead of holding locks from both pools and then processing the pools
    together, make freezing/thwaing per-pool - grab locks of one pool,
    process it, release it and then proceed to the next pool.
    
    While this patch changes processing order across pools, order within
    each pool remains the same.  As each pool is independent, this
    shouldn't break anything.
    
    This is part of an effort to remove global_cwq and make worker_pool
    the top level abstraction, which in turn will help implementing worker
    pools with user-specified attributes.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Reviewed-by: Lai Jiangshan <laijs@cn.fujitsu.com>

commit 40fcc069addb5a0f94bb31a86707022f4aba34f4
Author: Tejun Heo <tj@kernel.org>
Date:   Thu Jan 24 11:01:33 2013 -0800

    workqueue: make hotplug processing per-pool
    
    Instead of holding locks from both pools and then processing the pools
    together, make hotplug processing per-pool - grab locks of one pool,
    process it, release it and then proceed to the next pool.
    
    rebind_workers() is updated to take and process @pool instead of @gcwq
    which results in a lot of de-indentation.  gcwq_claim_assoc_and_lock()
    and its counterpart are replaced with in-line per-pool locking.
    
    While this patch changes processing order across pools, order within
    each pool remains the same.  As each pool is independent, this
    shouldn't break anything.
    
    This is part of an effort to remove global_cwq and make worker_pool
    the top level abstraction, which in turn will help implementing worker
    pools with user-specified attributes.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Reviewed-by: Lai Jiangshan <laijs@cn.fujitsu.com>

commit b7edaa5f36f782be684f57a568d2d1a39a6d5c77
Author: Tejun Heo <tj@kernel.org>
Date:   Thu Jan 24 11:01:33 2013 -0800

    workqueue: move global_cwq->lock to worker_pool
    
    Move gcwq->lock to pool->lock.  The conversion is mostly
    straight-forward.  Things worth noting are
    
    * In many places, this removes the need to use gcwq completely.  pool
      is used directly instead.  get_std_worker_pool() is added to help
      some of these conversions.  This also leaves get_work_gcwq() without
      any user.  Removed.
    
    * In hotplug and freezer paths, the pools belonging to a CPU are often
      processed together.  This patch makes those paths hold locks of all
      pools, with highpri lock nested inside, to keep the conversion
      straight-forward.  These nested lockings will be removed by
      following patches.
    
    This is part of an effort to remove global_cwq and make worker_pool
    the top level abstraction, which in turn will help implementing worker
    pools with user-specified attributes.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Reviewed-by: Lai Jiangshan <laijs@cn.fujitsu.com>

commit aaa42da5b4d253d79f07555612f5ef5b3e0179d3
Author: Tejun Heo <tj@kernel.org>
Date:   Thu Jan 24 11:01:33 2013 -0800

    workqueue: move global_cwq->cpu to worker_pool
    
    Move gcwq->cpu to pool->cpu.  This introduces a couple places where
    gcwq->pools[0].cpu is used.  These will soon go away as gcwq is
    further reduced.
    
    This is part of an effort to remove global_cwq and make worker_pool
    the top level abstraction, which in turn will help implementing worker
    pools with user-specified attributes.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Reviewed-by: Lai Jiangshan <laijs@cn.fujitsu.com>

commit 7e749b912208abae091f6a04c2f33a03e21c8093
Author: Tejun Heo <tj@kernel.org>
Date:   Thu Jan 24 11:01:33 2013 -0800

    workqueue: move busy_hash from global_cwq to worker_pool
    
    There's no functional necessity for the two pools on the same CPU to
    share the busy hash table.  It's also likely to be a bottleneck when
    implementing pools with user-specified attributes.
    
    This patch makes busy_hash per-pool.  The conversion is mostly
    straight-forward.  Changes worth noting are,
    
    * Large block of changes in rebind_workers() is moving the block
      inside for_each_worker_pool() as now there are separate hash tables
      for each pool.  This changes the order of operations but doesn't
      break anything.
    
    * Thre for_each_worker_pool() loops in gcwq_unbind_fn() are combined
      into one.  This again changes the order of operaitons but doesn't
      break anything.
    
    This is part of an effort to remove global_cwq and make worker_pool
    the top level abstraction, which in turn will help implementing worker
    pools with user-specified attributes.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Reviewed-by: Lai Jiangshan <laijs@cn.fujitsu.com>

commit 6f3b9343152cd0e8283492d205f21e33decfb32b
Author: Tejun Heo <tj@kernel.org>
Date:   Thu Jan 24 11:01:33 2013 -0800

    workqueue: record pool ID instead of CPU in work->data when off-queue
    
    Currently, when a work item is off-queue, work->data records the CPU
    it was last on, which is used to locate the last executing instance
    for non-reentrance, flushing, etc.
    
    We're in the process of removing global_cwq and making worker_pool the
    top level abstraction.  This patch makes work->data point to the pool
    it was last associated with instead of CPU.
    
    After the previous WORK_OFFQ_POOL_CPU and worker_poo->id additions,
    the conversion is fairly straight-forward.  WORK_OFFQ constants and
    functions are modified to record and read back pool ID instead.
    worker_pool_by_id() is added to allow looking up pool from ID.
    get_work_pool() replaces get_work_gcwq(), which is reimplemented using
    get_work_pool().  get_work_pool_id() replaces work_cpu().
    
    This patch shouldn't introduce any observable behavior changes.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Reviewed-by: Lai Jiangshan <laijs@cn.fujitsu.com>

commit 1ab87b4047fa5ce1ecc54845701230d0cf660207
Author: Tejun Heo <tj@kernel.org>
Date:   Thu Jan 24 11:01:33 2013 -0800

    workqueue: add worker_pool->id
    
    Add worker_pool->id which is allocated from worker_pool_idr.  This
    will be used to record the last associated worker_pool in work->data.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Reviewed-by: Lai Jiangshan <laijs@cn.fujitsu.com>

commit 900efafaa50d6943f381c031df2ec0edc12fa283
Author: Tejun Heo <tj@kernel.org>
Date:   Thu Jan 24 11:01:33 2013 -0800

    workqueue: introduce WORK_OFFQ_CPU_NONE
    
    Currently, when a work item is off queue, high bits of its data
    encodes the last CPU it was on.  This is scheduled to be changed to
    pool ID, which will make it impossible to use WORK_CPU_NONE to
    indicate no association.
    
    This patch limits the number of bits which are used for off-queue cpu
    number to 31 (so that the max fits in an int) and uses the highest
    possible value - WORK_OFFQ_CPU_NONE - to indicate no association.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Reviewed-by: Lai Jiangshan <laijs@cn.fujitsu.com>

commit eca3adfa100c79cc86929d89e73dafbaeb73a000
Author: Tejun Heo <tj@kernel.org>
Date:   Thu Jan 24 11:01:33 2013 -0800

    workqueue: make GCWQ_FREEZING a pool flag
    
    Make GCWQ_FREEZING a pool flag POOL_FREEZING.  This patch doesn't
    change locking - FREEZING on both pools of a CPU are set or clear
    together while holding gcwq->lock.  It shouldn't cause any functional
    difference.
    
    This leaves gcwq->flags w/o any flags.  Removed.
    
    While at it, convert BUG_ON()s in freeze_workqueue_begin() and
    thaw_workqueues() to WARN_ON_ONCE().
    
    This is part of an effort to remove global_cwq and make worker_pool
    the top level abstraction, which in turn will help implementing worker
    pools with user-specified attributes.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Reviewed-by: Lai Jiangshan <laijs@cn.fujitsu.com>

commit 3948eb539f1c49db2b8021318948fa0f3022f556
Author: Tejun Heo <tj@kernel.org>
Date:   Thu Jan 24 11:01:33 2013 -0800

    workqueue: make GCWQ_DISASSOCIATED a pool flag
    
    Make GCWQ_DISASSOCIATED a pool flag POOL_DISASSOCIATED.  This patch
    doesn't change locking - DISASSOCIATED on both pools of a CPU are set
    or clear together while holding gcwq->lock.  It shouldn't cause any
    functional difference.
    
    This is part of an effort to remove global_cwq and make worker_pool
    the top level abstraction, which in turn will help implementing worker
    pools with user-specified attributes.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Reviewed-by: Lai Jiangshan <laijs@cn.fujitsu.com>
    
    Conflicts:
    	kernel/workqueue.c

commit 1f931c10b34f13ceb809904bc026eaff23e52cd2
Author: Tejun Heo <tj@kernel.org>
Date:   Thu Jan 24 11:01:33 2013 -0800

    workqueue: use std_ prefix for the standard per-cpu pools
    
    There are currently two worker pools per cpu (including the unbound
    cpu) and they are the only pools in use.  New class of pools are
    scheduled to be added and some pool related APIs will be added
    inbetween.  Call the existing pools the standard pools and prefix them
    with std_.  Do this early so that new APIs can use std_ prefix from
    the beginning.
    
    This patch doesn't introduce any functional difference.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Reviewed-by: Lai Jiangshan <laijs@cn.fujitsu.com>

commit 5f5dd0f9b46dbfe959e91d379d8f6cb1fad1d856
Author: Tejun Heo <tj@kernel.org>
Date:   Thu Jan 24 11:01:32 2013 -0800

    workqueue: unexport work_cpu()
    
    This function no longer has any external users.  Unexport it.  It will
    be removed later on.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Reviewed-by: Lai Jiangshan <laijs@cn.fujitsu.com>

commit 86aa5b681980cd62641b6d78201293a2303804c0
Author: Tejun Heo <tj@kernel.org>
Date:   Fri Jan 18 14:05:55 2013 -0800

    workqueue: move struct worker definition to workqueue_internal.h
    
    This will be used to implement an inline function to query whether
    %current is a workqueue worker and, if so, allow determining which
    work item it's executing.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>

commit 318d83a621cd89ed0dd5dcd82536c3941f0225df
Author: Tejun Heo <tj@kernel.org>
Date:   Fri Jan 18 14:05:55 2013 -0800

    workqueue: rename kernel/workqueue_sched.h to kernel/workqueue_internal.h
    
    Workqueue wants to expose more interface internal to kernel/.  Instead
    of adding a new header file, repurpose kernel/workqueue_sched.h.
    Rename it to workqueue_internal.h and add include protector.
    
    This patch doesn't introduce any functional changes.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>

commit 62429b91aea93a5f271301692ff2beeed9f6477a
Author: Tejun Heo <tj@kernel.org>
Date:   Thu Jan 17 17:16:24 2013 -0800

    workqueue: set PF_WQ_WORKER on rescuers
    
    PF_WQ_WORKER is used to tell scheduler that the task is a workqueue
    worker and needs wq_worker_sleeping/waking_up() invoked on it for
    concurrency management.  As rescuers never participate in concurrency
    management, PF_WQ_WORKER wasn't set on them.
    
    There's a need for an interface which can query whether %current is
    executing a work item and if so which.  Such interface requires a way
    to identify all tasks which may execute work items and PF_WQ_WORKER
    will be used for that.  As all normal workers always have PF_WQ_WORKER
    set, we only need to add it to rescuers.
    
    As rescuers start with WORKER_PREP but never clear it, it's always
    NOT_RUNNING and there's no need to worry about it interfering with
    concurrency management even if PF_WQ_WORKER is set; however, unlike
    normal workers, rescuers currently don't have its worker struct as
    kthread_data().  It uses the associated workqueue_struct instead.
    This is problematic as wq_worker_sleeping/waking_up() expect struct
    worker at kthread_data().
    
    This patch adds worker->rescue_wq and start rescuer kthreads with
    worker struct as kthread_data and sets PF_WQ_WORKER on rescuers.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>

commit c01067789e60c31a6265a10e8f06eb3e4b7e0a1e
Author: Tejun Heo <tj@kernel.org>
Date:   Wed Dec 19 11:24:06 2012 -0800

    workqueue: fix find_worker_executing_work() brekage from hashtable conversion
    
    42f8570f43 ("workqueue: use new hashtable implementation") incorrectly
    made busy workers hashed by the pointer value of worker instead of
    work.  This broke find_worker_executing_work() which in turn broke a
    lot of fundamental operations of workqueue - non-reentrancy and
    flushing among others.  The flush malfunction triggered warning in
    disk event code in Fengguang's automated test.
    
     write_dev_root_ (3265) used greatest stack depth: 2704 bytes left
     ------------[ cut here ]------------
     WARNING: at /c/kernel-tests/src/stable/block/genhd.c:1574 disk_clear_events+0x\
    cf/0x108()
     Hardware name: Bochs
     Modules linked in:
     Pid: 3328, comm: ata_id Not tainted 3.7.0-01930-gbff6343 #1167
     Call Trace:
      [<ffffffff810997c4>] warn_slowpath_common+0x83/0x9c
      [<ffffffff810997f7>] warn_slowpath_null+0x1a/0x1c
      [<ffffffff816aea77>] disk_clear_events+0xcf/0x108
      [<ffffffff811bd8be>] check_disk_change+0x27/0x59
      [<ffffffff822e48e2>] cdrom_open+0x49/0x68b
      [<ffffffff81ab0291>] idecd_open+0x88/0xb7
      [<ffffffff811be58f>] __blkdev_get+0x102/0x3ec
      [<ffffffff811bea08>] blkdev_get+0x18f/0x30f
      [<ffffffff811bebfd>] blkdev_open+0x75/0x80
      [<ffffffff8118f510>] do_dentry_open+0x1ea/0x295
      [<ffffffff8118f5f0>] finish_open+0x35/0x41
      [<ffffffff8119c720>] do_last+0x878/0xa25
      [<ffffffff8119c993>] path_openat+0xc6/0x333
      [<ffffffff8119cf37>] do_filp_open+0x38/0x86
      [<ffffffff81190170>] do_sys_open+0x6c/0xf9
      [<ffffffff8119021e>] sys_open+0x21/0x23
      [<ffffffff82c1c3d9>] system_call_fastpath+0x16/0x1b
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Reported-by: Fengguang Wu <fengguang.wu@intel.com>
    Cc: Sasha Levin <sasha.levin@oracle.com>

commit d8ed9b6fb5f147fcb8a1b56646db5b5c27359a94
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Dec 18 10:35:02 2012 -0800

    workqueue: consider work function when searching for busy work items
    
    To avoid executing the same work item concurrenlty, workqueue hashes
    currently busy workers according to their current work items and looks
    up the the table when it wants to execute a new work item.  If there
    already is a worker which is executing the new work item, the new item
    is queued to the found worker so that it gets executed only after the
    current execution finishes.
    
    Unfortunately, a work item may be freed while being executed and thus
    recycled for different purposes.  If it gets recycled for a different
    work item and queued while the previous execution is still in
    progress, workqueue may make the new work item wait for the old one
    although the two aren't really related in any way.
    
    In extreme cases, this false dependency may lead to deadlock although
    it's extremely unlikely given that there aren't too many self-freeing
    work item users and they usually don't wait for other work items.
    
    To alleviate the problem, record the current work function in each
    busy worker and match it together with the work item address in
    find_worker_executing_work().  While this isn't complete, it ensures
    that unrelated work items don't interact with each other and in the
    very unlikely case where a twisted wq user triggers it, it's always
    onto itself making the culprit easy to spot.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Reported-by: Andrey Isakov <andy51@gmx.ru>
    Bugzilla: https://bugzilla.kernel.org/show_bug.cgi?id=51701
    Cc: stable@vger.kernel.org

commit 16c244d126b3d327ef10d7cf1deb636c74b8f331
Author: Sasha Levin <sasha.levin@oracle.com>
Date:   Mon Dec 17 10:01:23 2012 -0500

    workqueue: use new hashtable implementation and fix ondemand
    
    Switch workqueues to use the new hashtable implementation. This reduces the
    amount of generic unrelated code in the workqueues.
    
    This patch depends on d9b482c ("hashtable: introduce a small and naive
    hashtable") which was merged in v3.6.
    
    Acked-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Sasha Levin <sasha.levin@oracle.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>
    
    Conflicts:
    	kernel/workqueue.c

commit 43e6823db8132e3126400a8aef47296befa91114
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sat May 23 21:10:43 2015 +0200

    cpufreq: governor updates, thanks to @Dormianx and @Alucard24

commit ba96d3ace714312f9a9222964fe36f6e47ce4e59
Author: Rafael J. Wysocki <rjw@sisk.pl>
Date:   Wed Jul 11 22:42:58 2012 +0200

    PM / Sleep: Fix build warning in sysfs.c for CONFIG_PM_SLEEP unset
    
    The power/async device sysfs attribute is only used if both
    CONFIG_PM_ADVANCED_DEBUG and CONFIG_PM_SLEEP are set, but the code
    implementing it doesn't depend on CONFIG_PM_SLEEP.  As a result, a
    build warning appears if CONFIG_PM_ADVANCED_DEBUG is set and
    CONFIG_PM_SLEEP is not set.
    
    Fix it by adding a #ifdef CONFIG_PM_SLEEP around the code in
    question.
    
    Signed-off-by: Rafael J. Wysocki <rjw@sisk.pl>

commit fcd4857658950f07f5dea5111884697a45150425
Author: Nishanth Menon <nm@ti.com>
Date:   Tue Mar 26 18:01:49 2013 +0000

    PM / OPP: add documentation to RCU head in struct opp
    
    commit dde8437 (PM / OPP: RCU reclaim) introduced rcu_head for
    struct opp. This aids freeing using kfree_rcu. However, we missed
    adding documentation for the same. This generates kernel doc warning:
    Warning(drivers/base/power/opp.c:70): No description found for
    parameter 'head'
    
    Add documentation as appropriate.
    
    [rjw: Changelog]
    Signed-off-by: Nishanth Menon <nm@ti.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit 5069a93528ae304acdcf7bcbc76cf7e6efd0bb3d
Author: Mark Langsdorf <mark.langsdorf@calxeda.com>
Date:   Mon Jan 28 18:26:16 2013 +0000

    PM / OPP: Export more symbols for module usage
    
    Export cpufreq helpers in OPP to make the cpufreq-core0 and highbank-cpufreq
    drivers loadable as modules.
    
    Signed-off-by: Mark Langsdorf <mark.langsdorf@calxeda.com>
    Signed-off-by: Nishanth Menon <nm@ti.com>
    Acked-by: Shawn Guo <shawn.guo@linaro.org>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit e4eeae550c2f01fe5c917ef3752ed5e1de19b9c2
Author: Nishanth Menon <nm@ti.com>
Date:   Mon Jan 28 18:26:15 2013 +0000

    PM / OPP: switch exported symbols to GPL variant
    
    We are GPLV2 library, so be clear in the symbols exported as well.
    
    Signed-off-by: Nishanth Menon <nm@ti.com>
    Acked-by: Shawn Guo <shawn.guo@linaro.org>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit 051130127a870970bd345df02461c7be71fdf442
Author: Wei Yongjun <yongjun_wei@trendmicro.com.cn>
Date:   Wed Oct 31 01:29:17 2012 +0100

    PM / OPP: using kfree_rcu() to simplify the code
    
    The callback function of call_rcu() just calls a kfree(), so we
    can use kfree_rcu() instead of call_rcu() + callback function.
    
    dpatch engine is used to auto generate this patch.
    (https://github.com/weiyj/dpatch)
    
    Signed-off-by: Wei Yongjun <yongjun_wei@trendmicro.com.cn>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Git-commit: ea83f81b489be3be268ed7fabfe8dd94bdc45a29
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Change-Id: I90fd70a0c98666269e7fb7b11b7fd5132ede2db1
    Signed-off-by: Jeremy Gebben <jgebben@codeaurora.org>

commit f3ee75b96008837c4b09b14ddd91d52f1a33ce6d
Author: Nishanth Menon <nm@ti.com>
Date:   Wed Oct 24 22:00:12 2012 +0200

    PM / OPP: predictable fail results for opp_find* functions, v2
    
    Currently the opp_find* functions return -ENODEV when:
    a) it cant find a device (e.g. request for an OPP search on device
       which was not registered)
    b) When it cant find a match for the search strategy used
    
    This makes life a little in-efficient for users such as devfreq
    to make reasonable judgement before switching search strategies.
    
    So, standardize the return results as following:
     -EINVAL for bad pointer parameters
     -ENODEV when device cannot be found
     -ERANGE when search fails
    
    This has the following benefit for devfreq implementation:
    The search fails when an unregistered device pointer is provided.
    This is a trigger to change the search direction and search for
    a better fit, however, if we cannot differentiate between a valid
    search range failure Vs an unregistered device, second search goes
    through the same fail return condition. This can be avoided by
    appropriate handling of error return code.
    
    With this change, we also fix devfreq for the improved search
    strategy with updated error code.
    
    Signed-off-by: Nishanth Menon <nm@ti.com>
    Reviewed-by: Kevin Hilman <khilman@ti.com>
    Acked-by: MyungJoo Ham <myungjoo.ham@samsung.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Git-commit: 0779726cc265805d0f7c7dd1d791fa4076b31a9a
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Change-Id: I95c9d1aaf7ad8dbaff77ab3d57a7b8fe8d599764
    Signed-off-by: Jeremy Gebben <jgebben@codeaurora.org>

commit 40114495afa8366b1212e9e690078abefd50d850
Author: Liam Girdwood <lrg@ti.com>
Date:   Tue Oct 23 01:27:44 2012 +0200

    PM / OPP: Export symbols for module usage.
    
    Export the OPP functions for use by driver modules.
    
    Cc: "Rafael J. Wysocki" <rjw@sisk.pl>
    Cc: Kevin Hilman <khilman@ti.com>
    Cc: linux-pm@vger.kernel.org
    Cc: linux-kernel@vger.kernel.org
    
    [nm@ti.com: expansion of functions exported]
    Signed-off-by: Nishanth Menon <nm@ti.com>
    Signed-off-by: Liam Girdwood <lrg@ti.com>
    Acked-by: Kevin Hilman <khilman@ti.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Git-commit: 80126ce7aeb4e187429681ef8a7785b7dcd7a348
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Change-Id: Ic55436f62564ff634dbbbfb8ef75a54470d5eff2
    Signed-off-by: Jeremy Gebben <jgebben@codeaurora.org>

commit 0f63dd7dd10d34960cf4929f8c732af8d6a0321b
Author: Vincent Guittot <vincent.guittot@linaro.org>
Date:   Tue Oct 23 01:21:49 2012 +0200

    PM / OPP: RCU reclaim
    
    synchronize_rcu() blocks the caller of opp_enable/disbale
    for a complete grace period. This blocking duration prevents
    any intensive use of the functions. Replace synchronize_rcu()
    by call_rcu() which will call our function for freeing the old
    opp element.
    
    The duration of opp_enable() and opp_disable() will be no more
    dependant of the grace period.
    
    Signed-off-by: Vincent Guittot <vincent.guittot@linaro.org>
    Reviewed-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Git-commit: dde8437d06560366d8988c92b5774039ec703864
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Change-Id: I0732cbef840ff5f26b0be55697abd99741fce7aa
    Signed-off-by: Jeremy Gebben <jgebben@codeaurora.org>

commit 8d4209b2ca8faeb4668b9ed8da6ad7cc0b6f90f5
Author: Shawn Guo <shawn.guo@linaro.org>
Date:   Wed Sep 5 01:09:12 2012 +0200

    PM / OPP: Initialize OPP table from device tree
    
    With a lot of devices booting from device tree nowadays, it requires
    that OPP table can be initialized from device tree.  The patch adds
    a helper function of_init_opp_table together with a binding doc for
    that purpose.
    
    Signed-off-by: Shawn Guo <shawn.guo@linaro.org>
    Acked-by: Rob Herring <rob.herring@calxeda.com>
    Signed-off-by: Rafael J. Wysocki <rjw@sisk.pl>
    Git-commit: b496dfbc94ab86f970ef0167eaabe51f930aa5fb
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Change-Id: Ic6919b0c58774c2e97151f0eaeef325f3f83f93e
    Signed-off-by: Jeremy Gebben <jgebben@codeaurora.org>

commit 5b06acd9c19fe1d1c36df9672437ddacbf446e59
Author: Alexandre Courbot <acourbot@nvidia.com>
Date:   Sun Feb 3 01:29:26 2013 +0900

    gpiolib: use gpio_chips list in gpiochip_find
    
    Using the GPIO chips list is much faster than parsing the entire GPIO
    number space.
    
    Signed-off-by: Alexandre Courbot <acourbot@nvidia.com>
    Reviewed-by: Linus Walleij <linus.walleij@linaro.org>
    Signed-off-by: Grant Likely <grant.likely@secretlab.ca>

commit 97636aac0f8fee46855208dab34d26fcd6cbd5df
Author: Alexandre Courbot <acourbot@nvidia.com>
Date:   Mon Feb 4 17:42:04 2013 +0900

    gpiolib: add missing braces in gpio_direction_show
    
    Add missing braces in an if..else condition.
    
    Signed-off-by: Alexandre Courbot <acourbot@nvidia.com>
    Signed-off-by: Linus Walleij <linus.walleij@linaro.org>

commit 38d9abf7f43c3a0f733e88c0db642ec90c030019
Author: Alexandre Courbot <acourbot@nvidia.com>
Date:   Sun Feb 3 01:29:25 2013 +0900

    gpiolib: use gpio_chips list in gpiolib_sysfs_init
    
    Use the small list of GPIO chips instead of parsing the whole GPIO
    number space.
    
    Signed-off-by: Alexandre Courbot <acourbot@nvidia.com>
    Reviewed-by: Linus Walleij <linus.walleij@linaro.org>
    Signed-off-by: Grant Likely <grant.likely@secretlab.ca>

commit dda6b40367cbfd8db113e010defbe1c4183ebfe4
Author: Alexandre Courbot <acourbot@nvidia.com>
Date:   Sun Feb 3 01:29:24 2013 +0900

    gpiolib: link all gpio_chips using a list
    
    Add a list member to gpio_chip that allows all chips to be parsed
    quickly. The current method requires parsing the entire GPIO integer
    space, which is painfully slow. Using a list makes many chip operations
    that involve lookup or parsing faster, and also simplifies the code. It
    is also necessary to eventually get rid of the global gpio_desc[] array.
    
    The list of gpio_chips is always ordered by base GPIO number to ensure
    chips traversal is done in the right order.
    
    Signed-off-by: Alexandre Courbot <acourbot@nvidia.com>
    Reviewed-by: Linus Walleij <linus.walleij@linaro.org>
    Signed-off-by: Grant Likely <grant.likely@secretlab.ca>

commit 0e4f4b07380e6e5d8616587c0095f05985a95b4c
Author: Theodore Ts'o <tytso@mit.edu>
Date:   Thu May 14 18:37:30 2015 -0400

    ext4: don't save the error information if the block device is read-only
    
    Google-Bug-Id: 20939131
    Signed-off-by: Theodore Ts'o <tytso@mit.edu>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit b2ba89e9d92555284a751fc086f61069a04ea2e8
Author: Stephen Smalley <sds@tycho.nsa.gov>
Date:   Tue May 19 13:59:12 2015 -0400

    selinux: enable per-file labeling for debugfs files.
    
    Add support for per-file labeling of debugfs files so that
    we can distinguish them in policy.  This is particularly
    important in Android where certain debugfs files have to be writable
    by apps and therefore the debugfs directory tree can be read and
    searched by all.
    
    Since debugfs is entirely kernel-generated, the directory tree is
    immutable by userspace, and the inodes are pinned in memory, we can
    simply use the same approach as with proc and label the inodes from
    policy based on pathname from the root of the debugfs filesystem.
    Generalize the existing labeling support used for proc and reuse it
    for debugfs too.
    
    [sds:  Back-ported to 3.10.  superblock_security_struct flags field
    is only unsigned char in 3.10 so we have to redefine SE_SBGENFS.
    However, this definition is kernel-private, not exposed to userspace
    or stored anywhere persistent.]
    
    Signed-off-by: Stephen Smalley <sds@tycho.nsa.gov>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 9fd4581ec42b8cb05e7dcd752b4585508f901d4c
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sat May 23 20:34:56 2015 +0200

     msm: Add state notifiers to existing drivers
    
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 72d14054a300fb2466852cd4bc6e6827d10079bc
Author: Pranav Vashi <neobuddy89@gmail.com>
Date:   Sun Mar 1 17:36:44 2015 +0530

    msm: msm_limiter: Restore old path for parameters
    
    * Reverted parameters to old path
      /sys/kernel/msm_limiter/*
    
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit c60b7aff796a19b01971d7fc3c3f9e04c385438f
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sat May 23 20:33:09 2015 +0200

    msm_limiter:  msm: Fix usage of FB notifiers
    
    * Before this commit, notifiers were ineffective!
    
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit fd7d28865ac28b86db75834930d9b9608ecb421e
Author: Pranav Vashi <neobuddy89@gmail.com>
Date:   Wed Feb 25 02:26:59 2015 +0530

    msm: Add MSM limiter driver
    
    * Adapted for Shamu.
    * A "swiss knife" that features per-cpu total
      control.
    
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 84fe13c7aa642f20612890f00ca3c1f667bf9f67
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sat May 23 20:27:24 2015 +0200

    cpufreq: update cpu-boost, thanks do @neobuddy89

commit a503102ea6e82f6b8befade7b135c313038d6990
Author: Junjie Wu <junjiew@codeaurora.org>
Date:   Fri Mar 20 14:41:50 2015 -0700

    PM / devfreq: governor_cpufreq: Rewrite locking to avoid deadlocks
    
    A devfreq governor store in parallel with a cpu freq update can cause
    deadlock as shown below.
    
    Assume current devfreq governor is cpufreq, and user tries to change
    to some other governor.
    
    Write to sysfs store_governor   | cpufreq driver updating cpu freq
    ------------------------------- | -----------------------------------
    echo bw_hwmon > governor        |
                                    | takes rcu_read_lock and calls all
                                    | cpufreq transition callbacks for
                                    | PRECHANGE or POSTCHANGE
                                    |
    GOV_STOP on governor_cpufreq.   |
    unregister_cpufreq() accquires  |
    state_lock mutex.               |
                                    | try to accquire same state_lock in
                                    | cpufreq_trans_notifier(). Blocked.
    unregister from cpufreq         |
    transition notifier and wait for|
    all rcu_readers to finish.      |
                                Deadlock
    
    A similar deadlock can happen with governor change and policy notifier
    callbacks.
    
    The state_lock currently protects multiple unrelated critical
    sections: registering/unregistering of cpufreq notifiers, read/writing
    the device list, and tracking the cpu states and updating device
    frequencies. There is no need for register/unregister of the cpufreq
    notifiers to be mutually excluded against the other critical sections
    using the same lock.
    
    Split state_lock into two locks to protect the register/unregister of
    cpufreq notifiers from the rest of the critical sections.
    
    Signed-off-by: Pan Fang <fangpan@codeaurora.org>
    Signed-off-by: Arun KS <arunks@codeaurora.org>
    Signed-off-by: Junjie Wu <junjiew@codeaurora.org>
    Suggested-by: Saravana Kannan <skannan@codeaurora.org>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 7d9e471ca1718a205c200208601d4f8363d992af
Author: Junjie Wu <junjiew@codeaurora.org>
Date:   Wed Nov 5 11:49:10 2014 -0800

    PM / devfreq: Fix error handling in governor_cpufreq
    
    put_online_cpus() is not called if an error occurs during cpufreq
    notification registration. Fix the error path by calling it properly.
    
    Signed-off-by: Junjie Wu <junjiew@codeaurora.org>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 61efb3ac789ac9029168b729969625eb4460aa78
Author: Vikram Mulukutla <markivx@codeaurora.org>
Date:   Mon Nov 3 21:46:58 2014 -0800

    PM / devfreq: Workaround cpufreq REMOVE_POLICY versus hotplug lock race
    
    It is not possible to ensure the synchronization of REMOVE_POLICY
    notifications with CPU hotplug lock; {get,put}_online_cpus ensures
    that hotplug cannot happen, but it is still possible to receive
    REMOVE_POLICY notifications asynchronously while checking for online
    CPUs within a {get,put}_online_cpus critical section.
    
    Account for this by detecting that we haven't yet setup a local state
    when the REMOVE_POLICY notification comes in.
    
    Signed-off-by: Vikram Mulukutla <markivx@codeaurora.org>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 6b8f8cb787ae0c86439536fa4f398a222692b89e
Author: Saravana Kannan <skannan@codeaurora.org>
Date:   Tue Sep 30 18:48:21 2014 -0700

    PM / devfreq: Add timeout feature for cpufreq governor
    
    Some devfreq devices might need their frequency to be set only for a short
    time after the CPU frequency changes.
    
    For such devices, add a "timeout" tunable that determines for how many
    milliseconds the governor sets the device frequency based on the CPU
    frequency. After "timeout" milliseconds from the CPU frequency change, the
    governor will set the device frequency back to its minimum value.
    
    Signed-off-by: Saravana Kannan <skannan@codeaurora.org>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 18dd0ff17457bc8b68099dddf38e74262c1c5887
Author: Saravana Kannan <skannan@codeaurora.org>
Date:   Fri Apr 11 18:48:38 2014 -0700

    PM / devfreq: Generic cpufreq governor
    
    This devfreq governor is a generic implementation that can scale any
    devfreq device based on the current CPU frequency of all ONLINE CPUs. It
    allows for specifying CPU freq to devfreq mapping for specific devices.
    When such a mapping is not present, it defaults to scaling the device
    frequency in proportion to the CPU frequency.
    
    Signed-off-by: Saravana Kannan <skannan@codeaurora.org>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 0df503b8b74c20170fa6d910f83c6748cacda441
Author: Venkat Gopalakrishnan <venkatg@codeaurora.org>
Date:   Thu May 1 19:15:23 2014 -0700

    block/fs: keep track of the task that dirtied the page
    
    Background writes happen in the context of a background thread.
    It is very useful to identify the actual task that generated the
    request instead of background task that submited the request.
    Hence keep track of the task when a page gets dirtied and dump
    this task info while tracing. Not all the pages in the bio are
    dirtied by the same task but most likely it will be, since the
    sectors accessed on the device must be adjacent.
    
    Signed-off-by: Venkat Gopalakrishnan <venkatg@codeaurora.org>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 391eef320c95f0a232567db2784a8dfd781243d0
Author: Matias Bjrling <m@bjorling.me>
Date:   Wed Jun 11 23:43:54 2014 +0200

    block: remove WQ_POWER_EFFICIENT from kblockd
    
    blk-mq issues async requests through kblockd. To issue a work request on
    a specific CPU, kblockd_schedule_delayed_work_on is used. However, the
    specific CPU choice may not be honored, if the power_efficient option
    for workqueues is set. blk-mq requires that we have strict per-cpu
    scheduling, so it wont work properly if kblockd is marked
    POWER_EFFICIENT and power_efficient is set.
    
    Remove the kblockd WQ_POWER_EFFICIENT flag to prevent this behavior.
    This essentially reverts part of commit 695588f9454b, which added
    the WQ_POWER_EFFICIENT marker to kblockd.
    
    Signed-off-by: Matias Bjrling <m@bjorling.me>
    Signed-off-by: Jens Axboe <axboe@fb.com>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 2923a41bd8370b417c2cd35bbb0edaf858f200a3
Author: Viresh Kumar <viresh.kumar@linaro.org>
Date:   Wed Apr 24 17:12:56 2013 +0530

    block: queue work on power efficient wq
    
    Block layer uses workqueues for multiple purposes. There is no real dependency
    of scheduling these on the cpu which scheduled them.
    
    On a idle system, it is observed that and idle cpu wakes up many times just to
    service this work. It would be better if we can schedule it on a cpu which the
    scheduler believes to be the most appropriate one.
    
    This patch replaces normal workqueues with power efficient versions.
    
    Cc: Jens Axboe <axboe@kernel.dk>
    Signed-off-by: Viresh Kumar <viresh.kumar@linaro.org>
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 8ec8698521c75146ef60b658dbbe7731d19b6c9f
Author: Sravan Kumar Ambapuram <asravan@codeaurora.org>
Date:   Tue Mar 25 15:00:54 2014 +0530

    drivercore: Avoid adding devices without pm_ops to dpm list
    
    Devices without pm_ops are not required to be added in dpm list.
    It also helps in improving suspend/resume latencies.
    
    Signed-off-by: Sravan Kumar Ambapuram <asravan@codeaurora.org>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 458c87704c9ecdfb6ca3774092152dce6f92c88a
Author: Azhar Shaikh <azhars@codeaurora.org>
Date:   Fri Oct 17 16:27:59 2014 -0700

    PM: Fix a bug in dpm_suspend_late()
    
    If there is any error while executing "late suspend" callbacks, then
    the pm_runtime_enable() will not be called.
    Hence in case of an error call pm_runtime_enable() before exiting from
    dpm_suspend_late().
    
    Signed-off-by: Azhar Shaikh <azhars@codeaurora.org>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 02fab304134b4cf4726eb8c60dca87af6f10be05
Author: Zhang Rui <rui.zhang@intel.com>
Date:   Wed May 28 15:23:35 2014 +0800

    PM / sleep: unregister wakeup source when disabling device wakeup
    
    When enabling a device' wakeup capability, a wakeup source
    is created for the device automatically. But the wakeup source
    is not unregistered when disabling the device' wakeup capability.
    
    This results in zombie wakeup sources, after devices/drivers are unregistered.
    
    Signed-off-by: Zhang Rui <rui.zhang@intel.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 9649e6da944cac2050c54dc79fcf6e38233bf9ec
Author: Mark Brown <broonie@linaro.org>
Date:   Fri May 9 19:18:08 2014 +0100

    PM / OPP: Make OPP invisible to users in Kconfig
    
    The OPP code is an in kernel library selected by its users, there is no
    no architecture code required to implement it and enabling it without a
    user just increases the kernel size. Since the users select rather than
    depend on it just remove the ability to directly set the option from
    Kconfig.
    
    Signed-off-by: Mark Brown <broonie@linaro.org>
    Acked-by: Nishanth Menon <nm@ti.com>
    Acked-by: Viresh Kumar <viresh.kumar@linaro.org>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Signed-off-by: Matt Wagantall <mattw@codeaurora.org>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit d71fac9bb6b1a0505ac910e52e1a17d710b4ab84
Author: Joe Swantek <jswantek@motorola.com>
Date:   Thu Sep 18 10:55:16 2014 -0500

    power: catch wake-up requests in suspend_again
    
    It is possible during the processing of suspend_again() that a
    wake-up request (i.e. pm_stay_awake()) be made by a driver in response
    to an outside event (i.e. hw interrupt, etc.). Currently, when
    suspend_again() requests to re-enter suspend, suspend_enter() runs
    to re-enter suspend. The pm_wakeup_pending() function is called to check
    for any wake-up requests. However, this function is essentially disabled
    because the events_check_enabled flag is set to false by the previous
    run through suspend_enter(). This causes any wake-up request made
    during the processing of suspend_again() to be delayed until the next
    time the kernel resumes sometime in the future.
    
    To resolve this issue, if suspend_again() votes to re-enter
    suspend_enter() the count of wake-up requests is re-evaluated
    and the events_check_enabled flag set correctly based on the
    presence of wake-up requests that occurred during suspend_again().
    
    Signed-off-by: Joe Swantek <jswantek@motorola.com>
    Reviewed-by: Patrick Auchter <auchter@motorola.com>
    Reviewed-by: Christopher Fries <cfries@motorola.com>
    SLTApproved: Christopher Fries <cfries@motorola.com>
    Reviewed-by: Ravi Chebolu <arc095@motorola.com>
    Reviewed-by: Fred Fettinger <fettinge@motorola.com>
    Reviewed-by: Jeffrey Carlyle <jeff.carlyle@motorola.com>
    Reviewed-by: Russell Knize <rknize@motorola.com>
    Submit-Approved: Jira Key <jirakey@motorola.com>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 48fe937c951fc15bab6af96b823814b2ee731e8f
Author: Mahesh Sivasubramanian <msivasub@codeaurora.org>
Date:   Wed Aug 27 11:42:48 2014 -0600

    qos: Pass the list of cpus with affected qos to notifer
    
    Send the list of cpus whose qos has been affected along with the changed
    value. Driver listening in for notifier can use this to apply the qos value
    for the respective cpus.
    
    Signed-off-by: Mahesh Sivasubramanian <msivasub@codeaurora.org>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 95185e3194c671c421d621b8926dcf137e559edd
Author: Praveen Chidambaram <pchidamb@codeaurora.org>
Date:   Wed May 21 16:21:31 2014 -0600

    QoS: Enhance framework to support cpu/irq specific QoS requests
    
    QoS request for CPU_DMA_LATENCY can be better optimized if the request
    can be set only for the required cpus and not all cpus. This helps save
    power on other cores, while still gauranteeing the quality of service.
    
    Enhance the QoS constraints data structures to support target value for
    each core. Requests specify if the QoS is applicable to all cores
    (default) or to a selective subset of the cores or to a core(s), that the
    IRQ is affine to.
    
    QoS requests that need to track an IRQ can be set to apply only on the
    cpus to which the IRQ's smp_affinity attribute is set to. The QoS
    framework will automatically track IRQ migration between the cores. The
    QoS is updated to be applied only to the core(s) that the IRQ has been
    migrated to.
    
    Idle and interested drivers can request a PM QoS value for a constraint
    across all cpus, or a specific cpu or a set of cpus. Separate APIs have
    been added to request for individual cpu or a cpumask.  The default
    behaviour of PM QoS is maintained i.e, requests that do not specify a
    type of the request will continue to be effected on all cores.  Requests
    that want to specify an affinity of cpu(s) or an irq, can modify the PM
    QoS request data structures by specifying the type of the request and
    either the mask of the cpus or the IRQ number depending on the type.
    Updating the request does not reset the type of the request.
    
    The userspace sysfs interface does not support CPU/IRQ affinity.
    
    Signed-off-by: Praveen Chidambaram <pchidamb@codeaurora.org>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 9ac90242b319fced4ca7890ab846f71d5fe8c6a2
Author: Praveen Chidambaram <pchidamb@codeaurora.org>
Date:   Tue May 20 12:57:14 2014 -0600

    QoS: Modify data structures and function arguments for scalability.
    
    QoS add requests uses a handle to the priority list that is used
    internally to save the request, but this does not extend well. Also,
    dev_pm_qos structure definition seems to use a list object directly.
    The 'derivative' relationship seems to be broken.
    
    Use pm_qos_request objects instead of passing around the protected
    priority list object.
    
    Signed-off-by: Praveen Chidambaram <pchidamb@codeaurora.org>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit d75928a0271bb6fcf183b3017b935570f87bb1a9
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sat May 23 02:20:53 2015 +0200

    overclock: better overclock/underclock defintions

commit f60f8025d72d77b8b3630082f8d4e3c011af73ba
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sat May 23 01:53:30 2015 +0200

    cpufreq: add new frequencys for better touchboost

commit 44671dcf1a057925db9ac22568cd351a8e66b5fa
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sat May 23 01:43:28 2015 +0200

    qseecom: revert back to stock samsung version, my updated one got some RR on my phone

commit 77f610396149b16fd166cdd79654dee8e6979c31
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sat May 23 01:01:42 2015 +0200

     cpufreq: Add a notifer chain that governors can use to report informa.
    
    .tion
    
    Some modules can benefit from getting additional information cpufreq
    governors use to make frequency switch decisions.
    This change lays down a basic framework that the governors can use
    to report additional information (Eg: CPU's load) information to
    the clients that subscribe to cpufreq govinfo notifier chain.
    
    Signed-off-by: Rohit Gupta <rohgup@codeaurora.org>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 62763c30d8783918be1353a676f1b2a4f0b41d31
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sat May 23 00:58:37 2015 +0200

     Revert "cpufreq: make the "scaling_cur_freq" sysfs entry pollable"
    
    This reverts commit ee974af.
    
    sysfs_notify of scaling_cur_freq is no longer used by userspace
    components. Since sysfs_notify contends on a single mutex, it could
    add long delay to CPU frequency scaling.
    
    Remove the sysfs_notify() to speed up CPU frequency scaling.
    
    Signed-off-by: Junjie Wu <junjiew@codeaurora.org>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit dfe7b246eec5d852c597de59b73c56a89b42c3e9
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sat May 23 00:56:26 2015 +0200

    mm: add missing documentation for ksm deffered times

commit 907fdcf45d38eb3ef1780ddfbb5a78163c7c936b
Author: Jake Weinstein <xboxlover360@gmail.com>
Date:   Sun Sep 21 18:23:00 2014 -0500

    PM_QoS: add back comments to header

commit ef09a762e0d209d38e2e70a70a4d8c1c309e355c
Author: Sai Gurrappadi <sgurrappadi@nvidia.com>
Date:   Mon Feb 24 16:52:52 2014 -0800

    power: Update QoS Max boundaries to use INT_MAX
    
    Use INT_MAX rather than LONG_MAX as on a 64 bit system, the s32's
    target_value and default_value will overflow.
    
    Bug 1432878
    
    Change-Id: Id9720e79c286802594620db65d8f50303d1a88d7
    Signed-off-by: Sai Gurrappadi <sgurrappadi@nvidia.com>
    Reviewed-on: http://git-master/r/373795
    Reviewed-by: Automatic_Commit_Validation_User
    Reviewed-by: Samuel Russell <samuelr@nvidia.com>
    Reviewed-by: Diwakar Tundlam <dtundlam@nvidia.com>

commit 3676fe2c0311efeaa3896d6a4cf1632e4d61e13a
Author: walter79 <gzwalter79@gmail.com>
Date:   Fri May 22 23:23:47 2015 +0200

    build script jactive: use 0hulk_tw501_defconfig

commit f50d202c60f55c1408ffe534e444f81b335dbd63
Author: walter79 <gzwalter79@gmail.com>
Date:   Fri May 22 23:20:41 2015 +0200

    0hulk_tw501_defconfig: fix confict with jactive
    
    it is set via jf_eur_defconfig

commit 934e4af11a62ba96e79f4c48d70349e4aadcdf4d
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Fri May 22 15:26:35 2015 +0200

    Revert "qseecom: enable ce1 clock when sending RPMB service response to tz"
    
    This reverts commit fba94bee485f47e5df3c7f6d87654d2ad5d987e0.

commit 3b47e3b518d22804a974d4ecdb0596df6a412166
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Fri May 22 15:25:01 2015 +0200

    qseecom: fix small errors

commit fba94bee485f47e5df3c7f6d87654d2ad5d987e0
Author: Zhen Kong <zkong@codeaurora.org>
Date:   Tue Jul 9 17:12:55 2013 -0700

    qseecom: enable ce1 clock when sending RPMB service response to tz
    
    TZ need to do authentication when provisioning RPMB key on secure device,
    which requires accessing crypto HW. Hence, the CE clks needs to be enabled
    by qseecom prior to sending RPMB response to tz.
    
    Change-Id: I73e3844e758ebddd57639a6d477299fe306bb538
    Signed-off-by: Zhen Kong <zkong@codeaurora.org>

commit d8c9aac979587f6ff0e165678bcf6713a0050f76
Author: AnilKumar Chimata <anilc@codeaurora.org>
Date:   Wed Jun 12 13:08:37 2013 +0530

    qseecom: Fix issues in the return path of load external elf
    
    Fixes the issues during the return path of qseecom load external
    elf function. Improper return paths are present during the error
    handling cases. Wrong goto calls results in unwanted code clean-ups
    which might create problems. This patch corrects the return paths
    during the error cases.
    
    Change-Id: Icd526448e7be355acb438fff600e3e85664f75f5
    CRs-fixed: 498153
    Signed-off-by: AnilKumar Chimata <anilc@codeaurora.org>

commit 5e8b5b662ec799c1c8ebc59a74a27759eb510c18
Author: AnilKumar Chimata <anilc@codeaurora.org>
Date:   Wed Jun 12 12:43:55 2013 +0530

    qseecom: Fix return path of qseecom load app if app exists
    
    Fixes the issue during the return path of qseecom load_app. If
    the application is already up and running then second time load
    app should return with proper error. During this process SFAB
    disable clock vote is missed, which might create problem. So
    this patch adds the appropriate disable call for the SFAB clock
    which fixes the issue.
    
    Change-Id: I744a923f7e08fea85ef5bada5538f1a28c52e4e7
    CRs-fixed: 498153
    Signed-off-by: AnilKumar Chimata <anilc@codeaurora.org>

commit 03793f6ff4a6abf5844fd3692f435f3462a664a4
Author: Hariprasad Dhalinarasimha <hnamgund@codeaurora.org>
Date:   Sun May 19 21:22:10 2013 -0700

    qseecom: Add cache functionality
    
    Flush the buffers to physical memory before invoking
    scm_call to ensure data in buffers is not stale.
    
    Change-Id: If756e12e4cceb29f3d0e1a04618a037494f31cf7
    Signed-off-by: Mona Hossain <mhossain@codeaurora.org>
    Signed-off-by: Hariprasad Dhalinarasimha <hnamgund@codeaurora.org>

commit 8907657a34c653890079a6f386889c54c5c036ee
Author: Mona Hossain <mhossain@codeaurora.org>
Date:   Tue May 28 09:06:26 2013 -0700

    qseecom: Clear modified data after processing command
    
    When a client issues a send modified request to the driver,
    the driver modifies data within the message in the same
    shared buffer accessable by user space client.
    
    The driver replaces certain fields within the message with
    the physical address of the buffer (addressed by the ION fd
    sent by user space client). This modified information with
    the physical address needs to be cleared after the command
    is processed by secure app, to prevent user space client
    from having any access or information regarding the physical
    address.
    
    This patch clears the same modified data within the message
    after the command is processed by secure app.
    
    Change-Id: I429a223fba130e614cbd03dc1aa31c7680b34889
    Signed-off-by: Mona Hossain <mhossain@codeaurora.org>

commit ad09f2f3a47c631ff34a1c4d777eaedeb0644a59
Author: Mona Hossain <mhossain@codeaurora.org>
Date:   Wed May 22 09:26:50 2013 -0700

    qseecom: Fix race condition when loading app
    
    When two clients issue a 'qseecom_start_app' request to load
    and start the same app, check_if_app_exist() call may return
    an incorrect status of the app to have been already loaded.
    
    Fix is to protect/mutex_lock the entire region that checks
    if the app is already loaded and loads the app successfully.
    
    Change-Id: I7a23ae8c8935c7c0c2e7c3c6439742d3e8fe7b42
    Signed-off-by: Mona Hossain <mhossain@codeaurora.org>
    
    Conflicts:
    	drivers/misc/qseecom.c

commit 27da345319e03555ebd785add466df4c62aa6ce9
Author: Mona Hossain <mhossain@codeaurora.org>
Date:   Sun May 19 21:27:26 2013 -0700

    qseecom: Return failure if TZ version is incorrect
    
    Remove all QSEOS_VERSION_TZ13 related support and return
    error if version is less than TZ 1.4.
    
    Change-Id: If4768c59739186d89650b5ab1160d45218797ba0
    Signed-off-by: Mona Hossain <mhossain@codeaurora.org>

commit 05a7594523f2689b312ee90cb80f91b6b38bd506
Author: Mona Hossain <mhossain@codeaurora.org>
Date:   Wed May 15 16:49:29 2013 -0700

    qseecom: Remove support for QSEOS_VERSION_TZ13
    
    Remove support for interfacing with TZ version 1.3.
    The support for this is deprecated.
    
    Change-Id: Ib02f56d207966879ad4a9d72bf9931889d3bc6ff
    Signed-off-by: Mona Hossain <mhossain@codeaurora.org>
    
    Conflicts:
    	drivers/misc/qseecom.c

commit f7f2099e1e527c56c5eb307afb56d7d6cb6146bb
Author: Zhen Kong <zkong@codeaurora.org>
Date:   Mon May 13 23:55:42 2013 -0700

    qseecom: enable ce1 clock when setting PIPE key
    
    When setting PIPE key, TZ need to read key store from ssd partition
    again and re-authenticate it. Authenticating this requires accessing
    crypto HW. Hence the CE clks needs to be enabled by qseecom prior to
    issuing the request to set/clear pipe key.
    
    Change-Id: I3472e62416c24563b429801fa0d218b0df5e7149
    Signed-off-by: Zhen Kong <zkong@codeaurora.org>

commit c71837a21f05bdf23cc4897e8d37cef0e80e18a4
Author: Mona Hossain <mhossain@codeaurora.org>
Date:   Fri Mar 29 17:28:31 2013 -0700

    qseecom: Fix issue with incomplete command exiting prematurely
    
    A process waiting on a signal can be awaken by any signal. We
    need to only continue processing when the condition of the
    wait event is met.
    
    Change-Id: Ib2102babbb505876f89b04399729e6ff5a475605
    Signed-off-by: Mona Hossain <mhossain@codeaurora.org>

commit a0ac9aa9241995a3c9952baa75dc977399470b77
Author: Zhen Kong <zkong@codeaurora.org>
Date:   Mon Apr 15 11:04:54 2013 -0700

    qseecom: change key management api to support tz with SSD key store
    
    Key management request command id and structure are changed
    to support new tz with SSD key store. PIPE type is also added
    in set key request to indicate specific PIPE register. qseecom
    checks QSEE version to see if tz supports Key management.
    
    Change-Id: I8e88214c1f762c1ce4161bcc1d2196109b16409c
    Signed-off-by: Zhen Kong <zkong@codeaurora.org>

commit 15e8a9d2bd776b981158759fa39d6fd0b07b3d6c
Author: Hariprasad Dhalinarasimha <hnamgund@codeaurora.org>
Date:   Wed Apr 10 15:13:03 2013 -0700

    qseecom: Assign app_id at the valid place.
    
    app_id was always 0 as it was getting a default value. Fix is to
    assign the app_id return value of the function
    "__qseecom_check_app_exists"
    
    Change-Id: I21b866b42c40047653a0ab111c262f269c3de943
    CRs-fixed: 473405
    Signed-off-by: Hariprasad Dhalinarasimha <hnamgund@codeaurora.org>

commit 7affae8bb0daac00d557734520087cd562c38b42
Author: Mona Hossain <mhossain@codeaurora.org>
Date:   Thu Apr 18 12:08:58 2013 -0700

    qseecom: Change log message from warning to debug
    
    Kernel logs are being inundated with warning messages
    that is trigerring watchdog bark.
    
    Change-Id: I3ee3686ed8c4ff37c934369aa0fc8e01711cdc96
    Signed-off-by: Mona Hossain <mhossain@codeaurora.org>

commit 3910eb32f6c7c7036ac5f99c3ddd9a004b981b07
Author: Mona Hossain <mhossain@codeaurora.org>
Date:   Mon Apr 15 17:20:15 2013 -0700

    qseecom: Propagate error to process incomplete cmd.
    
    When an error to process an INCOMPLETE command is encountered,
    qseecom driver returns the error to HLOS client thereby leaving
    the secure app that requested service from listener hanging.
    
    This results in further communication to QSEE to be blocked.
    In order to unblock the QSEE, so that it can accept new commands
    from HLOS client, we need to return the status of error back to
    QSEE to propagate it to secure app.
    
    Change-Id: I69251bcc39f44759c0fd9c4b64e48a824c3ec80b
    Signed-off-by: Mona Hossain <mhossain@codeaurora.org>

commit 394d6bb890774abe746747e094ad182123e17a78
Author: Amir Samuelov <amirs@codeaurora.org>
Date:   Sun Mar 10 16:56:13 2013 +0200

    qseecom: Add ioctl for enterprise-security
    
    Add ioctl to get a sw-fuse state and
    add ioctl to save a partition hash.
    
    Change-Id: I45adfec72ea79ce7bc26fac27bbaaacae86cb7d9
    Signed-off-by: Amir Samuelov <amirs@codeaurora.org>
    Signed-off-by: Elad Levi <elevi@codeaurora.org>

commit 07ee7584e2d2d9bdc1f71d3baff697a9dfee606e
Author: Mona Hossain <mhossain@codeaurora.org>
Date:   Thu Apr 11 12:43:48 2013 -0700

    qseecom: Allow a generic client to vote for clks
    
    There are some generic clients that will need to enable and
    disable clks.  We need to account for these clients accessing
    the CE clks and disable them according when the handle is
    released.
    
    This is done by making the perf_enabled, fast_load_enabled flags
    part of the GENERIC handle.
    
    Change-Id: I572f1a41768f500371bacf39ac959c51e0095efc
    Signed-off-by: Mona Hossain <mhossain@codeaurora.org>

commit 361b3762ce17a071978b23d18fd090064851ad66
Author: Mona Hossain <mhossain@codeaurora.org>
Date:   Fri Apr 5 21:56:28 2013 -0700

    qseecom: Add case for releasing GENERIC handle
    
    Releasing a handle of generic type client fails.
    Fix is to add a case for handling a GENERIC type
    qseecom client handle in the release call.
    
    Change-Id: Ic54d8ad540113fe16bb755fe7ab0b56553f454fb
    Signed-off-by: Mona Hossain <mhossain@codeaurora.org>

commit 04f00f3688bd8ad96328e18e385dee47cae092ef
Author: Mona Hossain <mhossain@codeaurora.org>
Date:   Thu Feb 14 12:06:41 2013 -0800

    qseecom: Add support for key management scheme
    
    Add IOCTLs and APIs for creating and wiping CE HW pipe Keys.
    CE HW PIPE keys are not accessible by HLOS (xpu protected
    by secure domain).  So, HLOS needs to request TZ to set/clear
    these PIPE key registers.
    
    This patch adds support for issuing commands to TZ to generate
    and wipe keys.
    
    Change-Id: I43777bf20be4f82e37ca39b81b9a9e862e47cea7
    Signed-off-by: Mona Hossain <mhossain@codeaurora.org>

commit ef302b76e62ac73b322b0992f63d3380da48ec9b
Author: Hariprasad Dhalinarasimha <hnamgund@codeaurora.org>
Date:   Mon Feb 11 15:07:59 2013 -0800

    qseecom: Add support to issue secure service cmd.
    
     - Add a generic way to issue commands to secure services running
    on the secure domain (TrustZone). This is implemented
    in the qseecom_svc_cmd() API.
    
     - This API currently supports RPMB commands to the respective
    RPMB servcie running on TZ.
    
    Change-Id: I38148b7fd2cee72146368dd1b2f902aa234847b7
    Signed-off-by: Hariprasad Dhalinarasimha <hnamgund@codeaurora.org>

commit d439f61b6f357445cddc1581e1b1374345426c64
Author: Hariprasad Dhalinarasimha <hnamgund@codeaurora.org>
Date:   Thu Feb 28 17:55:51 2013 -0800

    msm:qseecom: Add validation checks for memory cleanup for qseecom
    
    Add validation checks to handle memory freeing &
    prevent NULL pointer access.
    
    Change-Id: I7383b10037ca0ce3f049de060dc70f3ad4b9ca55
    Signed-off-by: Hariprasad Dhalinarasimha <hnamgund@codeaurora.org>

commit 1ad0cb0dedcf75374a11c95635dcfc7d0f9bebe8
Author: Mona Hossain <mhossain@codeaurora.org>
Date:   Tue Feb 5 17:38:55 2013 -0800

    qseecom: Add a status field to the listener resp structure
    
    When a client thread waiting for a response from a listener,
    gets aborted pre-maturely, we need to signal the QSEE that
    the comamnd issued by the secure app to the listener service
    was not processed succesfully.
    
    Adding status field in the listener resp type enables the
    the qseecom driver to do the above.
    
    Change-Id: I4380dc8c314877b372dc67859c1446c10c4ecbdd
    Signed-off-by: Mona Hossain <mhossain@codeaurora.org>

commit d2a2eb8e2a6ea503d7118bf5aeb1a185bfd687d8
Author: Hariprasad Dhalinarasimha <hnamgund@codeaurora.org>
Date:   Thu Jan 31 18:32:32 2013 -0800

    qseecom: Release the memory after processing INCOMPLETE_CMD
    
    Release the memory after processing the INCOMPLETE_CMD,
    as the same memory location is used for processing
    INCOMPLETE_CMD
    
    Change-Id: I9fd305b0e2f272f61b9975d02e222ff543842d7d
    Signed-off-by: Hariprasad Dhalinarasimha <hnamgund@codeaurora.org>

commit c9bd1cfa65b60435944a6612b7cab4489ec21f7c
Author: Mona Hossain <mhossain@codeaurora.org>
Date:   Wed Jan 23 18:08:45 2013 -0800

    qseecom: Process incomplete load app command
    
    Add support to process an INCOMPLETE status returned
    when a request to load app is issued to QSEE.
    
    Change-Id: If75a5726d3288812ddc6a1e23c5d968e8bc4ee1c
    Signed-off-by: Mona Hossain <mhossain@codeaurora.org>

commit 063092eb807b64abbbddd81019022bc8bd7cdc93
Author: Hariprasad Dhalinarasimha <hnamgund@codeaurora.org>
Date:   Mon Oct 1 19:56:27 2012 -0700

    msm: Change the address for Trustzone buffer log.
    
    DDR memory is used and Trustzone buffer address is changed,
    so no need to do ioremap for the location unlike previously where
    the memory was again pointing to IMEM location.
    
    Change-Id: Ia30cf45bc71d2def36350779647e4b179e3bae96
    Signed-off-by: Hariprasad Dhalinarasimha <hnamgund@codeaurora.org>

commit e28d48e41ee93c803620f9bab511d6db7bf4b3e3
Author: Mona Hossain <mhossain@codeaurora.org>
Date:   Wed Nov 21 13:33:42 2012 -0800

    qseecom: Enhance secure app logging mechanism
    
    The existing QSEE application logs are written from QSEE to HLOS
    file system requiring a QSEE -> HLOS -> QSEE context switch for
    every logged message. This change eliminates the context switch by
    writing the log messages to a shared buffer.
    
    In addition, the output from the debugfs log buffer can be stored
    on a host PC rather than on a target with limited storage capacity.
    
    Change-Id: I299370a29cf9ca583a9629c1336308bf3a42e926
    Signed-off-by: Mona Hossain <mhossain@codeaurora.org>
    
    Conflicts:
    	drivers/misc/qseecom.c

commit 8003dd57e2b005ac9f364560c435ccecfd3b3a81
Author: Mona Hossain <mhossain@codeaurora.org>
Date:   Thu Jan 3 06:11:09 2013 -0800

    qseecom: Increase max number of sg entries to 512
    
    The number of scattered entries returned by ion_sg_table()
    for a non-contiguous memory is larger than 10 in cases where the
    memory is larger.
    
    This patch fixes issues seen in some clients that issue a large
    buffer and the number of scattered segments is large.
    
    The max of 512 is calculated based on the max packet size to not
    exceed 2MB (with each segment being the minumum of 4KB).
    
    Change-Id: Id45e0212466eb9cbc260d52054670981d4fc1062
    Signed-off-by: Mona Hossain <mhossain@codeaurora.org>

commit b321248f626dcad48d60b35fb1b14cd30562f9b4
Author: Hariprasad Dhalinarasimha <hnamgund@codeaurora.org>
Date:   Wed Jan 2 10:40:40 2013 -0800

    qseecom: Change the library name.
    
    Change the name of library to be in sync with TZ change.
    
    Change-Id: Id9e7851d50cdd0eea991561abb9ba57ebbac7ca2
    Signed-off-by: Hariprasad Dhalinarasimha <hnamgund@codeaurora.org>

commit b3d0f99289fb25995b8da9f891e33abc108e93eb
Author: Mona Hossain <mhossain@codeaurora.org>
Date:   Mon Dec 3 10:10:37 2012 -0800

    qseecom: Tie performance request to client handle
    
    When a user space client application crashes without
    disabling clk vote,  there is a possibiltity of crypto
    HW being left at its high bandwidth mode.  This results
    in power drain.
    
    This patch ties the performance request to the client
    handle.  When any client handle is released (done when
    the client handle is closed explicitly or when a client
    application crashes), appropriate calls are invoked to
    disable clk voting as part of closing the handle
    
    Change-Id: I7f801d642ec53e3e5bf18a5d3a6076f973bb5175
    Signed-off-by: Mona Hossain <mhossain@codeaurora.org>

commit 21a5e13b23cd01002d40163d8581bbb843399da3
Author: Mona Hossain <mhossain@codeaurora.org>
Date:   Fri Nov 23 14:42:20 2012 -0800

    qseecom: Fix memory leak when __check_if_app_exist() fail
    
    In qseecom_start_app(), when __check_if_app_exist() returns an
    error status, the function is exited  with a -EINVAL error.
    This result in not de-allocating memory that was allocated for
    the handle prior to calling __check_if_app_exists()
    
    Memory that was allocted for the handle, needs to be de-allocated
    before exiting with an error status when __check_if_app_exists()
    return a failure.
    
    The fix is to re-order the memory allocation of the handle after
    __check_if_app_exists() goes through successfully.
    
    Change-Id: I9dde552deb2296ab719ee815314c9da161a5d1da
    Signed-off-by: Mona Hossain <mhossain@codeaurora.org>

commit 89e40d17bede58c3bb236a86050b2f9a475831ed
Author: Mona Hossain <mhossain@codeaurora.org>
Date:   Thu Nov 15 20:09:08 2012 -0800

    qseecom: Report address of secure app region to QSEE
    
    QSEE verion 2 supports relocatable images. To support this
    a chunk of contiguous memory in the first 256MB  bank is
    reserved for loading the secure applications.  The location
    and size of this memory region is defined in the device tree
    data of the driver.
    
    This patch
    - Adds supports for retrieving the address and size of the
      memory region reserved for the secure app from the device
      tree data
    - Adds a new command to issue to QSEE for notifying this
      app region information (retrieved from device tree)
    
    Change-Id: I0310815bf42c3705e0f15bd69bd4b02c6f4c56b0
    Signed-off-by: Mona Hossain <mhossain@codeaurora.org>

commit 627a31fac98dc108ee6546868805e72d48912ba8
Author: Mona Hossain <mhossain@codeaurora.org>
Date:   Mon Oct 29 17:49:01 2012 -0700

    qseecom: Add support for loading commonlib image
    
    Load the common library image on secure domain during
    initialization of the driver.
    
    All secure app loaded on the secure domain (TZ) is currently
    refactored to not include all secure libraries. The libraries
    are now a part of separate image.  This is used by all secure app.
    This common lib image needs to be  loaded before any secure app
    can be loaded.
    
    This patch adds support for handling this new change based off
    the QSEE version running on secure domain.  The change is to
    load the common library image before loading any secure app.
    
    Change-Id: I1c7692b612ea4c7bddbca1af07ddf32a073d70a8
    Signed-off-by: Mona Hossain <mhossain@codeaurora.org>
    
    Conflicts:
    	drivers/misc/qseecom.c

commit 309a2636b8949f444fd05e6ce1cfe0496b6b2819
Author: Mona Hossain <mhossain@codeaurora.org>
Date:   Thu Nov 15 19:51:33 2012 -0800

    qseecom: Support Non-contiguous ION allocated memory
    
    Get the scatter gather entries for non contiguous Ion
    allocated memory and populate the list within the command
    message with the physical address of each chunks.
    
    Change-Id: I0e8d930525f50b6aac38b0bd3e7debec9024d1f0
    Signed-off-by: Mona Hossain <mhossain@codeaurora.org>

commit b671974777196cb76ac2aa04d363458cd56fef9d
Author: Hariprasad Dhalinarasimha <hnamgund@codeaurora.org>
Date:   Sat Jul 20 04:49:34 2013 +0530

    qseecom: Change pr_warning message to pr_debug
    
    Change warning message to debug message to reduce the excessive logging.
    These changes are applicable only if any process is trying to interrupt
    the blocking listener services.
    
    Change-Id: Ie1c9af0daf16ef2c39e198f30b0151e2246d79e3
    Signed-off-by: AnilKumar Chimata <anilc@codeaurora.org>
    Signed-off-by: Hariprasad Dhalinarasimha <hnamgund@codeaurora.org>

commit c91ebd10fa275d1d4ff713b752fb2bb841ba7a11
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Fri May 22 13:28:31 2015 +0200

    fastcharge: update to latest driver from Yank-lu, big thanks to Him
    we can now hopefully control the wireless charging progress and have some more values to choose
    
    msm:qseecom: Add validation checks for memory cleanup for qseecom
    
    Add validation checks to handle memory freeing &
    prevent NULL pointer access.
    
    Change-Id: I7383b10037ca0ce3f049de060dc70f3ad4b9ca55
    Signed-off-by: Hariprasad Dhalinarasimha <hnamgund@codeaurora.org>

commit 010ad6f51554c50f24e73275392595dc51ad0729
Author: Mona Hossain <mhossain@codeaurora.org>
Date:   Mon Feb 25 09:32:33 2013 -0800

    qseecom: Fix kernel panic
    
    There exist a  possibility for "handle" input being NULL.  We need
    to check for this before dereferencing it.
    
    Change-Id: I50c1c6855ad303e05b52e6e37d774551d9db4fc4
    Signed-off-by: Mona Hossain <mhossain@codeaurora.org>

commit b94c88f70478cc9c4e501ea9bc3c88c5a517b54e
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu May 21 19:31:54 2015 +0200

    cpufreq: set the right hardlimit freqs

commit e04040a020e100998d7d5478866c01d85dc0dac1
Author: yank555-lu <yank555.lu@gmail.com>
Date:   Sun Apr 20 13:09:25 2014 +0200

    fs/dyn_sync_cntrl: dynamic sync control v1.5 (faux123)
    
    The dynamic sync control interface uses new power_suspend interface
    
    While screen is on, file sync is disabled
    when screen is off, a file sync is called to flush all outstanding writes
    and restore file sync operation as normal.
    
    updated for 8974 kernels
    
    Signed-off-by: Paul Reioux <reioux@gmail.com>;
    
    fs/dyn_sync_cntrl: keep dynamic sync control off by default (Yank555.lu)

commit 40f65f31b53dd65a09c45b09d8baf7b6a4922248
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu May 21 19:06:50 2015 +0200

    config: update all previous enabled features

commit f81460adb9186cc983232488b645e687b380d3c0
Author: Ricardo Cerqueira <cyanogenmod@cerqueira.org>
Date:   Fri Mar 28 23:42:15 2014 +0000

    leds: At disable, set the trigger to default instead of wiping it
    
    Change-Id: Ifb4a3b3034c441d29fd1d9f209ed829adfc76a72

commit 0ae9f6efdaf80c71df3bfbad1b39372702fb20cd
Author: yank555-lu <yank555.lu@gmail.com>
Date:   Wed Dec 24 14:24:52 2014 +0100

    cpufreq: CPU max. hardlimit v2.2 (Yank555.lu)
    
    - touch detection completely rewritten (now device independant, more lightweight)
    - userspace dvfs lock code updated to hammerhead (was Note 3 code)
    - slight code cleanup
    - debug output switchable in defconfig
    
    SysFs Interface :
    
     * /sys/kernel/cpufreq_hardlimit/scaling_max_freq_screen_on (rw)
     *
     *   set or show the real hard CPU max frequency limit when screen is on
     *
     * /sys/kernel/cpufreq_hardlimit/scaling_max_freq_screen_off (rw)
     *
     *   set or show the real hard CPU max frequency limit when screen is off
     *
     * /sys/kernel/cpufreq_hardlimit/scaling_min_freq_screen_on (rw)
     *
     *   set or show the real hard CPU min frequency limit when screen is on
     *
     * /sys/kernel/cpufreq_hardlimit/scaling_min_freq_screen_off (rw)
     *
     *   set or show the real hard CPU min frequency limit when screen is off
     *
     * /sys/kernel/cpufreq_hardlimit/wakeup_kick_freq (rw)
     *
     *   set or show the wakeup kick frequency (scaling_min for delay time)
     *
     * /sys/kernel/cpufreq_hardlimit/wakeup_kick_delay (rw)
     *
     *   set or show the wakeup kick duration (in ms)
     *
     * /sys/kernel/cpufreq_hardlimit/touchboost_lo_freq (rw)
     *
     *   set or show touchboost low frequency
     *
     * /sys/kernel/cpufreq_hardlimit/touchboost_hi_freq (rw)
     *
     *   set or show touchboost high frequency
     *
     * /sys/kernel/cpufreq_hardlimit/touchboost_delay (rw)
     *
     *   set or show touchboost delay (0 = disabled, up to 10000ms)
     *
     * /sys/kernel/cpufreq_hardlimit/touchboost_eventcount (rw)
     *
     *   set or show touchboost eventcount necessary to go into high frequency (1-10)
     *
     * /sys/kernel/cpufreq_hardlimit/touchinput_dev_name (ro)
     *
     *   display the used touch device name
     *
     * /sys/kernel/cpufreq_hardlimit/userspace_dvfs_lock (rw)
     *
     *   0 = allow changes to scaling min/max
     *   1 = ignore (don't apply, but don't return an error)
     *   2 = refuse (don't apply, return EINVAL)
     *
     * /sys/kernel/cpufreq_hardlimit/available_frequencies (ro)
     *
     *   display list of available CPU frequencies for convenience
     *
     * /sys/kernel/cpufreq_hardlimit/current_limit_max (ro)
     *
     *   display current applied hardlimit for CPU max
     *
     * /sys/kernel/cpufreq_hardlimit/current_limit_min (ro)
     *
     *   display current applied hardlimit for CPU min
     *
     * /sys/kernel/cpufreq_hardlimit/version (ro)
     *
     *   display CPU freq hard limit version information

commit 2812f5f3c39d30fbee3bcf19b5eade2164c976bc
Author: yank555-lu <yank555.lu@gmail.com>
Date:   Mon Jul 21 00:55:41 2014 +0200

    cpufreq: CPU max. hardlimit v2.1 (Yank555.lu)
    
    - 2 level touchboost included in Hardlimit driver
    
    SysFs Interface :
    
     * /sys/kernel/cpufreq_hardlimit/scaling_max_freq_screen_on (rw)
     *
     *   set or show the real hard CPU max frequency limit when screen is on
     *
     * /sys/kernel/cpufreq_hardlimit/scaling_max_freq_screen_off (rw)
     *
     *   set or show the real hard CPU max frequency limit when screen is off
     *
     * /sys/kernel/cpufreq_hardlimit/scaling_min_freq_screen_on (rw)
     *
     *   set or show the real hard CPU min frequency limit when screen is on
     *
     * /sys/kernel/cpufreq_hardlimit/scaling_min_freq_screen_off (rw)
     *
     *   set or show the real hard CPU min frequency limit when screen is off
     *
     * /sys/kernel/cpufreq_hardlimit/wakeup_kick_freq (rw)
     *
     *   set or show the wakeup kick frequency (scaling_min for delay time)
     *
     * /sys/kernel/cpufreq_hardlimit/wakeup_kick_delay (rw)
     *
     *   set or show the wakeup kick duration (in ms)
     *
     * /sys/kernel/cpufreq_hardlimit/touchboost_lo_freq (rw)
     *
     *   set or show touchboost low frequency
     *
     * /sys/kernel/cpufreq_hardlimit/touchboost_hi_freq (rw)
     *
     *   set or show touchboost high frequency
     *
     * /sys/kernel/cpufreq_hardlimit/touchboost_delay (rw)
     *
     *   set or show touchboost delay (0 = disabled, up to 10000ms)
     *
     * /sys/kernel/cpufreq_hardlimit/touchboost_eventcount (rw)
     *
     *   set or show touchboost eventcount necessary to go into high frequency (1-10)
     *
     * /sys/kernel/cpufreq_hardlimit/userspace_dvfs_lock (rw)
     *
     *   0 = allow changes to scaling min/max
     *   1 = ignore (don't apply, but don't return an error)
     *   2 = refuse (don't apply, return EINVAL)
     *
     * /sys/kernel/cpufreq_hardlimit/available_frequencies (ro)
     *
     *   display list of available CPU frequencies for convenience
     *
     * /sys/kernel/cpufreq_hardlimit/current_limit_max (ro)
     *
     *   display current applied hardlimit for CPU max
     *
     * /sys/kernel/cpufreq_hardlimit/current_limit_min (ro)
     *
     *   display current applied hardlimit for CPU min
     *
     * /sys/kernel/cpufreq_hardlimit/version (ro)
     *
     *   display CPU freq hard limit version information

commit a13e2609bdabc12bcab5239b17cd0c91002ec14d
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu May 21 18:33:49 2015 +0200

    cpufreq: removed yankdemand cpu_time

commit c720e869bb3b656cb4d0749fa3b9546a49171a23
Author: yank555-lu <yank555.lu@gmail.com>
Date:   Sat Jul 12 16:37:55 2014 +0200

    cpufreq: add yankdemand governor
    
    Full stock (JB) ondemand governor with changed default tunable values aimed at lower battery consumption

commit 818b47e25b7839c80a6d052d32c814b64483838d
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu May 21 18:24:46 2015 +0200

    drivers: cpufreq: add new values for intellimm and intellidemand, switch back to older intelliactive and yankactive

commit b3fc6771b2431d1684b8dc469cd720ac704db6b0
Author: Rama Krishna Phani A <rphani@codeaurora.org>
Date:   Wed May 6 17:28:20 2015 +0530

    msm: sps: Handle low memory condition in SPS driver
    
    In low memory condition some bam client is sensitive to timing
    and may get time out while allocating memory through kzalloc.
    
    Add support to hold memory during connect and disconnect
    and to release memory during client driver's deregistration
    of its BAM.
    
    Change-Id: Iaf89dc542b62c95f055e7b06d788d896809b0bf8
    Signed-off-by: Rama Krishna Phani A <rphani@codeaurora.org>

commit 63140a188f5b7bdbf508d2a606ebb2b274f30d38
Author: Dorimanx <yuri@bynet.co.il>
Date:   Thu May 21 15:18:15 2015 +0300

    MSM RQ STATS: if default hotplug is off. dont show any calc info.
    
    Now mpdecision binary will rest in peace.

commit fc21d152addc0b0d54ad8514203e596875ad549b
Author: Srinivas Kandagatla <srinivas.kandagatla@st.com>
Date:   Tue Sep 18 08:10:28 2012 +0100

    dt: introduce of_get_child_by_name to get child node by name, also update wifi drivers
    
    This patch introduces of_get_child_by_name function to get a child node
    by its name in a given parent node.
    
    Without this patch each driver code has to iterate the parent and do
    a string compare, However having of_get_child_by_name libary function would
    avoid code duplication, errors and is more convenient.
    
    Change-Id: Id33184bb9f4b494e7121d7dde719d29206dcad52
    Signed-off-by: Srinivas Kandagatla <srinivas.kandagatla@st.com>
    Signed-off-by: Rob Herring <rob.herring@calxeda.com>

commit 2ed573401ea71dd0cfc8b4fa1be120d3cd2334c4
Author: Grant Likely <grant.likely@linaro.org>
Date:   Fri Oct 4 17:24:26 2013 +0100

    of: Fix iteration bug over CPU reg properties
    
    The size of each hwid in a cpu nodes 'reg' property is defined by the
    parents #address-cells property in the normal way. The cpu parsing code
    has a bug where it will overrun the end of the property if
    address-cells is greater than one. This commit fixes the problem by
    adjusting the array size by the number of address cells. It also makes
    sure address-cells isn't zero for that would cause an infinite loop.
    
    v2: bail if #address-cells is zero instead of forcing to
        OF_ROOT_NODE_ADDR_CELLS_DEFAULT. Forcing it will cause the reg
        property to be parsed incorrectly.
    
    Signed-off-by: Grant Likely <grant.likely@linaro.org>
    Cc: Rob Herring <rob.herring@calxeda.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit bddabdf91a92ee8e28f8be6d53e43bd38e5c4d8f
Author: David Miller <davem@davemloft.net>
Date:   Thu Oct 3 17:24:51 2013 -0400

    of: Make cpu node handling more portable.
    
    Use for_each_node_by_type() to iterate all cpu nodes in the
    system.
    
    Provide and overridable function arch_find_n_match_cpu_physical_id,
    which sees if the given device node matches 'cpu' and if so sets
    '*thread' when non-NULL to the cpu thread number within the core.
    
    The default implementation behaves the same as the existing code.
    
    Add a sparc64 implementation.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>
    Tested-by: Sudeep KarkadaNagesha <Sudeep.KarkadaNagesha@arm.com>
    Signed-off-by: Grant Likely <grant.likely@linaro.org>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 864111b8044702791a6b62ebd96a2eb99c9c34e5
Author: Grant Likely <grant.likely@linaro.org>
Date:   Thu Oct 3 21:04:31 2013 +0100

    of: fix unnecessary warning on missing /cpus node
    
    Not all DT platforms have all the cpus collected under a /cpus node.
    That just happens to be a details of FDT, ePAPR and PowerPC platforms.
    Sparc does something different, but unfortunately the current code
    complains with a warning if /cpus isn't there. This became a problem
    with commit "driver/core cpu: initialize of_node in cpu's
    device structure", which caused the function to get called for all
    architectures.
    
    This commit is a temporary fix to fail silently if the cpus node isn't
    present. A proper fix will come later to allow arch code to provide a
    custom mechanism for decoding the CPU hwid if the 'reg' property isn't
    appropriate.
    
    Signed-off-by: Grant Likely <grant.likely@linaro.org>
    Cc: David Miller <davem@davemloft.net>
    Cc: Sudeep KarkadaNagesha <Sudeep.KarkadaNagesha@arm.com>
    Cc: Rob Herring <rob.herring@calxeda.com>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 752f4949e6095cd75e5fb08d118f6cd0eb8a5047
Author: Sudeep KarkadaNagesha <sudeep.karkadanagesha@arm.com>
Date:   Thu Aug 15 14:01:40 2013 +0100

    of: move of_get_cpu_node implementation to DT core library
    
    This patch moves the generalized implementation of of_get_cpu_node from
    PowerPC to DT core library, thereby adding support for retrieving cpu
    node for a given logical cpu index on any architecture.
    
    The CPU subsystem can now use this function to assign of_node in the
    cpu device while registering CPUs.
    
    It is recommended to use these helper function only in pre-SMP/early
    initialisation stages to retrieve CPU device node pointers in logical
    ordering. Once the cpu devices are registered, it can be retrieved easily
    from cpu device of_node which avoids unnecessary parsing and matching.
    
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Grant Likely <grant.likely@linaro.org>
    Acked-by: Rob Herring <rob.herring@calxeda.com>
    Signed-off-by: Sudeep KarkadaNagesha <sudeep.karkadanagesha@arm.com>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 8c4064700c1d25bd04204952cf1ddc1556eee4af
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Sun Jun 3 22:04:34 2012 -0700

    irqdomain: Simple NUMA awareness.
    
    While common irqdesc allocation is node aware, the irqdomain code is not.
    
    Presently we observe a number of regressions/inconsistencies on
    NUMA-capable platforms:
    
    - Platforms using irqdomains with legacy mappings, where the
      irq_descs are allocated node-local and the irqdomain data
      structure is not.
    
    - Drivers implementing irqdomains will lose node locality
      regardless of the underlying struct device's node id.
    
    This plugs in NUMA node id proliferation across the various allocation
    callsites by way of_node_to_nid() node lookup. While of_node_to_nid()
    does the right thing for OF-capable platforms it doesn't presently handle
    the non-DT case. This is trivially dealt with by simply wraping in to
    numa_node_id() unconditionally.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Rob Herring <rob.herring@calxeda.com>
    Signed-off-by: Grant Likely <grant.likely@secretlab.ca>

commit 73f217911125610f2b9fa678afb54cf71460ef4e
Author: Alistair Strachan <alistair.strachan@imgtec.com>
Date:   Wed Apr 10 16:35:14 2013 -0700

    sync: Fix a race condition between release_obj and print_obj
    
    Before this change, a timeline would only be removed from the timeline
    list *after* the sync driver had its release_obj() called. However, the
    driver's release_obj() may free resources needed by print_obj().
    
    Although the timeline list is locked when print_obj() is called, it is
    not locked when release_obj() is called. If one CPU was in print_obj()
    when another was in release_obj(), the print_obj() may make unsafe
    accesses.
    
    It is not actually necessary to hold the timeline list lock when calling
    release_obj() if the call is made after the timeline is unlinked from
    the list, since there is no possibility another thread could be in --
    or enter -- print_obj() for that timeline.
    
    This change moves the release_obj() call to after the timeline is
    unlinked, preventing the above race from occurring.
    
    Signed-off-by: Alistair Strachan <alistair.strachan@imgtec.com>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 8493e0f9fdb444e9c6bc0891a1ebce8b61d8e230
Author: Sudeep KarkadaNagesha <sudeep.karkadanagesha@arm.com>
Date:   Mon Jun 17 12:58:45 2013 +0100

    driver/core: cpu: initialize of_node in cpu's device struture
    
    CPUs are also registered as devices but the of_node in these cpu
    devices are not initialized. Currently different drivers requiring
    to access cpu device node are parsing the nodes themselves and
    initialising the of_node in cpu device.
    
    The of_node in all the cpu devices needs to be initialized properly
    and at one place. The best place to update this is CPU subsystem
    driver when registering the cpu devices.
    
    The OF/DT core library now provides of_get_cpu_node to retrieve a cpu
    device node for a given logical index by abstracting the architecture
    specific details.
    
    This patch uses of_get_cpu_node to assign of_node when registering the
    cpu devices.
    
    Acked-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Acked-by: Rob Herring <rob.herring@calxeda.com>
    Signed-off-by: Sudeep KarkadaNagesha <sudeep.karkadanagesha@arm.com>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 656957273523d33ca654241ff2359847f9dd2c23
Author: Yasuaki Ishimatsu <isimatu.yasuaki@jp.fujitsu.com>
Date:   Mon Apr 29 15:08:50 2013 -0700

    numa, cpu hotplug: change links of CPU and node when changing node number by onlining CPU
    
    When booting x86 system contains memoryless node, node numbers of CPUs
    on memoryless node were changed to nearest online node number by
    init_cpu_to_node() because the node is not online.
    
    In my system, node numbers of cpu#30-44 and 75-89 were changed from 2 to
    0 as follows:
    
      $ numactl --hardware
      available: 2 nodes (0-1)
      node 0 cpus: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 30 31 32 33 34 35 36 37 38 39 40
      41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 75 76 77 78 79 80 81 82
      83 84 85 86 87 88 89
      node 0 size: 32394 MB
      node 0 free: 27898 MB
      node 1 cpus: 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 60 61 62 63 64 65 66
      67 68 69 70 71 72 73 74
      node 1 size: 32768 MB
      node 1 free: 30335 MB
    
    If we hot add memory to memoryless node and offine/online all CPUs on
    the node, node numbers of these CPUs are changed to correct node numbers
    by srat_detect_node() because the node become online.
    
    In this case, node numbers of cpu#30-44 and 75-89 were changed from 0 to
    2 in my system as follows:
    
      $ numactl --hardware
      available: 3 nodes (0-2)
      node 0 cpus: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 45 46 47 48 49 50 51 52 53 54 55
      56 57 58 59
      node 0 size: 32394 MB
      node 0 free: 27218 MB
      node 1 cpus: 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 60 61 62 63 64 65 66
      67 68 69 70 71 72 73 74
      node 1 size: 32768 MB
      node 1 free: 30014 MB
      node 2 cpus: 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 75 76 77 78 79 80 81
      82 83 84 85 86 87 88 89
      node 2 size: 16384 MB
      node 2 free: 16384 MB
    
    But "cpu to node" and "node to cpu" links were not changed as follows:
    
      $ ls /sys/devices/system/cpu/cpu30/|grep node
      node0
      $ ls /sys/devices/system/node/node0/|grep cpu30
      cpu30
    
    "numactl --hardware" shows that cpu30 belongs to node 2.  But sysfs
    links does not change.
    
    This patch changes "cpu to node" and "node to cpu" links when node
    number changed by onlining CPU.
    
    Signed-off-by: Yasuaki Ishimatsu <isimatu.yasuaki@jp.fujitsu.com>
    Cc: KOSAKI Motohiro <kosaki.motohiro@gmail.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: "Srivatsa S. Bhat" <srivatsa.bhat@linux.vnet.ibm.com>
    Cc: Greg KH <greg@kroah.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 2787f0d0f805dd8ac5da2f6709c4cd0d6f71f12c
Author: Arnd Bergmann <arnd@arndb.de>
Date:   Wed Apr 3 15:18:24 2013 +0000

    sysfs: fix crash_notes_size build warning
    
    commit "sysfs: Add crash_notes_size to export percpu
    note size" adds a printk that outputs a size_t value as %lu
    when it should be %zu, resulting in this warning.
    
    drivers/base/cpu.c: In function 'show_crash_notes_size':
    drivers/base/cpu.c:142:2: warning: format '%lu' expects argument of type 'long unsigned int', but argument 3 has type 'unsigned int' [-Wformat=]
    
    Reported-by: kbuild test robot <fengguang.wu@intel.com>
    Signed-off-by: Arnd Bergmann <arnd@arndb.de>
    Cc: Zhang Yanfei <zhangyanfei@cn.fujitsu.com>
    Cc: Simon Horman <horms@verge.net.au>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 41ce12c2160a451edfbef3153ae304e5ba684c5c
Author: Zhang Yanfei <zhangyanfei@cn.fujitsu.com>
Date:   Thu Mar 28 16:15:35 2013 +0800

    sysfs: Add crash_notes_size to export percpu note size
    
    For percpu notes, we are exporting only address and not size. So
    the userspace tool kexec-tools is putting an upper limit of 1024
    and putting the value in p_memsz and p_filesz fields. So the patch
    add the new sysfile crash_notes_size to export the exact percpu
    note size and let the kexec-tools parse it intead of using 1024.
    
    The idea came from Vivek Goyal. And a later patch will be sent to
    kexec-tools to let it parse the size.
    
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Cc: Vivek Goyal <vgoyal@redhat.com>
    Signed-off-by: Zhang Yanfei <zhangyanfei@cn.fujitsu.com>
    Acked-by: Simon Horman <horms@verge.net.au>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit bbe5f33abd2acc6c5960cce08217bd27de4fedc0
Author: Ralf Baechle <ralf@linux-mips.org>
Date:   Tue Jan 15 15:27:46 2013 +0100

    drivers/base/cpu.c: Fix typo in comment
    
    [ We should make fun of people who can't speel too, but then we'd have
      no time for any real work at all  - Linus ]
    
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit aaeb25b003950904295b8ac5f4ab6f2d0b87af8b
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed May 20 23:32:47 2015 +0200

    block: tune and update I/O

commit a62a8046564815ecc73fe554b579010d68a30325
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed May 20 23:12:43 2015 +0200

    power/main: some mixed updates

commit 4fa69255c436b0038aa399f10636d44f5ee11dde
Author: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
Date:   Sat Dec 22 23:59:01 2012 +0100

    PM: Move disabling/enabling runtime PM to late suspend/early resume
    
    Currently, the PM core disables runtime PM for all devices right
    after executing subsystem/driver .suspend() callbacks for them
    and re-enables it right before executing subsystem/driver .resume()
    callbacks for them.  This may lead to problems when there are
    two devices such that the .suspend() callback executed for one of
    them depends on runtime PM working for the other.  In that case,
    if runtime PM has already been disabled for the second device,
    the first one's .suspend() won't work correctly (and analogously
    for resume).
    
    To make those issues go away, make the PM core disable runtime PM
    for devices right before executing subsystem/driver .suspend_late()
    callbacks for them and enable runtime PM for them right after
    executing subsystem/driver .resume_early() callbacks for them.  This
    way the potential conflitcs between .suspend_late()/.resume_early()
    and their runtime PM counterparts are still prevented from happening,
    but the subtle ordering issues related to disabling/enabling runtime
    PM for devices during system suspend/resume are much easier to avoid.
    
    Reported-and-tested-by: Jan-Matthias Braun <jan_braun@gmx.net>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Reviewed-by: Ulf Hansson <ulf.hansson@linaro.org>
    Reviewed-by: Kevin Hilman <khilman@deeprootsystems.com>
    Cc: 3.4+ <stable@vger.kernel.org>

commit 283fe316aa0f76955bf39f66618f9193159cf417
Author: Rafael J. Wysocki <rjw@sisk.pl>
Date:   Mon Aug 6 01:45:11 2012 +0200

    PM / Runtime: Allow helpers to be called by early platform drivers
    
    Runtime PM helper functions, like pm_runtime_get_sync(), cannot be
    called by early platform device drivers, because the devices' power
    management locks are not initialized at that time.  This is quite
    inconvenient, so modify early_platform_add_devices() to initialize
    the devices power management locks as appropriate and make sure that
    they won't be initialized more than once if an early platform
    device is going to be used as a regular one later.
    
    Signed-off-by: Rafael J. Wysocki <rjw@sisk.pl>

commit 1455e3eb9fd001d1cf6dd3d939237cb38d90f634
Author: Rafael J. Wysocki <rjw@sisk.pl>
Date:   Mon Aug 6 01:46:39 2012 +0200

    PM / Domains: Move syscore flag from subsys data to struct device
    
    The syscore device PM flag is used to mark the devices (belonging to
    a PM domain) that should never be turned off, except for the system
    core (syscore) suspend/hibernation and resume stages.  That flag is
    stored in the device's struct pm_subsys_data object whose address is
    available from struct device.  However, in some situations it may be
    convenient to set that flag before the device is added to a PM
    domain, so it is better to move it directly to the "power" member of
    struct device.  Then, it can be checked by the routines in
    drivers/base/power/runtime.c and drivers/base/power/main.c, which is
    more straightforward.
    
    This also reduces the number of dev_gpd_data() invocations in the
    generic PM domains framework, so the overhead related to the syscore
    flag is slightly smaller.
    
    Signed-off-by: Rafael J. Wysocki <rjw@sisk.pl>
    Acked-by: Magnus Damm <damm@opensource.se>
    
    Conflicts:
    	drivers/base/power/main.c

commit 2c151ca124bd77d428778c21d7800d315ed59cba
Author: Rafael J. Wysocki <rjw@sisk.pl>
Date:   Mon Aug 6 01:45:54 2012 +0200

    PM / Domains: Rename the always_on device flag to syscore
    
    The always_on device flag is used to mark the devices (belonging to
    a PM domain) that should never be turned off, except for the system
    core (syscore) suspend/hibernation and resume stages.  Change name
    of that flag to "syscore" to better reflect its purpose.
    
    Signed-off-by: Rafael J. Wysocki <rjw@sisk.pl>
    Acked-by: Magnus Damm <damm@opensource.se>

commit 85039008f58bc7a993642ef6abaf40214b0ba30c
Author: Rafael J. Wysocki <rjw@sisk.pl>
Date:   Wed Jul 11 22:42:52 2012 +0200

    PM / Domains: Fix build warning for CONFIG_PM_RUNTIME unset
    
    The functions genpd_save_dev() and genpd_restore_dev() are not used
    for CONFIG_PM_RUNTIME unset, so move them under an appropriate
    #ifdef.
    
    Signed-off-by: Rafael J. Wysocki <rjw@sisk.pl>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 4ca06b83e990cb85c6481601fea94087d5b39d5a
Author: Sachin Kamat <sachin.kamat@linaro.org>
Date:   Wed Jul 11 12:25:49 2012 +0200

    PM / Domains: Replace plain integer with NULL pointer in domain.c file
    
    Fixes the following sparse warning:
    drivers/base/power/domain.c:1679:55: warning: Using plain integer as NULL pointer
    
    Signed-off-by: Sachin Kamat <sachin.kamat@linaro.org>
    Signed-off-by: Rafael J. Wysocki <rjw@sisk.pl>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit c81463237337453d1d57fdb72d65989441edebc0
Author: Sachin Kamat <sachin.kamat@linaro.org>
Date:   Tue Jul 10 21:47:07 2012 +0200

    PM / Domains: Add missing static storage class specifier in domain.c file
    
    Fixes the folloiwng sparse warning:
    drivers/base/power/domain.c:149:5:
    warning: symbol '__pm_genpd_poweron' was not declared. Should it be static?
    
    Signed-off-by: Sachin Kamat <sachin.kamat@linaro.org>
    Signed-off-by: Rafael J. Wysocki <rjw@sisk.pl>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit acf772e91aeaab5293cb87d99cdd96a2e93b8808
Author: Rafael J. Wysocki <rjw@sisk.pl>
Date:   Thu Jul 5 22:12:54 2012 +0200

    PM / Domains: Allow device callbacks to be added at any time
    
    Make it possible to modify device callbacks used by the generic PM
    domains core code at any time, not only after the device has been
    added to a domain.  This will allow device drivers to provide their
    own device PM domain callbacks even if they are registered before
    adding the devices to PM domains.
    
    For this purpose, use the observation that the struct
    generic_pm_domain_data object containing the relevant callback
    pointers may be allocated by pm_genpd_add_callbacks() and the
    callbacks may be set before __pm_genpd_add_device() is run for
    the given device.  This object will then be used by
    __pm_genpd_add_device(), but it has to be protected from
    premature removal by reference counting.
    
    Signed-off-by: Rafael J. Wysocki <rjw@sisk.pl>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 498a02c709bb83503dce92851118617770f6ba58
Author: Rafael J. Wysocki <rjw@sisk.pl>
Date:   Thu Jul 5 22:12:32 2012 +0200

    PM / Domains: Add device domain data reference counter
    
    Add a mechanism for counting references to the
    struct generic_pm_domain_data object pointed to by
    dev->power.subsys_data->domain_data if the device in question
    belongs to a generic PM domain.
    
    This change is necessary for a subsequent patch making it possible to
    allocate that object from within pm_genpd_add_callbacks(), so that
    drivers can attach their PM domain device callbacks to devices before
    those devices are added to PM domains.
    
    This patch has been tested on the SH7372 Mackerel board.
    
    Signed-off-by: Rafael J. Wysocki <rjw@sisk.pl>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit fd248fcdbb9830fcea5ae8933d536ff80da9029b
Author: Dorimanx <yuri@bynet.co.il>
Date:   Thu Sep 25 02:41:31 2014 +0300

    PM / Domains: small correction, commit was added before.

commit 8bb66994d3317e1e6543f1ae3dadc327e708762d
Author: Rafael J. Wysocki <rjw@sisk.pl>
Date:   Sat Jun 16 00:02:34 2012 +0200

    PM / Domains: Do not stop devices after restoring their states
    
    While resuming a device belonging to a PM domain,
    pm_genpd_runtime_resume() calls __pm_genpd_restore_device() to
    restore its state, if necessary.  The latter starts the device,
    using genpd_start_dev(), restores its state, using
    genpd_restore_dev(), and then stops it, using genpd_stop_dev().
    However, this last operation is not necessary, because the
    device is supposed to be operational after pm_genpd_runtime_resume()
    has returned and because of it pm_genpd_runtime_resume() has to
    call genpd_start_dev() once again for the "restored" device, which
    is inefficient.
    
    To make things more efficient, remove the call to genpd_stop_dev()
    from __pm_genpd_restore_device() and the direct call to
    genpd_start_dev() from pm_genpd_runtime_resume().  [Of course,
    genpd_start_dev() still has to be called by it for devices with the
    power.irq_safe flag set, because __pm_genpd_restore_device() is not
    executed for them.]
    
    This change has been tested on the SH7372 Mackerel board.
    
    Signed-off-by: Rafael J. Wysocki <rjw@sisk.pl>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit ebb0bc870e1b42948c6331d26fa40bed61f98ad8
Author: Rafael J. Wysocki <rjw@sisk.pl>
Date:   Sat Jun 16 00:02:22 2012 +0200

    PM / Domains: Use subsystem runtime suspend/resume callbacks by default
    
    Currently, the default "save state" and "restore state" routines
    for generic PM domains, pm_genpd_default_save_state() and
    pm_genpd_default_restore_state(), respectively, only use runtime PM
    callbacks provided by device drivers, but in general those callbacks
    need not provide the entire necessary functionality.  Namely, in
    general it may be necessary to execute subsystem (i.e. device type,
    device class or bus type) callbacks that will carry out all of the
    necessary operations.
    
    For this reason, modify pm_genpd_default_save_state() and
    pm_genpd_default_restore_state() to execute subsystem callbacks,
    if they are provided, and fall back to driver callbacks otherwise.
    
    Signed-off-by: Rafael J. Wysocki <rjw@sisk.pl>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 8ae2f07b6af9fa243d4e5b13f8b2355d2bd9980e
Author: Rafael J. Wysocki <rjw@sisk.pl>
Date:   Mon May 14 21:45:52 2012 +0200

    PM / Domains: Make it possible to add devices to inactive domains
    
    The generic PM domains core code currently requires domains to be in
    the "power on" state for adding devices to them, but this limitation
    turns out to be inconvenient in some situations, so remove it.
    
    For this purpose, make __pm_genpd_add_device() set the device's
    need_restore flag if the domain is in the "power off" state, so that
    the device's "restore state" (usually .runtime_resume()) callback
    is executed when it is resumed after the domain has been turned on.
    If the domain is in the "power on" state, the device's need_restore
    flag will be cleared by __pm_genpd_add_device(), so that its "save
    state" (usually .runtime_suspend()) callback is executed when the
    domain is about to be turned off.  However, since that default
    behavior need not be always desirable, add a helper function
    pm_genpd_dev_need_restore() allowing a device's need_restore flag
    to be set/unset at any time.
    
    Signed-off-by: Rafael J. Wysocki <rjw@sisk.pl>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 8250b503f612bed6f93a6173eddacc2a571e74a6
Author: Huang Ying <ying.huang@intel.com>
Date:   Mon May 7 21:35:45 2012 +0200

    PM / Domains: Fix link checking when add subdomain
    
    Current pm_genpd_add_subdomain() will allow duplicated link between
    master and slave domain.  This patch fixed it.
    
    Because when current pm_genpd_add_subdomain() checks whether the link
    between the master and slave generic PM domain already exists,
    slave_links instead of master_links of master domain is used.
    
    Signed-off-by: Huang Ying <ying.huang@intel.com>
    Signed-off-by: Rafael J. Wysocki <rjw@sisk.pl>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 5639bf04a7838b944fa2b9d5afbc83cf86bdc1f9
Author: Rafael J. Wysocki <rjw@sisk.pl>
Date:   Tue May 1 21:34:07 2012 +0200

    PM / Domains: Cache device stop and domain power off governor results, v3
    
    The results of the default device stop and domain power off governor
    functions for generic PM domains, default_stop_ok() and
    default_power_down_ok(), depend only on the timing data of devices,
    which are static, and on their PM QoS constraints.  Thus, in theory,
    these functions only need to carry out their computations, which may
    be time consuming in general, when it is known that the PM QoS
    constraint of at least one of the devices in question has changed.
    
    Use the PM QoS notifiers of devices to implement that.  First,
    introduce new fields, constraint_changed and max_off_time_changed,
    into struct gpd_timing_data and struct generic_pm_domain,
    respectively, and register a PM QoS notifier function when adding
    a device into a domain that will set those fields to 'true' whenever
    the device's PM QoS constraint is modified.  Second, make
    default_stop_ok() and default_power_down_ok() use those fields to
    decide whether or not to carry out their computations from scratch.
    
    The device and PM domain hierarchies are taken into account in that
    and the expense is that the changes of PM QoS constraints of
    suspended devices will not be taken into account immediately, which
    isn't guaranteed anyway in general.
    
    Signed-off-by: Rafael J. Wysocki <rjw@sisk.pl>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 0a57231f6b6006cde7e923b2d9e74551ab4791b8
Author: Rafael J. Wysocki <rjw@sisk.pl>
Date:   Tue May 1 21:33:53 2012 +0200

    PM / Domains: Make device removal more straightforward
    
    The removal of a device from a PM domain doesn't have to browse
    the domain's device list, because it can check directly if the
    device belongs to the given domain.  Moreover, it should clear
    the domain_data pointer in dev->power.subsys_data, because
    dev_pm_put_subsys_data(dev) may not remove dev->power.subsys_data
    and the stale domain data pointer may cause problems to happen.
    
    Rework pm_genpd_remove_device() taking the above observations into
    account.
    
    Signed-off-by: Rafael J. Wysocki <rjw@sisk.pl>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 7335679bf49d7e79ade24cfd23cb5be86c3566c0
Author: Rafael J. Wysocki <rjw@sisk.pl>
Date:   Sun Apr 29 22:54:36 2012 +0200

    PM / Runtime: Remove device fields related to suspend time, v2
    
    After the previous changes in default_stop_ok() and
    default_power_down_ok() for PM domains, there are two fields in
    struct dev_pm_info that aren't necessary any more,  suspend_time
    and max_time_suspended_ns.
    
    Remove those fields along with all of the code that accesses them,
    which simplifies the runtime PM framework quite a bit.
    
    Signed-off-by: Rafael J. Wysocki <rjw@sisk.pl>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 0277585bb0f34a4aa899b2485b3cde2df0e8ca15
Author: Rafael J. Wysocki <rjw@sisk.pl>
Date:   Sun Apr 29 22:54:17 2012 +0200

    PM / Domains: Rework default device stop governor function, v2
    
    The existing default device stop governor function for PM domains,
    default_stop_ok(), is supposed to check whether or not the device's
    PM QoS latency constraint will be violated if the device is stopped
    by pm_genpd_runtime_suspend().  However, the computations carried out
    by it don't reflect the definition of the PM QoS latency constrait in
    Documentation/ABI/testing/sysfs-devices-power.
    
    Make default_stop_ok() follow the definition of the PM QoS latency
    constrait.  In particular, make it take the device's start and stop
    latencies correctly.
    
    Add a new field, effective_constraint_ns, to struct gpd_timing_data
    and use it to store the difference between the device's PM QoS
    constraint and its resume latency for use by the device's parent
    (the effective_constraint_ns values for the children are used for
    computing the parent's one along with its PM QoS constraint).
    
    Remove the break_even_ns field from struct gpd_timing_data, because
    it's not used any more.
    
    Signed-off-by: Rafael J. Wysocki <rjw@sisk.pl>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit b7cb3b62c85fb872a7b4d9c3551fd6b41370d979
Author: Rafael J. Wysocki <rjw@sisk.pl>
Date:   Sun Apr 29 22:54:30 2012 +0200

    PM / Domains: Rework default domain power off governor function, v2
    
    The existing default domain power down governor function for PM
    domains, default_power_down_ok(), is supposed to check whether or not
    the PM QoS latency constraints of the devices in the domain will be
    violated if the domain is turned off by pm_genpd_poweroff().
    However, the computations carried out by it don't reflect the
    definition of the PM QoS latency constrait in
    Documentation/ABI/testing/sysfs-devices-power.
    
    Make default_power_down_ok() follow the definition of the PM QoS
    latency constrait.  In particular, make it only take latencies into
    account, because it doesn't matter how much time has elapsed since
    the domain's devices were suspended for the computation.
    
    Remove the break_even_ns and power_off_time fields from
    struct generic_pm_domain, because they are not necessary any more.
    
    Signed-off-by: Rafael J. Wysocki <rjw@sisk.pl>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit cda35e04e15dc419259bd980b6e2550bb01ebe35
Author: Rafael J. Wysocki <rjw@sisk.pl>
Date:   Tue Jul 3 19:07:42 2012 +0200

    PM / Domains: Add preliminary support for cpuidle, v2
    
    On some systems there are CPU cores located in the same power
    domains as I/O devices.  Then, power can only be removed from the
    domain if all I/O devices in it are not in use and the CPU core
    is idle.  Add preliminary support for that to the generic PM domains
    framework.
    
    First, the platform is expected to provide a cpuidle driver with one
    extra state designated for use with the generic PM domains code.
    This state should be initially disabled and its exit_latency value
    should be set to whatever time is needed to bring up the CPU core
    itself after restoring power to it, not including the domain's
    power on latency.  Its .enter() callback should point to a procedure
    that will remove power from the domain containing the CPU core at
    the end of the CPU power transition.
    
    The remaining characteristics of the extra cpuidle state, referred to
    as the "domain" cpuidle state below, (e.g. power usage, target
    residency) should be populated in accordance with the properties of
    the hardware.
    
    Next, the platform should execute genpd_attach_cpuidle() on the PM
    domain containing the CPU core.  That will cause the generic PM
    domains framework to treat that domain in a special way such that:
    
     * When all devices in the domain have been suspended and it is about
       to be turned off, the states of the devices will be saved, but
       power will not be removed from the domain.  Instead, the "domain"
       cpuidle state will be enabled so that power can be removed from
       the domain when the CPU core is idle and the state has been chosen
       as the target by the cpuidle governor.
    
     * When the first I/O device in the domain is resumed and
       __pm_genpd_poweron(() is called for the first time after
       power has been removed from the domain, the "domain" cpuidle
       state will be disabled to avoid subsequent surprise power removals
       via cpuidle.
    
    The effective exit_latency value of the "domain" cpuidle state
    depends on the time needed to bring up the CPU core itself after
    restoring power to it as well as on the power on latency of the
    domain containing the CPU core.  Thus the "domain" cpuidle state's
    exit_latency has to be recomputed every time the domain's power on
    latency is updated, which may happen every time power is restored
    to the domain, if the measured power on latency is greater than
    the latency stored in the corresponding generic_pm_domain structure.
    
    Signed-off-by: Rafael J. Wysocki <rjw@sisk.pl>
    Reviewed-by: Kevin Hilman <khilman@ti.com>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>
    
    Conflicts:
    	drivers/base/power/domain.c
    	include/linux/pm_domain.h

commit 9ef7b941fd417221c36414c7578d34e273b9ea90
Author: ShuoX Liu <shuox.liu@intel.com>
Date:   Tue Jul 3 19:05:31 2012 +0200

    cpuidle: move field disable from per-driver to per-cpu
    
    Andrew J.Schorr raises a question.  When he changes the disable setting on
    a single CPU, it affects all the other CPUs.  Basically, currently, the
    disable field is per-driver instead of per-cpu.  All the C states of the
    same driver are shared by all CPU in the same machine.
    
    The patch changes the `disable' field to per-cpu, so we could set this
    separately for each cpu.
    
    Signed-off-by: ShuoX Liu <shuox.liu@intel.com>
    Reported-by: Andrew J.Schorr <aschorr@telemetry-investments.com>
    Reviewed-by: Yanmin Zhang <yanmin_zhang@intel.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Rafael J. Wysocki <rjw@sisk.pl>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 8e58a81116cea8e483e0761ceefda972b59a3bdb
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed May 20 22:29:49 2015 +0200

    cpufreq: because of new intellidemand, fix some small stuff

commit a1951221d8e3e705c517eb1901f08464692cd1e5
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed May 20 22:25:53 2015 +0200

    config: add new debug mode for powersuspend, wakeup.c gapps removed

commit 8b23aac17589dff495e4fcf342f4d6504753febc
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed May 20 21:15:49 2015 +0200

    powersuspend: blank space cleanup

commit f0fd73d9e2eac351d2728354acdb7bb38dbb2e18
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed May 20 21:14:14 2015 +0200

     kernel/power/powersuspend: new PM kernel driver for Android w/o early
    
    _suspend v1.7 (faux123/Yank555.lu)
    
    - do only run state change if change actually requests a new state

commit 4f892976295fab52c2fc80cce6fa6270140bfa26
Author: pchiang <pchiang@nvidia.com>
Date:   Fri Jan 17 17:51:49 2014 +0800

    PM / QoS: check NULL and ENODEV on power.qos
    
    fix kernel panic when qos is not null
    
    Bug 1442069
    Bug 1430231
    
    Change-Id: I8810009e8f2a72730cfb253b245434d9e0806b9c
    Signed-off-by: pchiang <pchiang@nvidia.com>
    Reviewed-on: http://git-master/r/357069
    Reviewed-by: Terje Bergstrom <tbergstrom@nvidia.com>
    Reviewed-on: http://git-master/r/360175
    GVS: Gerrit_Virtual_Submit

commit 79956d8304b92fee0a9553575e9d69aa1647480d
Author: Bryan Wu <pengw@nvidia.com>
Date:   Thu Dec 19 18:24:12 2013 -0800

    PM / QoS: cleanup FLAGS notifiers during removing
    
    DEV_PM_QOS_FLAGS was introduced in commit
    c92b96f94cdc26e33ea490a9a0e02c16988a6dd6, but it was forgot to remove
    in dev_pm_qos_remove_notifier().
    
    So without this patch, kernel will enter an endless loop in
    notifier_chain_register() when reloading a module multiple times.
    
    Bug 1421146
    
    Change-Id: I6b98fe57a70f172d110cc5f257ce4f853a447802
    Signed-off-by: Bryan Wu <pengw@nvidia.com>
    Reviewed-on: http://git-master/r/347823
    Reviewed-by: Terje Bergstrom <tbergstrom@nvidia.com>

commit cc4d9a3a1c0a8d6a495edff2c65a66039ede591d
Author: Terje Bergstrom <tbergstrom@nvidia.com>
Date:   Wed Oct 9 15:21:42 2013 +0300

    PM / QoS: Add notifier for flags
    
    dev_pm_qos has a notifier for DEV_PM_QOS_LATENCY. Add a similar
    notifier for DEV_PM_QOS_FLAGS.
    
    Bug 1364240
    
    Change-Id: Ica4c58708855938818a1e75896503b9023b96573
    Signed-off-by: Terje Bergstrom <tbergstrom@nvidia.com>
    Reviewed-on: http://git-master/r/288810

commit f33c27386df89430f548f1c74a71035614363a84
Author: Terje Bergstrom <tbergstrom@nvidia.com>
Date:   Wed Oct 9 11:33:38 2013 +0300

    PM / QoS: add dev_pm_qos_update_request_timeout()
    
    Add dev_pm_qos_update_request_timeout() that works in the same way as
    pm_qos_request_timeout().
    
    Bug 1364240
    
    Change-Id: I9da700df443f48099eac929055e9fe2db4c2f540
    Signed-off-by: Terje Bergstrom <tbergstrom@nvidia.com>
    Reviewed-on: http://git-master/r/288808

commit b5d27032cb528422ca7ddb437ef5ee61c6c254be
Author: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
Date:   Tue Apr 2 01:25:24 2013 +0200

    PM / QoS: Avoid possible deadlock related to sysfs access
    
    Commit b81ea1b (PM / QoS: Fix concurrency issues and memory leaks in
    device PM QoS) put calls to pm_qos_sysfs_add_latency(),
    pm_qos_sysfs_add_flags(), pm_qos_sysfs_remove_latency(), and
    pm_qos_sysfs_remove_flags() under dev_pm_qos_mtx, which was a
    mistake, because it may lead to deadlocks in some situations.
    For example, if pm_qos_remote_wakeup_store() is run in parallel
    with dev_pm_qos_constraints_destroy(), they may deadlock in the
    following way:
    
     ======================================================
     [ INFO: possible circular locking dependency detected ]
     3.9.0-rc4-next-20130328-sasha-00014-g91a3267 #319 Tainted: G        W
     -------------------------------------------------------
     trinity-child6/12371 is trying to acquire lock:
      (s_active#54){++++.+}, at: [<ffffffff81301631>] sysfs_addrm_finish+0x31/0x60
    
     but task is already holding lock:
      (dev_pm_qos_mtx){+.+.+.}, at: [<ffffffff81f07cc3>] dev_pm_qos_constraints_destroy+0x23/0x250
    
     which lock already depends on the new lock.
    
     the existing dependency chain (in reverse order) is:
    
     -> #1 (dev_pm_qos_mtx){+.+.+.}:
            [<ffffffff811811da>] lock_acquire+0x1aa/0x240
            [<ffffffff83dab809>] __mutex_lock_common+0x59/0x5e0
            [<ffffffff83dabebf>] mutex_lock_nested+0x3f/0x50
            [<ffffffff81f07f2f>] dev_pm_qos_update_flags+0x3f/0xc0
            [<ffffffff81f05f4f>] pm_qos_remote_wakeup_store+0x3f/0x70
            [<ffffffff81efbb43>] dev_attr_store+0x13/0x20
            [<ffffffff812ffdaa>] sysfs_write_file+0xfa/0x150
            [<ffffffff8127f2c1>] __kernel_write+0x81/0x150
            [<ffffffff812afc2d>] write_pipe_buf+0x4d/0x80
            [<ffffffff812af57c>] splice_from_pipe_feed+0x7c/0x120
            [<ffffffff812afa25>] __splice_from_pipe+0x45/0x80
            [<ffffffff812b14fc>] splice_from_pipe+0x4c/0x70
            [<ffffffff812b1538>] default_file_splice_write+0x18/0x30
            [<ffffffff812afae3>] do_splice_from+0x83/0xb0
            [<ffffffff812afb2e>] direct_splice_actor+0x1e/0x20
            [<ffffffff812b0277>] splice_direct_to_actor+0xe7/0x200
            [<ffffffff812b15bc>] do_splice_direct+0x4c/0x70
            [<ffffffff8127eda9>] do_sendfile+0x169/0x300
            [<ffffffff8127ff94>] SyS_sendfile64+0x64/0xb0
            [<ffffffff83db7d18>] tracesys+0xe1/0xe6
    
     -> #0 (s_active#54){++++.+}:
            [<ffffffff811800cf>] __lock_acquire+0x15bf/0x1e50
            [<ffffffff811811da>] lock_acquire+0x1aa/0x240
            [<ffffffff81300aa2>] sysfs_deactivate+0x122/0x1a0
            [<ffffffff81301631>] sysfs_addrm_finish+0x31/0x60
            [<ffffffff812ff77f>] sysfs_hash_and_remove+0x7f/0xb0
            [<ffffffff813035a1>] sysfs_unmerge_group+0x51/0x70
            [<ffffffff81f068f4>] pm_qos_sysfs_remove_flags+0x14/0x20
            [<ffffffff81f07490>] __dev_pm_qos_hide_flags+0x30/0x70
            [<ffffffff81f07cd5>] dev_pm_qos_constraints_destroy+0x35/0x250
            [<ffffffff81f06931>] dpm_sysfs_remove+0x11/0x50
            [<ffffffff81efcf6f>] device_del+0x3f/0x1b0
            [<ffffffff81efd128>] device_unregister+0x48/0x60
            [<ffffffff82d4083c>] usb_hub_remove_port_device+0x1c/0x20
            [<ffffffff82d2a9cd>] hub_disconnect+0xdd/0x160
            [<ffffffff82d36ab7>] usb_unbind_interface+0x67/0x170
            [<ffffffff81f001a7>] __device_release_driver+0x87/0xe0
            [<ffffffff81f00559>] device_release_driver+0x29/0x40
            [<ffffffff81effc58>] bus_remove_device+0x148/0x160
            [<ffffffff81efd07f>] device_del+0x14f/0x1b0
            [<ffffffff82d344f9>] usb_disable_device+0xf9/0x280
            [<ffffffff82d34ff8>] usb_set_configuration+0x268/0x840
            [<ffffffff82d3a7fc>] usb_remove_store+0x4c/0x80
            [<ffffffff81efbb43>] dev_attr_store+0x13/0x20
            [<ffffffff812ffdaa>] sysfs_write_file+0xfa/0x150
            [<ffffffff8127f71d>] do_loop_readv_writev+0x4d/0x90
            [<ffffffff8127f999>] do_readv_writev+0xf9/0x1e0
            [<ffffffff8127faba>] vfs_writev+0x3a/0x60
            [<ffffffff8127fc60>] SyS_writev+0x50/0xd0
            [<ffffffff83db7d18>] tracesys+0xe1/0xe6
    
     other info that might help us debug this:
    
      Possible unsafe locking scenario:
    
            CPU0                    CPU1
            ----                    ----
       lock(dev_pm_qos_mtx);
                                    lock(s_active#54);
                                    lock(dev_pm_qos_mtx);
       lock(s_active#54);
    
      *** DEADLOCK ***
    
    To avoid that, remove the calls to functions mentioned above from
    under dev_pm_qos_mtx and introduce a separate lock to prevent races
    between functions that add or remove device PM QoS sysfs attributes
    from happening.
    
    Reported-by: Sasha Levin <sasha.levin@oracle.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit d67501e558acbcf5d18da62a2046ead7a64a8097
Author: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
Date:   Mon Mar 4 14:22:57 2013 +0100

    PM / QoS: Remove device PM QoS sysfs attributes at the right place
    
    Device PM QoS sysfs attributes, if present during device removal,
    are removed from within device_pm_remove(), which is too late,
    since dpm_sysfs_remove() has already removed the whole attribute
    group they belonged to.  However, moving the removal of those
    attributes to dpm_sysfs_remove() alone is not sufficient, because
    in theory they still can be re-added right after being removed by it
    (the device's driver is still bound to it at that point).
    
    For this reason, move the entire desctruction of device PM QoS
    constraints to dpm_sysfs_remove() and make it prevent any new
    constraints from being added after it has run.  Also, move the
    initialization of the power.qos field in struct device to
    device_pm_init_common() and drop the no longer needed
    dev_pm_qos_constraints_init().
    
    Reported-by: Sasha Levin <sasha.levin@oracle.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit 7ce6af0a23cb33f1efdbb9eb4693fdf0a411cdb2
Author: Rafael J. Wysocki <rjw@sisk.pl>
Date:   Mon Aug 6 01:45:11 2012 +0200

    PM / Runtime: Allow helpers to be called by early platform drivers
    
    Runtime PM helper functions, like pm_runtime_get_sync(), cannot be
    called by early platform device drivers, because the devices' power
    management locks are not initialized at that time.  This is quite
    inconvenient, so modify early_platform_add_devices() to initialize
    the devices power management locks as appropriate and make sure that
    they won't be initialized more than once if an early platform
    device is going to be used as a regular one later.
    
    Signed-off-by: Rafael J. Wysocki <rjw@sisk.pl>

commit b8eae0cc8939bc58571bc575fa92e0c127876c72
Author: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
Date:   Sun Mar 3 22:48:14 2013 +0100

    PM / QoS: Fix concurrency issues and memory leaks in device PM QoS
    
    The current device PM QoS code assumes that certain functions will
    never be called in parallel with each other (for example, it is
    assumed that dev_pm_qos_expose_flags() won't be called in parallel
    with dev_pm_qos_hide_flags() for the same device and analogously
    for the latency limit), which may be overly optimistic.  Moreover,
    dev_pm_qos_expose_flags() and dev_pm_qos_expose_latency_limit()
    leak memory in error code paths (req needs to be freed on errors)
    and __dev_pm_qos_drop_user_request() forgets to free the request.
    
    To fix the above issues put more things under the device PM QoS
    mutex to make them mutually exclusive and add the missing freeing
    of memory.
    
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit 91e2a243b79c4b7ed13a72a94f0bbceb33864fdb
Author: Lan Tianyu <tianyu.lan@intel.com>
Date:   Wed Jan 23 04:26:28 2013 +0800

    PM/Qos: Expose dev_pm_qos_flags symbol
    
    The dev_pm_qos_flags() will be used in the usb core which could be
    compiled as a module. This patch is to export it.
    
    Acked-by: Alan Stern <stern@rowland.harvard.edu>
    Signed-off-by: Lan Tianyu <tianyu.lan@intel.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit a8151728d87cd7a141cfa412e9d981a4627fa702
Author: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
Date:   Tue Dec 18 14:07:49 2012 +0100

    PM / QoS: Rename local variable in dev_pm_qos_add_ancestor_request()
    
    Local variable 'error' in dev_pm_qos_add_ancestor_request() need
    not contain error codes only, so rename it to 'ret'.
    
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit faed94f566d0db120fee81e48b1d143d67a16f55
Author: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
Date:   Sat Nov 24 10:10:51 2012 +0100

    PM / QoS: Handle device PM QoS flags while removing constraints
    
    PM QoS flags have to be handled by dev_pm_qos_constraints_destroy()
    in the same way as PM QoS resume latency constraints.  That is, if
    they have been exposed to user space, they have to be hidden from it
    and the list of flags requests has to be flushed before destroying
    the device's PM QoS object.  Make that happen.
    
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit f4d8c2a52772a21b180192eef0769a9e45ea6112
Author: Lan Tianyu <tianyu.lan@intel.com>
Date:   Thu Nov 8 11:14:08 2012 +0800

    PM / QoS: Resume device before exposing/hiding PM QoS flags
    
    Since dev_pm_qos_add_request(), dev_pm_qos_update_request() and
    dev_pm_qos_remove_request() for PM QoS flags should not be invoked
    when device in RPM_SUSPENDED, add pm_runtime_get_sync() and pm_runtime_put()
    around these functions in dev_pm_qos_expose_flags() and
    dev_pm_qos_hide_flags().
    
    [rjw: Modified the subject and changelog to better reflect the code
     changes made.]
    
    Signed-off-by: Lan Tianyu <tianyu.lan@intel.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit 2e48b5e5ab70231e0052224b071d18e08c8478f2
Author: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
Date:   Fri Nov 2 13:10:09 2012 +0100

    PM / QoS: Document request manipulation requirement for flags
    
    In fact, the callers of dev_pm_qos_add_request(),
    dev_pm_qos_update_request() and dev_pm_qos_remove_request() for
    requests of type DEV_PM_QOS_FLAGS need to ensure that the target
    device is not RPM_SUSPENDED before using any of these functions (or
    be prepared for the new PM QoS flags to take effect after the device
    has been resumed).  Document this in their kerneldoc comments.
    
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit d1c4ab70521a35e99cfbfc9e202c58c068489091
Author: Lan,Tianyu <tianyu.lan@intel.com>
Date:   Thu Nov 1 22:45:30 2012 +0100

    PM / QoS: Fix a free error in the dev_pm_qos_constraints_destroy()
    
    Free a wrong point to struct dev_pm_qos->latency which suppose to
    be the point to struct dev_pm_qos. The patch is to fix the issue.
    
    Signed-off-by: Lan Tianyu <tianyu.lan@intel.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit a01c6e5cde7bb12de62b2bd23a1a9b77023ca025
Author: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
Date:   Tue Oct 30 20:00:30 2012 +0100

    PM / QoS: Fix the return value of dev_pm_qos_update_request()
    
    Commit e39473d (PM / QoS: Make it possible to expose PM QoS device
    flags to user space) introduced __dev_pm_qos_update_request() to be
    called internally by dev_pm_qos_update_request(), but forgot to make
    the latter actually use the return value of the former.  Fix this
    mistake.
    
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit 6bd00b7b8a4bd51acf7aeee8f97709792c538b5e
Author: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
Date:   Wed Oct 24 02:08:18 2012 +0200

    PM / QoS: Make it possible to expose PM QoS device flags to user space
    
    Define two device PM QoS flags, PM_QOS_FLAG_NO_POWER_OFF
    and PM_QOS_FLAG_REMOTE_WAKEUP, and introduce routines
    dev_pm_qos_expose_flags() and dev_pm_qos_hide_flags() allowing the
    caller to expose those two flags to user space or to hide them
    from it, respectively.
    
    After the flags have been exposed, user space will see two
    additional sysfs attributes, pm_qos_no_power_off and
    pm_qos_remote_wakeup, under the device's /sys/devices/.../power/
    directory.  Then, writing 1 to one of them will update the
    PM QoS flags request owned by user space so that the corresponding
    flag is requested to be set.  In turn, writing 0 to one of them
    will cause the corresponding flag in the user space's request to
    be cleared (however, the owners of the other PM QoS flags requests
    for the same device may still request the flag to be set and it
    may be effectively set even if user space doesn't request that).
    
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Reviewed-by: Jean Pihet <j-pihet@ti.com>
    Acked-by: mark gross <markgross@thegnar.org>

commit 7b1174deeaaf2af8e740c1f53112b0d17382e261
Author: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
Date:   Tue Oct 23 01:09:12 2012 +0200

    PM / QoS: Introduce PM QoS device flags support
    
    Modify the device PM QoS core code to support PM QoS flags requests.
    
    First, add a new field of type struct pm_qos_flags called "flags"
    to struct dev_pm_qos for representing the list of PM QoS flags
    requests for the given device.  Accordingly, add a new "type" field
    to struct dev_pm_qos_request (along with an enum for representing
    request types) and a new member called "flr" to its data union for
    representig flags requests.
    
    Second, modify dev_pm_qos_add_request(), dev_pm_qos_update_request(),
    the internal routine apply_constraint() used by them and their
    existing callers to cover flags requests as well as latency
    requests.  In particular, dev_pm_qos_add_request() gets a new
    argument called "type" for specifying the type of a request to be
    added.
    
    Finally, introduce two routines, __dev_pm_qos_flags() and
    dev_pm_qos_flags(), allowing their callers to check which PM QoS
    flags have been requested for the given device (the caller is
    supposed to pass the mask of flags to check as the routine's
    second argument and examine its return value for the result).
    
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Reviewed-by: Jean Pihet <j-pihet@ti.com>
    Reviewed-by: mark gross <markgross@thegnar.org>

commit ae7c62da393d5e3443a15b92dbb3fe9a1f42e0b0
Author: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
Date:   Tue Oct 23 01:09:00 2012 +0200

    PM / QoS: Prepare struct dev_pm_qos_request for more request types
    
    The subsequent patches will use struct dev_pm_qos_request for
    representing both latency requests and flags requests.  To make that
    easier, put the node member of struct dev_pm_qos_request (under the
    name "pnode") into a union called "data" that will represent the
    request's  value and list node depending on its type.
    
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Reviewed-by: Jean Pihet <j-pihet@ti.com>
    Reviewed-by: mark gross <markgross@thegnar.org>

commit 4481395fdf6d7118e57afaf47ae96fe66ed3a74a
Author: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
Date:   Tue Oct 23 01:07:27 2012 +0200

    PM / QoS: Prepare device structure for adding more constraint types
    
    Currently struct dev_pm_info contains only one PM QoS constraints
    pointer reserved for latency requirements.  Since one more device
    constraints type (i.e. flags) will be necessary, introduce a new
    structure, struct dev_pm_qos, that eventually will contain all of
    the available device PM QoS constraints and replace the "constraints"
    pointer in struct dev_pm_info with a pointer to the new structure
    called "qos".
    
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Reviewed-by: Jean Pihet <j-pihet@ti.com>

commit d117d1411691c323d9ed376e39fcafc2ffe3e3ff
Author: Rafael J. Wysocki <rjw@sisk.pl>
Date:   Sun Apr 29 22:54:47 2012 +0200

    PM / QoS: Create device constraints objects on notifier registration
    
    The current behavior of dev_pm_qos_add_notifier() makes device PM QoS
    notifiers less than useful.  Namely, it silently returns success when
    called before any PM QoS constraints are added for the device, so the
    caller will assume that the notifier has been registered, but when
    someone actually adds some nontrivial constraints for the device
    eventually, the previous callers of dev_pm_qos_add_notifier()
    will not know about that and their notifier routines will not be
    executed (contrary to their expectations).
    
    To address this problem make dev_pm_qos_add_notifier() create the
    constraints object for the device if it is not present when the
    routine is called.
    
    Signed-off-by: Rafael J. Wysocki <rjw@sisk.pl>
    Acked-by : markgross <markgross@thegnar.org>

commit c472b59dfac43f73720bed5af6fe594b57b22220
Author: Sachin Kamat <sachin.kamat@linaro.org>
Date:   Tue Jul 17 22:38:26 2012 +0200

    PM / QoS: Use NULL pointer instead of plain integer in qos.c
    
    Fix the following sparse warning:
    drivers/base/power/qos.c:465:29: warning: Using plain integer as NULL pointer
    
    Signed-off-by: Sachin Kamat <sachin.kamat@linaro.org>
    Signed-off-by: Rafael J. Wysocki <rjw@sisk.pl>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 489578e327c626235c054b59e0ce8375e36f7e72
Author: Dorimanx <yuri@bynet.co.il>
Date:   Sat Sep 27 23:47:06 2014 +0300

    fix kernel/power/qos.c

commit b4ec2e1b2b4f200594061a94803b504f233c1c95
Author: MyungJoo Ham <myungjoo.ham@samsung.com>
Date:   Thu Sep 25 17:23:52 2014 -0500

    PM / QoS: Introduce new classes: DMA-Throughput and DVFS-Latency
    
    1. CPU_DMA_THROUGHPUT
    
    This might look simliar to CPU_DMA_LATENCY. However, there are H/W
    blocks that creates QoS requirement based on DMA throughput, not
    latency, while their (those QoS requester H/W blocks) services are
    short-term bursts that cannot be effectively responsed by DVFS
    mechanisms (CPUFreq and Devfreq).
    
    In the Exynos4412 systems that are being tested, such H/W blocks include
    MFC (multi-function codec)'s decoding and enconding features, TV-out
    (including HDMI), and Cameras. When the display is operated at 60Hz,
    each chunk of task should be done within 16ms and the workload on DMA is
    not well spread and fluctuates between frames; some frame requires more
    and some do not and within a frame, the workload also fluctuates
    heavily and the tasks within a frame are usually not parallelized; they
    are processed through specific H/W blocks, not CPU cores. They often
    have PPMU capabilities; however, they need to be polled very frequently
    in order to let DVFS mechanisms react properly. (less than 5ms).
    
    For such specific tasks, allowing them to request QoS requirements seems
    adequete because DVFS mechanisms (as long as the polling rate is 5ms or
    longer) cannot follow up with them. Besides, the device drivers know
    when to request and cancel QoS exactly.
    
    2. DVFS_LATENCY
    
    Both CPUFreq and Devfreq have response latency to a sudden workload
    increase. With near-100% (e.g., 95%) up-threshold, the average response
    latency is approximately 1.5 x polling-rate.
    
    A specific polling rate (e.g., 100ms) may generally fit for its system;
    however, there could be exceptions for that. For example,
    - When a user input suddenly starts: typing, clicking, moving cursors, and
      such, the user might need the full performance immediately. However,
      we do not know whether the full performance is actually needed or not
      until we calculate the utilization; thus, we need to calculate it
      faster with user inputs or any similar events. Specifying QoS on CPU
      processing power or Memory bandwidth at every user input is an
      overkill because there are many cases where such speed-up isn't
      necessary.
    - When a device driver needs a faster performance response from DVFS
      mechanism. This could be addressed by simply putting QoS requests.
      However, such QoS requests may keep the system running fast
      unnecessary in some cases, especially if a) the device's resource
      usage bursts with some duration (e.g., 100ms-long bursts) and
      b) the driver doesn't know when such burst come. MMC/WiFi often had
      such behaviors although there are possibilities that part (b) might
      be addressed with further efforts.
    
    The cases shown above can be tackled with putting QoS requests on the
    response time or latency of DVFS mechanism, which is directly related to
    its polling interval (if the DVFS mechanism is polling based).
    
    Signed-off-by: MyungJoo Ham <myungjoo.ham@samsung.com>
    Signed-off-by: Kyungmin Park <kyungmin.park@samsung.com>
    
    --
    Changes from RFC(v1)
    - Added omitted part (registering new classes)

commit 00aef0f5e88c31988b9be3365b1343d7b4bdb39c
Author: Sai Gurrappadi <sgurrappadi@nvidia.com>
Date:   Thu May 22 15:06:06 2014 -0700

    PM QoS: Fix pmqos notifiers in pm_qos_enabled_set
    
    Send out the notifiers if the enabled flag has been changed. This will
    ensure that the notifier fires in all cases.
    
    Bug 1516219
    
    Change-Id: Ie97422f61a9ad56f0ce194a99ce69193d429eadc
    Signed-off-by: Sai Gurrappadi <sgurrappadi@nvidia.com>
    Reviewed-on: http://git-master/r/413433
    Reviewed-by: Automatic_Commit_Validation_User
    Tested-by: Diwakar Tundlam <dtundlam@nvidia.com>
    Reviewed-by: Diwakar Tundlam <dtundlam@nvidia.com>

commit 983e543622130c1bb8d56dfb483110a8c86c593a
Author: Puneet Saxena <puneets@nvidia.com>
Date:   Mon Jan 27 16:30:29 2014 +0530

    Power: pmqos: Add emc freq pmqos constraint
    
    It adds min emc freq pmqos.
    
    Bug 1432476
    
    Change-Id: If05c2e8cceffc9ca071ed0b023c29e1ef2921245
    Signed-off-by: Puneet Saxena <puneets@nvidia.com>
    Reviewed-on: http://git-master/r/360379
    Reviewed-by: Automatic_Commit_Validation_User
    Reviewed-by: Aleksandr Frid <afrid@nvidia.com>
    Reviewed-by: Sachin Nikam <snikam@nvidia.com>

commit 1011d143951f98b483ca0f266f5597b7322d188d
Author: Sai Gurrappadi <sgurrappadi@nvidia.com>
Date:   Tue Jan 14 10:26:22 2014 -0800

    power: Fix coverity error
    
    Properly null-terminate userspace input string. Otherwise, the subsequent
    strsep() could continue off into arbitrary chunks of kmalloc() space
    that aren't part of the original string buffer.
    
    Change-Id: I3868dbcdd9df7e7172c001eb6bc41c605d48604b
    Signed-off-by: Sai Gurrappadi <sgurrappadi@nvidia.com>
    Reviewed-on: http://git-master/r/355578
    Reviewed-by: Paul Walmsley <pwalmsley@nvidia.com>
    Reviewed-by: Automatic_Commit_Validation_User
    Reviewed-by: Diwakar Tundlam <dtundlam@nvidia.com>

commit 4392db45a882e945e080a9f2836e566babfd3204
Author: Sai Gurrappadi <sgurrappadi@nvidia.com>
Date:   Mon Oct 21 10:44:23 2013 -0700

    power: Prefer min over max for online cpus
    
    We prefered min_online_cpus over max_online_cpus if min > max.
    min_wins is now true for online cpu PmQoS requests.
    
    Bug 1270839
    
    Change-Id: I2888538dd1a4616babb7cd1532264272de5cfe64
    Signed-off-by: Sai Gurrappadi <sgurrappadi@nvidia.com>
    Reviewed-on: http://git-master/r/301871
    Reviewed-by: Automatic_Commit_Validation_User
    Reviewed-by: Diwakar Tundlam <dtundlam@nvidia.com>

commit bcf9ba1dc097ad784596de06d8e1092fa46449de
Author: Sai Gurrappadi <sgurrappadi@nvidia.com>
Date:   Tue Oct 1 10:37:35 2013 -0700

    power: PM QoS support for bounded constraints
    
    Extended PM QoS to allow binding of two constraints. Bounded constraints
    add the following functionality:
    
    	- Priority for min/max bound requests. Targets bounds are set
    	  to satisfy all priorities (intersection of all ranges). If it
    	  is not possible to do so, higher priorities prevail
    	- Timeouts for bound requests
    	- Userspace interface that exposes bound requests
    
    PM QoS still supports its original kernelspace and userspace interfaces
    
    Bug 1270839
    Bug 1349096
    
    Change-Id: Ic83444912b330fc71335d9a5b59077b1d16496bd
    Signed-off-by: Sai Gurrappadi <sgurrappadi@nvidia.com>
    Reviewed-on: http://git-master/r/299037
    Reviewed-by: Automatic_Commit_Validation_User
    Reviewed-by: Paul Walmsley <pwalmsley@nvidia.com>
    Reviewed-by: Diwakar Tundlam <dtundlam@nvidia.com>

commit 6e1ec53c1532df0e688dce7c88fdf8de18e703cd
Author: Sai Gurrappadi <sgurrappadi@nvidia.com>
Date:   Tue Oct 1 10:01:27 2013 -0700

    pmqos: Replace spinlock with mutex for pm_qos_lock
    
    Using a spinlock (taken with irqsave) meant that pm_qos_lock couldn't be
    used to synchronize on the notifiers in order to ensure proper order of
    the notifications. This is needed in case where there might be two near
    simultaneous pmqos client requests for a bound on the same constraint;
    the notifiers in pm_qos_update_target for the two clients could
    potentially engage in a race.
    
    Example:
    
    Assume two requests are made (A, B with A coming first) for max cpufreq
    and these are the only requests currently available.
    
    Current behavior can result in:
    
    notify(max_cpu_freq, minof(A, B))
    notify(max_cpu_freq, minof(LONG_MAX, A))
    
    Expected behavior:
    
    notify(max_cpu_freq, minof(LONG_MAX, A))
    notify(max_cpu_freq, minof(A, B))
    
    Most of the PM QoS and Dev PM QoS requester clients were reviewed and
    none of them were found to be calling pm_qos_add/update/remove request
    from interrupt or atomic context since those calls include the blocking
    notifier call which cannot be done in atomic context.
    
    Change-Id: I2fb43cc38da4c701e4872b937dd82cd38f1a1c1e
    Signed-off-by: Sai Gurrappadi <sgurrappadi@nvidia.com>
    Reviewed-on: http://git-master/r/299036
    Reviewed-by: Automatic_Commit_Validation_User
    Reviewed-by: Diwakar Tundlam <dtundlam@nvidia.com>

commit 49f64bfd6e99685538d0fb6d895e61647bfcd780
Author: Jake Weinstein <xboxlover360@gmail.com>
Date:   Sun Sep 21 17:19:15 2014 -0500

    QoS: add back comments

commit df7514444d50a91c55aa35cef1a5f9c6ba914713
Author: Alex Frid <afrid@nvidia.com>
Date:   Mon Jul 22 21:26:11 2013 -0700

    PM QoS: Add GPU frequency limits to PM QoS
    
    Added GPU frequency min/max as PM QoS classes.
    
    Bug 1330780
    
    Change-Id: I2428c62748521c17e23b2df9ca409deda8b36160
    Signed-off-by: Alex Frid <afrid@nvidia.com>
    Reviewed-on: http://git-master/r/267702
    Reviewed-by: Ilan Aelion <iaelion@nvidia.com>
    Reviewed-by: Mitch Luban <mluban@nvidia.com>
    Reviewed-by: Yu-Huan Hsu <yhsu@nvidia.com>

commit 5536e60fcc6b432675c6343bc6f70454a363a014
Author: Li Li <lli5@nvidia.com>
Date:   Mon Jan 7 14:30:09 2013 -0800

    PM / QoS: export pm_qos_update_request_timeout()
    
    This pm_qos_update_request_timeout() was introduced without being exported.
    Should export it as all of the other PM QoS APIs so those drivers compiled as
    modules can use it.
    
    Change-Id: Ie51ce52db4ca633117fe18441c42b562220399e8
    Signed-off-by: Li Li <lli5@nvidia.com>
    Reviewed-on: http://git-master/r/189306
    Reviewed-by: Automatic_Commit_Validation_User
    GVS: Gerrit_Virtual_Submit
    Reviewed-by: Eric Miao <emiao@nvidia.com>
    Reviewed-by: Bharat Nihalani <bnihalani@nvidia.com>

commit c015fb74680c869b8c20c92e1dcf0debbf470487
Author: Antti P Miettinen <amiettinen@nvidia.com>
Date:   Mon Aug 20 19:36:38 2012 +0300

    PM QoS: Add disable parameter
    
    For testing purposes it is useful to be able to disable
    PM Qos.
    
    Bug 1020898
    Bug 917572
    
    Reviewed-on: http://git-master/r/124667
    
    Change-Id: I266f5b5730cfe4705197d8b09db7f9eda6766c7c
    Signed-off-by: Antti P Miettinen <amiettinen@nvidia.com>
    Signed-off-by: Varun Wadekar <vwadekar@nvidia.com>
    
    Rebase-Id: Re2088674f90436e0b9dd74310d5cda1f9e2868e4

commit 7fb4d2de1315f9800352199dab5e94454c812677
Author: Gaurav Sarode <gsarode@nvidia.com>
Date:   Fri Feb 17 15:25:43 2012 +0530

    PM Qos: Add min online cpus as PM QoS parameter
    
    Bug 940061
    
    Change-Id: Ibae842fdc3af3c92ec7e6125c602417110d8b55e
    Signed-off-by: Gaurav Sarode <gsarode@nvidia.com>
    Reviewed-on: http://git-master/r/84521
    Reviewed-by: Sachin Nikam <snikam@nvidia.com>
    Tested-by: Aleksandr Frid <afrid@nvidia.com>
    Reviewed-by: Diwakar Tundlam <dtundlam@nvidia.com>
    
    Rebase-Id: R830d4e99f1e03b61a8c4e52e11645b7ed2f10f56

commit d7ba58222179614f5d469604656060d9f9eac9bb
Author: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
Date:   Tue Oct 23 01:07:46 2012 +0200

    PM / QoS: Introduce request and constraint data types for PM QoS flags
    
    Introduce struct pm_qos_flags_request and struct pm_qos_flags
    representing PM QoS flags request type and PM QoS flags constraint
    type, respectively.  With these definitions the data structures
    will be arranged so that the list member of a struct pm_qos_flags
    object will contain the head of a list of struct pm_qos_flags_request
    objects representing all of the "flags" requests present for the
    given device.  Then, the effective_flags member of a struct
    pm_qos_flags object will contain the bitwise OR of the flags members
    of all the struct pm_qos_flags_request objects in the list.
    
    Additionally, introduce helper function pm_qos_update_flags()
    allowing the caller to manage the list of struct pm_qos_flags_request
    pointed to by the list member of struct pm_qos_flags.
    
    The flags are of type s32 so that the request's "value" field
    is always of the same type regardless of what kind of request it
    is (latency requests already have value fields of type s32).
    
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Reviewed-by: Jean Pihet <j-pihet@ti.com>
    Acked-by: mark gross <markgross@thegnar.org>

commit ac3297db24915db1fa302d9e404863dd13d604ce
Author: Luis Gonzalez Fernandez <luisgf@gmail.com>
Date:   Fri Sep 7 21:35:21 2012 +0200

    PM / QoS: Add return code to pm_qos_get_value function.
    
    pm_qos_get_value don't return a return code in all cases. It's sure that
    anything interesting happend after BUG() but this prevent any compilation
    warning.
    
    [rjw: Chaneged the new return value to PM_QOS_DEFAULT_VALUE.]
    
    Signed-off-by: Luis Gonzalez Fernandez <luisgf@gmail.com>
    Signed-off-by: Rafael J. Wysocki <rjw@sisk.pl>

commit d9ad8bc32c52100f67d6fb67bb105ca88e18218f
Author: Antti P Miettinen <amiettinen@nvidia.com>
Date:   Tue Dec 27 12:28:21 2011 +0200

    PM QoS: Add CPU frequency min/max as PM QoS params
    
    Add minimum and maximum CPU frequency as PM QoS parameters.
    
    Bug 888312
    
    Change-Id: I18abddded35a044a6ad8365035e31d1a2213a329
    Reviewed-on: http://git-master/r/72206
    Signed-off-by: Antti P Miettinen <amiettinen@nvidia.com>
    Signed-off-by: Varun Wadekar <vwadekar@nvidia.com>
    Reviewed-on: http://git-master/r/75883
    Reviewed-by: Automatic_Commit_Validation_User
    
    Rebase-Id: R1007bbef60489ecc81a9acd0ce3b0abfa9a05f3e

commit 9fe65da556a9cc2b75dd8186290951e54d6d706b
Author: Alex Frid <afrid@nvidia.com>
Date:   Fri Dec 16 13:44:23 2011 -0800

    PM QoS: Add max online cpus as PM QoS parameter
    
    Bug 894200
    
    Change-Id: Ieb009a13c6ef9bca2388e234eb973d65a4e3a58b
    Signed-off-by: Alex Frid <afrid@nvidia.com>
    Reviewed-on: http://git-master/r/71034
    Reviewed-by: Rohan Somvanshi <rsomvanshi@nvidia.com>
    Tested-by: Rohan Somvanshi <rsomvanshi@nvidia.com>
    
    Rebase-Id: R5791d3cb0bb66f3b8079f5a8af5fa758fb3c6705

commit 83028123aa05ebc463052ab8b7b33f7f6a04a6d6
Author: Arto Merilainen <amerilainen@nvidia.com>
Date:   Tue Aug 27 16:17:58 2013 +0300

    kernel: power: Add PM_USERSPACE_FROZEN workqueue
    
    Some device drivers require a callback to be called after the userspace
    processes are frozen. This patch adds PM_USERSPACE_FROZEN workqueue
    which is called after userspace processes are frozen but when the
    kernel threads are still functioning.
    
    Bug 1344551
    
    Change-Id: I0e6fd7e2473db168d01c88bc0192326ceea92ebe
    Signed-off-by: Arto Merilainen <amerilainen@nvidia.com>
    Reviewed-on: http://git-master/r/266774
    (cherry picked from commit d964493291ef87eea1a2ee47b5b66305bb18bcf3)
    Reviewed-on: http://git-master/r/274939
    Tested-by: Sang-Hun Lee <sanlee@nvidia.com>
    Reviewed-by: Kevin Huang (Eng-SW) <kevinh@nvidia.com>
    Reviewed-by: Mitch Luban <mluban@nvidia.com>

commit 2aa69a57bb76bf043ea2c6d5e353917752fb1a18
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed May 20 20:47:41 2015 +0200

     PM / Sleep: Remove ftrace_stop/start() from suspend and hibernate
    
    ftrace_stop() and ftrace_start() were added to the suspend and hibernate
    process because there was some function within the work flow that caused
    the system to reboot if it was traced. This function has recently been
    found (restore_processor_state()). Now there's no reason to disable
    function tracing while we are going into suspend or hibernate, which means
    that being able to trace this will help tremendously in debugging any
    issues with suspend or hibernate.
    
    This also means that the ftrace_stop/start() functions can be removed
    and simplify the function tracing code a bit.
    
    Link: http://lkml.kernel.org/r/1518201.VD9cU33jRU@vostro.rjw.lan
    
    Acked-by: "Rafael J. Wysocki" <rjw@rjwysocki.net>
    Reviewed-by: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit ad052afcce682c0ffb4ba05eb201881c8c6864c5
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed May 20 20:39:01 2015 +0200

    Revert "qseecom: Add support for loading commonlib image"
    
    This reverts commit 6e9071c0e6870689781670a72fc689a2ef816169.

commit 22301ef938c0d9a910f053e394f65101a782b3b6
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed May 20 20:38:50 2015 +0200

    Revert "treewide: Replace pil_get()/put() with subsystem_get()/put()"
    
    This reverts commit d6b3c855679b7c43c992c1db722ef718d4b26d50.

commit 6e9071c0e6870689781670a72fc689a2ef816169
Author: Mona Hossain <mhossain@codeaurora.org>
Date:   Mon Oct 29 17:49:01 2012 -0700

    qseecom: Add support for loading commonlib image
    
    Load the common library image on secure domain during
    initialization of the driver.
    
    All secure app loaded on the secure domain (TZ) is currently
    refactored to not include all secure libraries. The libraries
    are now a part of separate image.  This is used by all secure app.
    This common lib image needs to be  loaded before any secure app
    can be loaded.
    
    This patch adds support for handling this new change based off
    the QSEE version running on secure domain.  The change is to
    load the common library image before loading any secure app.
    
    Change-Id: I1c7692b612ea4c7bddbca1af07ddf32a073d70a8
    Signed-off-by: Mona Hossain <mhossain@codeaurora.org>

commit d6b3c855679b7c43c992c1db722ef718d4b26d50
Author: Stephen Boyd <sboyd@codeaurora.org>
Date:   Wed Jun 27 15:15:16 2012 -0700

    treewide: Replace pil_get()/put() with subsystem_get()/put()
    
    Do a tree wide sed to replace pil_get and pil_put() with their
    subsystem counterpart. With all the pil drivers migrated over to
    the new subsystem start/stop API this is essentially a no-op.
    
    We rename "q6" to "adsp" to be more in line with how future
    chipsets name the low power audio subsystem. This requires minor
    changes to userspace; hopefully no more changes will be
    necessary.
    
    Change-Id: I22dbbdcf7f5aaaa80394a6d70b5cfc24cdb32044
    Signed-off-by: Stephen Boyd <sboyd@codeaurora.org>o

commit c9dd5d906c518a4e812d825e5d98ed987a4aafa2
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed May 20 19:40:54 2015 +0200

    drivers: cpufreq: update some really nice stuff, thanks to the great @dorimanx!
    also fix an compiling error in ssp_dev.c

commit aaf79f6650a4a8d638febc98bbe63fed31e47c2f
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed May 20 15:19:41 2015 +0200

    Revert "net: wireless: bcmdhd: Enable IPv6 RA filter feature"
    
    This reverts commit 92d59a5bcaa5c8600eebc537a861ee5caabb349f.

commit ff49d1243ba33e88c125cf58df0b40318fa31eb4
Author: franciscofranco <franciscofranco.1990@gmail.com>
Date:   Sat Feb 1 16:57:51 2014 +0000

    bcmdhd: decrease rx wakelock time from 500ms to 250ms
    
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 5d98e2b5dae9b2461e454e8c51ffdacfd8636e4b
Author: Ruchi Kandoi <kandoiruchi@google.com>
Date:   Thu May 7 10:18:55 2015 -0700

    suspend: Return error when pending wakeup source is found.
    
    If a wakeup source is found to be pending in the last stage of suspend
    after syscore suspend then the device doesn't suspend but the error is
    not propogated which causes an error in the accounting for the number
    of suspend aborts and successful suspends.
    
    Change-Id: Ib63b4ead755127eaf03e3b303aab3c782ad02ed1
    Signed-off-by: Ruchi Kandoi <kandoiruchi@google.com>

commit e9c722bd14b3e4f05ddb5b448a94451885ce3b62
Author: Ecco Park <eccopark@broadcom.com>
Date:   Fri Nov 15 01:45:00 2013 -0600

    net: wireless: bcmdhd: Enable IPv6 RA filter feature
    
    * We will block IPv6 Router Advertisement multicast packet
      in suspend mode to save power on device.
    
    Bug: 11638281 Signed-off-by: Ecco Park <eccopark@broadcom.com>
    Change-Id: I88199a08ff6d73923e87dfa77acd2299eb89e8f4

commit 4c696264d8b0f7bf74f9fefc27d00f83167856bf
Author: franciscofranco <franciscofranco.1990@gmail.com>
Date:   Sat Feb 1 16:57:51 2014 +0000

    bcmdhd: decrease rx wakelock time from 500ms to 250ms for testing purposes.
    
    Signed-off-by: franciscofranco <franciscofranco.1990@gmail.com>

commit 2747737d95b04bccdcc4ad916ae09447c1ae52db
Author: dorimanx <yuri@bynet.co.il>
Date:   Fri Feb 14 02:12:20 2014 +0200

     net: wireless: bcmdhd: reduced the wakelock time of RX packet
    
    	Reduce time : 1secs -> 500ms
    	Problem : [Issue 11512235] 34% battery eaten overnight
    		  [Issue 11374623] OS + Wifi used 41% of battery,
    		  wlan_rx_wake
    Signed-off-by: Ecco Park <eccopark@broadcom.com>
    
    Signed-off-by: Paul Reioux <reioux@gmail.com>
    
    Conflicts:
    	drivers/net/wireless/bcmdhd/Makefile
    	drivers/net/wireless/bcmdhd/src/dhd/sys/dhd.h

commit 47e154d837e7d880febb5d16278890f6012a3011
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed May 20 14:31:24 2015 +0200

    input: replace more earlysuspend stuff with powersuspend

commit 9b61fd5b5bb4f661d62867beca66c71ab523b4bf
Author: sbrissen <sbrissen@hotmail.com>
Date:   Fri Mar 28 08:17:17 2014 -0400

    jf: cypress-touchkey - add keydisabler
    
    Change-Id: I85efd4c5b2d6a7283c430f5eca2a730ef6b03d18

commit 1580b46705be34256f40e09399f76e76688e7ec9
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Tue May 19 21:55:24 2015 +0200

    gud/net: fix my errors (again..)

commit b005c1ab94c2453d241d7f4c25e62735461943e2
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Mon May 18 15:15:35 2015 +0200

    wl_android: fixed compilation errors!

commit 432a8d97c95eb10281d8365d70207c6c68b670f4
Author: alucard24 <dmbaoh2@gmail.com>
Date:   Mon May 18 20:09:34 2015 +0200

    darkness and nightmare cpu govs: removed useless local cpu var!

commit 62ee849d6f157069f824bf3f50d4893dd76de3ef
Author: Jeff Vander Stoep <jeffv@google.com>
Date:   Wed Apr 29 11:14:23 2015 -0700

    SELinux: ss: Fix policy write for ioctl operations
    
    Security server omits the type field when writing out the contents of the
    avtab from /sys/fs/selinux/policy. This leads to a corrupt output. No impact
    on the running kernel or its loaded policy. Impacts CTS neverallow tests.
    
    Bug: 20665861
    Change-Id: I657e18013dd5a1f40052bc2b02dd8e0afee9bcfb
    Signed-off-by: Jeff Vander Stoep <jeffv@google.com>
    (cherry picked from commit 8cdfb356b51e29494ca0b9e4e86727d6f841a52d)

commit 39a6632f6d49799243bd09c25500967857088b63
Author: Jeff Vander Stoep <jeffv@google.com>
Date:   Mon Apr 20 17:45:42 2015 -0700

    SELinux: use deletion-safe iterator to free list
    
    This code is not exercised by policy version 26, but will be upon
    upgrade to policy version 30.
    
    Bug: 18087110
    Change-Id: I07c6f34607713294a6a12c43a64d9936f0602200
    Signed-off-by: Jeff Vander Stoep <jeffv@google.com>

commit 06ce96eba37b31aae780cc6f90021ac033c329ce
Author: Jeff Vander Stoep <jeffv@google.com>
Date:   Sat Apr 4 16:15:54 2015 -0700

    security: lsm_audit: add ioctl specific auditing
    
    Add information about ioctl calls to the LSM audit data. Log the
    file path and command number.
    
    Bug: 18087110
    Change-Id: Idbbd106db6226683cb30022d9e8f6f3b8fab7f84
    Signed-off-by: Jeff Vander Stoep <jeffv@google.com>

commit 49d55d33eca64b38e75fe4a0c0a933af04936f75
Author: Paul Moore <pmoore@redhat.com>
Date:   Tue Jan 28 14:45:41 2014 -0500

    selinux: add SOCK_DIAG_BY_FAMILY to the list of netlink message types
    
    commit 6a96e15096da6e7491107321cfa660c7c2aa119d upstream.
    
    The SELinux AF_NETLINK/NETLINK_SOCK_DIAG socket class was missing the
    SOCK_DIAG_BY_FAMILY definition which caused SELINUX_ERR messages when
    the ss tool was run.
    
     # ss
     Netid  State  Recv-Q Send-Q  Local Address:Port   Peer Address:Port
     u_str  ESTAB  0      0                  * 14189             * 14190
     u_str  ESTAB  0      0                  * 14145             * 14144
     u_str  ESTAB  0      0                  * 14151             * 14150
     {...}
     # ausearch -m SELINUX_ERR
     ----
     time->Thu Jan 23 11:11:16 2014
     type=SYSCALL msg=audit(1390493476.445:374):
      arch=c000003e syscall=44 success=yes exit=40
      a0=3 a1=7fff03aa11f0 a2=28 a3=0 items=0 ppid=1852 pid=1895
      auid=0 uid=0 gid=0 euid=0 suid=0 fsuid=0 egid=0 sgid=0 fsgid=0
      tty=pts0 ses=1 comm="ss" exe="/usr/sbin/ss"
      subj=unconfined_u:unconfined_r:unconfined_t:s0-s0:c0.c1023 key=(null)
     type=SELINUX_ERR msg=audit(1390493476.445:374):
      SELinux:  unrecognized netlink message type=20 for sclass=32
    
    Change-Id: I22218ec620bc3ee6396145f1c2ad8ed222648309
    Signed-off-by: Paul Moore <pmoore@redhat.com>

commit 0d3744a5d1a2d629aca52b52775dc988d16a4157
Author: Nicolas Dichtel <nicolas.dichtel@6wind.com>
Date:   Fri Apr 10 16:24:28 2015 +0200

    selinux/nlmsg: add XFRM_MSG_MAPPING
    
    commit bd2cba07381a6dba60bc1c87ed8b37931d244da1 upstream (net-next).
    
    This command is missing.
    
    Change-Id: Ida52130382e42355e5f3b39134aa61a1ea98026d
    Fixes: 3a2dfbe8acb1 ("xfrm: Notify changes in UDP encapsulation via netlink")
    CC: Martin Willi <martin@strongswan.org>
    Reported-by: Stephen Smalley <sds@tycho.nsa.gov>
    Signed-off-by: Nicolas Dichtel <nicolas.dichtel@6wind.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 483cbe1156a8dde8d85eb61c2fa6daf974443082
Author: Nicolas Dichtel <nicolas.dichtel@6wind.com>
Date:   Fri Apr 10 16:24:27 2015 +0200

    selinux/nlmsg: add XFRM_MSG_MIGRATE
    
    commit 8d465bb777179c4bea731b828ec484088cc9fbc1 upstream (net-next).
    
    This command is missing.
    
    Change-Id: Id2c9344ca1ab2c96e0b758ad1efb38e16cf23b86
    Fixes: 5c79de6e79cd ("[XFRM]: User interface for handling XFRM_MSG_MIGRATE")
    Reported-by: Stephen Smalley <sds@tycho.nsa.gov>
    Signed-off-by: Nicolas Dichtel <nicolas.dichtel@6wind.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 6ddc3592020917a3a1b066fcd7f14ccd1bc0a689
Author: Nicolas Dichtel <nicolas.dichtel@6wind.com>
Date:   Fri Apr 10 16:24:26 2015 +0200

    selinux/nlmsg: add XFRM_MSG_REPORT
    
    commit b0b59b0056acd6f157a04cc895f7e24692fb08aa upstream (net-next).
    
    This command is missing.
    
    Change-Id: I8fa3b1b9815296d3b001244d2212f79f5654bd01
    Fixes: 97a64b4577ae ("[XFRM]: Introduce XFRM_MSG_REPORT.")
    Reported-by: Stephen Smalley <sds@tycho.nsa.gov>
    Signed-off-by: Nicolas Dichtel <nicolas.dichtel@6wind.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit b1f048b1dbde60f9e8bf984a38e3dba1074d1438
Author: Nicolas Dichtel <nicolas.dichtel@6wind.com>
Date:   Wed Apr 8 18:36:42 2015 +0200

    selinux/nlmsg: add XFRM_MSG_[NEW|GET]SADINFO
    
    commit 5b5800fad072133e4a9c2efbf735baaac83dec86 upstream (net-next).
    
    These commands are missing.
    
    Change-Id: I3fd1d3d700592c653e1a5c5199125805d55aaa95
    Fixes: 28d8909bc790 ("[XFRM]: Export SAD info.")
    Signed-off-by: Nicolas Dichtel <nicolas.dichtel@6wind.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 361d4a1ec651b8556f542be8c16b321f6fb0f7a2
Author: Nicolas Dichtel <nicolas.dichtel@6wind.com>
Date:   Wed Apr 8 18:36:41 2015 +0200

    selinux/nlmsg: add XFRM_MSG_GETSPDINFO
    
    commit 5e6deebafb45fb271ae6939d48832e920b8fb74e upstream (net-next).
    
    This command is missing.
    
    Change-Id: Id0a0d9bf7a4af98a8f761fec902d1296138a911f
    Fixes: ecfd6b183780 ("[XFRM]: Export SPD info")
    Signed-off-by: Nicolas Dichtel <nicolas.dichtel@6wind.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 62a516edb6f8d350781865dd63da7ce43f0973b8
Author: Nicolas Dichtel <nicolas.dichtel@6wind.com>
Date:   Wed Apr 8 18:36:40 2015 +0200

    selinux/nlmsg: add XFRM_MSG_NEWSPDINFO
    
    commit 2b7834d3e1b828429faa5dc41a480919e52d3f31 upstream (net-next).
    
    This new command is missing.
    
    Change-Id: If511000c19aa9af7220ff775d88ace9834b35dcb
    Fixes: 880a6fab8f6b ("xfrm: configure policy hash table thresholds by netlink")
    Reported-by: Christophe Gouault <christophe.gouault@6wind.com>
    Signed-off-by: Nicolas Dichtel <nicolas.dichtel@6wind.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 863953677a4a3b87dfb06aaf67cda4aac1279ba6
Author: Sasha Levin <sasha.levin@oracle.com>
Date:   Mon Dec 29 14:39:01 2014 -0600

    KEYS: close race between key lookup and freeing
    
    When a key is being garbage collected, it's key->user would get put before
    the ->destroy() callback is called, where the key is removed from it's
    respective tracking structures.
    
    This leaves a key hanging in a semi-invalid state which leaves a window open
    for a different task to try an access key->user. An example is
    find_keyring_by_name() which would dereference key->user for a key that is
    in the process of being garbage collected (where key->user was freed but
    ->destroy() wasn't called yet - so it's still present in the linked list).
    
    This would cause either a panic, or corrupt memory.
    
    Change-Id: I01a5ec17916864929458caa9d0fbefea2ca2c5e2
    Signed-off-by: Sasha Levin <sasha.levin@oracle.com>
    
    Conflicts:
    	security/keys/gc.c

commit 3ac0de049acf57de4a23294fc6a4369bca2eaee4
Author: Christopher Yeoh <cyeoh@au1.ibm.com>
Date:   Thu May 31 16:26:42 2012 -0700

    aio/vfs: cleanup of rw_copy_check_uvector() and compat_rw_copy_check_uvector()
    
    A cleanup of rw_copy_check_uvector and compat_rw_copy_check_uvector after
    changes made to support CMA in an earlier patch.
    
    Rather than having an additional check_access parameter to these
    functions, the first paramater type is overloaded to allow the caller to
    specify CHECK_IOVEC_ONLY which means check that the contents of the iovec
    are valid, but do not check the memory that they point to.  This is used
    by process_vm_readv/writev where we need to validate that a iovec passed
    to the syscall is valid but do not want to check the memory that it points
    to at this point because it refers to an address space in another process.
    
    Signed-off-by: Chris Yeoh <yeohc@au1.ibm.com>
    Reviewed-by: Oleg Nesterov <oleg@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    
    Conflicts:
    	fs/compat.c

commit e9c14db3b39dffe1f5657408262d88322fd29a17
Author: Andrew Morton <akpm@linux-foundation.org>
Date:   Thu May 31 16:26:02 2012 -0700

    security/keys/keyctl.c: suppress memory allocation failure warning
    
    This allocation may be large.  The code is probing to see if it will
    succeed and if not, it falls back to vmalloc().  We should suppress any
    page-allocation failure messages when the fallback happens.
    
    Reported-by: Dave Jones <davej@redhat.com>
    Acked-by: David Howells <dhowells@redhat.com>
    Cc: James Morris <jmorris@namei.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit 1121b08c61d7121904a6eb9159581af132bf0f2a
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Tue Apr 24 02:44:49 2012 -0400

    TIF_NOTIFY_RESUME is defined on all targets now
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

commit 65b8d30657afa611aef4ad576d7eebfe9e35b27d
Author: David Howells <dhowells@redhat.com>
Date:   Fri May 11 10:56:56 2012 +0100

    KEYS: Add invalidation support
    
    Add support for invalidating a key - which renders it immediately invisible to
    further searches and causes the garbage collector to immediately wake up,
    remove it from keyrings and then destroy it when it's no longer referenced.
    
    It's better not to do this with keyctl_revoke() as that marks the key to start
    returning -EKEYREVOKED to searches when what is actually desired is to have the
    key refetched.
    
    To invalidate a key the caller must be granted SEARCH permission by the key.
    This may be too strict.  It may be better to also permit invalidation if the
    caller has any of READ, WRITE or SETATTR permission.
    
    The primary use for this is to evict keys that are cached in special keyrings,
    such as the DNS resolver or an ID mapper.
    
    Signed-off-by: David Howells <dhowells@redhat.com>

commit f865dc72bb005f45945799b1de812fc3b54f416b
Author: David Howells <dhowells@redhat.com>
Date:   Fri May 11 10:56:56 2012 +0100

    KEYS: Permit in-place link replacement in keyring list
    
    Make use of the previous patch that makes the garbage collector perform RCU
    synchronisation before destroying defunct keys.  Key pointers can now be
    replaced in-place without creating a new keyring payload and replacing the
    whole thing as the discarded keys will not be destroyed until all currently
    held RCU read locks are released.
    
    If the keyring payload space needs to be expanded or contracted, then a
    replacement will still need allocating, and the original will still have to be
    freed by RCU.
    
    Signed-off-by: David Howells <dhowells@redhat.com>

commit 8e763fb9426e178e865968823bdf0f9ab16187da
Author: David Howells <dhowells@redhat.com>
Date:   Fri May 11 10:56:56 2012 +0100

    KEYS: Perform RCU synchronisation on keys prior to key destruction
    
    Make the keys garbage collector invoke synchronize_rcu() prior to destroying
    keys with a zero usage count.  This means that a key can be examined under the
    RCU read lock in the safe knowledge that it won't get deallocated until after
    the lock is released - even if its usage count becomes zero whilst we're
    looking at it.
    
    This is useful in keyring search vs key link.  Consider a keyring containing a
    link to a key.  That link can be replaced in-place in the keyring without
    requiring an RCU copy-and-replace on the keyring contents without breaking a
    search underway on that keyring when the displaced key is released, provided
    the key is actually destroyed only after the RCU read lock held by the search
    algorithm is released.
    
    This permits __key_link() to replace a key without having to reallocate the key
    payload.  A key gets replaced if a new key being linked into a keyring has the
    same type and description.
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    Acked-by: Jeff Layton <jlayton@redhat.com>

commit efc74f388ee0a7823dc233224974ed501f1b60fb
Author: Jeff Vander Stoep <jeffv@google.com>
Date:   Wed Apr 8 11:27:46 2015 -0700

    SELinux: per-command whitelisting of ioctls
    
     note that this patch depends on a prior patch that is already in
     android-3.4 but has not apparently found its way into the msm 3.4
     branches (but is included in exynos and tegra),
     https://android-review.googlesource.com/#/c/92962/
    
    Extend the generic ioctl permission check with support for per-command
    filtering. Source/target/class sets including the ioctl permission may
    additionally include a set of commands. Example:
    
    allow <source> <target>:<class> { 0x8910-0x8926 0x892A-0x8935 }
    auditallow <source> <target>:<class> 0x892A
    
    When ioctl commands are omitted only the permissions are checked. This
    feature is intended to provide finer granularity for the ioctl
    permission which may be too imprecise in some circumstances. For
    example, the same driver may use ioctls to provide important and
    benign functionality such as driver version or socket type as well as
    dangerous capabilities such as debugging features, read/write/execute
    to physical memory or access to sensitive data. Per-command filtering
    provides a mechanism to reduce the attack surface of the kernel, and
    limit applications to the subset of commands required.
    
    The format of the policy binary has been modified to include ioctl
    commands, and the policy version number has been incremented to
    POLICYDB_VERSION_IOCTL_OPERATIONS=30 to account for the format change.
    
    Bug: 18087110
    Change-Id: Ibf0e36728f6f3f0d5af56ccdeddee40800af689d
    Signed-off-by: Jeff Vander Stoep <jeffv@google.com>
    
    Conflicts:
    	security/selinux/avc.c
    	security/selinux/hooks.c
    	security/selinux/ss/services.c

commit 9d54c46bf49690db3aa907e1f380206513ba00cf
Author: Richard Haines <richard_c_haines@btinternet.com>
Date:   Tue Nov 19 17:34:23 2013 -0500

    SELinux: Update policy version to support constraints info
    
    Update the policy version (POLICYDB_VERSION_CONSTRAINT_NAMES) to allow
    holding of policy source info for constraints.
    
    Upstream commit a660bec1d84ad19a39e380af129e207b3b8f609e
    
    Signed-off-by: Richard Haines <richard_c_haines@btinternet.com>
    Acked-by: Stephen Smalley <sds@tycho.nsa.gov>
    Signed-off-by: Paul Moore <pmoore@redhat.com>
    Change-Id: If419c7bfdea2f7006c9a62ea595f0cbfe5c78871
    
    Conflicts:
    	security/selinux/include/security.h

commit 6d164c366cf6ccb28f180ba6e9305d07041ba438
Author: Eric Paris <eparis@redhat.com>
Date:   Tue Mar 20 14:35:12 2012 -0400

    SELinux: add default_type statements
    
    Because Fedora shipped userspace based on my development tree we now
    have policy version 27 in the wild defining only default user, role, and
    range.  Thus to add default_type we need a policy.28.
    
    Upstream commit eed7795d0a2c9b2e934afc088e903fa2c17b7958
    
    Signed-off-by: Eric Paris <eparis@redhat.com>
    Change-Id: Icb3324af7f740249977a4559c2c5692c7fcc22a2
    
    Conflicts:
    	security/selinux/include/security.h

commit ac11e83cc9c09edd6b20fd90e3ca2ece632fd5c8
Author: Eric Paris <eparis@redhat.com>
Date:   Tue Mar 20 14:35:12 2012 -0400

    SELinux: allow default source/target selectors for user/role/range
    
    When new objects are created we have great and flexible rules to
    determine the type of the new object.  We aren't quite as flexible or
    mature when it comes to determining the user, role, and range.  This
    patch adds a new ability to specify the place a new objects user, role,
    and range should come from.  For users and roles it can come from either
    the source or the target of the operation.  aka for files the user can
    either come from the source (the running process and todays default) or
    it can come from the target (aka the parent directory of the new file)
    
    examples always are done with
    directory context: system_u:object_r:mnt_t:s0-s0:c0.c512
    process context: unconfined_u:unconfined_r:unconfined_t:s0-s0:c0.c1023
    
    [no rule]
    	unconfined_u:object_r:mnt_t:s0   test_none
    [default user source]
    	unconfined_u:object_r:mnt_t:s0   test_user_source
    [default user target]
    	system_u:object_r:mnt_t:s0       test_user_target
    [default role source]
    	unconfined_u:unconfined_r:mnt_t:s0 test_role_source
    [default role target]
    	unconfined_u:object_r:mnt_t:s0   test_role_target
    [default range source low]
    	unconfined_u:object_r:mnt_t:s0 test_range_source_low
    [default range source high]
    	unconfined_u:object_r:mnt_t:s0:c0.c1023 test_range_source_high
    [default range source low-high]
    	unconfined_u:object_r:mnt_t:s0-s0:c0.c1023 test_range_source_low-high
    [default range target low]
    	unconfined_u:object_r:mnt_t:s0 test_range_target_low
    [default range target high]
    	unconfined_u:object_r:mnt_t:s0:c0.c512 test_range_target_high
    [default range target low-high]
    	unconfined_u:object_r:mnt_t:s0-s0:c0.c512 test_range_target_low-high
    
    upstream commit aa893269de6277b44be88e25dcd5331c934c29c4
    
    Change-Id: Ic8f33d05793bf742c70c68ea79e33c7f40ffbd53
    Signed-off-by: Eric Paris <eparis@redhat.com>
    
    Conflicts:
    	security/selinux/include/security.h
    	security/selinux/ss/policydb.c

commit 05931b59d0acc237a0be5e2c51e8283a16b9a0c2
Author: Pablo Neira Ayuso <pablo@netfilter.org>
Date:   Sat Sep 8 02:53:53 2012 +0000

    netlink: kill netlink_set_nonroot
    
    Replace netlink_set_nonroot by one new field `flags' in
    struct netlink_kernel_cfg that is passed to netlink_kernel_create.
    
    This patch also renames NL_NONROOT_* to NL_CFG_F_NONROOT_* since
    now the flags field in nl_table is generic (so we can add more
    flags if needed in the future).
    
    Also adjust all callers in the net-next tree to use these flags
    instead of netlink_set_nonroot.
    
    Signed-off-by: Pablo Neira Ayuso <pablo@netfilter.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>
    
    Conflicts:
    	net/netlink/af_netlink.c

commit 0c91aed800a3d5b65d83f84d53b2878c0121724f
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Thu May 24 17:21:27 2012 -0600

    netlink: Make the sending netlink socket availabe in NETLINK_CB
    
    The sending socket of an skb is already available by it's port id
    in the NETLINK_CB.  If you want to know more like to examine the
    credentials on the sending socket you have to look up the sending
    socket by it's port id and all of the needed functions and data
    structures are static inside of af_netlink.c.  So do the simple
    thing and pass the sending socket to the receivers in the NETLINK_CB.
    
    I intend to use this to get the user namespace of the sending socket
    in inet_diag so that I can report uids in the context of the process
    who opened the socket, the same way I report uids in the contect
    of the process who opens files.
    
    Acked-by: David S. Miller <davem@davemloft.net>
    Acked-by: Serge Hallyn <serge.hallyn@canonical.com>
    Signed-off-by: Eric W. Biederman <ebiederm@xmission.com>

commit aedd3dd1ed77171e4e382991823b4c6c2a59d68a
Author: Pablo Neira Ayuso <pablo@netfilter.org>
Date:   Fri Jun 29 06:15:22 2012 +0000

    netlink: add nlk->netlink_bind hook for module auto-loading
    
    This patch adds a hook in the binding path of netlink.
    
    This is used by ctnetlink to allow module autoloading for the case
    in which one user executes:
    
     conntrack -E
    
    So far, this resulted in nfnetlink loaded, but not
    nf_conntrack_netlink.
    
    I have received in the past many complains on this behaviour.
    
    Signed-off-by: Pablo Neira Ayuso <pablo@netfilter.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 267ade5872b4e96c4b08405e2037ec9af9dd836f
Author: Eric Dumazet <edumazet@google.com>
Date:   Sun Apr 22 21:30:29 2012 +0000

    af_netlink: drop_monitor/dropwatch friendly
    
    Need to consume_skb() instead of kfree_skb() in netlink_dump() and
    netlink_unicast_kernel() to avoid false dropwatch positives.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit cca5681bd16c7fbc55f7dc601e4e11b0360479f5
Author: Eric Dumazet <edumazet@google.com>
Date:   Sun Apr 22 21:30:21 2012 +0000

    af_netlink: cleanups
    
    netlink_destroy_callback() move to avoid forward reference
    
    CodingStyle cleanups
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 5334ace77d6c300508a795bd2b0e21c9719d9854
Author: Eric Dumazet <edumazet@google.com>
Date:   Thu Apr 19 02:24:28 2012 +0000

    netlink: dont drop packet but consume it
    
    When we need to clone skb, we dont drop a packet.
    Call consume_skb() to not confuse dropwatch.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit f435cc79556ac8a3d7a06ec0913bf97f76fd634c
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Thu Oct 11 11:42:01 2012 -0400

    consitify do_mount() arguments
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

commit 4a9bea0babf4f3a684979136be4e5dd74a244136
Author: Pablo Neira Ayuso <pablo@netfilter.org>
Date:   Sat Sep 8 02:53:54 2012 +0000

    netlink: hide struct module parameter in netlink_kernel_create
    
    This patch defines netlink_kernel_create as a wrapper function of
    __netlink_kernel_create to hide the struct module *me parameter
    (which seems to be THIS_MODULE in all existing netlink subsystems).
    
    Suggested by David S. Miller.
    
    Signed-off-by: Pablo Neira Ayuso <pablo@netfilter.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>
    
    Conflicts:
    	drivers/gud/mobicore_kernelapi/main.c

commit a7fd42b58db96dc2b48a1ae6659dd58a4eab83a3
Author: Mel Gorman <mgorman@suse.de>
Date:   Tue Jul 31 16:44:44 2012 -0700

    selinux: tag avc cache alloc as non-critical
    
    Failing to allocate a cache entry will only harm performance not
    correctness.  Do not consume valuable reserve pages for something like
    that.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Acked-by: Eric Paris <eparis@redhat.com>
    Acked-by: Rik van Riel <riel@redhat.com>
    Cc: James Morris <jmorris@namei.org>
    Cc: Christoph Hellwig <hch@infradead.org>
    Cc: David S. Miller <davem@davemloft.net>
    Cc: Eric B Munson <emunson@mgebm.net>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Mike Christie <michaelc@cs.wisc.edu>
    Cc: Neil Brown <neilb@suse.de>
    Cc: Sebastian Andrzej Siewior <sebastian@breakpoint.cc>
    Cc: Trond Myklebust <Trond.Myklebust@netapp.com>
    Cc: Xiaotian Feng <dfeng@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit b04ea7fa71b820cf21345291321bf066bdf97eda
Author: Al Viro <viro@ZenIV.linux.org.uk>
Date:   Sat Jun 9 08:15:16 2012 +0100

    selinux: fix selinux_inode_setxattr oops
    
    OK, what we have so far is e.g.
    	setxattr(path, name, whatever, 0, XATTR_REPLACE)
    with name being good enough to get through xattr_permission().
    Then we reach security_inode_setxattr() with the desired value and size.
    Aha.  name should begin with "security.selinux", or we won't get that
    far in selinux_inode_setxattr().  Suppose we got there and have enough
    permissions to relabel that sucker.  We call security_context_to_sid()
    with value == NULL, size == 0.  OK, we want ss_initialized to be non-zero.
    I.e. after everything had been set up and running.  No problem...
    
    We do 1-byte kmalloc(), zero-length memcpy() (which doesn't oops, even
    thought the source is NULL) and put a NUL there.  I.e. form an empty
    string.  string_to_context_struct() is called and looks for the first
    ':' in there.  Not found, -EINVAL we get.  OK, security_context_to_sid_core()
    has rc == -EINVAL, force == 0, so it silently returns -EINVAL.
    All it takes now is not having CAP_MAC_ADMIN and we are fucked.
    
    All right, it might be a different bug (modulo strange code quoted in the
    report), but it's real.  Easily fixed, AFAICS:
    
    Deal with size == 0, value == NULL case in selinux_inode_setxattr()
    
    Cc: stable@vger.kernel.org
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>
    Tested-by: Dave Jones <davej@redhat.com>
    Reported-by: Dave Jones <davej@redhat.com>
    Signed-off-by: James Morris <james.l.morris@oracle.com>

commit 92ab7b9838b8dcb65df2774ac5e53838443fd412
Author: Eric Paris <eparis@redhat.com>
Date:   Fri Jul 6 14:13:30 2012 -0400

    SELinux: do not check open perms if they are not known to policy
    
    When I introduced open perms policy didn't understand them and I
    implemented them as a policycap.  When I added the checking of open perm
    to truncate I forgot to conditionalize it on the userspace defined
    policy capability.  Running an old policy with a new kernel will not
    check open on open(2) but will check it on truncate.  Conditionalize the
    truncate check the same as the open check.
    
    Signed-off-by: Eric Paris <eparis@redhat.com>
    Cc: stable@vger.kernel.org # 3.4.x
    Signed-off-by: James Morris <james.l.morris@oracle.com>

commit 439b0211f2f90e878e7c844282c8da501cd0b5db
Author: Dorimanx <yuri@bynet.co.il>
Date:   Mon May 18 01:11:00 2015 +0300

    after commit a31f2d17b331db970259e875b7223d3aba7e3821 need to update all the rest used.
    and fix my compiling stuff

commit 50d19308d474d546ba71b5e6271279ea31c56fb5
Author: Pablo Neira Ayuso <pablo@netfilter.org>
Date:   Fri Jun 29 06:15:21 2012 +0000

    netlink: add netlink_kernel_cfg parameter to netlink_kernel_create
    
    This patch adds the following structure:
    
    struct netlink_kernel_cfg {
            unsigned int    groups;
            void            (*input)(struct sk_buff *skb);
            struct mutex    *cb_mutex;
    };
    
    That can be passed to netlink_kernel_create to set optional configurations
    for netlink kernel sockets.
    
    I've populated this structure by looking for NULL and zero parameters at the
    existing code. The remaining parameters that always need to be set are still
    left in the original interface.
    
    That includes optional parameters for the netlink socket creation. This allows
    easy extensibility of this interface in the future.
    
    This patch also adapts all callers to use this new interface.
    
    Signed-off-by: Pablo Neira Ayuso <pablo@netfilter.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 56431362f1f5d5aa4c5b6c05f23f71acc2939cac
Author: David S. Miller <davem@davemloft.net>
Date:   Tue Jun 26 21:41:57 2012 -0700

    selinux: netlink: Move away from NLMSG_PUT().
    
    And use nlmsg_data() while we're here too.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 7d130c4385c2eb6576845dd8a336c9fe41c30df2
Author: Alban Crequy <alban.crequy@collabora.co.uk>
Date:   Mon May 14 03:56:39 2012 +0000

    netfilter: selinux: switch hook PFs to nfproto
    
    This patch is a cleanup. Use NFPROTO_* for consistency with other
    netfilter code.
    
    Signed-off-by: Alban Crequy <alban.crequy@collabora.co.uk>
    Reviewed-by: Javier Martinez Canillas <javier.martinez@collabora.co.uk>
    Reviewed-by: Vincent Sanders <vincent.sanders@collabora.co.uk>
    Signed-off-by: Pablo Neira Ayuso <pablo@netfilter.org>

commit 0211f73508115a98050efa72c3a2d09cabb7aaf9
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Wed May 30 13:30:51 2012 -0400

    split ->file_mmap() into ->mmap_addr()/->mmap_file()
    
    ... i.e. file-dependent and address-dependent checks.
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

commit 28b840740771688fdeed453d4aab862b4340e09f
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Wed May 30 13:11:37 2012 -0400

    split cap_mmap_addr() out of cap_file_mmap()
    
    ... switch callers.
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

commit 8a5f85dffb39a5633538a2074816ba3df94aa4ea
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Mon Apr 2 19:40:47 2012 -0400

    selinuxfs snprintf() misuses
    
    a) %d does _not_ produce a page worth of output
    b) snprintf() doesn't return negatives - it used to in old glibc, but
    that's the kernel...
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

commit 7f17141f7ea7082c9208fd4390122e62b1079513
Author: Eric Paris <eparis@redhat.com>
Date:   Thu Apr 5 13:51:53 2012 -0400

    SELinux: remove unused common_audit_data in flush_unauthorized_files
    
    We don't need this variable and it just eats stack space.  Remove it.
    
    Signed-off-by: Eric Paris <eparis@redhat.com>

commit 4c5c1f4be6118f4edf5b180312b8a61fe598d1db
Author: Wanlong Gao <gaowanlong@cn.fujitsu.com>
Date:   Wed Mar 7 22:17:14 2012 +0800

    SELinux: avc: remove the useless fields in avc_add_callback
    
    avc_add_callback now just used for registering reset functions
    in initcalls, and the callback functions just did reset operations.
    So, reducing the arguments to only one event is enough now.
    
    Signed-off-by: Wanlong Gao <gaowanlong@cn.fujitsu.com>
    Signed-off-by: Eric Paris <eparis@redhat.com>

commit dcd328a42e27a33c8bfb016d0c5e0b15e511cf4a
Author: Wanlong Gao <gaowanlong@cn.fujitsu.com>
Date:   Wed Mar 7 22:17:13 2012 +0800

    SELinux: replace weak GFP_ATOMIC to GFP_KERNEL in avc_add_callback
    
    avc_add_callback now only called from initcalls, so replace the
    weak GFP_ATOMIC to GFP_KERNEL, and mark this function __init
    to make a warning when not been called from initcalls.
    
    Signed-off-by: Wanlong Gao <gaowanlong@cn.fujitsu.com>
    Signed-off-by: Eric Paris <eparis@redhat.com>

commit 687a9839e174265ae939bdaae3ad8e510d54bbe2
Author: Eric Paris <eparis@redhat.com>
Date:   Wed Apr 4 15:01:43 2012 -0400

    SELinux: unify the selinux_audit_data and selinux_late_audit_data
    
    We no longer need the distinction.  We only need data after we decide to do an
    audit.  So turn the "late" audit data into just "data" and remove what we
    currently have as "data".
    
    Signed-off-by: Eric Paris <eparis@redhat.com>

commit 2fb7ee3df885af4237810b800837346d07fe71aa
Author: Eric Paris <eparis@redhat.com>
Date:   Wed Apr 4 15:01:43 2012 -0400

    SELinux: remove auditdeny from selinux_audit_data
    
    It's just takin' up space.
    
    Signed-off-by: Eric Paris <eparis@redhat.com>

commit eb941cf6bd9252081827a3979648e2c17d429783
Author: Eric Paris <eparis@redhat.com>
Date:   Wed Apr 4 15:01:43 2012 -0400

    LSM: do not initialize common_audit_data to 0
    
    It isn't needed.  If you don't set the type of the data associated with
    that type it is a pretty obvious programming bug.  So why waste the cycles?
    
    Signed-off-by: Eric Paris <eparis@redhat.com>

commit ebd040168629b877bba0beead9dd8757c1f5a259
Author: Eric Paris <eparis@redhat.com>
Date:   Wed Apr 4 15:01:43 2012 -0400

    LSM: remove the task field from common_audit_data
    
    There are no legitimate users.  Always use current and get back some stack
    space for the common_audit_data.
    
    Signed-off-by: Eric Paris <eparis@redhat.com>

commit ac125939e870a8db5d4af1a23920c486b86e37af
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Tue May 19 19:36:07 2015 +0200

    rq_stats: add missing bricked.hotplug

commit 80a4c5e7b1af301c1ee829219ebbacc736ce7a42
Author: Eric Paris <eparis@redhat.com>
Date:   Wed Apr 4 15:01:42 2012 -0400

    LSM: remove the COMMON_AUDIT_DATA_INIT type expansion
    
    Just open code it so grep on the source code works better.
    
    Signed-off-by: Eric Paris <eparis@redhat.com>

commit 8f4d6c3c19cbc4e1ccef0ffde22f48272c6a8089
Author: Eric Paris <eparis@redhat.com>
Date:   Wed Apr 4 15:01:42 2012 -0400

    SELinux: move common_audit_data to a noinline slow path function
    
    selinux_inode_has_perm is a hot path.  Instead of declaring the
    common_audit_data on the stack move it to a noinline function only used in
    the rare case we need to send an audit message.
    
    Signed-off-by: Eric Paris <eparis@redhat.com>

commit cbb97bf49cf5a45e0c9f3e2b5258281db79d6f21
Author: Eric Paris <eparis@redhat.com>
Date:   Wed Apr 4 15:01:42 2012 -0400

    SELinux: remove inode_has_perm_noadp
    
    Both callers could better be using file_has_perm() to get better audit
    results.
    
    Signed-off-by: Eric Paris <eparis@redhat.com>

commit b33d2b6b164a2e0292c4010d6b19692591f216e6
Author: Eric Paris <eparis@redhat.com>
Date:   Wed Apr 4 15:01:42 2012 -0400

    SELinux: delay initialization of audit data in selinux_inode_permission
    
    We pay a rather large overhead initializing the common_audit_data.
    Since we only need this information if we actually emit an audit
    message there is little need to set it up in the hot path.  This patch
    splits the functionality of avc_has_perm() into avc_has_perm_noaudit(),
    avc_audit_required() and slow_avc_audit().  But we take care of setting
    up to audit between required() and the actual audit call.  Thus saving
    measurable time in a hot path.
    
    Signed-off-by: Stephen Smalley <sds@tycho.nsa.gov>
    Signed-off-by: Eric Paris <eparis@redhat.com>

commit 8e4f3c329410b2d6e6b4712cf7c8c109daa985f5
Author: Jeff Vander Stoep <jeffv@google.com>
Date:   Wed Apr 8 11:27:46 2015 -0700

    SELinux: per-command whitelisting of ioctls
    
     note that this patch depends on a prior patch that is already in
     android-3.4 but has not apparently found its way into the msm 3.4
     branches (but is included in exynos and tegra),
     https://android-review.googlesource.com/#/c/92962/
    
    Extend the generic ioctl permission check with support for per-command
    filtering. Source/target/class sets including the ioctl permission may
    additionally include a set of commands. Example:
    
    allow <source> <target>:<class> { 0x8910-0x8926 0x892A-0x8935 }
    auditallow <source> <target>:<class> 0x892A
    
    When ioctl commands are omitted only the permissions are checked. This
    feature is intended to provide finer granularity for the ioctl
    permission which may be too imprecise in some circumstances. For
    example, the same driver may use ioctls to provide important and
    benign functionality such as driver version or socket type as well as
    dangerous capabilities such as debugging features, read/write/execute
    to physical memory or access to sensitive data. Per-command filtering
    provides a mechanism to reduce the attack surface of the kernel, and
    limit applications to the subset of commands required.
    
    The format of the policy binary has been modified to include ioctl
    commands, and the policy version number has been incremented to
    POLICYDB_VERSION_IOCTL_OPERATIONS=30 to account for the format change.
    
    Bug: 18087110
    Change-Id: Ibf0e36728f6f3f0d5af56ccdeddee40800af689d
    Signed-off-by: Jeff Vander Stoep <jeffv@google.com>

commit a3442183d53e45afafdd4f19851c18c1f96b3bda
Author: Eric Paris <eparis@redhat.com>
Date:   Wed Apr 4 13:46:46 2012 -0400

    SELinux: remove needless sel_div function
    
    I'm not really sure what the idea behind the sel_div function is, but it's
    useless.  Since a and b are both unsigned, it's impossible for a % b < 0.
    That means that part of the function never does anything.  Thus it's just a
    normal /.  Just do that instead.  I don't even understand what that operation
    was supposed to mean in the signed case however....
    
    If it was signed:
    sel_div(-2, 4) == ((-2 / 4) - ((-2 % 4) < 0))
    		  ((0)      - ((-2)     < 0))
    		  ((0)      - (1))
    		  (-1)
    
    What actually happens:
    sel_div(-2, 4) == ((18446744073709551614 / 4) - ((18446744073709551614 % 4) < 0))
    		  ((4611686018427387903)      - ((2 < 0))
    		  (4611686018427387903        - 0)
    		  ((unsigned int)4611686018427387903)
    		  (4294967295)
    
    Neither makes a whole ton of sense to me.  So I'm getting rid of the
    function entirely.
    
    Signed-off-by: Eric Paris <eparis@redhat.com>

commit c4f9c338d86fabfc40805fde735fcb4d57af7b54
Author: Eric Paris <eparis@redhat.com>
Date:   Wed Apr 4 13:45:49 2012 -0400

    SELinux: audit failed attempts to set invalid labels
    
    We know that some yum operation is causing CAP_MAC_ADMIN failures.  This
    implies that an RPM is laying down (or attempting to lay down) a file with
    an invalid label.  The problem is that we don't have any information to
    track down the cause.  This patch will cause such a failure to report the
    failed label in an SELINUX_ERR audit message.  This is similar to the
    SELINUX_ERR reports on invalid transitions and things like that.  It should
    help run down problems on what is trying to set invalid labels in the
    future.
    
    Resulting records look something like:
    type=AVC msg=audit(1319659241.138:71): avc:  denied  { mac_admin } for pid=2594 comm="chcon" capability=33 scontext=unconfined_u:unconfined_r:unconfined_t:s0-s0:c0.c1023 tcontext=unconfined_u:unconfined_r:unconfined_t:s0-s0:c0.c1023 tclass=capability2
    type=SELINUX_ERR msg=audit(1319659241.138:71): op=setxattr invalid_context=unconfined_u:object_r:hello:s0
    type=SYSCALL msg=audit(1319659241.138:71): arch=c000003e syscall=188 success=no exit=-22 a0=a2c0e0 a1=390341b79b a2=a2d620 a3=1f items=1 ppid=2519 pid=2594 auid=0 uid=0 gid=0 euid=0 suid=0 fsuid=0 egid=0 sgid=0 fsgid=0 tty=pts0 ses=1 comm="chcon" exe="/usr/bin/chcon" subj=unconfined_u:unconfined_r:unconfined_t:s0-s0:c0.c1023 key=(null)
    type=CWD msg=audit(1319659241.138:71):  cwd="/root" type=PATH msg=audit(1319659241.138:71): item=0 name="test" inode=785879 dev=fc:03 mode=0100644 ouid=0 ogid=0 rdev=00:00 obj=unconfined_u:object_r:admin_home_t:s0
    
    Signed-off-by: Eric Paris <eparis@redhat.com>

commit 82a2d4d3d4d9e18b34562113304ccc63843e68ff
Author: Eric Paris <eparis@redhat.com>
Date:   Wed Apr 4 13:45:40 2012 -0400

    SELinux: rename dentry_open to file_open
    
    dentry_open takes a file, rename it to file_open
    
    Signed-off-by: Eric Paris <eparis@redhat.com>
    
    Conflicts:
    	security/security.c

commit 88fa79b81c75fa6af328ca25c6e2c57d5a1a5215
Author: Eric Paris <eparis@redhat.com>
Date:   Wed Apr 4 13:45:34 2012 -0400

    SELinux: check OPEN on truncate calls
    
    In RH BZ 578841 we realized that the SELinux sandbox program was allowed to
    truncate files outside of the sandbox.  The reason is because sandbox
    confinement is determined almost entirely by the 'open' permission.  The idea
    was that if the sandbox was unable to open() files it would be unable to do
    harm to those files.  This turns out to be false in light of syscalls like
    truncate() and chmod() which don't require a previous open() call.  I looked
    at the syscalls that did not have an associated 'open' check and found that
    truncate(), did not have a seperate permission and even if it did have a
    separate permission such a permission owuld be inadequate for use by
    sandbox (since it owuld have to be granted so liberally as to be useless).
    This patch checks the OPEN permission on truncate.  I think a better solution
    for sandbox is a whole new permission, but at least this fixes what we have
    today.
    
    Signed-off-by: Eric Paris <eparis@redhat.com>

commit 6a43cefeb49a253958a893b636e6500267ee4a0c
Author: Eric Paris <eparis@redhat.com>
Date:   Thu Feb 16 15:08:39 2012 -0500

    SELinux: loosen DAC perms on reading policy
    
    There is no reason the DAC perms on reading the policy file need to be root
    only.  There are selinux checks which should control this access.
    
    Signed-off-by: Eric Paris <eparis@redhat.com>

commit fa1d859b65e1d571fc7c8eda7b6760d8222e1590
Author: Eric Paris <eparis@redhat.com>
Date:   Thu Feb 16 15:08:39 2012 -0500

    SELinux: allow seek operations on the file exposing policy
    
    sesearch uses:
    lseek(3, 0, SEEK_SET)                   = -1 ESPIPE (Illegal seek)
    
    Make that work.
    
    Signed-off-by: Eric Paris <eparis@redhat.com>

commit b496548ba790aa8a27e149525c52a47e308325ba
Author: Dorimanx <yuri@bynet.co.il>
Date:   Mon Apr 27 13:37:58 2015 +0300

    MSM RQ STATS: Sync with KK kernel.

commit 4ad3235fb5d59c5a21932057d7bf9cd2360f636d
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Tue May 19 19:21:19 2015 +0200

    config: update files

commit ef9fa883e538dcb7e275ddac2e525817b3b8cf55
Author: Stephen Boyd <sboyd@codeaurora.org>
Date:   Mon Dec 8 18:21:24 2014 -0800

    soc: qcom: rq_stats: Always show idle time left
    
    The def_timer_ms file indicates how much time has passed since we
    wrote to the file to start the timer, but the time is only
    updated when the work runs, instead of on demand when the file is
    read. If we're poll(3)ing this file and another file and the
    other file wakes our process up from sleep we won't know without
    measuring time ourselves. Make this easier on userspace by always
    showing the amount of time since we wrote the file so that we can
    figure out how long we were asleep for.
    
    Change-Id: I06135a94a41a5cd13454a92f2de7e63683ede4a5
    Signed-off-by: Stephen Boyd <sboyd@codeaurora.org>

commit 766194eeb304253e98f2da7f07a08becd67465dd
Author: Pranav Vashi <neobuddy89@gmail.com>
Date:   Tue Sep 16 23:43:29 2014 +0530

    msm: msm_rq_stats: Add proper attribute to enable hotplug
    
    * hotplug_disable attribute resets when device suspend and
      sets when device resumes.
    * Thereby, the value is dynamic. Perhaps, this value is read
      by userspace, so we don't want to mess with it further.
    * Restore hotplug_disable to original format of read-only.
    * Introduce hotplug_enable attribute which will do the job.
    * We have "enable" attribute for other hotplug drivers, so
      this implementation aligns with them.
    
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>
    
    Dorimanx Note:
    Ported for my rq_stats code.
    
    merged also the bricked hotplug on/off

commit a4489b9cb3a3008bcdefb6aa07dd1beb1b5916d1
Author: Christopher R. Palmer <crpalmer@gmail.com>
Date:   Fri Sep 12 18:05:10 2014 -0400

    msm: msm_rq_stats: Tone down mpdecision
    
    Add a new Kconfig to enable more conservative computations sent
    to mpdecision:
    
     * Do a time window weighted load computation instead of just a sum
     * Do an sliding window averaged time computation for number runnable
    
    The purpose of these changes is to make mpdecision less likely to
    keep extra cores online.  It currently has a tendancy to run more
    cores running than needed and this can even cause some lag issues.
    
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 6c6c85abc9e0a1806018bccaeb29e4d3e3bca99f
Author: alucard24 <dmbaoh2@gmail.com>
Date:   Thu Aug 14 13:33:36 2014 +0200

    msm_rq_stats: fixed system suspend/resume if lock_hotplug_disabled is equal to 0 and resuming is coming.

commit d66cc94972da7bc7669a2bcc61c1df62276bb833
Author: Dorimanx <yuri@bynet.co.il>
Date:   Mon Apr 27 12:58:49 2015 +0300

    update io_is_busy for arch/arm/mach-msm/msm_rq_stats.c

commit f728eedbe2566fe66a1f21e6793909904bb1840b
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Mon May 26 12:44:17 2014 +0200

    Fixed msm_rq_stats disabling enabling.

commit cf282a544f69eaf66f884375f28fe6cfcc76fdf2
Author: myfluxi <linflux@arcor.de>
Date:   Sat May 24 12:00:52 2014 +0200

    msm: rq_stats: Disable hotplug and stats by default
    
    Userspace closed source mpdecision binary is nessecary for PowerHAL to work.
    As it depends on rq_stats, let's at least disable hotplugging and stats
    collection by default. /sys/devices/system/cpu/cpu0/rq-stats/hotplug_disable
    is now writable, so hotplug can be reenabled.
    
    Also, add some bits related to current cpu policy.

commit 22fa4642062834bd3638b7cec952e08c5176595d
Author: myfluxi <linflux@arcor.de>
Date:   Wed May 14 18:04:21 2014 +0200

    msm: rq_stats: Calculate load based on current freq limit
    
    We are not using rq_stats any more so this commit is just for the
    sake of completeness.  Prior to this patch, load has been calculated
    based on an initial value of policy.cpuinfo.max_freq. This value may
    change for several reasons: userspace settings, thermal throttling
    (hello hammerhead!) or extended frequency tables where the absolute
    maximum level is often not used during regular operations. This
    results in unreliable calculations. Consider current policy.max to
    fix this behaviour.
    
    Conflicts:
    	arch/arm/mach-msm/msm_rq_stats.c

commit 1ef1ad9d63ded808a18c589e769800a63a56bcb8
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Tue May 19 19:10:35 2015 +0200

    fixing some compiling errors

commit d3d2f133af61a3eadc2afd258d6201b53419bb5f
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Thu Apr 9 12:55:33 2015 +0200

    crypto: sha1 - implement base layer for SHA-1
    
    To reduce the number of copies of boilerplate code throughout
    the tree, this patch implements generic glue for the SHA-1
    algorithm. This allows a specific arch or hardware implementation
    to only implement the special handling that it needs.
    
    The users need to supply an implementation of
    
      void (sha1_block_fn)(struct sha1_state *sst, u8 const *src, int blocks)
    
    and pass it to the SHA-1 base functions. For easy casting between the
    prototype above and existing block functions that take a 'u32 state[]'
    as their first argument, the 'state' member of struct sha1_state is
    moved to the base of the struct.
    
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>

commit 55fb30004e90932759dedac3dadfe1309dfd02ba
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Tue May 19 14:18:45 2015 +0200

    crypto: fix my merge fault

commit 334c288a7820ec4615a2c9a20385dfd60a3a27b8
Author: Stephan Mueller <smueller@chronox.de>
Date:   Sat Feb 28 20:50:40 2015 +0100

    crypto: algif - enable AEAD interface compilation
    
    Enable compilation of the AEAD AF_ALG support and provide a Kconfig
    option to compile the AEAD AF_ALG support.
    
    Signed-off-by: Stephan Mueller <smueller@chronox.de>
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>

commit c77bd88b34f7efe80e763b2957f8c75685ca0e9b
Author: Stephan Mueller <smueller@chronox.de>
Date:   Sat Feb 28 20:50:00 2015 +0100

    crypto: algif - add AEAD support
    
    This patch adds the AEAD support for AF_ALG.
    
    The implementation is based on algif_skcipher, but contains heavy
    modifications to streamline the interface for AEAD uses.
    
    To use AEAD, the user space consumer has to use the salg_type named
    "aead".
    
    The AEAD implementation includes some overhead to calculate the size of
    the ciphertext, because the AEAD implementation of the kernel crypto API
    makes implied assumption on the location of the authentication tag. When
    performing an encryption, the tag will be added to the created
    ciphertext (note, the tag is placed adjacent to the ciphertext). For
    decryption, the caller must hand in the ciphertext with the tag appended
    to the ciphertext. Therefore, the selection of the used memory
    needs to add/subtract the tag size from the source/destination buffers
    depending on the encryption type. The code is provided with comments
    explaining when and how that operation is performed.
    
    A fully working example using all aspects of AEAD is provided at
    http://www.chronox.de/libkcapi.html
    
    Signed-off-by: Stephan Mueller <smueller@chronox.de>
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>

commit 6dcd524c682c47e916a69425e412fb0cb80e955b
Author: Jarod Wilson <jarod@redhat.com>
Date:   Fri Jan 23 12:42:15 2015 -0500

    crypto: testmgr - mark rfc4106(gcm(aes)) as fips_allowed
    
    This gcm variant is popular for ipsec use, and there are folks who would
    like to use it while in fips mode. Mark it with fips_allowed=1 to
    facilitate that.
    
    CC: LKML <linux-kernel@vger.kernel.org>
    CC: Stephan Mueller <smueller@atsec.com>
    Signed-off-by: Jarod Wilson <jarod@redhat.com>
    Acked-by: Stephan Mueller <smueller@atsec.com>
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>

commit ea6fb3926261099536273c45576524b0acabafe2
Author: Herbert Xu <herbert@gondor.apana.org.au>
Date:   Fri Jan 16 19:51:20 2015 +1100

    crypto: seqiv - Ensure that IV size is at least 8 bytes
    
    Since seqiv is designed for IPsec we need to be able to accomodate
    the whole IPsec sequence number in order to ensure the uniqueness
    of the IV.
    
    This patch forbids any algorithm with an IV size of less than 8
    from using it.  This should have no impact on existing users since
    they all have an IV size of 8.
    
    Reported-by: Maciej ?enczykowski <zenczykowski@gmail.com>
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>
    Acked-by: Maciej ?enczykowski <zenczykowski@gmail.com>

commit 78e2fb35f3f2caf20b34866ebb48f69e1653ef2d
Author: Wei Yongjun <yongjun_wei@trendmicro.com.cn>
Date:   Wed Jan 14 09:14:41 2015 +0800

    crypto: algif_rng - fix sparse non static symbol warning
    
    Fixes the following sparse warnings:
    
    crypto/algif_rng.c:185:13: warning:
     symbol 'rng_exit' was not declared. Should it be static?
    
    Signed-off-by: Wei Yongjun <yongjun_wei@trendmicro.com.cn>
    Acked-by: Stephan Mueller <smueller@chronox.de>
    Acked-by: Neil Horman <nhorman@tuxdriver.com>
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>

commit fcc57e2a0fd62141a929489984e184955d638c26
Author: Stephan Mueller <smueller@chronox.de>
Date:   Thu Dec 25 23:00:39 2014 +0100

    crypto: algif_rng - enable RNG interface compilation
    
    Enable compilation of the RNG AF_ALG support and provide a Kconfig
    option to compile the RNG AF_ALG support.
    
    Signed-off-by: Stephan Mueller <smueller@chronox.de>
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>

commit 3659634842e0b677c5a34b163138e9bd6630a106
Author: Stephan Mueller <smueller@chronox.de>
Date:   Thu Dec 25 23:00:06 2014 +0100

    crypto: algif_rng - add random number generator support
    
    This patch adds the random number generator support for AF_ALG.
    
    A random number generator's purpose is to generate data without
    requiring the caller to provide any data. Therefore, the AF_ALG
    interface handler for RNGs only implements a callback handler for
    recvmsg.
    
    The following parameters provided with a recvmsg are processed by the
    RNG callback handler:
    
    	* sock - to resolve the RNG context data structure accessing the
    	  RNG instance private to the socket
    
    	* len - this parameter allows userspace callers to specify how
    	  many random bytes the RNG shall produce and return. As the
    	  kernel context for the RNG allocates a buffer of 128 bytes to
    	  store random numbers before copying them to userspace, the len
    	  parameter is checked that it is not larger than 128. If a
    	  caller wants more random numbers, a new request for recvmsg
    	  shall be made.
    
    The size of 128 bytes is chose because of the following considerations:
    
    	* to increase the memory footprint of the kernel too much (note,
    	  that would be 128 bytes per open socket)
    
    	* 128 is divisible by any typical cryptographic block size an
    	  RNG may have
    
    	* A request for random numbers typically only shall supply small
    	  amount of data like for keys or IVs that should only require
    	  one invocation of the recvmsg function.
    
    Note, during instantiation of the RNG, the code checks whether the RNG
    implementation requires seeding. If so, the RNG is seeded with output
    from get_random_bytes.
    
    A fully working example using all aspects of the RNG interface is
    provided at http://www.chronox.de/libkcapi.html
    
    Signed-off-by: Stephan Mueller <smueller@chronox.de>
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>

commit c67396cbf8b82589bbc048faf4261e7d3af19fc8
Author: Stephan Mueller <smueller@chronox.de>
Date:   Tue Dec 23 09:34:03 2014 +0100

    crypto: af_alg - zeroize key data
    
    alg_setkey should zeroize the sensitive data after use.
    
    Signed-off-by: Stephan Mueller <smueller@chronox.de>
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>

commit 2cc82b453fa27c6ad8ae5b7b9046a16667ecf853
Author: Tadeusz Struk <tadeusz.struk@intel.com>
Date:   Mon Dec 8 12:03:42 2014 -0800

    crypto: algif - Mark sgl end at the end of data
    
    algif_skcipher sends 127 sgl buffers for encryption regardless of how
    many buffers acctually have data to process, where the few first with
    valid len and the rest with zero len. This is not very eficient.
    This patch marks the last one with data as the last one to process.
    
    Signed-off-by: Tadeusz Struk <tadeusz.struk@intel.com>
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>

commit 0d978ba458630cf46b2cd1a9aab0451c63ffc9ec
Author: Stephan Mueller <smueller@chronox.de>
Date:   Sun Dec 7 23:21:42 2014 +0100

    crypto: af_alg - add setsockopt for auth tag size
    
    Use setsockopt on the tfm FD to provide the authentication tag size for
    an AEAD cipher. This is achieved by adding a callback function which is
    intended to be used by the AEAD AF_ALG implementation.
    
    The optlen argument of the setsockopt specifies the authentication tag
    size to be used with the AEAD tfm.
    
    Signed-off-by: Stephan Mueller <smueller@chronox.de>
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>

commit 1da6a3b7b9fa32fc10d1870d90accafb81b43460
Author: Joshua I. James <joshua@cybercrimetech.com>
Date:   Fri Dec 5 15:00:10 2014 +0900

    crypto: api - fixed style erro in algapi.c
    
    Fixed style error identified by checkpatch.
    
    WARNING: Missing a blank line after declarations
    +               int err = crypto_remove_alg(&inst->alg, &users);
    +               BUG_ON(err);
    
    Signed-off-by: Joshua I. James <joshua@cybercrimetech.com>
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>

commit 6320bd8c3de0ede443bf419e926f3107ca1f3929
Author: Joshua I. James <joshua@cybercrimetech.com>
Date:   Fri Dec 5 14:44:54 2014 +0900

    crypto: ahash - fixed style error in ahash.c
    
    Fixed style error identified by checkpatch.
    
    WARNING: Missing a blank line after declarations
    +               unsigned int unaligned = alignmask + 1 - (offset & alignmask);
    +               if (nbytes > unaligned)
    
    Signed-off-by: Joshua I. James <joshua@cybercrimetech.com>
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>

commit 3fb19599ce81ca426706c439fdf493aab1e594c7
Author: Joshua I. James <joshua@cybercrimetech.com>
Date:   Fri Dec 5 14:38:40 2014 +0900

    crypto: af_alg - fixed style error in af_alg.c
    
    Fixed style error identified by checkpatch.
    
    ERROR: space required before the open parenthesis '('
    +               switch(cmsg->cmsg_type) {
    
    Signed-off-by: Joshua I. James <joshua@cybercrimetech.com>
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>

commit 67c3301c2bb012d4581c5a7d0858275d04c86fb1
Author: Joshua I. James <joshua@cybercrimetech.com>
Date:   Fri Dec 5 14:24:44 2014 +0900

    crypto: aead - fixed style error in aead.c
    
    Fixed style error identified by checkpatch.
    
    ERROR: do not use assignment in if condition
    +       if ((err = crypto_register_instance(tmpl, inst))) {
    
    Signed-off-by: Joshua I. James <joshua@cybercrimetech.com>
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>

commit 56dd3d925196f1425b1d0ec4215ab7f0ebad6ead
Author: Joshua I. James <joshua@cybercrimetech.com>
Date:   Fri Dec 5 14:06:16 2014 +0900

    crypto: ablkcipher - fixed style errors in ablkcipher.c
    
    Fixed style errors reported by checkpatch.
    
    WARNING: Missing a blank line after declarations
    +       u8 *end_page = (u8 *)(((unsigned long)(start + len - 1)) & PAGE_MASK);
    +       return max(start, end_page);
    
    WARNING: line over 80 characters
    +               scatterwalk_start(&walk->out, scatterwalk_sg_next(walk->out.sg));
    
    WARNING: Missing a blank line after declarations
    +               int err = ablkcipher_copy_iv(walk, tfm, alignmask);
    +               if (err)
    
    ERROR: do not use assignment in if condition
    +       if ((err = crypto_register_instance(tmpl, inst))) {
    
    Signed-off-by: Joshua I. James <joshua@cybercrimetech.com>
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>

commit 157efaf75c93300e55810d307e7f6f3d31c36d7e
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Sat Apr 11 15:32:34 2015 +0200

    crypto: arm - workaround for building with old binutils
    
    Old versions of binutils (before 2.23) do not yet understand the
    crypto-neon-fp-armv8 fpu instructions, and an attempt to build these
    files results in a build failure:
    
    arch/arm/crypto/aes-ce-core.S:133: Error: selected processor does not support ARM mode `vld1.8 {q10-q11},[ip]!'
    arch/arm/crypto/aes-ce-core.S:133: Error: bad instruction `aese.8 q0,q8'
    arch/arm/crypto/aes-ce-core.S:133: Error: bad instruction `aesmc.8 q0,q0'
    arch/arm/crypto/aes-ce-core.S:133: Error: bad instruction `aese.8 q0,q9'
    arch/arm/crypto/aes-ce-core.S:133: Error: bad instruction `aesmc.8 q0,q0'
    
    Since the affected versions are still in widespread use, and this breaks
    'allmodconfig' builds, we should try to at least get a successful kernel
    build. Unfortunately, I could not come up with a way to make the Kconfig
    symbol depend on the binutils version, which would be the nicest solution.
    
    Instead, this patch uses the 'as-instr' Kbuild macro to find out whether
    the support is present in the assembler, and otherwise emits a non-fatal
    warning indicating which selected modules could not be built.
    
    Signed-off-by: Arnd Bergmann <arnd@arndb.de>
    Link: http://storage.kernelci.org/next/next-20150410/arm-allmodconfig/build.log
    Fixes: 864cbeed4ab22d ("crypto: arm - add support for SHA1 using ARMv8 Crypto Instructions")
    [ard.biesheuvel:
     - omit modules entirely instead of building empty ones if binutils is too old
     - update commit log accordingly]
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>

commit 97a0989da2cc6bda193293f320a1697ea1113af2
Author: Arnd Bergmann <arnd@arndb.de>
Date:   Sat Apr 11 10:48:44 2015 +0200

    crypto: arm/sha256 - avoid sha256 code on ARMv7-M
    
    The sha256 assembly implementation can deal with all architecture levels
    from ARMv4 to ARMv7-A, but not with ARMv7-M. Enabling it in an
    ARMv7-M kernel results in this build failure:
    
    arm-linux-gnueabi-ld: error: arch/arm/crypto/sha256_glue.o: Conflicting architecture profiles M/A
    arm-linux-gnueabi-ld: failed to merge target specific data of file arch/arm/crypto/sha256_glue.o
    
    This adds a Kconfig dependency to prevent the code from being disabled
    for ARMv7-M.
    
    Signed-off-by: Arnd Bergmann <arnd@arndb.de>
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>

commit de8cf5426aa1c5f65b5e0bf23e1fbd90607252ca
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Thu Apr 9 12:55:43 2015 +0200

    crypto: arm/sha2-ce - move SHA-224/256 ARMv8 implementation to base layer
    
    This removes all the boilerplate from the existing implementation,
    and replaces it with calls into the base layer.
    
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>

commit 02b1d0493c98a827e88f6e9fdcdf58282bd70c94
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Thu Apr 9 12:55:42 2015 +0200

    crypto: arm/sha256 - move SHA-224/256 ASM/NEON implementation to base layer
    
    This removes all the boilerplate from the existing implementation,
    and replaces it with calls into the base layer.
    
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>

commit d41948f4fdf6bd509695ab0468d1a713a683bfa6
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Thu Apr 9 12:55:41 2015 +0200

    crypto: arm/sha1-ce - move SHA-1 ARMv8 implementation to base layer
    
    This removes all the boilerplate from the existing implementation,
    and replaces it with calls into the base layer.
    
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>

commit 50b4cf4677761a2f453badc31524b19cfaf2d3a8
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Thu Apr 9 12:55:40 2015 +0200

    crypto: arm/sha1_neon - move SHA-1 NEON implementation to base layer
    
    This removes all the boilerplate from the existing implementation,
    and replaces it with calls into the base layer.
    
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>

commit 4d15c94209280799cd6edd343322c016ecf7a4f9
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Thu Apr 9 12:55:39 2015 +0200

    crypto: arm/sha1 - move SHA-1 ARM asm implementation to base layer
    
    This removes all the boilerplate from the existing implementation,
    and replaces it with calls into the base layer.
    
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>

commit 8f344c42bb5d3221b3a9a943385e6212277b0465
Author: Sami Tolvanen <samitolvanen@google.com>
Date:   Fri Apr 3 18:03:40 2015 +0800

    crypto: arm/sha256 - Add optimized SHA-256/224
    
    Add Andy Polyakov's optimized assembly and NEON implementations for
    SHA-256/224.
    
    The sha256-armv4.pl script for generating the assembly code is from
    OpenSSL commit 51f8d095562f36cdaa6893597b5c609e943b0565.
    
    Compared to sha256-generic these implementations have the following
    tcrypt speed improvements on Motorola Nexus 6 (Snapdragon 805):
    
      bs    b/u      sha256-neon  sha256-asm
      16    16       x1.32        x1.19
      64    16       x1.27        x1.15
      64    64       x1.36        x1.20
      256   16       x1.22        x1.11
      256   64       x1.36        x1.19
      256   256      x1.59        x1.23
      1024  16       x1.21        x1.10
      1024  256      x1.65        x1.23
      1024  1024     x1.76        x1.25
      2048  16       x1.21        x1.10
      2048  256      x1.66        x1.23
      2048  1024     x1.78        x1.25
      2048  2048     x1.79        x1.25
      4096  16       x1.20        x1.09
      4096  256      x1.66        x1.23
      4096  1024     x1.79        x1.26
      4096  4096     x1.82        x1.26
      8192  16       x1.20        x1.09
      8192  256      x1.67        x1.23
      8192  1024     x1.80        x1.26
      8192  4096     x1.85        x1.28
      8192  8192     x1.85        x1.27
    
    Where bs refers to block size and b/u to bytes per update.
    
    Signed-off-by: Sami Tolvanen <samitolvanen@google.com>
    Cc: Andy Polyakov <appro@openssl.org>
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>

commit efd1c48e85b5898139df2d1ced76d5cdca039608
Author: Stephan Mueller <smueller@chronox.de>
Date:   Mon Mar 30 22:09:53 2015 +0200

    crypto: aes-ce - mark ARMv8 AES helper ciphers
    
    Flag all ARMv8 AES helper ciphers as internal ciphers to prevent
    them from being called by normal users.
    
    Signed-off-by: Stephan Mueller <smueller@chronox.de>
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>

commit ebf24207f2f7bf0035e5f8d598484e2c865da907
Author: Stephan Mueller <smueller@chronox.de>
Date:   Mon Mar 30 22:09:27 2015 +0200

    crypto: aesbs - mark NEON bit sliced AES helper ciphers
    
    Flag all NEON bit sliced AES helper ciphers as internal ciphers to
    prevent them from being called by normal users.
    
    Signed-off-by: Stephan Mueller <smueller@chronox.de>
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>

commit a6aa7e6869c2a888d8d93b294f9cf4b151f1d273
Author: Stephan Mueller <smueller@chronox.de>
Date:   Mon Mar 30 22:02:36 2015 +0200

    crypto: ghash-ce - mark GHASH ARMv8 vmull.p64 helper ciphers
    
    Flag all GHASH ARMv8 vmull.p64 helper ciphers as internal ciphers
    to prevent them from being called by normal users.
    
    Signed-off-by: Stephan Mueller <smueller@chronox.de>
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>

commit de4fb216e8532a3c381b02b4311d0265fbc4f032
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Mon Mar 23 21:33:09 2015 +0100

    crypto: arm/ghash - fix big-endian bug in ghash
    
    This fixes a bug in the new v8 Crypto Extensions GHASH code
    that only manifests itself in big-endian mode.
    
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>

commit b39d8bc37796d2bb2109e7fc774cfb26ce563db6
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Tue Mar 10 09:47:48 2015 +0100

    crypto: arm - add support for GHASH using ARMv8 Crypto Extensions
    
    This implements the GHASH hash algorithm (as used by the GCM AEAD
    chaining mode) using the AArch32 version of the 64x64 to 128 bit
    polynomial multiplication instruction (vmull.p64) that is part of
    the ARMv8 Crypto Extensions.
    
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>

commit a04ff0313ef7c11cb07ed05d0b2129e42da8fc59
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Tue Mar 10 09:47:47 2015 +0100

    crypto: arm - AES in ECB/CBC/CTR/XTS modes using ARMv8 Crypto Extensions
    
    This implements the ECB, CBC, CTR and XTS asynchronous block ciphers
    using the AArch32 versions of the ARMv8 Crypto Extensions for AES.
    
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>

commit a4978fb7a36b5c5cc377efc0c1aa4d5a263d1f14
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Tue Mar 10 09:47:46 2015 +0100

    crypto: arm - add support for SHA-224/256 using ARMv8 Crypto Extensions
    
    This implements the SHA-224/256 secure hash algorithm using the AArch32
    versions of the ARMv8 Crypto Extensions for SHA2.
    
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>

commit 0a0309efc257859620349d1fb68baf6aaae42bdf
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Tue Mar 10 09:47:45 2015 +0100

    crypto: arm - add support for SHA1 using ARMv8 Crypto Instructions
    
    This implements the SHA1 secure hash algorithm using the AArch32
    versions of the ARMv8 Crypto Extensions for SHA1.
    
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>

commit 19dc586d5b95e4af38c8360e83361794aed2eaaa
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Tue May 19 14:00:37 2015 +0200

    crypto: add missing new modules names

commit 79e06f39986f35c4871395ac98052f0de0431154
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Tue Mar 10 09:47:44 2015 +0100

    crypto: arm - move ARM specific Kconfig definitions to a dedicated file
    
    This moves all Kconfig symbols defined in crypto/Kconfig that depend
    on CONFIG_ARM to a dedicated Kconfig file in arch/arm/crypto, which is
    where the code that implements those features resides as well.
    
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>

commit b4ce05f72a9b914821f96c604618fb4704dc82f1
Author: Tim Chen <tim.c.chen@linux.intel.com>
Date:   Thu Jul 31 10:29:51 2014 -0700

    crypto: sha-mb - multibuffer crypto infrastructure
    
    This patch introduces the multi-buffer crypto daemon which is responsible
    for submitting crypto jobs in a work queue to the responsible multi-buffer
    crypto algorithm.  The idea of the multi-buffer algorihtm is to put
    data streams from multiple jobs in a wide (AVX2) register and then
    take advantage of SIMD instructions to do crypto computation on several
    buffers simultaneously.
    
    The multi-buffer crypto daemon is also responsbile for flushing the
    remaining buffers to complete the computation if no new buffers arrive
    for a while.
    
    Signed-off-by: Tim Chen <tim.c.chen@linux.intel.com>
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>

commit 23c81ba840868bb3d6149ff43f4383722de23e44
Author: Tim Chen <tim.c.chen@linux.intel.com>
Date:   Thu Jan 23 03:25:47 2014 -0800

    CRC32C: Add soft module dependency to load other accelerated crc32c modules
    
    We added the soft module dependency of crc32c module alias
    to generic crc32c module so other hardware accelerated crc32c
    modules could get loaded and used before the generic version.
    We also renamed the crypto/crc32c.c containing the generic
    crc32c crypto computation to crypto/crc32c_generic.c according
    to convention.
    
    Signed-off-by: Tim Chen <tim.c.chen@linux.intel.com>
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>

commit 6d4c0649470cfe6aeed4a613e6909b8c0b6f3cfa
Author: Dmitry Kasatkin <d.kasatkin@samsung.com>
Date:   Mon May 6 15:40:01 2013 +0300

    crypto: provide single place for hash algo information
    
    This patch provides a single place for information about hash algorithms,
    such as hash sizes and kernel driver names, which will be used by IMA
    and the public key code.
    
    Changelog:
    - Fix sparse and checkpatch warnings
    - Move hash algo enums to uapi for userspace signing functions.
    
    Signed-off-by: Dmitry Kasatkin <d.kasatkin@samsung.com>
    Signed-off-by: Mimi Zohar <zohar@linux.vnet.ibm.com>
    Acked-by: Herbert Xu <herbert@gondor.apana.org.au>

commit 44c5811d3552cec3f0c9b990191a7c30d37cc8cc
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Fri Sep 20 09:55:40 2013 +0200

    crypto: create generic version of ablk_helper
    
    Create a generic version of ablk_helper so it can be reused
    by other architectures.
    
    Acked-by: Jussi Kivilinna <jussi.kivilinna@iki.fi>
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>

commit 64828ebf4629ac32a0a6f3e86b3badbb1c9c311a
Author: Chanho Min <chanho.min@lge.com>
Date:   Mon Jul 8 16:01:51 2013 -0700

    crypto: add lz4 Cryptographic API
    
    Add support for lz4 and lz4hc compression algorithm using the lib/lz4/*
    codebase.
    
    [akpm@linux-foundation.org: fix warnings]
    Signed-off-by: Chanho Min <chanho.min@lge.com>
    Cc: "Darrick J. Wong" <djwong@us.ibm.com>
    Cc: Bob Pearson <rpearson@systemfabricworks.com>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: Herbert Xu <herbert@gondor.hengli.com.au>
    Cc: Yann Collet <yann.collet.73@gmail.com>
    Cc: Kyungsik Lee <kyungsik.lee@lge.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit 42439ac55d1c5a5fae3f315253b9e49509d3f803
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Tue May 19 13:40:20 2015 +0200

    crypto: add missing cmac

commit 536080114a194eef71b5cf3bbf724f327984aaeb
Author: Jussi Kivilinna <jussi.kivilinna@iki.fi>
Date:   Mon Apr 8 10:48:44 2013 +0300

    crypto: add CMAC support to CryptoAPI
    
    Patch adds support for NIST recommended block cipher mode CMAC to CryptoAPI.
    
    This work is based on Tom St Denis' earlier patch,
     http://marc.info/?l=linux-crypto-vger&m=135877306305466&w=2
    
    Cc: Tom St Denis <tstdenis@elliptictech.com>
    Signed-off-by: Jussi Kivilinna <jussi.kivilinna@iki.fi>
    Acked-by: David S. Miller <davem@davemloft.net>
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>
    
    Conflicts:
    	crypto/tcrypt.c
    	crypto/testmgr.c

commit 8659e436ee0fb79081fbeb1519f7b1bab2f3ba69
Author: Jussi Kivilinna <jussi.kivilinna@mbnet.fi>
Date:   Tue Nov 13 11:43:14 2012 +0200

    crypto: cast5/cast6 - move lookup tables to shared module
    
    CAST5 and CAST6 both use same lookup tables, which can be moved shared module
    'cast_common'.
    
    Signed-off-by: Jussi Kivilinna <jussi.kivilinna@mbnet.fi>
    Acked-by: David S. Miller <davem@davemloft.net>
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>
    
    Conflicts:
    	arch/x86/crypto/cast5-avx-x86_64-asm_64.S
    	arch/x86/crypto/cast6-avx-x86_64-asm_64.S
    	crypto/Kconfig

commit 805268c70dfb6f54440ff19505998d132507a47a
Author: David Howells <dhowells@redhat.com>
Date:   Thu Sep 13 15:17:21 2012 +0100

    KEYS: Implement asymmetric key type
    
    Create a key type that can be used to represent an asymmetric key type for use
    in appropriate cryptographic operations, such as encryption, decryption,
    signature generation and signature verification.
    
    The key type is "asymmetric" and can provide access to a variety of
    cryptographic algorithms.
    
    Possibly, this would be better as "public_key" - but that has the disadvantage
    that "public key" is an overloaded term.
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    Signed-off-by: Rusty Russell <rusty@rustcorp.com.au>
    
    Conflicts:
    	crypto/Makefile

commit 844a6a32e7b4efba3077e4193c0d07886b0dc812
Author: Seth Jennings <sjenning@linux.vnet.ibm.com>
Date:   Thu Jul 19 09:42:41 2012 -0500

    powerpc/crypto: add 842 crypto driver
    
    This patch add the 842 cryptographic API driver that
    submits compression requests to the 842 hardware compression
    accelerator driver (nx-compress).
    
    If the hardware accelerator goes offline for any reason
    (dynamic disable, migration, etc...), this driver will use LZO
    as a software failover for all future compression requests.
    For decompression requests, the 842 hardware driver contains
    a software implementation of the 842 decompressor to support
    the decompression of data that was compressed before the accelerator
    went offline.
    
    Signed-off-by: Robert Jennings <rcj@linux.vnet.ibm.com>
    Signed-off-by: Seth Jennings <sjenning@linux.vnet.ibm.com>
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>

commit e00edd96312a8c7f367e0cc39a38187caf213826
Author: Johannes Goetzfried <Johannes.Goetzfried@informatik.stud.uni-erlangen.de>
Date:   Wed Jul 11 19:38:12 2012 +0200

    crypto: cast6 - prepare generic module for optimized implementations
    
    Rename cast6 module to cast6_generic to allow autoloading of optimized
    implementations. Generic functions and s-boxes are exported to be able to use
    them within optimized implementations.
    
    Signed-off-by: Johannes Goetzfried <Johannes.Goetzfried@informatik.stud.uni-erlangen.de>
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>

commit c22cfc40ec359af6439e0e8f9755b355fb2900a8
Author: Johannes Goetzfried <Johannes.Goetzfried@informatik.stud.uni-erlangen.de>
Date:   Wed Jul 11 19:37:04 2012 +0200

    crypto: cast5 - prepare generic module for optimized implementations
    
    Rename cast5 module to cast5_generic to allow autoloading of optimized
    implementations. Generic functions and s-boxes are exported to be able to use
    them within optimized implementations.
    
    Signed-off-by: Johannes Goetzfried <Johannes.Goetzfried@informatik.stud.uni-erlangen.de>
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>
    
    Conflicts:
    	crypto/cast5_generic.c

commit 20c3bbd6a006f344521acca946c71923b93e472d
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Thu Feb 26 07:22:05 2015 +0000

    crypto: arm/aes update NEON AES module to latest OpenSSL version
    
    This updates the bit sliced AES module to the latest version in the
    upstream OpenSSL repository (e620e5ae37bc). This is needed to fix a
    bug in the XTS decryption path, where data chunked in a certain way
    could trigger the ciphertext stealing code, which is not supposed to
    be active in the kernel build (The kernel implementation of XTS only
    supports round multiples of the AES block size of 16 bytes, whereas
    the conformant OpenSSL implementation of XTS supports inputs of
    arbitrary size by applying ciphertext stealing). This is fixed in
    the upstream version by adding the missing #ifndef XTS_CHAIN_TWEAK
    around the offending instructions.
    
    The upstream code also contains the change applied by Russell to
    build the code unconditionally, i.e., even if __LINUX_ARM_ARCH__ < 7,
    but implemented slightly differently.
    
    Cc: stable@vger.kernel.org
    Fixes: e4e7f10bfc40 ("ARM: add support for bit sliced AES using NEON instructions")
    Reported-by: Adrian Kotelba <adrian.kotelba@gmail.com>
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Tested-by: Milan Broz <gmazyland@gmail.com>
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>

commit 56556e046615f01824750a35a74e1e85a59bafe0
Author: Julia Lawall <Julia.Lawall@lip6.fr>
Date:   Sun Nov 30 18:03:48 2014 +0100

    crypto: arm - replace memset by memzero_explicit
    
    Memset on a local variable may be removed when it is called just before the
    variable goes out of scope.  Using memzero_explicit defeats this
    optimization.  A simplified version of the semantic patch that makes this
    change is as follows: (http://coccinelle.lip6.fr/)
    
    // <smpl>
    @@
    identifier x;
    type T;
    @@
    
    {
    ... when any
    T x[...];
    ... when any
        when exists
    - memset
    + memzero_explicit
      (x,
    -0,
      ...)
    ... when != x
        when strict
    }
    // </smpl>
    
    This change was suggested by Daniel Borkmann <dborkman@redhat.com>
    
    Signed-off-by: Julia Lawall <Julia.Lawall@lip6.fr>
    Acked-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>

commit c295a5fd50371604d5273f06467822f262e02dd9
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Tue May 19 13:26:45 2015 +0200

    arch: remove some unused files

commit 6c92f33ee63465bfc5df03008dca0b2a48fd8c4e
Author: Kees Cook <keescook@chromium.org>
Date:   Thu Nov 20 17:05:53 2014 -0800

    crypto: prefix module autoloading with "crypto-"
    
    This prefixes all crypto module loading with "crypto-" so we never run
    the risk of exposing module auto-loading to userspace via a crypto API,
    as demonstrated by Mathias Krause:
    
    https://lkml.org/lkml/2013/3/4/70
    
    Signed-off-by: Kees Cook <keescook@chromium.org>
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>
    
    Conflicts:
    	arch/arm64/crypto/aes-ce-ccm-glue.c
    	arch/arm64/crypto/aes-glue.c
    	arch/powerpc/crypto/sha1.c
    	arch/s390/crypto/aes_s390.c
    	arch/s390/crypto/des_s390.c
    	arch/s390/crypto/ghash_s390.c
    	arch/s390/crypto/sha1_s390.c
    	arch/s390/crypto/sha256_s390.c
    	arch/s390/crypto/sha512_s390.c
    	arch/sparc/crypto/aes_glue.c
    	arch/sparc/crypto/camellia_glue.c
    	arch/sparc/crypto/crc32c_glue.c
    	arch/sparc/crypto/des_glue.c
    	arch/sparc/crypto/md5_glue.c
    	arch/sparc/crypto/sha1_glue.c
    	arch/sparc/crypto/sha256_glue.c
    	arch/sparc/crypto/sha512_glue.c
    	arch/x86/crypto/camellia_aesni_avx2_glue.c
    	arch/x86/crypto/camellia_aesni_avx_glue.c
    	arch/x86/crypto/cast5_avx_glue.c
    	arch/x86/crypto/cast6_avx_glue.c
    	arch/x86/crypto/crc32-pclmul_glue.c
    	arch/x86/crypto/crc32c-intel_glue.c
    	arch/x86/crypto/crct10dif-pclmul_glue.c
    	arch/x86/crypto/des3_ede_glue.c
    	arch/x86/crypto/serpent_avx2_glue.c
    	arch/x86/crypto/serpent_avx_glue.c
    	arch/x86/crypto/sha256_ssse3_glue.c
    	arch/x86/crypto/sha512_ssse3_glue.c
    	arch/x86/crypto/twofish_avx_glue.c
    	crypto/842.c
    	crypto/cast5_generic.c
    	crypto/cast6_generic.c
    	crypto/crc32.c
    	crypto/crc32c_generic.c
    	crypto/crct10dif_generic.c
    	crypto/lz4.c
    	crypto/lz4hc.c
    	drivers/crypto/qat/qat_common/adf_ctl_drv.c
    	drivers/crypto/ux500/cryp/cryp_core.c
    	drivers/crypto/ux500/hash/hash_core.c
    	drivers/s390/crypto/ap_bus.c

commit def2091e864fccd0c0e0bdd23343ca0342de88d0
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Tue Aug 5 21:15:19 2014 +0100

    ARM: 8125/1: crypto: enable NEON SHA-1 for big endian
    
    This tweaks the SHA-1 NEON code slightly so it works correctly under big
    endian, and removes the Kconfig condition preventing it from being
    selected if CONFIG_CPU_BIG_ENDIAN is set.
    
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

commit cef3c3349e9e601616d37055d860b0fa42eb3317
Author: Jussi Kivilinna <jussi.kivilinna@iki.fi>
Date:   Tue Jul 29 17:14:09 2014 +0100

    ARM: 8118/1: crypto: sha1/make use of common SHA-1 structures
    
    Common SHA-1 structures are defined in <crypto/sha.h> for code sharing.
    
    This patch changes SHA-1/ARM glue code to use these structures.
    
    Acked-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Jussi Kivilinna <jussi.kivilinna@iki.fi>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

commit d5a87c36171e2fe854379269e4b6cae03e3ad74b
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Mon Oct 7 15:43:53 2013 +0100

    ARM: add .gitignore entry for aesbs-core.S
    
    This avoids this file being incorrectly added to git.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

commit 3384d7e0e9e7a1094140fed4967ed372e0e5ef47
Author: alucard24 <dmbaoh2@gmail.com>
Date:   Tue Jan 27 21:06:20 2015 +0100

    net: wireless: bcmdhd: Reduce scan dwell time to reduce power
    Scan dwell time is reduced for 40/80 to 20/40 (normal)
    and 20/30 (power-save). Scan quality reduction is ~ 20%.
    Big thanks to @dorimanx.
    
    Conflicts:
    	drivers/net/wireless/bcmdhd/dhd.h
    	drivers/net/wireless/bcmdhd/dhd_linux.c
    	drivers/net/wireless/bcmdhd/wl_cfg80211.c

commit edef24d1d5fd8e6ff88bf4d7818393c283d5e573
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Tue May 19 12:15:04 2015 +0200

    Revert "power: disable powersuspend again and add back earlysuspend" also fix some wifi stuff
    
    This reverts commit 411434e36ca93d63a9d7a88d3f8231dee3743313.
    
    Conflicts:
    	kernel/power/earlysuspend.c

commit a549bfc0f30f5eb7db4c877d8d86cf8d8382ac15
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Mon May 18 23:12:16 2015 +0200

    Revert "Revert "PM / Runtime: Asyncronous idle|suspend devices at system resume""
    
    This reverts commit 79ba2ac2185abbcc1ec40441a3a2b1e358fea2d3.

commit a4fa2037a8f13cb2c6262e58ec46600ed8c5d7d8
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Mon May 18 23:11:36 2015 +0200

    Revert "Revert all commits I made in the last two days. Mainly because of incompatible stuff. Maybe I will revert this commit here in the feature"
    
    This reverts commit 74208ddc2eec00785c944ea68448ac247d326992.

commit e7e8e3d6a2ee6d77f6c0a5398d3ce1237e963704
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Mon May 18 23:10:01 2015 +0200

    mach-msm: revert cpufreq.c to default and so do the same with spm and spm-v2

commit 79ba2ac2185abbcc1ec40441a3a2b1e358fea2d3
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Mon May 18 21:47:01 2015 +0200

    Revert "PM / Runtime: Asyncronous idle|suspend devices at system resume"
    
    This reverts commit a3e33ccf49393334af843835f20170a6fdd03f8d.

commit 74208ddc2eec00785c944ea68448ac247d326992
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Mon May 18 21:07:47 2015 +0200

    Revert all commits I made in the last two days. Mainly because of incompatible stuff. Maybe I will revert this commit here in the feature

commit c23ed31c3594595aab9a620a31493c0768adf026
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Mon May 18 00:18:21 2015 +0200

    cpufreq: add Hyper governor (thanks to @Dorimanx) and fix small stuff in earlysuspend

commit 411434e36ca93d63a9d7a88d3f8231dee3743313
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Mon May 18 00:09:49 2015 +0200

    power: disable powersuspend again and add back earlysuspend

commit 0470686fe9c39d034ee3c97f319c5a5c23602f4d
Author: Ruchi Kandoi <kandoiruchi@google.com>
Date:   Tue Oct 14 17:43:21 2014 -0700

    power: Avoids bogus error messages for the suspend aborts.
    
    Avoids printing bogus error message "tasks refusing to freeze", in cases
    where pending wakeup source caused the suspend abort.
    
    Signed-off-by: Ruchi Kandoi <kandoiruchi@google.com>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>
    
    Conflicts:
    	kernel/power/process.c

commit a5c79aeb3d44932ba41dde1587359ae7c7ae1dcc
Author: Colin Cross <ccross@android.com>
Date:   Wed Jul 24 17:41:33 2013 -0700

    freezer: set PF_SUSPEND_TASK flag on tasks that call freeze_processes
    
    Calling freeze_processes sets a global flag that will cause any
    process that calls try_to_freeze to enter the refrigerator.  It
    skips sending a signal to the current task, but if the current
    task ever hits try_to_freeze, all threads will be frozen and the
    system will deadlock.
    
    Set a new flag, PF_SUSPEND_TASK, on the task that calls
    freeze_processes.  The flag notifies the freezer that the thread
    is involved in suspend and should not be frozen.  Also add a
    WARN_ON in thaw_processes if the caller does not have the
    PF_SUSPEND_TASK flag set to catch if a different task calls
    thaw_processes than the one that called freeze_processes, leaving
    a task with PF_SUSPEND_TASK permanently set on it.
    
    Threads that spawn off a task with PF_SUSPEND_TASK set (which
    swsusp does) will also have PF_SUSPEND_TASK set, preventing them
    from freezing while they are helping with suspend, but they need
    to be dead by the time suspend is triggered, otherwise they may
    run when userspace is expected to be frozen.  Add a WARN_ON in
    thaw_processes if more than one thread has the PF_SUSPEND_TASK
    flag set.
    
    Reported-and-tested-by: Michael Leun <lkml20130126@newton.leun.net>
    Signed-off-by: Colin Cross <ccross@android.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    (cherry picked from commit 2b44c4db2e2f1765d35163a861d301038e0c8a75)
    Change-Id: I12e00d2ba61db827c07b8f4ef48e88a722f02d71
    Reviewed-on: http://git-master/r/259126
    Reviewed-by: Prashant Gaikwad <pgaikwad@nvidia.com>
    Tested-by: Prashant Gaikwad <pgaikwad@nvidia.com>
    Reviewed-by: Bharat Nihalani <bnihalani@nvidia.com>

commit a2f48a9120ec7ce23dcc64d0c8c27d779b2d10d2
Author: Colin Cross <ccross@android.com>
Date:   Mon May 6 23:50:10 2013 +0000

    freezer: shorten freezer sleep time using exponential backoff
    
    All tasks can easily be frozen in under 10 ms, switch to using
    an initial 1 ms sleep followed by exponential backoff until
    8 ms.  Also convert the printed time to ms instead of centiseconds.
    
    Change-Id: I7b198b16eefb623c2b0fc45dce50d9bca320afdc
    Acked-by: Pavel Machek <pavel@ucw.cz>
    Acked-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Colin Cross <ccross@android.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    
    Conflicts:
    	kernel/power/process.c

commit 0d0249ddb97c2b82f4b95e50ef8b90d7920b0500
Author: Dorimanx <yuri@bynet.co.il>
Date:   Sun May 17 16:37:16 2015 +0300

    Reverted all WAKEUP SOURCES LOGGING CODE. we get NULL pointer from ktime_get

commit e4ebc743da62180140985ff9eefee33b3e2292fd
Author: Rafael J. Wysocki <rjw@sisk.pl>
Date:   Sat May 5 21:57:28 2012 +0200

    PM / Sleep: User space wakeup sources garbage collector Kconfig option
    
    Make it possible to configure out the user space wakeup sources
    garbage collector for debugging and default Android builds.
    
    Change-Id: I85ca6caa92c8e82d863f0fa58d8861b5571c1b4a
    Signed-off-by: Rafael J. Wysocki <rjw@sisk.pl>
    Acked-by: Arve Hjnnevg <arve@android.com>
    Git-commit: 4e585d25e120f1eae0a3a8bf8f6ebc7692afec18
    Git-repo: git://codeaurora.org/kernel/msm.git
    Signed-off-by: Anurag Singh <anursing@codeaurora.org>

commit 08174a83eb31c9695c68add2a4b8b796935c3cf9
Author: Rafael J. Wysocki <rjw@sisk.pl>
Date:   Sat May 5 21:57:20 2012 +0200

    PM / Sleep: Make the limit of user space wakeup sources configurable
    
    Make it possible to configure out the check against the limit of
    user space wakeup sources for debugging and default Android builds.
    
    Change-Id: I8f74d7c8391627df970d2df666938069b012e2fe
    Signed-off-by: Rafael J. Wysocki <rjw@sisk.pl>
    Acked-by: Arve Hjnnevg <arve@android.com>
    Git-commit: c73893e2ca731b4a81ae59246ab57979aa188777
    Git-repo: git://codeaurora.org/kernel/msm.git
    Signed-off-by: Anurag Singh <anursing@codeaurora.org>

commit 8b94f7544c6640e999a47b9fc9be69fd67d3e274
Author: Arve Hjnnevg <arve@android.com>
Date:   Fri Mar 16 17:44:42 2012 -0700

    PM / Sleep: Add wake lock api wrapper on top of wakeup sources
    
    Change-Id: Icaad02fe1e8856fdc2e4215f380594a5dde8e002
    Signed-off-by: Arve Hjnnevg <arve@android.com>
    Git-commit: e9911f4efdc55af703b8b3bb8c839e6f5dd173bb
    Git-repo: git://codeaurora.org/kernel/msm.git
    [anursing@codeaurora.org: replace existing implementation, resolve
    merge conflicts]
    Signed-off-by: Anurag Singh <anursing@codeaurora.org>

commit 36bac0f427f4d375766ec55bf0771ff663970ff6
Author: Arve Hjnnevg <arve@android.com>
Date:   Fri Mar 16 17:44:42 2012 -0700

    PM / Sleep: Add wake lock api wrapper on top of wakeup sources
    
    Change-Id: Icaad02fe1e8856fdc2e4215f380594a5dde8e002
    Signed-off-by: Arve Hjnnevg <arve@android.com>

commit 2ab529edcfc9b44441825ec5bb521dc6924806c9
Author: alucard24 <dmbaoh2@gmail.com>
Date:   Sat Apr 18 15:31:12 2015 +0200

    wifi bcmdhd: Makefile cosmetic changes.

commit d1fd10ab6c4cfff6c33c5208dd844ae088410827
Author: alucard24 <dmbaoh2@gmail.com>
Date:   Sat Apr 4 14:50:55 2015 +0200

    bcmdhd: updated makefile flags in order to fix wifi suspend while screen is off and wifi is in use.
    
    Conflicts:
    	drivers/net/wireless/bcmdhd/Makefile

commit 2ff00e35343e9dd04ec3671c4229620c0556863d
Author: alucard24 <dmbaoh2@gmail.com>
Date:   Fri Mar 20 20:32:19 2015 +0100

    bcmdhd/dhd_linux: missing powersuspend define!

commit 94dfa8b26311acbdf141d6769f0e96f4c28bdd87
Author: alucard24 <dmbaoh2@gmail.com>
Date:   Sun Oct 12 20:19:38 2014 +0200

    wifi: replaced early suspend with power suspend definition
    and some other compiling fixes

commit 4e51a95d01c98ad30c3f6b5691ba72389e8f2979
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun May 17 18:50:48 2015 +0200

    Revert "Add s2w, d2w, w2w and p2w"
    
    This reverts commit ead900959830aa5e7e15cf78b3e4012a9e126df3.

commit f8e65b6f031e0f49c9e2eac1f44a5d4aa4cf523a
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun May 17 18:50:38 2015 +0200

    Revert "Move all screen on/off stuff from suspend/resume routines and hook it directly to screen on/off from board file"
    
    This reverts commit 9a2a66b893af3ac40ff3bdb4fe050a0e23e9d2c7.

commit 1ae45a6e1027c0ece631abc08e4d4349325de142
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun May 17 18:50:27 2015 +0200

    Revert "Make s2w and all the others modify irqs while screen is off when ktweaker sets them on boot or if sysfs are written to while screen is off thru adb"
    
    This reverts commit 98f8afde12956c0b554a988b4bb8b784ed992761.

commit 2c2260997e2a0e1150f1277232c2565c04606eeb
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun May 17 18:50:16 2015 +0200

    Revert "Screen wake: Add wake_options_prox_max to let user configure the prox value to block wake functions"
    
    This reverts commit bb4630afed7f5c27e2713ea2708a86fe16633bc9.

commit 9d339cd7adaf6ba810b188f72c55eca47a9991c2
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun May 17 18:50:04 2015 +0200

    Revert "wake functions: Add relay to prox file for prox_max, add wake lock monitor work, and some general clean up"
    
    This reverts commit cc53d237a9c5f07a696b62062827b222c15ffd73.

commit afea81e5a343da8247cd2b2108ee0237727098ae
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun May 17 18:49:48 2015 +0200

    Revert "wake funcs: Re-add synaptics_rmi4_reinit_device function but remove duplicate call to the synaptics_rmi4_f51_set_custom_rezero function"
    
    This reverts commit 01c94fcd4d02ed70d65f1b37705f882f7992809c.

commit 9e7a30a90065f261b9944c818cb0a857d5405786
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun May 17 18:49:37 2015 +0200

    Revert "wake funcs: Tweak Double tap to have a limit of 2 seconds and block more than one finger.  Also make sure wake lock is deleted if it is being held with LED notif option and then we are now in block LED mode"
    
    This reverts commit 5dd35d901d87de367fe9341c0d9c1705325b8364.

commit b630293e97eef4fcac269fc20f4a663dae4c3b82
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun May 17 18:49:24 2015 +0200

    Revert "wake funcs: Remove isbooted flag"
    
    This reverts commit 7490564d05727e080b7d17b6631a82174ea3cb69.

commit 42c8128d82fedd66e6b4750f0d16dc2b7d2eb907
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun May 17 18:49:12 2015 +0200

    Revert "wake funcs: Improve wake lock monitor code to ensure it holds it for specified time and kick starts it if it has been killed"
    
    This reverts commit 04fb6d39f5054b3afe71121d3e76a27fc4048429.

commit a371d3543d05586d51d790e0bd5f7999fef76dd1
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun May 17 18:49:00 2015 +0200

    Revert "wake funcs: Make sure main_prox_data is not NULL before checking variables or adding sensors"
    
    This reverts commit 0ca84738f419ef61210f093f606660d70b9df63d.

commit a1e6377e956a11463cb1495d16a1b296cf10844a
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun May 17 18:48:51 2015 +0200

    Revert "wake funcs: Prevent null reference to gdev during charging with device off mode"
    
    This reverts commit c10e495141eb7e0f45507a2cf91523f5f036e1eb.

commit 14a9e84a67b92ec533930d97b3578760dea7fdb6
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun May 17 18:48:41 2015 +0200

    Revert "wake funcs: Added turbo wake up mode, now calling pwrkey directly and modded pwrkey function to do turbo button for any governor"
    
    This reverts commit 123774908ba24bc6c7b99967b50cf99457840b58.

commit f70fbf76127edcee797d85d62657172e86c364a7
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun May 17 18:48:24 2015 +0200

    Revert "wake funcs: Add more protection while on a phone call"
    
    This reverts commit dd492b8ba36415662497563fe06fc8b343e92b5a.

commit c53fb1ffadc83d78b5b13f735741f5d6358df381
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun May 17 18:48:14 2015 +0200

    Revert "Add double tap 2 sleep and some prox fixes from the 2 sleep functions battling"
    
    This reverts commit 659aedc55ab5a108d199d6a8765a1ce63baf0167.

commit c2f87d0dd49af8641cba1086e92f393333cc5097
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun May 17 18:48:01 2015 +0200

    Revert "Bring back in synaptics_rmi4_f51_set_custom_rezero and synaptics_rmi4_reset_device functions to make sure screen is initialized"
    
    This reverts commit 3a29a601f30c6161943d99ee40f5bb5dbefb139d.

commit 9d8b506f3f2f8fcdf1fec74ad78a9f5df9451c81
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun May 17 18:47:52 2015 +0200

    Revert "synaptics: Readd some of the screen off flags from stock source"
    
    This reverts commit 089792ffdbfddfeecc78b78bceb5eece29707aee.

commit ca3a5045bdb25c6fbce9105c3244b85cd519d1c8
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun May 17 18:47:38 2015 +0200

    Revert "SSP: Break out some pieces of ssp.h into a seperate file so I can include only the pieces I need for screen and sensor wake functions"
    
    This reverts commit b61e967022cfff80b6ffb7b61e3d67fede04ac91.

commit c814714ca187de19c083e0eeb91880c06b9b7c80
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun May 17 18:46:19 2015 +0200

    mach-msm: add bricked hotplug, thanks to @neobuddy89 and @dorimanx
    enable powersuspend and fix some stuff refering to it

commit 24490b2e90a9d2f9de07fe676a6b8e6d118cae8e
Author: Tejun Heo <tj@kernel.org>
Date:   Fri Jan 11 13:37:33 2013 +0100

    PM: don't use [delayed_]work_pending()
    
    There's no need to test whether a (delayed) work item is pending
    before queueing, flushing or cancelling it, so remove work_pending()
    tests used in those cases.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit 651d1bb21f499215001f39b1c2a9953fdcb93c7c
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun May 17 18:16:42 2015 +0200

    power: small fixes

commit 1a415b1b3d2b30519062954ef700f91eec30a009
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun May 17 18:13:09 2015 +0200

    power: add new powersuspend driver, thanks to @Faux123, @dorimanx, @alucard24 and @yank555-lu

commit e360501ca6b059d47e2f8a552aed2352dc6032f6
Author: Joe Swantek <w98568@motorola.com>
Date:   Fri Feb 21 05:32:36 2014 -0600

    power: quickwakeup: initial driver
    
    This driver allows clients to register code to be called upon wakeup
    so that clients can perform specific checks and vote to drop the system
    back into suspend without fully resuming if the wake reason was a special
    quick wakeup event.
    
    Propagation of (CR)
    
    Change-Id: I791ff02f34b616e6da2b68efc2d4eb4449e5bbe7
    Signed-off-by: James Wylder <jwylder1@motorola.com>
    Signed-off-by: Jared Suttles <jsuttles@motorola.com>
    Signed-off-by: Joe Swantek <w98568@motorola.com>
    Reviewed-on: http://gerrit.pcs.mot.com/611835
    SLTApproved: Slta Waiver <sltawvr@motorola.com>
    Tested-by: Jira Key <jirakey@motorola.com>
    Reviewed-by: Joseph Swantek <jswantek@motorola.com>
    Submit-Approved: Jira Key <jirakey@motorola.com>

commit fd135493f67d6eb7caab5360ecca6f3dd2383d81
Author: Anurag Singh <anursing@codeaurora.org>
Date:   Sun Mar 10 18:29:40 2013 -0700

    power: Remove legacy wakelock code.
    
    Remove parts of code that use the legacy wakelock
    implementation as we make the switch to the upstream
    kernel's wakeup sources-based implementation.
    
    Change-Id: Idab9e3dd54e8a256b059c88606c9368f7ddb1c1b
    Signed-off-by: Anurag Singh <anursing@codeaurora.org>
    
    Conflicts:
    	kernel/power/process.c
    	kernel/power/suspend.c

commit fdb06443ef062d4d9dcdb66a28825b06dc79cf05
Author: Russ W. Knize <Russ.Knize@motorola.com>
Date:   Fri Feb 28 17:47:48 2014 -0600

    power: make sync on suspend optional
    
    On embedded devices with built-in batteries, it is not so
    important to sync the file systems before suspend.  The chance
    of losing power during suspend are no greater than they are
    when the system is awake.  The sync operations can greatly
    increase suspend latency when the system has accrued many dirty
    pages and/or the target storage devices are not particularly
    fast.
    
    This commit adds a kernel config option to allow file system
    sync in the suspend and earlysuspend paths to be disabled.  It is
    enabled by default.  It also exposes two command line options to
    control this feature: suspendsync and earlysuspendsync.
    
    Signed-off-by: Russ Knize <Russ.Knize@motorola.com>
    Reviewed-on: http://gerrit.pcs.mot.com/614512
    SLTApproved: Slta Waiver <sltawvr@motorola.com>
    Tested-by: Jira Key <jirakey@motorola.com>
    Reviewed-by: Igor Kovalenko <igork@motorola.com>
    Submit-Approved: Jira Key <jirakey@motorola.com>
    [neo: Disabled by default.]
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>
    
    Dorimanx Note:
    Added ONE time sync when screen goes OFF just in case.
    function static int enter_state(suspend_state_t state)
    is executed many times during long suspend, on every cpu wakeup
    so now we will not SYNC filesystem each time, there is not need for that.
    
    This should save little battery power, and reduce suspend cpu time.

commit 7323d6236f7b2d44aea6032ef3c80e82d9cc3dbc
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Tue Jun 24 23:38:08 2014 -0400

    PM / Sleep: Remove ftrace_stop/start() from suspend and hibernate
    
    ftrace_stop() and ftrace_start() were added to the suspend and hibernate
    process because there was some function within the work flow that caused
    the system to reboot if it was traced. This function has recently been
    found (restore_processor_state()). Now there's no reason to disable
    function tracing while we are going into suspend or hibernate, which means
    that being able to trace this will help tremendously in debugging any
    issues with suspend or hibernate.
    
    This also means that the ftrace_stop/start() functions can be removed
    and simplify the function tracing code a bit.
    
    Link: http://lkml.kernel.org/r/1518201.VD9cU33jRU@vostro.rjw.lan
    
    Acked-by: "Rafael J. Wysocki" <rjw@rjwysocki.net>
    Reviewed-by: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 81a320186ac90372c29bc0b13d269552637a10c6
Author: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
Date:   Thu May 15 23:29:57 2014 +0200

    ACPI / PM: Hold ACPI scan lock over the "freeze" sleep state
    
    The "freeze" sleep state suffers from the same issue that was
    addressed by commit ad07277e82de (ACPI / PM: Hold acpi_scan_lock over
    system PM transitions) for ACPI sleep states, that is, things break
    if ->remove() is called for devices whose system resume callbacks
    haven't been executed yet.
    
    It also can be addressed in the same way, by holding the ACPI scan
    lock over the "freeze" sleep state and PM transitions to and from
    that state, but ->begin() and ->end() platform operations for the
    "freeze" sleep state are needed for this purpose.
    
    This change has been tested on Acer Aspire S5 with Thunderbolt.
    
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit 6d9eb27d595c90161ff5c74bde03f0ea977b0879
Author: Viresh Kumar <viresh.kumar@linaro.org>
Date:   Mon Mar 10 19:31:51 2014 +0530

    PM / suspend: Remove unnecessary !!
    
    Double ! or !! are normally required to get 0 or 1 out of a expression. A
    comparision always returns 0 or 1 and hence there is no need to apply double !
    over it again.
    
    Signed-off-by: Viresh Kumar <viresh.kumar@linaro.org>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit fdea3887415bd47b2b7b108a0a01a20fd64fdbd7
Author: Brandt, Todd E <todd.e.brandt@intel.com>
Date:   Thu Jul 11 07:44:35 2013 +0000

    PM / Sleep: increase ftrace coverage in suspend/resume
    
    Change where ftrace is disabled and re-enabled during system
    suspend/resume to allow tracing of device driver pm callbacks.
    Ftrace will now be turned off when suspend reaches
    disable_nonboot_cpus() instead of at the very beginning of system
    suspend.
    
    Ftrace was disabled during suspend/resume back in 2008 by
    Steven Rostedt as he discovered there was a conflict in the
    enable_nonboot_cpus() call (see commit f42ac38 "ftrace: disable
    tracing for suspend to ram").  This change preserves his fix by
    disabling ftrace, but only at the function where it is known
    to cause problems.
    
    The new change allows tracing of the device level code for better
    debug.
    
    [rjw: Changelog]
    Signed-off-by: Todd Brandt <todd.e.brandt@intel.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit e603333e4e46a4a01a0c21888a28a5714393907a
Author: Bernie Thompson <bhthompson@chromium.org>
Date:   Sat Jun 1 00:47:43 2013 +0000

    PM / wakeup: Adjust messaging for wake events during suspend
    
    This adds in a new message to the wakeup code which adds an
    indication to the log that suspend was cancelled due to a wake event
    occouring during the suspend sequence. It also adjusts the message
    printed in suspend.c to reflect the potential that a suspend was
    aborted, as opposed to a device failing to suspend.
    
    Without these message adjustments one can end up with a kernel log
    that says that a device failed to suspend with no actual device
    suspend failures, which can be confusing to the log examiner.
    
    Signed-off-by: Bernie Thompson <bhthompson@chromium.org>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    
    Conflicts:
    	kernel/power/suspend.c

commit d91cd34d56c42f4f27ab253a51c9c63cd5142556
Author: Zhang Rui <rui.zhang@intel.com>
Date:   Wed Mar 27 03:36:10 2013 +0000

    PM / sleep: add TEST_PLATFORM support for freeze state
    
    Invoke freeze_enter() after suspend_test(TEST_PLATFORM) being invoked.
    
    So when setting /sys/power/pm_test to "platform", it can be used to
    check if freeze state is working well after all devices are suspended
    and before processors are blocked,
    
    Signed-off-by: Zhang Rui <rui.zhang@intel.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit 457109e78d905259ccf40b1a6a31d0848e99d21a
Author: Stepan Moskovchenko <stepanm@codeaurora.org>
Date:   Tue Jul 9 16:59:15 2013 -0700

    PM / Sleep: Clean up remnants of workqueue-based sync
    
    When legacy wakelock code was removed in commit
    f85607a715a74c65db812cd3901022888257f966, some of the code
    for moving calls to sys_sync() from suspend paths into a
    workqueue item had not been properly removed. Specifically,
    one of the call sites to suspend_sys_sync_wait() has been
    mistakenly replaced with a call to sys_sync(), which is not
    necessary because the corresponding instance of
    suspend_sys_sync_queue() was already replaced with
    sys_sync(). Clean up the remnants of the legacy wakelock
    code by removing the extraneous call to sys_sync() and
    restoring some of the surrounding printk statements that
    had been moved to suspend_sys_sync_queue() and subsequently
    lost.
    
    CRs-Fixed: 498669
    Change-Id: Ifb2ede7808560f456c824d3d6359a4541c51b73f
    Signed-off-by: Stepan Moskovchenko <stepanm@codeaurora.org>
    
    Conflicts:
    	kernel/power/process.c
    	kernel/power/suspend.c

commit 61104083dc6fc811e253c94114b6ed8fda39e39b
Author: Li Fei <fei.li@intel.com>
Date:   Fri Feb 1 08:56:03 2013 +0000

    suspend: enable freeze timeout configuration through sys
    
    At present, the value of timeout for freezing is 20s, which is
    meaningless in case that one thread is frozen with mutex locked
    and another thread is trying to lock the mutex, as this time of
    freezing will fail unavoidably.
    And if there is no new wakeup event registered, the system will
    waste at most 20s for such meaningless trying of freezing.
    
    With this patch, the value of timeout can be configured to smaller
    value, so such meaningless trying of freezing will be aborted in
    earlier time, and later freezing can be also triggered in earlier
    time. And more power will be saved.
    In normal case on mobile phone, it costs real little time to freeze
    processes. On some platform, it only costs about 20ms to freeze
    user space processes and 10ms to freeze kernel freezable threads.
    
    Signed-off-by: Liu Chuansheng <chuansheng.liu@intel.com>
    Signed-off-by: Li Fei <fei.li@intel.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    
    Conflicts:
    	kernel/power/main.c

commit 329c07a8b6b643b69d1c76d5c75563f529aca80b
Author: Daniel Walter <sahne@0x90.at>
Date:   Tue Oct 23 01:20:35 2012 +0200

    PM / sysfs: replace strict_str* with kstrto*
    
    Replace strict_strtoul() with kstrtoul() in pm_async_store() and
    pm_qos_power_write().
    
    [rjw: Modified subject and changelog.]
    
    Signed-off-by: Daniel Walter <sahne@0x90.at>
    Acked-by: Pavel Machek <pavel@ucw.cz>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit 4c496b80026831dcb41c34fe9e38c3902a658dc7
Author: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
Date:   Mon May 26 13:40:53 2014 +0200

    PM / sleep: Use valid_state() for platform-dependent sleep states only
    
    Use the observation that, for platform-dependent sleep states
    (PM_SUSPEND_STANDBY, PM_SUSPEND_MEM), a given state is either
    always supported or always unsupported and store that information
    in pm_states[] instead of calling valid_state() every time we
    need to check it.
    
    Also do not use valid_state() for PM_SUSPEND_FREEZE, which is always
    valid, and move the pm_test_level validity check for PM_SUSPEND_FREEZE
    directly into enter_state().
    
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    
    Conflicts:
    	kernel/power/suspend.c

commit a6232a2e3793183cc37a7d71e5a52d4c456e5cbe
Author: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
Date:   Mon May 26 13:40:47 2014 +0200

    PM / sleep: Add state field to pm_states[] entries
    
    To allow sleep states corresponding to the "mem", "standby" and
    "freeze" lables to be different from the pm_states[] indexes of
    those strings, introduce struct pm_sleep_state, consisting of
    a string label and a state number, and turn pm_states[] into an
    array of objects of that type.
    
    This modification should not lead to any functional changes.
    
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit 215e80322bc3c089a718e2ddaffef3068c513e55
Author: Rafael J. Wysocki <rjw@sisk.pl>
Date:   Thu Jun 21 00:19:33 2012 +0200

    PM / Sleep: Separate printing suspend times from initcall_debug
    
    Change the behavior of the newly introduced
    /sys/power/pm_print_times attribute so that its initial value
    depends on initcall_debug, but setting it to 0 will cause device
    suspend/resume times not to be printed, even if initcall_debug has
    been set.  This way, the people who use initcall_debug for reasons
    other than PM debugging will be able to switch the suspend/resume
    times printing off, if need be.
    
    Signed-off-by: Rafael J. Wysocki <rjw@sisk.pl>
    Reviewed-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Acked-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 760200dc8088d4b2cab4297be0fdfab60c7666e2
Author: Sameer Nanda <snanda@chromium.org>
Date:   Tue Jun 19 22:23:33 2012 +0200

    PM / Sleep: add knob for printing device resume times
    
    Added a new knob called /sys/power/pm_print_times. Setting it to 1
    enables printing of time taken by devices to suspend and resume.
    Setting it to 0 disables this printing (unless overridden by
    initcall_debug kernel command line option).
    
    Signed-off-by: Sameer Nanda <snanda@chromium.org>
    Acked-by: Greg KH <gregkh@linuxfoundation.org>
    Signed-off-by: Rafael J. Wysocki <rjw@sisk.pl>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit bd70b767ea5b7fc072955e838806f9b09671ad9c
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun May 17 04:32:52 2015 +0200

    kernel: some overall fixes

commit 8f535762b55a21270f792f03f7ad9cf5f4cef2bf
Author: alucard24 <dmbaoh2@gmail.com>
Date:   Sun Nov 9 15:21:16 2014 +0100

    Imported wakeup reason from @dorimanx branch

commit 3584b75b0d66f3e73dc6fb3fadb148e495bf771d
Author: Todd Poynor <toddpoynor@google.com>
Date:   Sat Aug 11 22:17:02 2012 +0000

    PM / Sleep: Print name of wakeup source that aborts suspend
    
    A driver or app may repeatedly request a wakeup source while the system
    is attempting to enter suspend, which may indicate a bug or at least
    point out a highly active system component that is responsible for
    decreased battery life on a mobile device.  Even when the incidence
    of suspend abort is not severe, identifying wakeup sources that
    frequently abort suspend can be a useful clue for power management
    analysis.
    
    In some cases the existing stats can point out the offender where there is
    an unexpectedly high activation count that stands out from the others, but
    in other cases the wakeup source frequently taken just after the rest of
    the system thinks its time to suspend might not stand out in the overall
    stats.
    
    It is also often useful to have information about what's been happening
    recently, rather than totals of all activity for the system boot.
    
    It's suggested to dump a line about which wakeup source
    aborted suspend to aid analysis of these situations.
    
    Signed-off-by: Todd Poynor <toddpoynor@google.com>
    Signed-off-by: Rafael J. Wysocki <rjw@sisk.pl>
    
    Conflicts:
    	drivers/base/power/wakeup.c

commit d85bb9f282c1eca9875fbb80b89545d38d10e2d9
Author: Rafael J. Wysocki <rjw@sisk.pl>
Date:   Sun Apr 29 22:53:42 2012 +0200

    PM / Sleep: Add user space interface for manipulating wakeup sources, v3
    
    Android allows user space to manipulate wakelocks using two
    sysfs file located in /sys/power/, wake_lock and wake_unlock.
    Writing a wakelock name and optionally a timeout to the wake_lock
    file causes the wakelock whose name was written to be acquired (it
    is created before is necessary), optionally with the given timeout.
    Writing the name of a wakelock to wake_unlock causes that wakelock
    to be released.
    
    Implement an analogous interface for user space using wakeup sources.
    Add the /sys/power/wake_lock and /sys/power/wake_unlock files
    allowing user space to create, activate and deactivate wakeup
    sources, such that writing a name and optionally a timeout to
    wake_lock causes the wakeup source of that name to be activated,
    optionally with the given timeout.  If that wakeup source doesn't
    exist, it will be created and then activated.  Writing a name to
    wake_unlock causes the wakeup source of that name, if there is one,
    to be deactivated.  Wakeup sources created with the help of
    wake_lock that haven't been used for more than 5 minutes are garbage
    collected and destroyed.  Moreover, there can be only WL_NUMBER_LIMIT
    wakeup sources created with the help of wake_lock present at a time.
    
    The data type used to track wakeup sources created by user space is
    called "struct wakelock" to indicate the origins of this feature.
    
    This version of the patch includes an rbtree manipulation fix from John Stultz.
    
    Change-Id: Icb452cfd54362b49dcb1cff88345928a2528ad97
    Signed-off-by: Rafael J. Wysocki <rjw@sisk.pl>
    Acked-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Reviewed-by: NeilBrown <neilb@suse.de>
    Git-commit: b86ff9820fd5df69295273b9aa68e58786ffc23f
    Git-repo: git://codeaurora.org/kernel/msm.git
    [anursing@codeaurora.org: replace existing implementation, resolve
    merge conflicts]
    Signed-off-by: Anurag Singh <anursing@codeaurora.org>
    Conflicts:
    	kernel/power/wakelock.c

commit 6ac80272abbe20d12046cf6cfbc8e4330fbe83e2
Author: Rafael J. Wysocki <rjw@sisk.pl>
Date:   Sun Apr 29 22:53:32 2012 +0200

    PM / Sleep: Add "prevent autosleep time" statistics to wakeup sources
    
    Android uses one wakelock statistics that is only necessary for
    opportunistic sleep.  Namely, the prevent_suspend_time field
    accumulates the total time the given wakelock has been locked
    while "automatic suspend" was enabled.  Add an analogous field,
    prevent_sleep_time, to wakeup sources and make it behave in a similar
    way.
    
    Change-Id: I4b9719d05da020757d7cc21ed3b52b7c32261bea
    Signed-off-by: Rafael J. Wysocki <rjw@sisk.pl>
    Acked-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Git-commit: 55850945e872531644f31fefd217d61dd15dcab8
    Git-repo: git://codeaurora.org/kernel/msm.git
    Signed-off-by: Anurag Singh <anursing@codeaurora.org>

commit 4ef35fc81e011c59ea0a4445951a26fad4f5ba6c
Author: Rafael J. Wysocki <rjw@sisk.pl>
Date:   Sun Apr 29 22:53:22 2012 +0200

    PM / Sleep: Implement opportunistic sleep, v2
    
    Introduce a mechanism by which the kernel can trigger global
    transitions to a sleep state chosen by user space if there are no
    active wakeup sources.
    
    It consists of a new sysfs attribute, /sys/power/autosleep, that
    can be written one of the strings returned by reads from
    /sys/power/state, an ordered workqueue and a work item carrying out
    the "suspend" operations.  If a string representing the system's
    sleep state is written to /sys/power/autosleep, the work item
    triggering transitions to that state is queued up and it requeues
    itself after every execution until user space writes "off" to
    /sys/power/autosleep.
    
    That work item enables the detection of wakeup events using the
    functions already defined in drivers/base/power/wakeup.c (with one
    small modification) and calls either pm_suspend(), or hibernate() to
    put the system into a sleep state.  If a wakeup event is reported
    while the transition is in progress, it will abort the transition and
    the "system suspend" work item will be queued up again.
    
    Change-Id: Ic3214de009c64feab606e93811bd442ccfc49d86
    Signed-off-by: Rafael J. Wysocki <rjw@sisk.pl>
    Acked-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Reviewed-by: NeilBrown <neilb@suse.de>
    Git-commit: 7483b4a4d9abf9dcf1ffe6e805ead2847ec3264e
    Git-repo: git://codeaurora.org/kernel/msm.git
    [anursing@codeaurora.org: replace existing implementation, resolve
    merge conflicts]
    Signed-off-by: Anurag Singh <anursing@codeaurora.org>

commit 9e007dbd18b3caaf436edf2afe5de06e2f3d4b21
Author: Arve Hjnnevg <arve@android.com>
Date:   Sun Apr 29 22:53:02 2012 +0200

    PM / Sleep: Add wakeup_source_activate and wakeup_source_deactivate tracepoints
    
    Add tracepoints to wakeup_source_activate and wakeup_source_deactivate.
    Useful for checking that specific wakeup sources overlap as expected.
    
    Change-Id: I1fa803411a11e50a4904fc97758dcbe39fa82516
    Signed-off-by: Arve Hjnnevg <arve@android.com>
    Acked-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Signed-off-by: Rafael J. Wysocki <rjw@sisk.pl>
    Git-commit: 6791e36c4a40e8930e08669e60077eea6770c429
    Git-repo: git://codeaurora.org/kernel/msm.git
    Signed-off-by: Anurag Singh <anursing@codeaurora.org>

commit 48166f1f45f94aa3fd038db15d918fcf519417df
Author: Rafael J. Wysocki <rjw@sisk.pl>
Date:   Sun Apr 29 22:52:52 2012 +0200

    PM / Sleep: Change wakeup source statistics to follow Android
    
    Wakeup statistics used by Android are slightly different from what we
    have in wakeup sources at the moment and there aren't any known
    users of those statistics other than Android, so modify them to make
    it easier for Android to switch to wakeup sources.
    
    This removes the struct wakeup_source's hit_cout field, which is very
    rough and therefore not very useful, and adds two new fields,
    wakeup_count and expire_count.  The first one tracks how many times
    the wakeup source is activated with events_check_enabled set (which
    roughly corresponds to the situations when a system power transition
    to a sleep state is in progress and would be aborted by this wakeup
    source if it were the only active one at that time) and the second
    one is the number of times the wakeup source has been activated with
    a timeout that expired.
    
    Additionally, the last_time field is now updated when the wakeup
    source is deactivated too (previously it was only updated during
    the wakeup source's activation), which seems to be what Android does
    with the analogous counter for wakelocks.
    
    Change-Id: I049c27e64273b42bfb1a79528f4b5997a64390e8
    Signed-off-by: Rafael J. Wysocki <rjw@sisk.pl>
    Acked-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 0cf49b5639a203ed387391cdab342f315bdbe8cc
Author: Rafael J. Wysocki <rjw@sisk.pl>
Date:   Sun Apr 29 22:52:34 2012 +0200

    PM / Sleep: Use wait queue to signal "no wakeup events in progress"
    
    The current wakeup source deactivation code doesn't do anything when
    the counter of wakeup events in progress goes down to zero, which
    requires pm_get_wakeup_count() to poll that counter periodically.
    Although this reduces the average time it takes to deactivate a
    wakeup source, it also may lead to a substantial amount of unnecessary
    polling if there are extended periods of wakeup activity.  Thus it
    seems reasonable to use a wait queue for signaling the "no wakeup
    events in progress" condition and remove the polling.
    
    Change-Id: I716c4de1a57df619f77e4883e61cde64bcaff0a5
    Signed-off-by: Rafael J. Wysocki <rjw@sisk.pl>
    Acked-by: mark gross <markgross@thegnar.org>
    Acked-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit d4b5e7e6add2c5aba70758df39714136880c818e
Author: Zhang Rui <rui.zhang@intel.com>
Date:   Wed Feb 6 13:00:36 2013 +0100

    PM: Introduce suspend state PM_SUSPEND_FREEZE, fix some compiling errors
    Update msm_hotplug and intelli-plug, thanks to @Alucard24, @dorimanx and @neobuddy89
    
    PM_SUSPEND_FREEZE state is a general state that
    does not need any platform specific support, it equals
    frozen processes + suspended devices + idle processors.
    
    Compared with PM_SUSPEND_MEMORY,
    PM_SUSPEND_FREEZE saves less power
    because the system is still in a running state.
    PM_SUSPEND_FREEZE has less resume latency because it does not
    touch BIOS, and the processors are in idle state.
    
    Compared with RTPM/idle,
    PM_SUSPEND_FREEZE saves more power as
    1. the processor has longer sleep time because processes are frozen.
       The deeper c-state the processor supports, more power saving we can get.
    2. PM_SUSPEND_FREEZE uses system suspend code path, thus we can get
       more power saving from the devices that does not have good RTPM support.
    
    This state is useful for
    1) platforms that do not have STR, or have a broken STR.
    2) platforms that have an extremely low power idle state,
       which can be used to replace STR.
    
    The following describes how PM_SUSPEND_FREEZE state works.
    1. echo freeze > /sys/power/state
    2. the processes are frozen.
    3. all the devices are suspended.
    4. all the processors are blocked by a wait queue
    5. all the processors idles and enters (Deep) c-state.
    6. an interrupt fires.
    7. a processor is woken up and handles the irq.
    8. if it is a general event,
       a) the irq handler runs and quites.
       b) goto step 4.
    9. if it is a real wake event, say, power button pressing, keyboard touch, mouse moving,
       a) the irq handler runs and activate the wakeup source
       b) wakeup_source_activate() notifies the wait queue.
       c) system starts resuming from PM_SUSPEND_FREEZE
    10. all the devices are resumed.
    11. all the processes are unfrozen.
    12. system is back to working.
    
    Known Issue:
    The wakeup of this new PM_SUSPEND_FREEZE state may behave differently
    from the previous suspend state.
    Take ACPI platform for example, there are some GPEs that only enabled
    when the system is in sleep state, to wake the system backk from S3/S4.
    But we are not touching these GPEs during transition to PM_SUSPEND_FREEZE.
    This means we may lose some wake event.
    But on the other hand, as we do not disable all the Interrupts during
    PM_SUSPEND_FREEZE, we may get some extra "wakeup" Interrupts, that are
    not available for S3/S4.
    
    The patches has been tested on an old Sony laptop, and here are the results:
    
    Average Power:
    1. RPTM/idle for half an hour:
       14.8W, 12.6W, 14.1W, 12.5W, 14.4W, 13.2W, 12.9W
    2. Freeze for half an hour:
       11W, 10.4W, 9.4W, 11.3W 10.5W
    3. RTPM/idle for three hours:
       11.6W
    4. Freeze for three hours:
       10W
    5. Suspend to Memory:
       0.5~0.9W
    
    Average Resume Latency:
    1. RTPM/idle with a black screen: (From pressing keyboard to screen back)
       Less than 0.2s
    2. Freeze: (From pressing power button to screen back)
       2.50s
    3. Suspend to Memory: (From pressing power button to screen back)
       4.33s
    
    >From the results, we can see that all the platforms should benefit from
    this patch, even if it does not have Low Power S0.
    
    Signed-off-by: Zhang Rui <rui.zhang@intel.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit 8ac4cab5739ab6c4ffe1ca2d65fcb0b1b82c255b
Author: Rama Krishna Phani A <rphani@codeaurora.org>
Date:   Fri Jan 30 15:40:24 2015 +0530

    msm: sps: Support low memory condition in SPS driver
    
    In low memory conditions BAM DMUX is getting time out while allocating
    memory through kzalloc.
    
    Add Flag support in kzalloc while allocating memory and avoid delay
    during memory allocation
    
    CRs-Fixed: 790361
    Change-Id: I01bad1642ea9f38ffd723b51ccb8addcd3efbbcf
    Signed-off-by: Rama Krishna Phani A <rphani@codeaurora.org>

commit a6058cf2c345e3b979c6af62d9b8d57be697d9d5
Author: Venkat Devarasetty <vdevaras@codeaurora.org>
Date:   Wed Nov 20 19:59:47 2013 +0530

    msm: spm: set pc mode bit for SAW2_3.0
    
    Set the power collapse bitin SAW2_SPM_CTL register.
    This bit is used to differentiate other low power modes
    from power collapse mode. This information is used for
    contolling DBGWRDUP input signal. If PC_MODE is set DBGPWRDUP
    signal is de-asserted as soon as SPM_WAIT_ACK becomes HIGH.
    When the PC mode is not set it will help in early exit from
    clock gating.
    
    Signed-off-by: Venkat Devarasetty <vdevaras@codeaurora.org>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit e4b09763544accaa45f5b7a1ed1bb2f0468ed699
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun May 17 02:47:31 2015 +0200

    mach-msm: spm-v2 fix my merge problem

commit bf6463c3653cb9973bb619840fb3c10ca176f917
Author: Murali Nalajala <mnalajal@codeaurora.org>
Date:   Mon Oct 21 16:48:24 2013 +0530

    msm: spm-v2: Use proper register offsets for SAW2_v3.0
    
    Current code get the register offsets for SAW2 based
    on hardcode macros. Use SAW2 major, minor numbers to
    get the proper register offsets to different versions
    of SAW2 controllers(i.e: SAW2_v2.1 and SAW2_v3.0) and
    this patch remove the SAW2_v1.1 controller related code.
    
    Signed-off-by: Murali Nalajala <mnalajal@codeaurora.org>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit b10b9266dd144eab2167c9c5ef2131fe88240c48
Author: Archana Sathyakumar <asathyak@codeaurora.org>
Date:   Tue Sep 24 11:31:46 2013 -0600

    msm: spm-v2: Flush the PMIC_DATA registers
    
    Different versions of SAW use different PMIC_DATA registers to
    save the current voltage, which is used to restore voltage when
    exiting low power modes. Flush the appropriate PMIC_DATA registers
    depending on the SAW versions.
    
    Signed-off-by: Archana Sathyakumar <asathyak@codeaurora.org>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 3837236b9cabc571a138531a73ab037aa18a0796
Author: Mahesh Sivasubramanian <msivasub@codeaurora.org>
Date:   Thu Jun 27 13:16:21 2013 -0600

    msm: spm-v2: Store voltage levels into PMIC_DATA3
    
    On certain targets, where the SPM controls the power supply for each
    core, the SPM power down sequence takes care of restoring the Voltage
    level after a power collapse to the appropriate sleep level. The active
    voltage level is stored in the PMIC_DATA_3 register and SPM state
    machine uses the data in this register to restore the voltage to the
    current active level.
    
    Signed-off-by: Mahesh Sivasubramanian <msivasub@codeaurora.org>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 57aee10b9adeee72cfb6bf5ecc9edca21e8dedb3
Author: myfluxi <linflux@arcor.de>
Date:   Mon Jan 26 22:52:58 2015 +0100

    msm: watchdog_v2: Print IRQ during bark
    
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 4009c01e22d1424570fd8147cf9bcf0003ec6f48
Author: Ameya Thakur <ameyat@codeaurora.org>
Date:   Wed Oct 16 17:23:51 2013 -0700

    msm: msm_watchdog_v2: Increase watchdog bark timeout during panic
    
    We now wait for 10 seconds during panic handling before triggerring a
    watchdog bark. This is done because the earlier timeout of 4 seconds
    was not sufficient for all targets which resulted in the device resetting
    in the middle of the panic handler.
    
    Signed-off-by: Ameya Thakur <ameyat@codeaurora.org>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit f4bfae35c3e8d7c36f581895630e22070a7f6696
Author: Ramesh Gupta Guntha <rggupt@codeaurora.org>
Date:   Fri May 2 19:39:50 2014 +0530

    qcom: watchdog-v2: Update last_pet during the suspend and resume
    
    Currently last_pet time is not getting modified during the device
    suspend and resume. Hence update last_pet variable accordingly.
    
    Signed-off-by: Prasad Sodagudi <psodagud@codeaurora.org>
    Signed-off-by: Ramesh Gupta Guntha <rggupt@codeaurora.org>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit cc8e9ab8bf66ffd72ebf75409b46c1e7ece4d0b1
Author: Karthikeyan Ramasubramanian <kramasub@codeaurora.org>
Date:   Thu Dec 12 16:55:43 2013 -0700

    msm: ipc: Migrate to wakeup sources from wake locks
    
    Wakelock usage has been deprected. Update IPC Router to use wakeup
    sources.
    
    Signed-off-by: Karthikeyan Ramasubramanian <kramasub@codeaurora.org>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 530cd3b874c3d94060796c512989b2112facdbaa
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Thu Apr 18 18:15:44 2013 +0100

    ARM: cpu hotplug: remove majority of cache flushing from platforms
    
    Remove the majority of cache flushing calls from the individual platform
    files.  This is now handled by the core code.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit cbf6005dda81949427ff0f3265009fe74c7ff2e4
Author: Patrick Daly <pdaly@codeaurora.org>
Date:   Fri Feb 28 15:26:30 2014 -0800

    msm: clock: Fix invalid pointer dereferences on error condition
    
    Check whether a clock pointer is an error value before deferencing it.
    
    Signed-off-by: Patrick Daly <pdaly@codeaurora.org>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit c9cfca35581ce8eeb454b6e622ff3079a5ecc58d
Author: Jeff Hugo <jhugo@codeaurora.org>
Date:   Tue Jan 27 16:19:00 2015 -0700

    soc: qcom: bam_dmux: Sync SSR with disconnect/reconnect
    
    The disconnect and reconnect bam events access the hardware which may not
    be permitted durring subsystem restart due to the risk of a stalled access.
    
    Put an explicit sync event between SSR and the disconnect/reconnect events
    to eliminate any risk of hardware accesses which are not permitted.
    
    CRs-Fixed: 771505
    Change-Id: I206c1cf6f18955320aa20c1dac56eb2173675c4c
    Signed-off-by: Jeffrey Hugo <jhugo@codeaurora.org>

commit ad1a60a2de3bb1bcb8c0a4fd1ef3e7af757ecc23
Author: Rama Krishna Phani A <rphani@codeaurora.org>
Date:   Tue Feb 3 19:17:40 2015 +0530

    soc: qcom: bam_dmux: Use SPS atomic allocation flag
    
    Bam_dmux sometimes calls sps_connect() during time sensitive operations.
    Sps_connect() internally has some memory allocations which may stall during
    low memory conditions and can cause a timeout in bam_dmux.
    
    SPS supports a configuration flag which changes these memory allocations
    to atomic.  Use this flag to prevent allocations from stalling and leading
    to timeouts.
    
    CRs-Fixed: 790361
    Change-Id: Icde45f7b08a4b4a2b664c40d088fd338b48ac8fa
    Signed-off-by: Rama Krishna Phani A <rphani@codeaurora.org>

commit 730b1e5e45afa68b522d96b726a2fc0dbbba3498
Author: Arun Kumar Neelakantam <aneela@codeaurora.org>
Date:   Mon Dec 29 11:14:46 2014 +0530

    driver: soc: bam_dmux: Fix spinlock lock-up
    
    Client invoking the BAM DMUX function from callback notifier and
    calling the callback notifier by acquiring channel spinlock triggers
    spinlock lock-up.
    
    Remove the notifier callback from the spinlock context.
    
    Signed-off-by: Arun Kumar Neelakantam <aneela@codeaurora.org>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit cebdc63eefb27e9b61fe2d472176721e0695d18d
Author: myfluxi <linflux@arcor.de>
Date:   Fri Jan 23 19:37:58 2015 +0100

    msm: bam_dmux: Reduce debug logs to avoid watchdog bite
    
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit dc45f22bbf03cfc5576c67af189b1cb074c39b70
Author: Girish Mahadevan <girishm@codeaurora.org>
Date:   Fri Jul 25 15:32:40 2014 -0600

    msm: msm_bus: Fix inadvertent stomp on BKE_EN register
    
    While setting the QoS mode of fixed mode masters bad masking
    operations is causing accidental stomp on the upper nibble of the
    BKE_EN register.
    
    Signed-off-by: Girish Mahadevan <girishm@codeaurora.org>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 0cf21798697fd3c61ac7701811e259e1e8957e8a
Author: myfluxi <linflux@arcor.de>
Date:   Thu Jan 15 23:09:08 2015 +0100

    missing parts for commit: msm: cpufreq: Add pm notifiers
    
    Also, fix a merge inconsistency that hapenend in:
    I136841405806c07e755919859e649ded0c719abb
    
    Change-Id: I6f6424cfaade24922910d1ab27a1aa341af2e10d

commit ccff4c07e3ca23c4b814a228f03b6889352d316c
Author: Junjie Wu <junjiew@codeaurora.org>
Date:   Thu Jan 30 13:28:37 2014 -0800

    msm: cpufreq: Remove use of cpu_is_* API
    
    cpu_is_* API is deprecated. Relevant targets now rely on is_sync for
    proper cpufreq mask setup. Remove unnecessary cpu_is_* calls from
    cpufreq.c
    
    Change-Id: I326d24c24f37b34fba36dbf9fc08aa5071180b51
    Signed-off-by: Junjie Wu <junjiew@codeaurora.org>
    (cherry picked from commit 8954ea33151f1ae91b6ed9d9909eb7f245839c07)
    (cherry picked from commit 84029ff02c16a682243b57db75fec860927a3515)

commit b06cf5b3ea017785d592574f9de1621b18ac223d
Author: Syed Rameez Mustafa <rameezmustafa@codeaurora.org>
Date:   Thu Jan 23 15:58:46 2014 -0800

    msm: cpufreq: Remove redundant check in cpu_set_freq
    
    Recent code refactoring has led to the unintentional consequence
    of having redundant checks for the success of clk_set_rate. Remove
    the redundant check.
    
    Change-Id: I6cb7bdec15afb833519fac3232836ce9445cba48
    Signed-off-by: Syed Rameez Mustafa <rameezmustafa@codeaurora.org>

commit 3f400194d7d0b851c8018f73f8be8be23c7c3164
Author: Junjie Wu <junjiew@codeaurora.org>
Date:   Wed Mar 26 17:10:32 2014 -0700

    qcom-cpufreq: Fix hotplug blocking logic
    
    Change 2b63b10 qcom-cpufreq: Block hotplug until cpufreq is ready
    intended to block hotplug until qcom-cpufreq driver has got all
    clocks. However, its implementation still leaves a small window when
    CPU 0 has got its clock but others haven't. If a hotplug happens on
    another CPU that doesn't have its clock pointer, refcount of that CPU
    clock will again be wrong.
    
    Fix the issue by introducing a ready flag.
    
    Change-Id: I1e05b063b0584088f717b82709a3f1d55bc6561b
    Signed-off-by: Junjie Wu <junjiew@codeaurora.org>

commit 9a59340167956b287a26d54fb89fc138b5987eed
Author: Junjie Wu <junjiew@codeaurora.org>
Date:   Fri Feb 21 11:50:56 2014 -0800

    qcom-cpufreq: Block hotplug until cpufreq is ready
    
    Hotplug before qcom-cpufreq is ready could lead to inconsistent CPU
    clock state. Block hotplug by returning NOTIFY_BAD in hotplug callback
    until qcom-cpufreq is probed.
    
    Change-Id: I72a2f98c083c9b21b95ecafdb5a5be7a7682e842
    Signed-off-by: Junjie Wu <junjiew@codeaurora.org>

commit 6bfe5afd4ceb88b07aee721ebfdd92f76b63982f
Author: Junjie Wu <junjiew@codeaurora.org>
Date:   Thu Nov 14 17:52:10 2013 -0800

    msm: cpufreq: Remove acpuclock calls from cpufreq
    
    msm-cpufreq has moved to use generic clock APIs for getting and
    setting clock frequency. Remove the usage of deprecated acpuclock
    APIs.
    
    Change-Id: Ia05a86a58e031a3ec4ea59cb00fba1c33cddc78b
    Signed-off-by: Junjie Wu <junjiew@codeaurora.org>

commit c80ff98a704d6ac450bd20b5a625d3b743377ba8
Author: Junjie Wu <junjiew@codeaurora.org>
Date:   Mon Dec 1 21:21:00 2014 -0800

    qcom-cpufreq: Restore CPU frequency during resume
    
    qcom-cpufreq blocks CPU frequency change request during suspend, because
    its dependencies might be suspended. Thus a freq change request would
    fail silently, and CPU clock won't change until first frequency update
    is requested after system comes out of suspend. This creates a period
    when thermal driver cannot perform frequency mitigation, even though
    policy->min/max have been correctly updated.
    
    Check each online CPU's policy during resume to correct any frequency
    violation as soon as possible.
    
    Change-Id: I3be79cf91e7d5e361314020c9806b770823c0b72
    Signed-off-by: Junjie Wu <junjiew@codeaurora.org>

commit a460ca2807eb51845fdc82010dc4b89e8c4abc4c
Author: myfluxi <linflux@arcor.de>
Date:   Thu Jan 15 23:09:08 2015 +0100

    msm: cpufreq: Add pm notifiers
    
    Change-Id: I6f6424cfaade24922910d1ab27a1aa341af2e10d

commit 6c639f0f330374352e046e6335166798d363048c
Author: Matt Wagantall <mattw@codeaurora.org>
Date:   Thu Apr 24 15:08:30 2014 -0700

    qcom-cpufreq: Remove use of device_suspended in the hotplug path
    
    The CPU_DOWN_PREPARE notifier in cpufreq.c already takes care
    of performing a CPUFREQ_GOV_STOP, which waits for in-progress
    frequency switches to complete and prevents any additional
    frequency switches from being performed. Remove the redundant
    code for accomplishing the same thing from qcom-cpufreq.
    
    Besides being good cleanup, this change should also resolve
    a potential deadlock in out-of-memory conditions. The scenario
    arises if CPU1, executing in msm_cpufreq_target(), holds the
    qcom-cpufreq suspend_mutex while waiting for the completion
    at the end of that same function. In an out-of-memory condition,
    the work performing that completion may not be able to
    immediately run.
    
    At the same time, CPU2 may be executing CPU_DOWN_PREPARE hotplug
    notifiers. It will be holding the global hotplug mutex at this
    time, and will eventually become blocked in
    msm_cpufreq_cpu_callback() while waiting for the CPU1 to release
    the device_suspended lock.
    
    If invoked, the out-of-memory killer will attempt to acquire
    the hotplug mutex via get_online_cpus() as part of
    out_of_memory().  This lock is already held by CPU2. CPU2 will
    not release the lock until the device_suspended lock is released
    by CPU1. The device_suspended lock will not be release until the
    completion in msm_cpufreq_target() is marked as complete, which
    won't happen until the out-of-memory killer is able to free
    some memory. Nasty deadlock.
    
    Clearly, the out-of-memory condition itself needs to be resolved,
    but there is no harm in resolving the deadlock also.
    
    Change-Id: I5f59864aeb12a8c2aa81de16446af09a6d514da4
    Signed-off-by: Matt Wagantall <mattw@codeaurora.org>

commit 151773934dc1dfe63f15485e6c3367ab98be41c5
Author: alucard24 <dmbaoh2@gmail.com>
Date:   Fri Oct 10 22:44:15 2014 +0200

    msm cpufreq: msm freq limit leads on dvfs freq limit

commit 5562912a20c62aa15aa31745936251e0e8fb4f51
Author: Vikram Mulukutla <markivx@codeaurora.org>
Date:   Wed Apr 2 15:42:17 2014 -0700

    msm: cpufreq: Move the enabling of the CPU/L2 clocks to CPU_STARTING
    
    It is a requirement of PLLs in MSM SoCs that the PLL power
    supplies be enabled while the PLL is running. If the PLL is
    locked and outputs enabled, turning off the regulator(s)
    supplying the PLL will cause the PLL to enter an unpredictable
    state.
    
    Now in the CPU_UP_PREPARE notifier, the CPU clocks are prepared
    and enabled, causing the source HFPLLs to also turn on. Note
    that the CPU isn't clocked yet. It is possible that execution is
    pre-empted and the CPU running the notifier enters power
    collapse. If all other CPUs also enter power collapse, then it
    is possible for an RPM notification to go out, allowing the RPM
    to transition the Apps processor to its sleep set. This can
    result in the HFPLL supplies being turned off while the HFPLL is
    running, violating the requirement mentioned above. Once the CPU
    is unclamped, the CPU is effectively unclocked, due to the HFPLL
    being in an unknown state.
    
    There is a check that is enabled in the PM code's CPU_UP_PREPARE
    notifier callback. This check ensures that the problematic RPM
    notification cannot occur until the core that is being brought
    online also enters power collapse. However, there is no ordering
    guarantee between that PM's hotplug notifier callback's execution
    and the cpufreq hotplug notifier callback's execution. This
    ordering depends on program link order, which is unreliable.
    
    It is necessary to ensure that once the HFPLL is enabled, the
    RPM cannot transition apps to its sleep set. Move the enabling
    of the CPU clocks to the CPU_STARTING notifier, which runs on
    the CPU bring brought online. The CPU_STARTING notifier is
    guaranteed to run after the CPU_UP_PREPARE notifier, which
    implies that the aforementioned do-not-notify-rpm check is
    executed *before* the HFPLL is enabled. Therefore even if all
    cores enter power collapse after the HFPLL is enabled, the
    HFPLL supplies are guaranteed to stay on, or the CPU clock
    is switched to the safe source and the HFPLL is turned off.
    
    CRs-Fixed: 622738
    Change-Id: I136841405806c07e755919859e649ded0c719abb
    Signed-off-by: Vikram Mulukutla <markivx@codeaurora.org>

commit 784f03a96cb1199270e7b3e2903e68565dc7511a
Author: Patrick Daly <pdaly@codeaurora.org>
Date:   Wed Feb 5 18:43:41 2014 -0800

    msm: cpufreq: Correct synchronous cpu logic
    
    Fix an incorrect patch conflict resolution which results in failing to
    initialize the cpufreq policy correctly when using an old acpuclock driver
    version.
    
    Signed-off-by: Patrick Daly <pdaly@codeaurora.org>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 454a99c47f23305143fd503541e5dc959f483978
Author: Patrick Daly <pdaly@codeaurora.org>
Date:   Thu Dec 19 15:20:47 2013 -0800

    msm: cpufreq: Avoid NULL clock pointer references
    
    The MSM CPUfreq driver defines a mapping from cpu number to clock
    pointer. On SOCs with synchronous cpus, only the mapping for cpu#0
    is defined, even though there may be more than 1 cpu. Handle this
    case by skipping certain unnecessary initialization steps for SOCs
    with synchronous cpus.
    
    Conflicts:
    	arch/arm/mach-msm/cpufreq.c
    
    Change-Id: Iaeba7a8615e3cb7da2c725250db92595450bfa46
    Signed-off-by: Patrick Daly <pdaly@codeaurora.org>

commit 09b93f2d3f09e5c699e20715420e6d960934dcf3
Author: Xiaogang Cui <xiaogang@codeaurora.org>
Date:   Mon Sep 2 10:57:54 2013 +0800

    msm: cpufreq: Add msm_cpufreq debugfs file to show CPU -> L2/BW mapping
    
    Example output:
    cat /sys/kernel/debug/clk/msm_cpufreq
      CPU (KHz)  L2 (KHz)  Mem (MBps)
         300000    300000         600
         422400    422400        1200
         652800    499200        1600
         729600    576000        2456
         883200    576000        2456
         960000    960000        3680
        1036800   1036800        3680
        1190400   1036800        3680
        1267200   1267200        6400
        1497600   1497600        6400
        1574400   1574400        6400
        1728000   1651200        6400
        1958400   1728000        7448
        2265600   1728000        7448
    
    Change-Id: I20df8c3778c0fcd0ef1f369acb1cb72cf5334812
    Signed-off-by: Xiaogang Cui <xiaogang@codeaurora.org>

commit c29a8c5e8e50ff95f7ba7d272a4831f3f7dba643
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Thu May 15 01:58:57 2014 +0200

    Optimized drivers and msm cpufreq

commit f37899e5c12fa3bedd6f240c1963523078a25e43
Author: Paul Reioux <reioux@gmail.com>
Date:   Thu May 23 11:48:57 2013 -0500

    arch/arm/mach-msm/cpufreq.c: reduce dmesg log spam
    
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit 425d22e7ce7a68164c2901ba0565499b23d24d6c
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sat May 16 18:20:20 2015 +0200

    config: enable the new KERNEL_TEXT_MPU_PROT

commit f4a67e9246529a77c73274a456520b382ad599d0
Author: Dmitry Shmidt <dimitrysh@google.com>
Date:   Tue Jan 13 13:52:49 2015 -0800

    irq: pm: Remove unused variable
    
    Change-Id: Ie4311b554628af878cd80fd0abc03b2be294f0bf
    Signed-off-by: Dmitry Shmidt <dimitrysh@google.com>

commit 5a0a9f25bace378f454efec2d398971fd483c2da
Author: Lorenzo Colitti <lorenzo@google.com>
Date:   Thu Nov 27 15:12:10 2014 +0900

    Make suspend abort reason logging depend on CONFIG_PM_SLEEP
    
    This unbreaks the build on architectures such as um that do not
    support CONFIG_PM_SLEEP.
    
    Change-Id: Ia846ed0a7fca1d762ececad20748d23610e8544f
    Signed-off-by: Lorenzo Colitti <lorenzo@google.com>

commit 5344977a9b59792775a1ab105dafa96e7c47446c
Author: Ruchi Kandoi <kandoiruchi@google.com>
Date:   Wed Apr 8 15:42:29 2015 -0700

    wakeup: Add last wake up source logging for suspend abort reason.
    
    There is a possibility that a wakeup source event is received after
    the device prepares to suspend which might cause the suspend to abort.
    
    This patch adds the functionality of reporting the last active wakeup
    source which is currently not active but caused the suspend to abort reason
    via the /sys/kernel/power/last_wakeup_reason file.
    
    Change-Id: I1760d462f497b33e425f5565cb6cff5973932ec3
    Signed-off-by: Ruchi Kandoi <kandoiruchi@google.com>

commit 24afb26322c2c698138694b339c2e1d68c5fba00
Author: jinqian <jinqian@google.com>
Date:   Tue Apr 14 13:47:35 2015 -0700

    power: increment wakeup_count when save_wakeup_count failed.
    
    user-space aborts suspend attempt if writing wakeup_count failed.
    Count the write failure towards wakeup_count.
    
    Signed-off-by: jinqian <jinqian@google.com>
    Change-Id: Ic0123ac7ef31564700b1f6b5f2234275ac104244

commit 7fc42fee34576c59dc5d5ae138929306d4d68c90
Author: John Stultz <john.stultz@linaro.org>
Date:   Thu Sep 6 23:19:06 2012 +0200

    PM / wakeup: Use irqsave/irqrestore for events_lock
    
    Jon Medhurst (Tixy) recently noticed a problem with the
    events_lock usage. One of the Android patches that uses
    wakeup_sources calls wakeup_source_add() with irqs disabled.
    However, the event_lock usage in wakeup_source_add() uses
    spin_lock_irq()/spin_unlock_irq(), which reenables interrupts.
    This results in lockdep warnings.
    
    The fix is to use spin_lock_irqsave()/spin_lock_irqrestore()
    instead for the events_lock.
    
    References: https://bugs.launchpad.net/linaro-landing-team-arm/+bug/1037565
    Reported-and-debugged-by: Jon Medhurst (Tixy) <tixy@linaro.org>
    Signed-off-by: John Stultz <john.stultz@linaro.org>
    Signed-off-by: Rafael J. Wysocki <rjw@sisk.pl>

commit cec6694abfcea1caa9cc8b0f331d1b7699b468dc
Author: jinqian <jinqian@google.com>
Date:   Mon Apr 13 14:12:47 2015 -0700

    power: validate wakeup source before activating it.
    
    A rogue wakeup source not registered in wakeup_sources list is not visible
    from wakeup_sources_stats_show. Check if the wakeup source is registered
    properly by looking at the timer struct.
    
    Signed-off-by: jinqian <jinqian@google.com>
    Change-Id: Id4b2fdb3cf7fecb2fe7576b2305cd337974665c9
    
    Conflicts:
    	drivers/base/power/wakeup.c

commit 7b97b3b51e780b450d2e48900f57d26501040492
Author: jinqian <jinqian@google.com>
Date:   Wed Mar 25 16:18:44 2015 -0700

    Power: Report suspend times from last_suspend_time
    
    This node epxorts two values separated by space.
    From left to right:
    1. time spent in suspend/resume process
    2. time spent sleep in suspend state
    
    Change-Id: I2cb9a9408a5fd12166aaec11b935a0fd6a408c63

commit c4afd6b7efa19135bbb16eca460a53b02b07e13c
Author: Dmitry Shmidt <dimitrysh@google.com>
Date:   Fri Oct 31 16:05:46 2014 -0700

    power: Add check_wakeup_reason() to verify wakeup source irq
    
    Wakeup reason is set before driver resume handlers are called.
    It is cleared before driver suspend handlers are called, on
    PM_SUSPEND_PREPARE.
    
    Change-Id: I04218c9b0c115a7877e8029c73e6679ff82e0aa4
    Signed-off-by: Dmitry Shmidt <dimitrysh@google.com>

commit 0433146f4f0a95acd60df7023f79f3d6f4c559b5
Author: Ruchi Kandoi <kandoiruchi@google.com>
Date:   Wed Oct 29 10:36:27 2014 -0700

    power: Adds functionality to log the last suspend abort reason.
    
    Extends the last_resume_reason to log suspend abort reason. The abort
    reasons will have "Abort:" appended at the start to distinguish itself
    from the resume reason.
    
    Signed-off-by: Ruchi Kandoi <kandoiruchi@google.com>
    Change-Id: I3207f1844e3d87c706dfc298fb10e1c648814c5f

commit 23a44a1e969e7df812eaab4f04188cf8cd3c4eab
Author: Benoit Goby <benoit@android.com>
Date:   Mon Feb 14 21:32:43 2011 -0800

    PM: Add watchdog to catch lockup during device resume
    
    Refactor the dpm suspend watchdog code and add watchdogs
    on resume too. The dpm wachdog prints the stack trace and
    reboots the system if a device takes more than 12 seconds
    to suspend or resume.
    
    Change-Id: If00c047a17b80bdc13a8426393c698bc450a7347
    Signed-off-by: Benoit Goby <benoit@android.com>
    
    Conflicts:
    	drivers/base/power/main.c

commit a3e33ccf49393334af843835f20170a6fdd03f8d
Author: Ulf Hansson <ulf.hansson@linaro.org>
Date:   Fri Apr 12 09:41:06 2013 +0000

    PM / Runtime: Asyncronous idle|suspend devices at system resume
    
    Use the asyncronous runtime PM API when returning the runtime
    reference for the device after the system resume is completed.
    
    By using the asyncronous runtime PM API we don't have to wait
    for each an every device to become idle|suspended. Instead we
    can move on and handle the next device in queue.
    
    Signed-off-by: Ulf Hansson <ulf.hansson@linaro.org>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit f1261fe5eda5cb2cbbde5a2aa5e8197395e7331d
Author: Ming Lei <ming.lei@canonical.com>
Date:   Fri Aug 17 22:06:59 2012 +0800

    PM / Sleep: introduce dpm_for_each_dev
    
    dpm_list and its pm lock provide a good way to iterate all
    devices in system. Except this way, there is no other easy
    way to iterate devices in system.
    
    firmware loader need to cache firmware images for devices
    before system sleep, so introduce the function to meet its
    demand.
    
    Reported-by: Fengguang Wu <fengguang.wu@intel.com>
    Signed-off-by: Ming Lei <ming.lei@canonical.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 6ffee1b4c908a24d6238447be1fb6096778e9856
Author: Sachin Kamat <sachin.kamat@linaro.org>
Date:   Tue Jul 17 22:38:08 2012 +0200

    PM / Sleep: Add missing static storage class specifiers in main.c
    
    Fix the following sparse warnings:
    drivers/base/power/main.c:48:1: warning: symbol 'dpm_prepared_list' was not declared. Should it be static?
    drivers/base/power/main.c:49:1: warning: symbol 'dpm_suspended_list' was not declared. Should it be static?
    drivers/base/power/main.c:50:1: warning: symbol 'dpm_late_early_list' was not declared. Should it be static?
    drivers/base/power/main.c:51:1: warning: symbol 'dpm_noirq_list' was not declared. Should it be static?
    
    Signed-off-by: Sachin Kamat <sachin.kamat@linaro.org>
    Signed-off-by: Rafael J. Wysocki <rjw@sisk.pl>

commit 39ca0481d76906d5987de89d15fed5e13225e834
Author: Rafael J. Wysocki <rjw@sisk.pl>
Date:   Mon Aug 6 01:44:28 2012 +0200

    PM: Reorganize device PM initialization
    
    Make the device power management initialization more straightforward
    by moving the initialization of common (i.e. used by both runtime PM
    and system suspend) fields to a separate routine.
    
    Signed-off-by: Rafael J. Wysocki <rjw@sisk.pl>

commit 26409a74a7b44bd4bfd188a43c79516bc8c9787e
Author: Preeti U Murthy <preeti@linux.vnet.ibm.com>
Date:   Mon Jul 9 10:12:56 2012 +0200

    PM / cpuidle: System resume hang fix with cpuidle
    
    On certain bios, resume hangs if cpus are allowed to enter idle states
    during suspend [1].
    
    This was fixed in apci idle driver [2].But intel_idle driver does not
    have this fix. Thus instead of replicating the fix in both the idle
    drivers, or in more platform specific idle drivers if needed, the
    more general cpuidle infrastructure could handle this.
    
    A suspend callback in cpuidle_driver could handle this fix. But
    a cpuidle_driver provides only basic functionalities like platform idle
    state detection capability and mechanisms to support entry and exit
    into CPU idle states. All other cpuidle functions are found in the
    cpuidle generic infrastructure for good reason that all cpuidle
    drivers, irrepective of their platforms will support these functions.
    
    One option therefore would be to register a suspend callback in cpuidle
    which handles this fix. This could be called through a PM_SUSPEND_PREPARE
    notifier. But this is too generic a notfier for a driver to handle.
    
    Also, ideally the job of cpuidle is not to handle side effects of suspend.
    It should expose the interfaces which "handle cpuidle 'during' suspend"
    or any other operation, which the subsystems call during that respective
    operation.
    
    The fix demands that during suspend, no cpus should be allowed to enter
    deep C-states. The interface cpuidle_uninstall_idle_handler() in cpuidle
    ensures that. Not just that it also kicks all the cpus which are already
    in idle out of their idle states which was being done during cpu hotplug
    through a CPU_DYING_FROZEN callbacks.
    
    Now the question arises about when during suspend should
    cpuidle_uninstall_idle_handler() be called. Since we are dealing with
    drivers it seems best to call this function during dpm_suspend().
    Delaying the call till dpm_suspend_noirq() does no harm, as long as it is
    before cpu_hotplug_begin() to avoid race conditions with cpu hotpulg
    operations. In dpm_suspend_noirq(), it would be wise to place this call
    before suspend_device_irqs() to avoid ugly interactions with the same.
    
    Ananlogously, during resume.
    
    References:
    [1] https://bugs.launchpad.net/ubuntu/+source/linux/+bug/674075.
    [2] http://marc.info/?l=linux-pm&m=133958534231884&w=2
    
    Reported-and-tested-by: Dave Hansen <dave@linux.vnet.ibm.com>
    Signed-off-by: Preeti U Murthy <preeti@linux.vnet.ibm.com>
    Reviewed-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Signed-off-by: Rafael J. Wysocki <rjw@sisk.pl>

commit c11e3743d1615e1a1db27577880f86ddabd88808
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sat May 16 17:52:43 2015 +0200

    Revert "PM / Sleep: add knob for printing device resume times"
    
    This reverts commit 67114d49c071722ae7f0114a63dc211febdfe82a.

commit 67114d49c071722ae7f0114a63dc211febdfe82a
Author: Sameer Nanda <snanda@chromium.org>
Date:   Tue Jun 19 22:23:33 2012 +0200

    PM / Sleep: add knob for printing device resume times
    
    Added a new knob called /sys/power/pm_print_times. Setting it to 1
    enables printing of time taken by devices to suspend and resume.
    Setting it to 0 disables this printing (unless overridden by
    initcall_debug kernel command line option).
    
    Signed-off-by: Sameer Nanda <snanda@chromium.org>
    Acked-by: Greg KH <gregkh@linuxfoundation.org>
    Signed-off-by: Rafael J. Wysocki <rjw@sisk.pl>

commit 17dab06571bed35c50f647961a4c44ea8b47186c
Author: Ruchi Kandoi <kandoiruchi@google.com>
Date:   Thu Apr 24 14:31:57 2014 -0700

    Power: Changes the permission to read only for sysfs file
    /sys/kernel/wakeup_reasons/last_resume_reason
    
    Change-Id: I8ac568a7cb58c31decd379195de517ff3c6f9c65
    Signed-off-by: Ruchi Kandoi <kandoiruchi@google.com>

commit 931308f747b9c0379159233dccb92e3a59cd5cc6
Author: Greg Hackmann <ghackmann@google.com>
Date:   Mon Mar 10 14:21:30 2014 -0700

    power: wakeup_reason: rename irq_count to irqcount
    
    On x86, irq_count conflicts with a declaration in
    arch/x86/include/asm/processor.h
    
    Change-Id: I3e4fde0ff64ef59ff5ed2adc0ea3a644641ee0b7
    Signed-off-by: Greg Hackmann <ghackmann@google.com>

commit fa8390491fc83c91e106fb70e4f575d04608fa9b
Author: Ruchi Kandoi <kandoiruchi@google.com>
Date:   Fri Mar 7 12:54:30 2014 -0800

    Power: Add guard condition for maximum wakeup reasons
    
    Ensure the array for the wakeup reason IRQs does not overflow.
    
    Change-Id: Iddc57a3aeb1888f39d4e7b004164611803a4d37c
    Signed-off-by: Ruchi Kandoi <kandoiruchi@google.com>
    (cherry picked from commit b5ea40cdfcf38296535f931a7e5e7bf47b6fad7f)

commit fdb5c6bbf0ffcebd2509ffc967893b78426c3ec6
Author: Ruchi Kandoi <kandoiruchi@google.com>
Date:   Thu Feb 20 19:47:38 2014 -0800

    POWER: fix compile warnings in log_wakeup_reason
    
    Change I81addaf420f1338255c5d0638b0d244a99d777d1 introduced compile
    warnings, fix these.
    
    Change-Id: I05482a5335599ab96c0a088a7d175c8d4cf1cf69
    Signed-off-by: Ruchi Kandoi <kandoiruchi@google.com>

commit a3e740d3c94119e3749e60a3bba0b6a928d03774
Author: Ruchi Kandoi <kandoiruchi@google.com>
Date:   Wed Feb 19 15:30:47 2014 -0800

    Power: add an API to log wakeup reasons
    
    Add API log_wakeup_reason() and expose it to userspace via sysfs path
    /sys/kernel/wakeup_reasons/last_resume_reason
    
    Change-Id: I81addaf420f1338255c5d0638b0d244a99d777d1
    Signed-off-by: Ruchi Kandoi <kandoiruchi@google.com>

commit 62d2476e6838ae5c23a006309d8d214ba7f1dabc
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sat May 16 17:46:10 2015 +0200

    mach-msm: remove merge "HEAD"

commit 1ed2ba25857c7fab50f6f611640b3330042a2d41
Author: Patrick Daly <pdaly@codeaurora.org>
Date:   Thu Feb 12 16:16:39 2015 -0800

    scm-mpu: Add additional memory protection options
    
    Add support for a scm call to protect a contiguous physical address range.
    Add a config option to use this api to protect the kernel text section.
    Otherwise, the user may specify the region through module parameters.
    
    CRs-Fixed: 797891
    Change-Id: Ie751ec72deb1a1692093559fe8c6784e8bf912a2
    Signed-off-by: Patrick Daly <pdaly@codeaurora.org>
    [schikk@codeaurora.org: 3.4 kernel doesn't have
    drivers/soc/qcom folder, new file scm-mpu.c has
    been added in arch/arm/mach-msm/.
    3.4 kernel doesn't support function scm_call2 ,
    so replaced the function call with scm_call which
    is supported in 3.4 kernel ]
    Signed-off-by: Swetha Chikkaboraiah <schikk@codeaurora.org>

commit 5cce158bef216afe49400503c6f7bd9dde02ca8a
Author: myfluxi <linflux@arcor.de>
Date:   Mon Oct 20 21:35:30 2014 +0200

    usb: dwc3: Fix race between pm_resume and runtime_resume

commit a09c6abee4da05301f828327694f6ec6b7c59887
Author: Amit Pundir <amit.pundir@linaro.org>
Date:   Fri Jan 16 05:41:10 2015 +0530

    usb: gadget: check for accessory device before disconnecting HIDs
    
    While disabling ConfigFS Android gadget, android_disconnect() calls
    kill_all_hid_devices(), if CONFIG_USB_CONFIGFS_F_ACC is enabled, to free
    the registered HIDs without checking whether the USB accessory device
    really exist or not. If USB accessory device doesn't exist then we run into
    following kernel panic:
    ----8<----
    [ 136.724761] Unable to handle kernel NULL pointer dereference at virtual address 00000064
    [ 136.724809] pgd = c0204000
    [ 136.731924] [00000064] *pgd=00000000
    [ 136.737830] Internal error: Oops: 5 [#1] SMP ARM
    [ 136.738108] CPU: 0 PID: 0 Comm: swapper/0 Not tainted 3.18.0-rc4-00400-gf75300e-dirty #76
    [ 136.742788] task: c0fb19d8 ti: c0fa4000 task.ti: c0fa4000
    [ 136.750890] PC is at _raw_spin_lock_irqsave+0x24/0x60
    [ 136.756246] LR is at kill_all_hid_devices+0x24/0x114
    ---->8----
    
    This patch adds a test to check if USB Accessory device exists before freeing HIDs.
    
    Change-Id: Ie229feaf0de3f4f7a151fcaa9a994e34e15ff73b
    Signed-off-by: Amit Pundir <amit.pundir@linaro.org>
    (cherry picked from commit 32a71bce154cb89a549b9b7d28e8cf03b889d849)

commit 3f5efdd3b41c2694ca18cfee78541e784a3c8f97
Author: Jack Pham <jackp@codeaurora.org>
Date:   Tue Feb 18 22:16:19 2014 -0800

    usb: gadget: android: Save/restore ep0 completion function
    
    The android_setup() function currently gives the f_accessory
    setup function first opportunity to handle control requests
    in order to support Android Open Accessory (AOA) hosts. That
    function makes use of cdev->req and overrides its completion
    function, but not in all cases. Thus, if a later request uses
    the same request pointer but doesn't (re)set req->complete it
    could result in the wrong completion function being called and
    causing invalid memory access.
    
    One way to fix this would be to explicitly set req->complete in
    all cases but that might require auditing all function drivers
    that have ep0 handling. Instead, note that the composite device
    had already initially set cdev->req->complete and simply cache
    and restore that pointer at the start of android_setup().
    
    Change-Id: I33bcd17bd20687a349d537d1013b52a2afef6996
    Signed-off-by: Jack Pham <jackp@codeaurora.org>

commit c17aa4dee82fb69c486bf1a7fef8f69312c6e8c0
Author: Mike Lockwood <lockwood@google.com>
Date:   Tue Aug 5 16:13:05 2014 -0700

    USB: gadget: f_mtp: Fix integer overflow when transferring large files.
    
    The ALIGN() macro will overflow if given large integers, and MTP uses
    0xFFFFFFFF for SendObject data packet size when the file is >= 4 gig.
    
    Signed-off-by: Mike Lockwood <lockwood@google.com>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 9585e9c5e65070218cf9cc321b7719ed56285f52
Author: Jesper Juhl <jj@chaosbits.net>
Date:   Mon Apr 9 22:52:04 2012 +0200

    usb/storage/ene_ub6250: Remove redundant NULL check before release_firmware() and pointless assignment
    
    release_firmware() tests for a NULL pointer, so it's redundant to do
    that checking before calling it.
    
    Additionally, in ene_load_bincode(), 'sd_fw' is a local variable so
    setting it to NULL just before it goes out of scope is completely
    pointless, so remove that assignment.
    
    Signed-off-by: Jesper Juhl <jj@chaosbits.net>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 0de1d8df091c7f8db9da54c304dc224d385603b4
Author: Joe Perches <joe@perches.com>
Date:   Fri Apr 19 11:44:00 2013 -0700

    usb: storage: Convert US_DEBUGP to usb_stor_dbg
    
    Use a more current logging style with dev_printk
    where possible.
    
    o Convert uses of US_DEBUGP to usb_stor_dbg
    o Add "struct us_data *" to usb_stor_dbg uses
    o usb_stor_dbg now uses struct device */dev_vprint_emit
    o Removed embedded function names
    o Coalesce formats
    o Remove trailing whitespace
    o Remove useless OOM messages
    o Remove useless function entry/exit logging
    o Convert some US_DEBUGP uses to dev_info and dev_dbg
    
    Object size is slightly reduced when debugging
    is enabled, slightly increased with no debugging
    because some initialization and removal messages
    are now always emitted.
    
    Signed-off-by: Joe Perches <joe@perches.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 99bc5f3892f95d2d04b477df280c5f6faeadd779
Author: Jeffrin Jose <ahiliation@yahoo.co.in>
Date:   Thu May 17 00:33:36 2012 +0530

    USB: storage: fixed several trailing white spaces issues.
    
    Fixed several trailing white spaces issues found
    by checkpatch.pl tool in drivers/usb/storage/usb.c
    
    Signed-off-by: Jeffrin Jose <ahiliation@yahoo.co.in>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 21f0cd95d8e88aeb0dd84938e8ec907e5f605a03
Author: Jeffrin Jose <ahiliation@yahoo.co.in>
Date:   Thu May 17 00:34:28 2012 +0530

    USB: storage: fixed keyword related space issues.
    
    Fixed keyword related space issues found by
    checkpatch.pl tool in drivers/usb/storage/usb.c
    
    Signed-off-by: Jeffrin Jose <ahiliation@yahoo.co.in>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit a65d64e9c752d7d6aa68d00d641e1b0a3b0e0b2a
Author: Tim Gardner <tim.gardner@canonical.com>
Date:   Fri Jul 27 10:53:21 2012 -0600

    USB: storage: ene_ub6250: Use macros for firmware names
    
    Advertise firmware files using MODULE_FIRMWARE macros.
    
    Fix a debug string: SD_RDWR_PATTERN --> SD_RW_PATTERN
    
    Signed-off-by: Tim Gardner <tim.gardner@canonical.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 1d45e59f4521b8abac4a807985c566372abcc269
Author: Joe Perches <joe@perches.com>
Date:   Thu Apr 18 14:17:14 2013 -0700

    usb: storage: Fix link error
    
    Fix allmodconfig link error introduced by commit 75b9130e8a
    ("usb: storage: Add usb_stor_dbg, reduce object size")
    
    Export the symbol usb_stor_dbg.
    Add export.h
    
    Reported-by: Stephen Rothwell <sfr@canb.auug.org.au>
    Signed-off-by: Joe Perches <joe@perches.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit da0ce820b256773eecde12e230acdd37abd975e0
Author: Joe Perches <joe@perches.com>
Date:   Tue Apr 16 22:44:07 2013 -0700

    usb: storage: Add usb_stor_dbg, reduce object size
    
    Reduce the size of the objects by consolidating
    the duplicated USB_STORAGE into a single function.
    
    Add function usb_stor_dbg to emit debugging messages.
    Always validate the format and arguments.
    Reduce the number of uses of CONFIG_USB_STORAGE_DEBUG.
    
    Reduces size of objects ~7KB when CONFIG_USB_STORAGE_DEBUG
    is set.
    
    $ size drivers/usb/storage/built-in.o*
       text	   data	    bss	    dec	    hex	filename
     140133	  55296	  70312	 265741	  40e0d	drivers/usb/storage/built-in.o.new
     147494	  55248	  70296	 273038	  42a8e	drivers/usb/storage/built-in.o.old
    
    Signed-off-by: Joe Perches <joe@perches.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit bc41e7f9cf9cceabbd867347c42654a30135b49c
Author: ChandanaKishori Chiluveru <cchilu@codeaurora.org>
Date:   Tue Jul 22 12:41:52 2014 +0530

    usb: msm_otg: allow peripheral mode switch in more host states
    
    The current code allows peripheral mode switch in only A_HOST state
    using user mode otg control debug interface. The target can also be
    in A_WAIT_BCON and A_SUSPEND while operating as a host. Allow peripheral
    mode switch in these states as well. Also check the PMIC id status
    when the otg control is set to PMIC.
    
    Change-Id: I6557b38ce5615e5c7ac3e670086bbfa98507bda7
    Signed-off-by: ChandanaKishori Chiluveru <cchilu@codeaurora.org>

commit b52a24a67ec551d9abd8bd95ea1475424e65d0a0
Author: Anson Jacob <ansonkuzhumbil@gmail.com>
Date:   Wed Jul 16 18:26:35 2014 +0800

    usb: gadget: f_audio_source: Fixed USB Audio Class Interface Descriptor
    
    Fixed Android Issue #56549.
    
    When both Vendor Class and Audio Class are activated for AOA 2.0,
    the baInterfaceNr of the AudioControl Interface Descriptor points
    to wrong interface numbers. They should be pointing to
    Audio Control Device and Audio Streaming interfaces.
    
    Replaced baInterfaceNr with the correct value.
    
    Change-Id: Ib20f20c1d65887bac051b7ff9453dc53fc23a8d0
    Signed-off-by: Anson Jacob <ansonkuzhumbil@gmail.com>

commit a76242b97f6ec29584f0b37b32ab715cd9b6e84f
Author: Anson Jacob <ansonkuzhumbil@gmail.com>
Date:   Mon Jun 23 19:07:44 2014 +0800

    usb: gadget: f_accessory: Enabled Zero Length Packet (ZLP) for acc_write
    
    Accessory connected to Android Device requires
    Zero Length Packet (ZLP) to be written when data
    transferred out from the Android device are multiples
    of wMaxPacketSize (64bytes (Full-Speed) / 512bytes (High-Speed))
    to end the transfer.
    
    Change-Id: Ib2c2c0ab98ef9afa10e74a720142deca5c0ed476
    Signed-off-by: Anson Jacob <ansonkuzhumbil@gmail.com>

commit 235f58aecd9137686b0e6ab15498a20cf6b2afda
Author: keunyoung <keunyoung@google.com>
Date:   Wed Jan 29 12:41:50 2014 -0800

    fix false disconnect due to a signal sent to the reading process
    
    - In the current implementation, when a signal is sent to the reading process,
      read is cancelled by calling usb_ep_dequeue, which lead into calling
      acc_complete_out with ECONNRESET, but the current logic treats it as
      disconnection, which makes the device inaccessible until cable is actually
      disconnected.
    - The fix calls disconnect only when ESHUTDOWN error is passed.
    - If data has already arrived while trying cancelling, the data is marked
      as available, and it will be read out on the next read. This is necessary
      as USB bulk is assumed to guarantee no data loss.
    
    Signed-off-by: keunyoung <keunyoung@google.com>

commit a1ba396640315934797bd315cc92c48bd2debf95
Author: Pavankumar Kondeti <pkondeti@codeaurora.org>
Date:   Fri Apr 11 15:45:28 2014 +0530

    USB: storage: Allow UICC cards to perform house keeping operations
    
    Some UICC cards needs to perform the flash related operations during idle
    time. The idle time is detected by the card by counting the consecutive
    TEST_UNIT_READY commands. If the card does not find idle time, it perform
    them while processing a WRITE(10) command. If the card is kept busy which
    happens during continuous mass storage interfaces, it takes lot of time to
    respond to the commands on the CCID interface.
    
    Introduce a quirk called US_FL_TUR_AFTER_WRITE for sending 5 consecutive
    TEST_UNIT_READY commands for every 8 WRITE(10) commands. This approach
    forces the card to perform house keeping operations during continuous
    mass storage transfers. The test result indicate no degradation in
    the performance.
    
    Signed-off-by: Pavankumar Kondeti <pkondeti@codeaurora.org>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 7eda3c4c8eb840f00e6fdc64e8806797236914ce
Merge: 21bfc31 94172e3
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sat May 16 17:21:55 2015 +0200

    Merge branch 'master' of https://github.com/Tkkg1994/Hulk-Kernel-V2

commit 21bfc31cb2392a948b2e32d9e986ac4743603125
Author: Saket Saurabh <ssaurabh@codeaurora.org>
Date:   Thu Feb 20 13:00:02 2014 +0530

    usb: Avoid incorrect memory leaks reported by kmemleak tool.
    
    Certain memory allocations are not properly tracked by the kmemleak
    tool, which causes it to incorrectly detect memory leaks. Notify the
    tool by using kmemleak_not_leak() to ignore the memory allocation
    so that incorrect leak reports are avoided.
    
    Signed-off-by: Saket Saurabh <ssaurabh@codeaurora.org>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 175949ee35e47763b2aa8edea161f7cedbd1372e
Author: Mike Lockwood <lockwood@google.com>
Date:   Thu Mar 6 15:49:39 2014 -0800

    USB: gadget: mtp: Fix hang in ioctl(MTP_RECEIVE_FILE) for WritePartialObject
    
    In receive_file_work, we now queue reads with length rounded up to
    max packet size rather than mtp_rx_req_len.
    Otherwise the read request will never complete.
    
    Signed-off-by: Mike Lockwood <lockwood@google.com>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit f642ac49546ae14398d403300868d9ef5555b24e
Author: dorimanx <yuri@bynet.co.il>
Date:   Wed May 21 14:48:20 2014 +0300

    USB KEYBOARD/MOUSE: Added on/off trigger to driver. READ MORE.
    
    When was ON by default I have seen that USB is always HID device.
    
    And i have always received 2 connection sounds and vibrate...
    
    It's annoying! so now all good, one alert for connection, and USB
    is Android Device, and when I have connect USB Keyboard or mouse or both
    via wireless connector (Microsoft keyboard + mouse Infra-Red)
    
    it's works! phone detect mouse + keyboard and i can use them on screen!
    
    the On/off trigger seems not needed to use the HID and can be OFF.
    
    I have left it for future use, for compatibility if needed.

commit 7da398ee2691198c2471d630bb05766336fdce22
Author: Pranav Vashi <neobuddy89@gmail.com>
Date:   Mon May 19 00:27:36 2014 +0530

    drivers: usb: Use Android device as USB keyboard/mouse
    
    Adapt to 3.4.y 8974/hammerhead
    Base project: https://github.com/pelya/android-keyboard-gadget
    Required app: https://play.google.com/store/apps/details?id=remote.hid.keyboard.client
    
    Steps:
    * Start ADB Debugging
    * Start app
    * Connect device to system via USB cable
    
    All credits to @pelya for this sweet hack.
    
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit f301ae72fab8eaf784879004261859f6c9058fdb
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sat May 16 17:14:17 2015 +0200

    cpufreq: add new ondemandplus, thanks to @dorimanx and @Alucard24 for their great work!

commit 94172e37f406bde5a0b3f104d194beaf4e5d4052
Author: Luca Grifo <luca.grifo@outlook.com>
Date:   Sat May 16 16:59:44 2015 +0200

    USB: storage: ene_ub6250: Use kmemdup instead of kmalloc + memcpy
    
    This issue was reported by coccicheck using the semantic patch
    at scripts/coccinelle/api/memdup.cocci
    
    Signed-off-by: Benoit Taine <benoit.taine@lip6.fr>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit b61e967022cfff80b6ffb7b61e3d67fede04ac91
Author: ktoonsez <ktoonsez@gmail.com>
Date:   Sat Mar 1 21:19:08 2014 -0700

    SSP: Break out some pieces of ssp.h into a seperate file so I can include only the pieces I need for screen and sensor wake functions

commit 089792ffdbfddfeecc78b78bceb5eece29707aee
Author: ktoonsez <ktoonsez@gmail.com>
Date:   Fri Apr 4 14:17:15 2014 -0700

    synaptics: Readd some of the screen off flags from stock source

commit 3a29a601f30c6161943d99ee40f5bb5dbefb139d
Author: ktoonsez <ktoonsez@gmail.com>
Date:   Wed Mar 26 19:57:08 2014 -0700

    Bring back in synaptics_rmi4_f51_set_custom_rezero and synaptics_rmi4_reset_device functions to make sure screen is initialized

commit 659aedc55ab5a108d199d6a8765a1ce63baf0167
Author: ktoonsez <ktoonsez@gmail.com>
Date:   Wed Mar 12 16:34:09 2014 -0700

    Add double tap 2 sleep and some prox fixes from the 2 sleep functions battling
    
    Conflicts:
    	drivers/input/touchscreen/synaptics_i2c_rmi.c

commit dd492b8ba36415662497563fe06fc8b343e92b5a
Author: ktoonsez <ktoonsez@gmail.com>
Date:   Wed Mar 12 08:32:44 2014 -0700

    wake funcs: Add more protection while on a phone call

commit 123774908ba24bc6c7b99967b50cf99457840b58
Author: ktoonsez <ktoonsez@gmail.com>
Date:   Mon Mar 10 22:29:45 2014 -0700

    wake funcs: Added turbo wake up mode, now calling pwrkey directly and modded pwrkey function to do turbo button for any governor
    Conflicts:
    	drivers/input/misc/pmic8xxx-pwrkey.c

commit c10e495141eb7e0f45507a2cf91523f5f036e1eb
Author: ktoonsez <ktoonsez@gmail.com>
Date:   Mon Mar 10 18:22:08 2014 -0700

    wake funcs: Prevent null reference to gdev during charging with device off mode

commit 0ca84738f419ef61210f093f606660d70b9df63d
Author: ktoonsez <ktoonsez@gmail.com>
Date:   Mon Mar 10 17:23:55 2014 -0700

    wake funcs: Make sure main_prox_data is not NULL before checking variables or adding sensors
    
    Conflicts:
    	drivers/input/touchscreen/synaptics_i2c_rmi.c

commit 04fb6d39f5054b3afe71121d3e76a27fc4048429
Author: ktoonsez <ktoonsez@gmail.com>
Date:   Mon Mar 10 07:56:44 2014 -0700

    wake funcs: Improve wake lock monitor code to ensure it holds it for specified time and kick starts it if it has been killed

commit 7490564d05727e080b7d17b6631a82174ea3cb69
Author: ktoonsez <ktoonsez@gmail.com>
Date:   Sun Mar 9 15:04:58 2014 -0700

    wake funcs: Remove isbooted flag

commit 5dd35d901d87de367fe9341c0d9c1705325b8364
Author: ktoonsez <ktoonsez@gmail.com>
Date:   Sun Mar 9 13:19:01 2014 -0700

    wake funcs: Tweak Double tap to have a limit of 2 seconds and block more than one finger.  Also make sure wake lock is deleted if it is being held with LED notif option and then we are now in block LED mode
    
    Conflicts:
    	drivers/leds/leds-an30259a.c

commit 01c94fcd4d02ed70d65f1b37705f882f7992809c
Author: ktoonsez <ktoonsez@gmail.com>
Date:   Fri Mar 7 17:08:30 2014 -0700

    wake funcs: Re-add synaptics_rmi4_reinit_device function but remove duplicate call to the synaptics_rmi4_f51_set_custom_rezero function

commit cc53d237a9c5f07a696b62062827b222c15ffd73
Author: ktoonsez <ktoonsez@gmail.com>
Date:   Fri Mar 7 09:34:49 2014 -0700

    wake functions: Add relay to prox file for prox_max, add wake lock monitor work, and some general clean up

commit bb4630afed7f5c27e2713ea2708a86fe16633bc9
Author: ktoonsez <ktoonsez@gmail.com>
Date:   Thu Mar 6 19:52:02 2014 -0700

    Screen wake: Add wake_options_prox_max to let user configure the prox value to block wake functions

commit 98f8afde12956c0b554a988b4bb8b784ed992761
Author: ktoonsez <ktoonsez@gmail.com>
Date:   Thu Mar 6 16:56:29 2014 -0700

    Make s2w and all the others modify irqs while screen is off when ktweaker sets them on boot or if sysfs are written to while screen is off thru adb

commit 9a2a66b893af3ac40ff3bdb4fe050a0e23e9d2c7
Author: ktoonsez <ktoonsez@gmail.com>
Date:   Thu Mar 6 16:38:25 2014 -0700

    Move all screen on/off stuff from suspend/resume routines and hook it directly to screen on/off from board file
    
    Conflicts:
    	drivers/input/touchscreen/synaptics_i2c_rmi.c

commit ead900959830aa5e7e15cf78b3e4012a9e126df3
Author: ktoonsez <ktoonsez@gmail.com>
Date:   Wed Mar 5 20:24:30 2014 -0700

    Add s2w, d2w, w2w and p2w
    and fix darkness governor
    Conflicts:
    	drivers/input/touchscreen/synaptics_i2c_rmi.c

commit 7c6c3b5bb7289e2713888c2ef57eab0b68c109d5
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Fri May 15 23:46:53 2015 +0200

    cpufreq: update nightmare and darkness governor thanks to @Alucard24 and @Dorimanx

commit 1ade2778a5a44f532e0841529194da4906d7e3ca
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Fri May 15 23:45:10 2015 +0200

    config: small changes

commit abb2ca237052c9b50905d67c61d6f74d42d1bdd3
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu May 14 19:34:08 2015 +0200

    mach-msm: acpuclock: remove OC_ultimate and build it with normal OC

commit 65c67b14e94a93477b025991b70161b75a1f0ee8
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu May 14 19:11:15 2015 +0200

    config: add GPU overclock and UKSM
    fix compiling errors in uksm.c, update to V1.2.3

commit cd48b1435820fced5665673c38db0c011ec31146
Author: Nai Xia <nai.xia@gmail.com>
Date:   Thu Apr 3 13:45:38 2014 +0800

    MM interfaces changes in upstream

commit d62d98688017065d2b6b4aa23844fccd3689ea43
Author: Nai Xia <nai.xia@gmail.com>
Date:   Sat Jul 6 15:15:02 2013 +0800

    Legacy interface random32 to prandom_u32

commit e89b0dc9fee1d20c93bed36c3a88932d2548d288
Author: Nai Xia <nai.xia@gmail.com>
Date:   Wed May 1 22:29:04 2013 +0800

    Adapt to upstream change of ksm_does_need_to_copy()
    
    to ksm_might_need_to_copy()

commit 1406dd085cc77e4e61f219d49912c21bf308d937
Author: Nai Xia <nai.xia@gmail.com>
Date:   Wed May 1 21:47:12 2013 +0800

    Adapt to upstream hlist_for_each_* change

commit cd85841aa0fe98753c74ccc844aaf9cc44b2d255
Author: Nai Xia <nai.xia@gmail.com>
Date:   Sat Mar 16 11:39:02 2013 +0800

    add back uksm_remove_vma() in vma_adjust
    
    This line was dropped due to a conflict in v3.8 upstream
    
    Conflicts:
    	mm/mmap.c

commit 4a2efd35556a090818ece2ecaa4973f446eae3e3
Author: Nai Xia <nai.xia@gmail.com>
Date:   Fri Mar 15 14:54:24 2013 +0800

    anon_vma_unlock to anon_vma_unlock_read in v3.8

commit 64bf4035cbb6f192b5d86b109e9500be394a7b41
Author: Nai Xia <nai.xia@gmail.com>
Date:   Mon Dec 31 15:53:38 2012 +0800

    UKSM 0.1.2.2

commit 73b21a0bb29623f79179c9939cfc9ce2f2e0ced8
Author: Nai Xia <nai.xia@gmail.com>
Date:   Thu Oct 18 19:43:27 2012 +0800

    Don't loop too long for busy mms

commit 300d8aaf5f2ef18adf7f7a384a76119dd28021fd
Author: Nai Xia <nai.xia@gmail.com>
Date:   Thu Oct 18 20:20:42 2012 +0800

    Mute page allocation failure warning message

commit 13b80db9e116f889b89aefd267db6091bcd43341
Author: Nai Xia <nai.xia@gmail.com>
Date:   Mon Dec 31 15:31:41 2012 +0800

    Keep up with changes in 3.7 upstream

commit af3c1db4e2c5a0368be41d7dc4259921dabc7ee2
Author: Nai Xia <nai.xia@gmail.com>
Date:   Sat Oct 13 10:56:14 2012 +0800

    UKSM 0.1.2.1

commit eff9536faa166b421e79a4f18b19ceb5a5b0f671
Author: Nai Xia <nai.xia@gmail.com>
Date:   Sat Oct 13 09:41:19 2012 +0800

    Get around an issue of "task_sched_runtime()" in -ck
    
    Other:
    * refine scan_time_to_sleep()
    * removing many trailing blanks

commit d2371aa3e2d6ade0d2389603ba74d0d0fbaa6666
Author: Nai Xia <nai.xia@gmail.com>
Date:   Wed Oct 10 20:50:14 2012 +0800

    Add some error handling logic(e.g. for OOM)

commit df504b9d5a07151732e04947d3c08412ae87dd42
Author: Nai Xia <nai.xia@gmail.com>
Date:   Tue Oct 9 18:58:08 2012 +0800

    Fix compiling regarding kmap_atomic()

commit e77854b1d7436577577133d925f1cb78e4cd6a3f
Author: Nai Xia <nai.xia@gmail.com>
Date:   Thu Jul 26 13:46:48 2012 +0800

    UKSM 0.1.2

commit afd0889f837e0c082a0cf122cbcab4e22a4e3f4f
Author: Nai Xia <nai.xia@gmail.com>
Date:   Sun Jul 8 09:23:58 2012 +0800

    Use slot->pages instead of vma_pages in uksm_vma_enter()

commit 3217818a6f73394f6027aee6e05a559ba269b14a
Author: Nai Xia <nai.xia@gmail.com>
Date:   Fri Jun 29 13:38:43 2012 +0800

    printk cleanup

commit 17b3176db3251b27813b4375915fa661411c6feb
Author: Nai Xia <nai.xia@gmail.com>
Date:   Fri Jun 29 13:12:59 2012 +0800

    UKSM 0.1.2-beta2

commit 8f6e84cf40f196ae61a5e122fdc4d95839f27a11
Author: Nai Xia <nai.xia@gmail.com>
Date:   Fri Jun 29 00:52:42 2012 +0800

    Possiblely the final commit before beta2

commit fb21b335e2d19afecd13a2d4d38f22fd28d46a29
Author: Nai Xia <nai.xia@gmail.com>
Date:   Thu Jun 28 15:36:47 2012 +0800

    Fully canceled the two tree design

commit 5edeab975f8d29729d8a18c4c88073fdde5c9305
Author: Nai Xia <nai.xia@gmail.com>
Date:   Thu Jun 28 00:13:05 2012 +0800

    revert the two tree design

commit abc68d35982971ee0cf350f5cac03d2e198cf1d5
Author: Nai Xia <nai.xia@gmail.com>
Date:   Wed Jun 27 23:06:00 2012 +0800

    Only use pages_merged as the dedup ratio value.
    
    We are doing even sampling.

commit b15f8a3cea4cfc77c0a9129cc773aa9ca7d83dfc
Author: Nai Xia <nai.xia@gmail.com>
Date:   Wed Jun 27 22:05:07 2012 +0800

    Special consideration for mutual duplication VMA
    
    add vma_slot_dedup list

commit aa6943b66c0a17cc40c5143935faff8655b1a812
Author: Nai Xia <nai.xia@gmail.com>
Date:   Wed Jun 27 18:56:08 2012 +0800

    inc_uksm_pages_scanned() for zero pages

commit fecd531c74ae99d6879bfd9b33487d272fc29624
Author: Nai Xia <nai.xia@gmail.com>
Date:   Wed Jun 27 17:21:45 2012 +0800

    introduce fully_scanned_round

commit 9d55adb36fb20ae29e9cfd1d3db490e2c82d2d9c
Author: Nai Xia <nai.xia@gmail.com>
Date:   Wed Jun 27 16:26:30 2012 +0800

    Do rshash_adjust() only when all slots fully scanned.

commit ca1490591722f8dbf94f314536ace6812193a13b
Author: Nai Xia <nai.xia@gmail.com>
Date:   Mon Jun 25 21:45:23 2012 +0800

    Add back hash_round_finished() test
    
    When the hash strength is changed, the unstable tree lose its meaning,
    we have to reset it.

commit a6ef78b31ca58525565cb794fbb7537ade7b0106
Author: Nai Xia <nai.xia@gmail.com>
Date:   Mon Jun 25 20:10:32 2012 +0800

    clear a root->enter_node bug

commit fd77b874e5ca0f15a9bfd1b476eebdb5156d3d2a
Author: Nai Xia <nai.xia@gmail.com>
Date:   Mon Jun 25 13:31:02 2012 +0800

    make tree node shorter for debug
    
    do uksm_calc_rung_step() after swap two vma trees.

commit a9800724d691b3a00019f6f5bd0701fd3b2e99fa
Author: Nai Xia <nai.xia@gmail.com>
Date:   Mon Jun 25 11:16:09 2012 +0800

    Possible bug fix and a prink
    
    Conflicts:
    	lib/sradix-tree.c

commit 6b29a77ec4d33ce5445bb620f73806672ca03430
Author: Nai Xia <nai.xia@gmail.com>
Date:   Mon Jun 25 02:13:04 2012 +0800

    Fix a speed control bug

commit b3c0076e4d4344640cc26eeb73aa33dd41f9023d
Author: Nai Xia <nai.xia@gmail.com>
Date:   Mon Jun 25 01:19:27 2012 +0800

    Clear a racing bug for exiting mm

commit 5fbf52ea6c1c6f3e2f22c38fd10f3a0273327dcc
Author: Nai Xia <nai.xia@gmail.com>
Date:   Mon Jun 25 00:19:22 2012 +0800

    try to clean a racing bug

commit 4e40f27ee0666c99764f454ef937fb35f7d840b1
Author: Nai Xia <nai.xia@gmail.com>
Date:   Sun Jun 24 23:40:46 2012 +0800

    Using two trees to manage scanned slots
    
    rung_up/down during scanning
    
    almost bug free
    
    Conflicts:
    	lib/sradix-tree.c

commit 376e65386eed70e76faf99085a0251b515b55190
Author: Nai Xia <nai.xia@gmail.com>
Date:   Sat Jun 23 17:12:28 2012 +0800

    Final change before going to beta

commit 5900cfc4124e6151b2d1254c62710f210efbb980
Author: Nai Xia <nai.xia@gmail.com>
Date:   Sat Jun 23 16:44:54 2012 +0800

    make the scan_ladder[0] has 500 msec coverage

commit 0b00b480623af40a3270d3cb2db270099e2dca5f
Author: Nai Xia <nai.xia@gmail.com>
Date:   Sat Jun 23 15:44:04 2012 +0800

    This should be the last commit before going to beta

commit 08920ce32df6df2928d195b073b3d7d33193cb43
Author: Nai Xia <nai.xia@gmail.com>
Date:   Sat Jun 23 13:37:19 2012 +0800

    For full zero pages, do not create rmap_item

commit 836d46fa4eb80f0a69d0c6e0763415ec20e6573e
Author: Nai Xia <nai.xia@gmail.com>
Date:   Thu Jun 21 09:46:54 2012 +0800

    Almost bug free, tested for one night

commit 9550a15b375a4ac276ee0030b7ac886e2ec66a85
Author: Nai Xia <nai.xia@gmail.com>
Date:   Wed Jun 20 22:13:46 2012 +0800

    do list_splice in locks in uksm_enter_all_slots

commit d16d1451e698c95701b284297ea36c3a6573d0b6
Author: Nai Xia <nai.xia@gmail.com>
Date:   Wed Jun 20 21:49:20 2012 +0800

    do not enter slot->vma-anon_vma == NULL

commit 219759d3af66e439a8beab9e29e596f614f288cc
Author: Nai Xia <nai.xia@gmail.com>
Date:   Wed Jun 20 20:15:50 2012 +0800

    No need to do remove_rmap_item_from_tree(rmap_item)
    
    When collision happens in unstable tree merge

commit 74e5eb3f2f8b415d2c144a67a32997a71257ff97
Author: Nai Xia <nai.xia@gmail.com>
Date:   Wed Jun 20 14:51:33 2012 +0800

    Almost ok can dedup now. To do intensive bug test.

commit 369e6f94e728a04cb5971bf52de4d4c51897acc8
Author: Nai Xia <nai.xia@gmail.com>
Date:   Wed Jun 20 00:05:53 2012 +0800

    compiles ok, before doing real test

commit fd88e8edc44bbd2f3f102007e92106fdfaac819b
Author: Nai Xia <nai.xia@gmail.com>
Date:   Tue Jun 19 14:01:45 2012 +0800

    tree debug almost ok, prepare merge into uksm

commit 884b8e838d755aa92415f3dffe926881f7a04bb3
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu May 14 17:18:28 2015 +0200

     Make file change before test
    
    Conflicts: trival fix
    
    	lib/Makefile

commit 4dc97644e264167031bf3f031964015aadce828e
Author: Nai Xia <nai.xia@gmail.com>
Date:   Mon Jun 18 11:59:58 2012 +0800

    compile ok, before test

commit 513ad91ca7a496a5e8575f11255908a655c41cac
Author: Nai Xia <nai.xia@gmail.com>
Date:   Mon Jun 18 11:26:44 2012 +0800

    before doing test
    
    Conflicts:
    
    	mm/uksm.c

commit 2edad6f483caa0121f5c432e7f26a1c26dc6d62d
Author: Nai Xia <nai.xia@gmail.com>
Date:   Sat Jun 16 17:39:45 2012 +0800

    prepare to profile

commit fa49d2a6ee98e46f0fbea64d4dcb60456587a34e
Author: Nai Xia <nai.xia@gmail.com>
Date:   Sat Jun 16 15:25:50 2012 +0800

    Give a 1 second upper bound for uksm_sleep_real

commit 812a9f786b7063438e07b85fc9a2e7cb2d486423
Author: Nai Xia <nai.xia@gmail.com>
Date:   Sat Jun 16 11:19:49 2012 +0800

    fix a tiny bug in scan_time_to_sleep()

commit 84413725ea04656dc207ba6e0c8a5ae7dd475557
Author: Nai Xia <nai.xia@gmail.com>
Date:   Fri Jun 15 21:10:50 2012 +0800

    split out of the usr_spt interfaces.

commit f09b2fc96f171d6f5ac1b92a4a3679e4d1d411d8
Author: Nai Xia <nai.xia@gmail.com>
Date:   Tue Jun 12 20:52:11 2012 +0800

    Add some advanced speed control interfaces

commit 50eaef447ebc57c3d0a1400373fd9fbc7a736152
Author: Nai Xia <nai.xia@gmail.com>
Date:   Mon Jun 11 22:04:33 2012 +0800

    UKSM 0.1.2-alpha2

commit 90eff9864872e0b342f400119bcf71ec0a77cc6f
Author: Nai Xia <nai.xia@gmail.com>
Date:   Mon Jun 11 22:02:10 2012 +0800

    Introduced uksm_abundant_threshold
    
    and add back missing uksm_thrash_threshold code

commit 2bae30ce004b386426feaa7eb492493a6d90648e
Author: Nai Xia <nai.xia@gmail.com>
Date:   Mon Jun 11 19:35:07 2012 +0800

    When tree rmap inserted to stable, no dedup account

commit 0ff5e790ea97074b159f78e357b1a1ef73f74b93
Author: Nai Xia <nai.xia@gmail.com>
Date:   Mon Jun 11 00:28:44 2012 +0800

    debug ok

commit 9e76e8a70d0fdefe885e19486fe6443eb6542a1a
Author: Nai Xia <nai.xia@gmail.com>
Date:   Sun Jun 10 23:43:38 2012 +0800

    simplify of dedup cal

commit 7e2d9b9e6ebaea6db41153a121f8e88223c351f5
Author: Nai Xia <nai.xia@gmail.com>
Date:   Sun Jun 10 18:57:11 2012 +0800

    Before tweaking fully scanned and dedup calculation

commit 10eb093dd14ac50106dd52ac5bbb6157df794d07
Author: Nai Xia <nai.xia@gmail.com>
Date:   Sun Jun 10 14:49:27 2012 +0800

    tweaking main scan loop and mm_sem
    
    before tweaking scan round

commit eeeab458b1b63501fa0bf6fde127670f48c270a8
Author: Nai Xia <nai.xia@gmail.com>
Date:   Sun Jun 10 00:30:40 2012 +0800

    end of the day

commit f283517b05dd2e509ab8ed25d28b1883f6bb60f4
Author: Nai Xia <nai.xia@gmail.com>
Date:   Sat Jun 9 23:59:13 2012 +0800

    almost ok

commit f5a907eff2b83e4b61ce6602045b3db4b8ea7db8
Author: Nai Xia <nai.xia@gmail.com>
Date:   Sat Jun 9 23:23:42 2012 +0800

    tmp commit

commit d28385db18016f1bd357271b42884e06270037c7
Author: Nai Xia <nai.xia@gmail.com>
Date:   Fri Jun 8 00:22:54 2012 +0800

    Fix a bug of data struct crashing

commit fba7263967252fdc6eb48339d1cbc8832b86f8fe
Author: Nai Xia <nai.xia@gmail.com>
Date:   Thu Jun 7 16:37:08 2012 +0800

    UKSM 0.1.2-alpha1

commit 6f32b7b7dec5af53e7e707ffcea772a0c5464ea9
Author: Nai Xia <nai.xia@gmail.com>
Date:   Wed Jun 6 00:23:50 2012 +0800

    Some new designs:
    
    * More precise per-page time estimation. Gather at least 10 virutal page
      scan time for average once a time.
    * Better page scan pattern, for each rung a round is finished in a
      predictable and controlable time. Introduced advance_current_scan()
      and uksm_calc_rung_step()
    * Performance cpu_governor with a group of preset values:
      uksm_cpu_preset.
    * Major scan loop clean up and some other related refactoring.
    * Sometimes printk can hurt time estimation badly.

commit d9527633f3f29fbde8f647a87408385d979a3efa
Author: Baoquan He <baoquan.he@gmail.com>
Date:   Mon May 21 22:02:21 2012 +0800

    UKSM: add parameter max_cpu_percentage
    
    parameter max_cpu_percentage which range from 0 99. Value 100 can
    be set, but will be changed to 99. uksm_scan_batch_pages is
    calculateddynamically if max_cpu_percentage isn't zero.
    
    delete cal_ladder_pages_to_scan  in func scan_batch_pages_store.

commit fcfd07c27d8af2ff4184d81776aecbf79c473eb7
Author: Nai Xia <nai.xia@gmail.com>
Date:   Sun May 13 20:22:43 2012 +0800

    UKSM: add VM_MERGEABLE to vm_flags before vma_merge()

commit b11839d2200ceae2afe06fe342f3000a8007c315
Author: Figo.zhang <figo1802@gmail.com>
Date:   Thu May 10 20:19:19 2012 +0800

    UKSM: update anon_page counter when a anon_page remap to uksm_zero_page

commit cf13ecdfc837d0274e458365b608e04ee112fe34
Author: Nai Xia <nai.xia@gmail.com>
Date:   Tue May 22 13:47:02 2012 +0800

    UKSM: Fix compiling error when porting to v3.4

commit 01a79abe0e5162bb9b24e7cd309a7991c70cdf86
Author: Nai Xia <nai.xia@gmail.com>
Date:   Sat May 5 21:12:43 2012 +0800

    UKSM: Documentation and some comment fix.

commit 9990d0e74de4449c24b067589ee787a4f2c8d979
Author: Nai Xia <nai.xia@gmail.com>
Date:   Thu May 3 15:00:22 2012 +0800

    UKSM: missing *ptep in try_merge_rmap_item()

commit f751142eb534d0677035fad5010d2bc56243836a
Author: Nai Xia <nai.xia@gmail.com>
Date:   Mon Apr 30 14:29:30 2012 +0800

    UKSM: let the default uksm_sleep_jiffies 10 msecs

commit 2e2c4324f11af37a1eb200db77d0dd79595eea4c
Author: Nai Xia <nai.xia@gmail.com>
Date:   Wed Apr 25 10:22:14 2012 +0800

    UKSM: remove inline modifier for uksm_vma_add_new
    
    Which can trigger rare compiling failures.

commit 46ceed8087d4a36b81836090f2e6eb92ad4ef2d3
Author: Nai Xia <nai.xia@gmail.com>
Date:   Wed Apr 25 09:05:21 2012 +0800

    UKSM: usr_spt timer fix, reset when jiffies wraps

commit 523d1ee6f1705ac2c63542a5f326539c6fa53b08
Author: Nai Xia <nai.xia@gmail.com>
Date:   Wed Apr 25 00:00:52 2012 +0800

    UKSM: Fix possible usr_spt timer overflow bug

commit 0e50e9ebd210aee1b3a737472edcd7b67c9100b6
Author: Nai Xia <nai.xia@gmail.com>
Date:   Mon Apr 23 15:44:53 2012 +0800

    UKSM: user support program enhancement
    
    Now users can control usr_spt_msg and usr_spt_flags

commit b71e8ac3eb9bf21ae5bf3d9f432e85a011d8c926
Author: Nai Xia <nai.xia@gmail.com>
Date:   Mon Apr 23 09:38:23 2012 +0800

    UKSM: add user support program and its interface

commit 1169e45a456b6d7c97b6ebe48b7961fdc51ce9fd
Author: Nai Xia <nai.xia@gmail.com>
Date:   Fri Apr 6 20:55:47 2012 +0800

    fix build error: drop_anon_vma() -> put_anon_vma()

commit 2261ec83fdded36145b497564742dca84e0bf688
Author: Nai Xia <nai.xia@gmail.com>
Date:   Thu Apr 5 14:19:43 2012 +0800

    This is a combination of 8 commits.
    
    === The first commit's message is:
    uksm-0.1.1 port step1:
    
    add support for uksm in files other than ksm.c and ksm.h
    
    === This is the 2nd commit message:
    
    uksm-0.1.1 port step2: add uksm.c and uksm.h
    
    === This is the 3rd commit message:
    
    Adding Kbuild support for UKSM, rename KSM to KSM_LEGACY
    
    === This is the 4th commit message:
    
    uksm-0.1.1 port step4: make ksm and uksm coexist
    
    === This is the 5th commit message:
    
    zero page patch bug fix:
    
    1. account zero page number when it get unmapped in several places
    2. in cmp_and_merge_page() update the inter-duplication table to
    make it cooperate with the basic ladder algorithm.
    
    === This is the 6th commit message:
    
    make uksm.h more comfortable with ksm.h
    
    === This is the 7th commit message:
    
    Clear NR_KSM_PAGES_SHARING related code
    
    The couting of NR_KSM_PAGES_SHARING is not perfect, racing make
    it very inaccurate. So remove it.
    
    === This is the 8th commit message:
    
    Massive renaming ksm_* in uksm.c to uksm_*
    
    And also clean up many CONFIG_UKSMs
    
    Conflicts:
    
    	fs/exec.c
    
    Conflicts: introduced in v3.3
    
    	include/linux/mmzone.h
    
    Conflicts: introduced in v3.4, trival noop line
    
    	fs/exec.c

commit dfe5e73ccc129a9fdcc67403a7a8aa977b7ae593
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu May 14 17:05:15 2015 +0200

    Revert "drivers/touchscreen: add sweep2wake"
    
    This reverts commit 3859e09d5a739276e39ad03337ab6aefe2fdb3bd.

commit 6bf41c79c667ef646355308d7cf78552e429c013
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu May 14 17:05:01 2015 +0200

    Revert "sweep2wake: make tunables accessible via sysfs interface"
    
    This reverts commit 1e661e5081b057aebeee5378bc5fda1dbd98687e.

commit a9bd660b79f20dab9f0b4e8c01c60e3bedd1517c
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu May 14 17:04:50 2015 +0200

    Revert "sweep2wake: disable debugging :p"
    
    This reverts commit 79e30a9ec86499972d27005414d0d7e94f9dddb7.

commit cf89c44cf3a899d4329e6672ab8d96d5f97c57c5
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu May 14 17:04:38 2015 +0200

    Revert "touch: sync sweep2wake and dt2wake to @showp1984's latest driver"
    
    This reverts commit 60f40d35815afe840f34393684fb822dcc6e618f.

commit bc602ccbdb9d7f88657001cb4f5026caa55b520b
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu May 14 17:04:26 2015 +0200

    Revert "touchscreen: fix my issue"
    
    This reverts commit ed2f120bfe7fdf751fcd9e224cd799def3429e8f.

commit 12e8c34b16a948ca1f4dbb57f770c542a9181560
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu May 14 17:03:42 2015 +0200

    Revert "Revert "Revert "mm: add and enable UKSM"""
    
    This reverts commit 619240195ebd0703a2814d76f02ed545f2f07e10.

commit 619240195ebd0703a2814d76f02ed545f2f07e10
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu May 14 16:44:25 2015 +0200

    Revert "Revert "mm: add and enable UKSM""
    
    This reverts commit 57362dfcf878eab0ed5f7c86450a22dacc30ef66.

commit 57362dfcf878eab0ed5f7c86450a22dacc30ef66
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu May 14 16:05:56 2015 +0200

    Revert "mm: add and enable UKSM"
    
    This reverts commit 000263bf44ab9f50ac0264512885c566d7dd2d0d.

commit 000263bf44ab9f50ac0264512885c566d7dd2d0d
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu May 14 15:37:41 2015 +0200

    mm: add and enable UKSM

commit 64f8137600203d7e4058a63786f0a631f67528ba
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Tue May 12 11:32:39 2015 +0200

    cpufreq: remove alucard governor, because it needs more stuff to work correctly
    sorry @Alucard_24 to include it without informing you.
    Your work is great, I really appreciate it. Hope you don't mind. Have a nice time and keep on rocking!

commit ed2f120bfe7fdf751fcd9e224cd799def3429e8f
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Tue May 12 11:31:11 2015 +0200

    touchscreen: fix my issue

commit 60f40d35815afe840f34393684fb822dcc6e618f
Author: Paul Reioux <reioux@gmail.com>
Date:   Mon Mar 24 01:39:37 2014 -0700

    touch: sync sweep2wake and dt2wake to @showp1984's latest driver
    
    Thanks to @showp1984 for his original s2w and dt2w contributions!
    
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit 79e30a9ec86499972d27005414d0d7e94f9dddb7
Author: faux123 <reioux@gmail.com>
Date:   Thu Dec 27 19:42:19 2012 -0800

    sweep2wake: disable debugging :p
    
    Signed-off-by: faux123 <reioux@gmail.com>

commit 1e661e5081b057aebeee5378bc5fda1dbd98687e
Author: faux123 <reioux@gmail.com>
Date:   Wed Dec 26 09:34:49 2012 -0800

    sweep2wake: make tunables accessible via sysfs interface
    
    also removed heigh limit check for ON operation
    
    Signed-off-by: faux123 <reioux@gmail.com>

commit 3859e09d5a739276e39ad03337ab6aefe2fdb3bd
Author: Dennis Rassmann <showp1984@gmail.com>
Date:   Wed Jul 24 18:00:44 2013 -0500

    drivers/touchscreen: add sweep2wake
    
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit 1eb242a296a93444f3f70ef7c7077a1d5a1abaf3
Author: Paul Reioux <reioux@gmail.com>
Date:   Wed Jun 19 01:28:22 2013 -0500

    GPU Overclock: Initial Krait 600 ab GPU overclocking
    
    add 360 MHz and 600 MHz options
    make 360 MHz nominal case and 600 MHz turbo case
    
    Signed-off-by: Paul Reioux <reioux@gmail.com>
    
    Conflicts:
    	arch/arm/mach-msm/Kconfig

commit d5ac91028b055560b3282b34d8d5fda1517e7ef3
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed May 6 20:36:20 2015 +0200

    cpufreq: add back defintions for minima

commit 8b5ca7e16cf07b7b71a3e303528d25c2cbba1a1e
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed May 6 17:22:36 2015 +0200

    mach-msm: gpu: go back to STOCK

commit d53ecafce9b7c8abfd70634b18da733a9f465a61
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed May 6 12:02:30 2015 +0200

    mach-msm: gpu: try some other stuff

commit 04c44f6210fa7d8fb9ec2252753c0b7cc018e8db
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed May 6 11:33:15 2015 +0200

    Revert "mach-msm: clock: some gpu optimisations"
    
    This reverts commit bc12bcb977d4f5edeb953511a852a3c916d0914e.
    
    Conflicts:
    	arch/arm/configs/jf_defconfig

commit e51c7b414bd9b5d43e3768cb84bef2c1199b5cde
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed May 6 08:52:36 2015 +0200

    cpufreq: update alucard, darkness and nightmare

commit bc12bcb977d4f5edeb953511a852a3c916d0914e
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Mon May 4 21:31:25 2015 +0200

    mach-msm: clock: some gpu optimisations

commit 991dada4d230b9204398b1b9a95b8237a3c96d0f
Author: Matt Wagantall <mattw@codeaurora.org>
Date:   Wed Apr 10 19:37:58 2013 -0700

    msm: clock: Update memory retention clock flags
    
    Introduce CLKFAG_{NO}RETAIN_PERIPH, which will eventually be used on
    8974 to control collapse of the core memory periphery. To avoid
    confusion with CLKFAG_{NO}RETAIN, rename those existing flag to
    CLKFAG_{NO}RETAIN_MEM, to better indicate that they control retention
    of the core's memory.
    
    Change-Id: I078bcc7026095ab4b94dc57362220a5b05621bec
    Signed-off-by: Matt Wagantall <mattw@codeaurora.org>

commit 68a3901405bd43656d6c2cd87ccc73ddd67a988c
Author: Matt Wagantall <mattw@codeaurora.org>
Date:   Mon Apr 1 15:22:47 2013 -0700

    msm: footswitch-8x60: Allow specification of per-fs reset delays
    
    Custom reset rates can be specified for clocks via fs_clk_data. If
    these rates are very low, then the default reset delay of 1us may
    not be sufficient. Address this by allowing custom reset delays to
    be specified on a per-footswitch bases.
    
    Change-Id: Id9b9377d79436a7fe5007a8f37216a456d66ce1a
    Signed-off-by: Matt Wagantall <mattw@codeaurora.org>

commit 156cac85f64bb96f2e395368e00820f99fdeabae
Author: Harsh Vardhan Dwivedi <hdwivedi@codeaurora.org>
Date:   Sun Oct 28 20:54:17 2012 -0600

    msm: kgsl: Add separate GPU shader memory mapping
    
    Add separate kernel memory mapping for GPU shader memory.
    Previously, both the register and the shader memory were mapped
    as one entity, with the mapping length being equal to combined
    size of register memory and shader memory. Now, we separate the
    shader memory mapping to help in cases of GPU devices where the
    shader and register memory may not be adjacent. This helps to
    dump the shader memory in the postmortem snapshot. By having a
    separate mapping for shader memory, the snapshot dump routine
    simply reads in the shader memory range specified and dumps.
    
    Change-Id: I30e59dffe849fefcfbbaf0e5245cc1024ccedcb1
    Signed-off-by: Harsh Vardhan Dwivedi <hdwivedi@codeaurora.org>
    
    Conflicts:
    	drivers/gpu/msm/adreno_a3xx_snapshot.c

commit 9ed57881e5251c7f624d8912e58448e2ac495452
Author: Kevin Matlage <kmatlage@codeaurora.org>
Date:   Tue Oct 23 17:10:56 2012 -0600

    msm: kgsl: Return 8064v1.1 chip ID for v1.2
    
    Return the 8064v1.1 chip ID for v1.2 because there
    appear to be no significant HW changes between the
    revisions. This allows for the right SW workarounds
    to be applied and fixes a major perf regression.
    
    Note that v1.1 and v1.2 are currently considered the
    exact same when looking at the bootloader chip ID, so
    we have no way of knowing which it truly is.
    
    Change-Id: Ia94b6b8af4d8378ee32fdde5f714b0cfd8e74f89
    Signed-off-by: Kevin Matlage <kmatlage@codeaurora.org>

commit 9ba33dbe1bf987ec3350d5c331fe3e23d4dcecd4
Author: Matt Wagantall <mattw@codeaurora.org>
Date:   Mon Apr 1 14:57:45 2013 -0700

    msm: clock-8960: Add 1.8MHz rate to 8064's gfx3d_clk plan
    
    This rate will be used by the 'footswitch-8x60' driver when
    controlling GFX3D power.
    
    Change-Id: Ib796114dc90b468f1c0aa014a9db699f3b854adf
    Signed-off-by: Matt Wagantall <mattw@codeaurora.org>

commit fe233eca585ac0793b81bb8a4ddc42e300eb429c
Author: Matt Wagantall <mattw@codeaurora.org>
Date:   Mon Apr 1 16:18:37 2013 -0700

    msm: footswitch-8x60: Update 8064 footswitch_enable() sequence for GFX3D
    
    Update 8064's GFX3D enable sequence per hardware designer recommendations
    to improve system stability when responding to the inrush current event.
    
    Differences from the standard footswitch_enable() sequence include:
     - Decreasing the reset_rate from 27MHz to 1.8MHz, and increasing
       reset_delay_us from 1us to 10us to compensate for the slower clock.
     - Enable clocks for reset propagation only after the core has been
       powered on.
     - Forcefully limit the AFAB and EBI1 clocks to a low value (decided
       by the RPM) by means of a write to the RPM_CTL RPM resource.
     - Removal of the reset toggle after powering on the core, which (while
       harmless) is not applicable to 8064.
    
    Change-Id: I943842b56ff96b2e6077a419566d91ac184a6fda
    Signed-off-by: Matt Wagantall <mattw@codeaurora.org>

commit a52eb5021319c8b8213a82239f483d907f658e4a
Author: Saravana Kannan <skannan@codeaurora.org>
Date:   Tue Jul 2 10:31:18 2013 -0700

    msm: clock: Fix list rate to handle rate > LONG_MAX
    
    While at it, also convert the list_rate return type from int to long to be
    consistent with the rest of the ops.
    
    Change-Id: Ib5bc230d82472d17ddcc99956cb6f10ff21df5ff
    Signed-off-by: Saravana Kannan <skannan@codeaurora.org>

commit 7dd0ca15a877964d67c18261e3630aacd9a03f39
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Mon May 4 17:21:08 2015 +0200

    GPU: delete unused stuff

commit cecaf5a75dad3aac6bf7d99eccc3251db1423f2c
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Mon May 4 17:13:02 2015 +0200

    Revert "drivers: gpu: revert trustzone back to stock"
    
    This reverts commit 905a47c388e3f6eac3705bd7ed1105eb8e1e3fba.

commit 5e0e41cc6f60ab86db4ff7f1141f1bd3861d2039
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Mon May 4 12:06:22 2015 +0200

    cpufreq: add alucard gov and update darkness+nightmare governor

commit 905a47c388e3f6eac3705bd7ed1105eb8e1e3fba
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Mon May 4 00:22:44 2015 +0200

    drivers: gpu: revert trustzone back to stock

commit 34123dacb3478a0f0016631088752222cfcd75c6
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun May 3 23:55:21 2015 +0200

    release V1.3

commit 73d613f20325a4518e1c3d72bb9157197aa38efc
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun May 3 23:37:10 2015 +0200

    gpu: set correctly FLOOR in trustzone

commit 28b4575a1b5b18e2660f9810f9462ec254aef86f
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun May 3 21:52:36 2015 +0200

    Correctly set all UC stuff

commit e5175e2c01bf975c72fc1d0ba18f9bab7825b116
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun May 3 19:36:01 2015 +0200

    Revert " cpufreq: interactive: Use correct kobj when creating sysfs"
    
    This reverts commit 1f816e068b062c5893c02dd4bf6b90647d51d549.

commit 1f816e068b062c5893c02dd4bf6b90647d51d549
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun May 3 18:18:14 2015 +0200

     cpufreq: interactive: Use correct kobj when creating sysfs
    
    cpufreq_global_kobject is no longer initialized during cpufreq_core_init.
    Fix sysfs creation by properly requesting the kobject based on
    have_governor_per_policy().
    
    Change-Id: I5f9cff68043dad8822952bd43227d948a934a1c7
    Signed-off-by: Junjie Wu <junjiew@codeaurora.org>

commit 0f70319a093d04ebb78ac992c8789530d6a71f9c
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun May 3 18:08:06 2015 +0200

     input: touchboost: conservative samples every 10ms, no point in limit
    
    ing input_time to update its value in 50ms intervals
    
    Signed-off-by: franciscofranco <franciscofranco.1990@gmail.com>

commit d2a5f01f9d23b491a93900dafa0df1404832eb69
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun May 3 18:07:32 2015 +0200

     input: add generic touch event listener
    
    Signed-off-by: franciscofranco <franciscofranco.1990@gmail.com>

commit c6ae86b101dbb8788ee010199451250e16e3e02e
Author: Ananta Kishore K <akollipa@codeaurora.org>
Date:   Fri Oct 24 16:02:15 2014 +0530

    msm: kgsl: Wake up GPU only for touch events
    
    Register the KGSL input event handler only for
    touch input events to make sure input event handler
    is called only for touch input events.
    
    Change-Id: I68938fbbf870bd3bce83cb057b504674753d6de8
    Signed-off-by: Ananta Kishore K <akollipa@codeaurora.org>
    Signed-off-by: franciscofranco <franciscofranco.1990@gmail.com>

commit 22fd530a7ec933027f78419f65908d89998f3002
Author: myfluxi <linflux@arcor.de>
Date:   Tue May 20 23:37:58 2014 +0200

    msm: kgsl: Schedule adreno_start in a high priority workqueue
    
    Better safe than sorry. Also, return early if we didn't set a _wake_timeout.
    
    Signed-off-by: franciscofranco <franciscofranco.1990@gmail.com>

commit efa2248cdead198ec983717970c4164197c49378
Author: myfluxi <linflux@arcor.de>
Date:   Tue May 20 22:56:51 2014 +0200

    msm: kgsl: Fix nice level for higher priority GPU start thread
    
    Before, we were actually setting the lowest possible priority.
    
    Signed-off-by: franciscofranco <franciscofranco.1990@gmail.com>
    Signed-off-by: Tkkg1994 <luca.grifo@outlook.com>

commit ea05ac4cf08fdbbe125c5ef19b5ac31e6985e488
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun May 3 17:46:08 2015 +0200

     msm: kgsl: Allow GPU start to run in a high priority queue
    
    The GPU start sequence needs to run with low latency when coming
    out of slumber for the first draw command. For those cases allow the
    start function to be queued into a high priority workqueue to reduce
    the chance of it getting scheduled out.
    
    Change-Id: Ic0dedbada7ef4d63b54e826862fbefdfb71a092a
    Signed-off-by: Jordan Crouse <jcrouse@codeaurora.org>
    Signed-off-by: franciscofranco <franciscofranco.1990@gmail.com>
    
    Signed-off-by: Tkkg1994 <luca.grifo@outlook.com>

commit 1fce3ea7e3bef335566332ef432033f4499a0e5a
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun May 3 17:29:56 2015 +0200

     msm: kgsl: Add an input handler to power up the GPU on a touch event
    
    On a device where the primary input is a touchscreen nobody is
    surprised that touch events are often the catalyst for drawing
    frames. If the GPU is in slumber we can get a jump on the action
    by watching for touch events and turning the GPU on in anticipation
    of a draw call. Register for events from all input drivers
    returning EV_ABS events and queue a work task to power up the GPU
    if it is currently in slumber.
    
    Change-Id: Ic0dedbadbdc96ef9ddd78252ea65f8e1cdd00110
    Signed-off-by: Jordan Crouse <jcrouse@codeaurora.org>
    Signed-off-by: franciscofranco <franciscofranco.1990@gmail.com>

commit 570093f37a0d212b41401fb615976b68bc9da88b
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun May 3 01:06:44 2015 +0200

     ipv4: Missing sk_nulls_node_init() in ping_unhash().
    
    If we don't do that, then the poison value is left in the ->pprev
    backlink.
    
    This can cause crashes if we do a disconnect, followed by a connect().
    
    Change-Id: I021261c6639339a0d027ee5cb086ba7301db334a
    Tested-by: Linus Torvalds <torvalds@linux-foundation.org>
    Reported-by: Wen Xu <hotdog3645@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 2779db92ee2398ee913e3230dd4be7839c6ba792
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun May 3 01:05:18 2015 +0200

     msm_fb: Suppress a log message.
    
    It's way too intrusive when switching the display on.
    
    Change-Id: Ie65179696ddb4fb238f03b659c73b06a772d2dfc

commit ab56ca1f6c5e3914eda33e15ad46735d70f5b058
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun May 3 01:04:11 2015 +0200

     writeback: fix a subtle race condition in I_DIRTY clearing
    
    commit 9c6ac78eb3521c5937b2dd8a7d1b300f41092f45 upstream.
    
    After invoking ->dirty_inode(), __mark_inode_dirty() does smp_mb() and
    tests inode->i_state locklessly to see whether it already has all the
    necessary I_DIRTY bits set.  The comment above the barrier doesn't
    contain any useful information - memory barriers can't ensure "changes
    are seen by all cpus" by itself.
    
    And it sure enough was broken.  Please consider the following
    scenario.
    
     CPU 0					CPU 1
     -------------------------------------------------------------------------------
    
    					enters __writeback_single_inode()
    					grabs inode->i_lock
    					tests PAGECACHE_TAG_DIRTY which is clear
     enters __set_page_dirty()
     grabs mapping->tree_lock
     sets PAGECACHE_TAG_DIRTY
     releases mapping->tree_lock
     leaves __set_page_dirty()
    
     enters __mark_inode_dirty()
     smp_mb()
     sees I_DIRTY_PAGES set
     leaves __mark_inode_dirty()
    					clears I_DIRTY_PAGES
    					releases inode->i_lock
    
    Now @inode has dirty pages w/ I_DIRTY_PAGES clear.  This doesn't seem
    to lead to an immediately critical problem because requeue_inode()
    later checks PAGECACHE_TAG_DIRTY instead of I_DIRTY_PAGES when
    deciding whether the inode needs to be requeued for IO and there are
    enough unintentional memory barriers inbetween, so while the inode
    ends up with inconsistent I_DIRTY_PAGES flag, it doesn't fall off the
    IO list.
    
    The lack of explicit barrier may also theoretically affect the other
    I_DIRTY bits which deal with metadata dirtiness.  There is no
    guarantee that a strong enough barrier exists between
    I_DIRTY_[DATA]SYNC clearing and write_inode() writing out the dirtied
    inode.  Filesystem inode writeout path likely has enough stuff which
    can behave as full barrier but it's theoretically possible that the
    writeout may not see all the updates from ->dirty_inode().
    
    Fix it by adding an explicit smp_mb() after I_DIRTY clearing.  Note
    that I_DIRTY_PAGES needs a special treatment as it always needs to be
    cleared to be interlocked with the lockless test on
    __mark_inode_dirty() side.  It's cleared unconditionally and
    reinstated after smp_mb() if the mapping still has dirty pages.
    
    Also add comments explaining how and why the barriers are paired.
    
    Lightly tested.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Jan Kara <jack@suse.cz>
    Cc: Mikulas Patocka <mpatocka@redhat.com>
    Cc: Jens Axboe <axboe@kernel.dk>
    Cc: Al Viro <viro@zeniv.linux.org.uk>
    Reviewed-by: Jan Kara <jack@suse.cz>
    Signed-off-by: Jens Axboe <axboe@fb.com>
    Signed-off-by: Zefan Li <lizefan@huawei.com>

commit f46ba0438a5a8a50ebdb9ce93d7df94b2e01c184
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun May 3 01:02:00 2015 +0200

     kernel: Restrict permissions of /proc/iomem.
    
    The permissions of /proc/iomem currently are -r--r--r--. Everyone can
    see its content. As iomem contains information about the physical memory
    content of the device, restrict the information only to root.
    
    Change-Id: If0be35c3fac5274151bea87b738a48e6ec0ae891
    CRs-Fixed: 786116
    Signed-off-by: Biswajit Paul <biswajitpaul@codeaurora.org>
    Signed-off-by: Avijit Kanti Das <avijitnsec@codeaurora.org>

commit 2a305e15dcec6e2db23f9aff7a6ae5ba9651b616
Author: Stephen Boyd <sboyd@codeaurora.org>
Date:   Tue Apr 9 10:57:54 2013 -0700

    msm: scm: Add scm_call_atomic3
    
    Some drivers need to pass three arguments atomically. Add support
    for it.
    
    Change-Id: Id0f962400c5c4f721eef44069fc3db1779248ae3
    Signed-off-by: Stephen Boyd <sboyd@codeaurora.org>

commit 8af8cce13fe1f8a062775e13b04ceec3d13d7ae4
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sat May 2 18:10:32 2015 +0200

    Revert " msm: kgsl: Check constraint state coming out of slumber"
    
    This reverts commit 74bccc79187ac0a98d90d22c65c692372f6a45a9.

commit 026e5b5f75d55e3971af14e0c33147d30bfaa071
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Fri May 1 20:09:33 2015 +0200

     msm: kgsl: support busy stats based policy in tz
    
    Gpu busy percentage based Dcvs policy requires more
    arguments to the secure tz call. A new function is
    required to support it.
    
    Change-Id: I434a9bbc68d112378d10eb2795cef9ee65c3e23e
    Signed-off-by: Suman Tatiraju <sumant@codeaurora.org>

commit afc53c09192a35579ab78f53d206d0c6b1abfe76
Author: Lucille Sylvester <lsylvest@codeaurora.org>
Date:   Fri Jan 18 17:09:02 2013 -0700

    msm: kgsl: Add intermediate power levels
    
    Make use of the two independent power domains to
    add intermediate levels.  Attempt to just increase
    the bus frequency before also raising the GPU
    frequency.
    
    Change-Id: Ie2109df001f96e36e03959288aa957d63c0ef4a7
    Signed-off-by: Lucille Sylvester <lsylvest@codeaurora.org>

commit 38f02ce93bda23d5190516fa734ac984de58fa75
Author: Suman Tatiraju <sumant@codeaurora.org>
Date:   Mon Apr 1 09:04:58 2013 -0700

    msm: kgsl: set active pwrlevel to init pwrlevel after power collapse
    
    Between kgsl_open and kgsl_release calls active pwrlevel could be
    set to anything based on the content. However after a power collapse
    always power up the GPU and set the clock to init level.
    
    Change-Id: Ic2858218afa9c1047864ab8551b3495b7d752952
    Signed-off-by: Suman Tatiraju <sumant@codeaurora.org>

commit ae97526bc13953514ca8708f13cc77b418eae76c
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Fri May 1 19:13:04 2015 +0200

     msm: kgsl: Remove the skip counting code from the trustzone policy
    
    No need to turn the algorithm off now that there is explict binning.
    It's not a performance win and it confuses whether DCVS is opperational.
    
    Change-Id: Ifa1646e8dde0a792c3d22d5e1cf9e139b0363f08
    Signed-off-by: Lucille Sylvester <lsylvest@codeaurora.org>

commit cfd461a316b2f778e73146941a90adff0d09eefc
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Fri May 1 19:11:06 2015 +0200

     msm: kgsl: Bump the frequency all the way to turbo for long ibs.
    
    Process the enormous chunks of data faster.
    
    Change-Id: Idf3fd0bfddc1cd1ac00f87dd72e403b81d70b170
    Signed-off-by: Lucille Sylvester <lsylvest@codeaurora.org>
    Signed-off-by: Carter Cooper <ccooper@codeaurora.org>

commit 74bccc79187ac0a98d90d22c65c692372f6a45a9
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Fri May 1 19:07:18 2015 +0200

     msm: kgsl: Check constraint state coming out of slumber
    
    Only set the default constraint when we come out of slumber if there
    is no current constraint set. The current behavior will always override
    the constraint that was set when coming out of slumber.
    
    Signed-off-by: Carter Cooper <ccooper@codeaurora.org>
    Change-Id: I58a5e2338bbee64e885edf697e83869820be2c22

commit 8c8495e8655398c4ee7972f7d7abe43a197f41fe
Author: Iliyan Malchev <malchev@google.com>
Date:   Wed Feb 4 19:35:25 2015 -0800

    kgsl: switch back to allocating from highmem
    
    Commit 344c3acdd8e436ccf7c4726422e2af85f5fb5a0a switched kgsl to allocating
    from highmem in page-sized chunks.  While this reduced pressure on highmem, it
    may have increased pressure on lowmem to the point where lowmemorykiller
    activity due to GFP_KERNEL requests rose.
    
    b/19236185 Phone runs for about day, then apps start getting constantly killed
    	   by lowmemorykiller
    
    Change-Id: I92e82cc668cd10d8845d12e7c1bad6d980bc3956
    Signed-off-by: Iliyan Malchev <malchev@google.com>

commit 5cba5604bf0e9bdf755ccb2fb1a8582714bfa1c6
Author: Iliyan Malchev <malchev@google.com>
Date:   Tue Jan 6 22:01:11 2015 -0800

    kgsl: allocate from lowmem
    
    Also, simplify _kgsl_sharedmem_page_alloc now that we only allocate GFP_KERNEL
    in PAGE_SIZE chunks.
    
    This reduces the non-movable highmem order-0 allocations from 15% to less than
    2% of the total.  This this, 98% of highmem is now movable, hence compactable.
    This in turn leaves us room to later enable compaction outside of the
    direct-reclaim path, which should improve responsiveness on highly-fragmented
    devices further.
    
    b/18069309 N5 running L can't run as many apps simultaneously as running K
    
    Change-Id: I810c0100e30156bb2877d6f6b8cecf244f733ed9
    Signed-off-by: Iliyan Malchev <malchev@google.com>

commit 542f0e53d61b1f5e8e449f04f6bc280f92c01897
Author: Iliyan Malchev <malchev@google.com>
Date:   Sat Dec 20 15:16:09 2014 -0800

    kgsl: allocate shared memory one page at a time
    
    This helps greatly alleviate pressure on the page allocator on devices running
    for longer periods of time, with highly-fragmented memory.  Of all allocations
    of order > 0, KGSL allocations consistently represent 70%, with another 20%
    coming from order-1 allocations due to calls to fork() and clone().
    
    b/18069309 N5 running L can't run as many apps simultaneously as running K
    
    Change-Id: I66ad445aa851ff80707c6807b3f546485ae7bb2c
    Signed-off-by: Iliyan Malchev <malchev@google.com>

commit 255210b097f5346a7a1493706aa4e11a814b4433
Author: Iliyan Malchev <malchev@google.com>
Date:   Wed Nov 19 21:16:50 2014 -0800

    kgsl: do not vmap/memset to zero-out pages
    
    b/18402205 External reports: Video playback failing on Flo after upgrade to
    	   Lollipop
    
    Change-Id: I358328ba2bd543d77e4218f32b0695c2f6f6e6c9
    Signed-off-by: Iliyan Malchev <malchev@google.com>

commit 7c018451993f3db2c1bfa5d274b06027bd51762e
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Fri May 1 18:39:57 2015 +0200

     msm: kgsl: Mark mmapped objects with VM_DONTCOPY
    
    Add VM_DONTCOPY to the default set of mmap flags to keep VM objects
    from being copied on fork() and causing issues.  KGSL file descriptors
    copied to a child are not expected to be usable.
    
    Change-Id: Ic0dedbad85c07118a931ccb9f7a6fd0507da3e5a
    Signed-off-by: Jordan Crouse <jcrouse@codeaurora.org>

commit 33c7f84ff752e8aaa0d63aa10fe2c7330783400e
Author: Jordan Crouse <jcrouse@codeaurora.org>
Date:   Tue May 6 09:36:40 2014 -0600

    msm: kgsl: Don't set VM_IO on mmap()ed GPU memory objects
    
    VM_IO prevents mapped memory from being peeked by ptrace(). That
    kind of protection isn't really needed for nominal GPU buffers.
    A process given itself up to ptrace() already expects to be
    examined so there is no additional risk to let the parent examine
    GPU buffers too.  This is done universally now, but there is no
    reason why we wouldn't let the process choose which buffers to
    keep private in the future.
    
    That said; there is more of a concern about including GPU buffers
    in a core dump since that is a more permanent and less secure
    record of the memory so add VM_DONTDUMP for all GPU buffers to
    protect against that.
    
    CRs-Fixed: 654751
    Change-Id: Ic0dedbade91a2ec458bcb27eff3312d4ec6e4389
    Signed-off-by: Jordan Crouse <jcrouse@codeaurora.org>

commit 7bc6233fb90cf763a1a4e331f3a71eba8fddac7b
Author: Jordan Crouse <jcrouse@codeaurora.org>
Date:   Mon Mar 24 16:03:59 2014 -0600

    msm: kgsl: Use a constant mask for the memops vmflags
    
    We don't need to define a function to return a constant.  Save
    ourselves some source code and some .text space too.
    
    Change-Id: Ic0dedbadc72c2fdd473cd4369ed772c84a923a15
    Signed-off-by: Jordan Crouse <jcrouse@codeaurora.org>

commit 894608b4ddaa73df2ce60de4830af3483b930038
Merge: 74e0edf ece60db
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Fri May 1 18:11:29 2015 +0200

    Merge branch 'master' of https://github.com/Tkkg1994/Hulk-Kernel-V2

commit ece60db179ba57faf40e3feac6900cbf035c7aaa
Merge: 60a1362 e7e21d6
Author: walter79 <gzwalter79@gmail.com>
Date:   Fri May 1 17:25:57 2015 +0200

    Merge pull request #2 from walter79/master
    
    jactive: update jactive_eur_defconfig

commit e7e21d601dbae99cbd5825f4741544fa1328b8a8
Author: walter79 <gzwalter79@gmail.com>
Date:   Fri May 1 17:24:58 2015 +0200

    jactive: update jactive_eur_defconfig

commit 60a1362c4ed946eb31f24220300d9c2c1060cf5f
Merge: 9386acd e93c775
Author: walter79 <gzwalter79@gmail.com>
Date:   Fri May 1 13:46:09 2015 +0200

    Merge pull request #1 from walter79/master
    
    Add support for GT-I9295 Samsung S4 Active

commit e93c7750241add782473e32a606063b69b84c43a
Author: walter79 <gzwalter79@gmail.com>
Date:   Fri May 1 13:38:00 2015 +0200

    Add support for GT-I9295 Samsung S4 Active

commit 74e0edfcdc32ccf3d69a341a1e9227cab889deda
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Apr 30 23:53:47 2015 +0200

    cpufreq: remove lagfree governor because of incompatible stuff

commit ede5c823efd967b5387d1df422d4714a3eb8a120
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Apr 30 22:46:05 2015 +0200

    config: fix **** smartass governor :D

commit 66e6880c72ae3514e617113503f4cc8c14d784d6
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Apr 30 14:20:36 2015 +0200

    cpufreq: add smartass2

commit 5a8c71e484f004ec44449308f6b5f0ad4632dea6
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Apr 30 14:15:14 2015 +0200

    cpufreq: add lagfree governor

commit 9386acd1516cd1a2230a671bda242d25f4a34637
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Apr 29 19:25:19 2015 +0200

     jf: set proper compass calibration point for vzw model
    
    Change-Id: I4634d9cdb436efca15a3698402a87a8d2995551e

commit 0eb986b0689d76b710d6e353bde114268a4dcfca
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Apr 29 11:09:07 2015 +0200

     block: add REQ_URGENT to request flags
    
    This patch adds a new flag to be used in cmd_flags field of struct request
    for marking request as urgent.
    Urgent request is the one that should be given priority currently handled
    (regular) request by the device driver. The decision of a request urgency
    is taken by the scheduler.
    
    Change-Id: Ic20470987ef23410f1d0324f96f00578f7df8717
    Signed-off-by: Tatyana Brokhman <tlinder@codeaurora.org>

commit d1efb317ef494efe8fdb626ca1b2106bb609112c
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Apr 29 10:11:17 2015 +0200

    config: enable row scheduler

commit b76a08ea6b53c336e5686b82bd7e9bd663de84b9
Author: Tatyana Brokhman <tlinder@codeaurora.org>
Date:   Tue Dec 4 16:04:15 2012 +0200

    block: Add API for urgent request handling
    
    This patch add support in block & elevator layers for handling
    urgent requests. The decision if a request is urgent or not is taken
    by the scheduler. Urgent request notification is passed to the underlying
    block device driver (eMMC for example). Block device driver may decide to
    interrupt the currently running low priority request to serve the new
    urgent request. By doing so READ latency is greatly reduced in read&write
    collision scenarios.
    
    Note that if the current scheduler doesn't implement the urgent request
    mechanism, this code path is never activated.
    
    Change-Id: I8aa74b9b45c0d3a2221bd4e82ea76eb4103e7cfa
    Signed-off-by: Tatyana Brokhman <tlinder@codeaurora.org>

commit 4cfb4b1dd36582cb217983b183161fa44e4dbb05
Author: Tatyana Brokhman <tlinder@codeaurora.org>
Date:   Tue Dec 4 15:54:43 2012 +0200

    block: Add support for reinsert a dispatched req
    
    Add support for reinserting a dispatched request back to the
    scheduler's internal data structures.
    This capability is used by the device driver when it chooses to
    interrupt the current request transmission and execute another (more
    urgent) pending request. For example: interrupting long write in order
    to handle pending read. The device driver re-inserts the
    remaining write request back to the scheduler, to be rescheduled
    for transmission later on.
    
    Add API for verifying whether the current scheduler
    supports reinserting requests mechanism. If reinsert mechanism isn't
    supported by the scheduler, this code path will never be activated.
    
    Change-Id: I5c982a66b651ebf544aae60063ac8a340d79e67f
    Signed-off-by: Tatyana Brokhman <tlinder@codeaurora.org>

commit 899c3970b232a827a9d7f0b8a8b76c3db74288a6
Author: Tanya Brokhman <tlinder@codeaurora.org>
Date:   Sun Jun 22 16:11:03 2014 +0300

    block: row: Fix crash when adding a new field in bio struct
    
    When adding new field to struct bio there is a crash in the removed
    code lines. This issue was introduced by commit
    80a8f0f87bee18283e9ca0a8966ec97ad9f084e5 "block: row-iosched idling
    triggered by readahead pages"
    
    (Partly) reverting this patch till root cause is fixed (on FS level).
    
    Change-Id: Ie82bc806ea52a6370b57aa15455c85b2db10d0da
    Signed-off-by: Tanya Brokhman <tlinder@codeaurora.org>

commit 9a8b9e3612245fa54bd4c95c6ee45a64c73639f1
Author: Tatyana Brokhman <tlinder@codeaurora.org>
Date:   Thu Jul 25 13:45:21 2013 +0300

    block: row: Remove warning massage from add_request
    
    Regular priority queues is marked as "starved" if it skipped a dispatch
    due to being empty. When a new request is added to a "starved" queue
    it will be marked as urgent.
    The removed WARN_ON was warning about an impossible case when a regular
    priority (read) queue was marked as starved but wasn't empty. This is
    a possible case due to the bellow:
    If the device driver fetched a read request that is pending for
    transmission and an URGENT request arrives, the fetched read will be
    reinserted back to the scheduler. Its possible that the queue it will be
    reinserted to was marked as "starved" in the meanwhile due to being empty.
    
    CRs-fixed: 517800
    Change-Id: Iaae642ea0ed9c817c41745b0e8ae2217cc684f0c
    Signed-off-by: Tatyana Brokhman <tlinder@codeaurora.org>

commit 3324691ae36ab0a97ade525f91b364d8d33d3369
Author: Tatyana Brokhman <tlinder@codeaurora.org>
Date:   Tue Jul 2 14:43:13 2013 +0300

    block: row: change hrtimer_cancel to hrtimer_try_to_cancel
    
    Calling hrtimer_cancel with interrupts disabled can result in a livelock.
    When flushing plug list in the block layer interrupts are disabled and an
    hrtimer is used when adding requests from that plug list to the scheduler.
    In this code flow, if the hrtimer (which is used for idling) is set, it's
    being canceled by calling hrtimer_cancel. hrtimer_cancel will perform
    the following in an endless loop:
    1. try cancel the timer
    2. if fails - rest_cpu
    the cancellation can fail if the timer function already started. Since
    interrupts are disabled it can never complete.
    This patch reduced the number of times the hrtimer lock is taken while
    interrupts are disabled by calling hrtimer_try_co_cancel. the later will
    try to cancel the timer just once and return with an error code if fails.
    
    CRs-fixed: 499887
    Change-Id: I25f79c357426d72ad67c261ce7cb503ae97dc7b9
    Signed-off-by: Tatyana Brokhman <tlinder@codeaurora.org>

commit 23849ffc7597d1d630c6913e08debedd1cf9cc34
Author: Lee Susman <lsusman@codeaurora.org>
Date:   Sun Jun 23 16:27:40 2013 +0300

    block: row-iosched idling triggered by readahead pages
    
    In the current implementation idling is triggered only by request
    insertion frequency. This heuristic is not very accurate and may hit
    random requests that shouldn't trigger idling. This patch uses the
    PG_readahead flag in struct page's flags, which indicates that the page
    is part of a readahead window, to start idling upon dispatch of a request
    associated with a readahead page.
    
    The above readehead flag is used together with the existing
    insertion-frequency trigger. The frequency timer will catch read requests
    which are not part of a readahead window, but are still part of a
    sequential stream (and therefore dispatched in small time intervals).
    
    Change-Id: Icb7145199c007408de3f267645ccb842e051fd00
    Signed-off-by: Lee Susman <lsusman@codeaurora.org>

commit e542a8f84d704a13c815ef99abf355cbc4761b3a
Author: Maya Erez <merez@codeaurora.org>
Date:   Sun Apr 14 15:19:52 2013 +0300

    block: row: Fix starvation tolerance values
    
    The current starvation tolerance values increase the boot time
    since high priority SW requests are delayed by regular priority requests.
    In order to overcome this, increase the starvation tolerance values.
    
    Change-Id: I9947fca9927cbd39a1d41d4bd87069df679d3103
    Signed-off-by: Tatyana Brokhman <tlinder@codeaurora.org>
    Signed-off-by: Maya Erez <merez@codeaurora.org>

commit 0e90bb74e513310545a586c0574451cc71c5bf51
Author: Tatyana Brokhman <tlinder@codeaurora.org>
Date:   Thu Mar 21 11:04:02 2013 +0200

    block: row: Update sysfs functions
    
    All ROW (time related) configurable parameters are stored in ms so there
    is no need to convert from/to ms when reading/updating them via sysfs.
    
    Change-Id: Ib6a1de54140b5d25696743da944c076dd6fc02ae
    Signed-off-by: Tatyana Brokhman <tlinder@codeaurora.org>

commit 3f4c1c09070e0bff55ab4776660288af82a44e3e
Author: Tatyana Brokhman <tlinder@codeaurora.org>
Date:   Thu Mar 21 13:02:07 2013 +0200

    block: row: Prevent starvation of regular priority by high priority
    
    At the moment all REGULAR and LOW priority requests are starved as long as
    there are HIGH priority requests to dispatch.
    This patch prevents the above starvation by setting a starvation limit the
    REGULAR\LOW priority requests can tolerate.
    
    Change-Id: Ibe24207982c2c55d75c0b0230f67e013d1106017
    Signed-off-by: Tatyana Brokhman <tlinder@codeaurora.org>

commit b805355f7c367855964019b8452a9499f6bf9b02
Author: Tatyana Brokhman <tlinder@codeaurora.org>
Date:   Tue Mar 12 21:17:18 2013 +0200

    block: row: Re-design urgent request notification mechanism
    
    When ROW scheduler reports to the block layer that there is an urgent
    request pending, the device driver may decide to stop the transmission
    of the current request in order to handle the urgent one. This is done
    in order to reduce the latency of an urgent request. For example:
    long WRITE may be stopped to handle an urgent READ.
    
    This patch updates the ROW URGENT notification policy to apply with the
    below:
    
    - Don't notify URGENT if there is an un-completed URGENT request in driver
    - After notifying that URGENT request is present, the next request
      dispatched is the URGENT one.
    - At every given moment only 1 request can be marked as URGENT.
      Independent of it's location (driver or scheduler)
    
    Other changes to URGENT policy:
    - Only READ queues are allowed to notify of an URGENT request pending.
    
    CR fix:
    If a pending urgent request (A) gets merged with another request (B)
    A is removed from scheduler queue but is not removed from
    rd->pending_urgent_rq.
    
    CRs-Fixed: 453712
    Change-Id: I321e8cf58e12a05b82edd2a03f52fcce7bc9a900
    Signed-off-by: Tatyana Brokhman <tlinder@codeaurora.org>

commit a09a92be8fa58ff4194cdd691e754207fe5d8339
Author: Shashank Babu Chinta Venkata <sbchin@codeaurora.org>
Date:   Tue Feb 26 17:33:55 2013 -0800

    Revert "block: row: Re-design urgent request notification mechanism"
    
    The revert fixes frequent boot up failures on 8974.
    
    This reverts commit 0c3b048d1fae87db150e9ff729a9608e5346e042.
    
    Change-Id: I181513382a128724ce08980ad2f14cd5943c27bd
    Signed-off-by: Shashank Babu Chinta Venkata <sbchin@codeaurora.org>

commit ef2f7a2130ce748503ee7c40b6235faa552f212d
Author: Tatyana Brokhman <tlinder@codeaurora.org>
Date:   Mon Feb 18 11:38:58 2013 +0200

    block: row: Re-design urgent request notification mechanism
    
    This patch updates the ROW URGENT notification policy to apply with the
    bellow:
    - Don't notify URGENT if there is an un-completed URGENT request in driver
    - After notifying that URGENT request is present, the next request
      dispatched is the URGENT one.
    - At every given moment only 1 request can be marked as URGENT.
      Independent of it's location (driver or scheduler)
    
    Other changes to URGENT policy:
    - Only queues that are allowed to notify of an URGENT request pending
    are READ queues
    
    Change-Id: I17ff73125bc7a760cea1116b466115a2df635a14
    Signed-off-by: Tatyana Brokhman <tlinder@codeaurora.org>

commit a2b2781555dd6042a8e509b0400e3bcf4bc06216
Author: Tatyana Brokhman <tlinder@codeaurora.org>
Date:   Thu Jan 24 16:17:27 2013 +0200

    block: row: Update initial values of ROW data structures
    
    This patch sets the initial values of internal ROW
    parameters.
    
    Change-Id: I38132062a7fcbe2e58b9cc757e55caac64d013dc
    Signed-off-by: Tatyana Brokhman <tlinder@codeaurora.org>
    [smuckle@codeaurora.org: ported from msm-3.7]
    Signed-off-by: Steve Muckle <smuckle@codeaurora.org>

commit df21ec640c36356f586299a860149e6121a730a6
Author: Tatyana Brokhman <tlinder@codeaurora.org>
Date:   Thu Jan 24 15:08:40 2013 +0200

    block: row: Don't notify URGENT if there are un-completed urgent req
    
    When ROW scheduler reports to the block layer that there is an urgent
    request pending, the device driver may decide to stop the transmission
    of the current request in order to handle the urgent one. If the current
    transmitted request is an urgent request - we don't want it to be
    stopped.
    Due to the above ROW scheduler won't notify of an urgent request if
    there are urgent requests in flight.
    
    Change-Id: I2fa186d911b908ec7611682b378b9cdc48637ac7
    Signed-off-by: Tatyana Brokhman <tlinder@codeaurora.org>

commit 0788f2386ab097102738da4d322e8d70a8b618e6
Author: Tatyana Brokhman <tlinder@codeaurora.org>
Date:   Thu Jan 17 20:56:07 2013 +0200

    block: row: Idling mechanism re-factoring
    
    At the moment idling in ROW is implemented by delayed work that uses
    jiffies granularity which is not very accurate. This patch replaces
    current idling mechanism implementation with hrtime API, which gives
    nanosecond resolution (instead of jiffies).
    
    Change-Id: I86c7b1776d035e1d81571894b300228c8b8f2d92
    Signed-off-by: Tatyana Brokhman <tlinder@codeaurora.org>

commit 2e9d891037297521a9406fc28b54d3214587f48d
Author: Tatyana Brokhman <tlinder@codeaurora.org>
Date:   Wed Jan 23 17:15:49 2013 +0200

    block: row: Dispatch requests according to their io-priority
    
    This patch implements "application-hints" which is a way the issuing
    application can notify the scheduler on the priority of its request.
    This is done by setting the io-priority of the request.
    This patch reuses an already existing mechanism of io-priorities developed
    for CFQ. Please refer to kernel/Documentation/block/ioprio.txt for
    usage example and explanations.
    
    Change-Id: I228ec8e52161b424242bb7bb133418dc8b73925a
    Signed-off-by: Tatyana Brokhman <tlinder@codeaurora.org>

commit 0989800cbb09de13efb88f760119aaf2a7c0666e
Author: Tatyana Brokhman <tlinder@codeaurora.org>
Date:   Sat Jan 12 16:23:18 2013 +0200

    block: row: Aggregate row_queue parameters to one structure
    
    Each ROW queues has several parameters which default values are defined
    in separate arrays. This patch aggregates all default values into one
    array.
    The values in question are:
     - is idling enabled for the queue
     - queue quantum
     - can the queue notify on urgent request
    
    Change-Id: I3821b0a042542295069b340406a16b1000873ec6
    Signed-off-by: Tatyana Brokhman <tlinder@codeaurora.org>

commit 62099bb7866d2ab5654680c6dcea18b5bcfa397e
Author: Tatyana Brokhman <tlinder@codeaurora.org>
Date:   Sat Jan 12 16:21:47 2013 +0200

    block: row: fix sysfs functions - idle_time conversion
    
    idle_time was updated to be stored in msec instead of jiffies.
    So there is no need to convert the value when reading from user or
    displaying the value to him.
    
    Change-Id: I58e074b204e90a90536d32199ac668112966e9cf
    Signed-off-by: Tatyana Brokhman <tlinder@codeaurora.org>

commit ca992a22fdfee369785d722c18f91676ddefbccd
Author: Tatyana Brokhman <tlinder@codeaurora.org>
Date:   Sat Jan 12 16:21:12 2013 +0200

    block: row: Insert dispatch_quantum into struct row_queue
    
    There is really no point in keeping the dispatch quantum
    of a queue outside of it. By inserting it to the row_queue
    structure we spare extra level in accessing it.
    
    Change-Id: Ic77571818b643e71f9aafbb2ca93d0a92158b199
    Signed-off-by: Tatyana Brokhman <tlinder@codeaurora.org>

commit 71a9ced8e3c2a3a6e69527f57763792af3148dc3
Author: Tatyana Brokhman <tlinder@codeaurora.org>
Date:   Sun Jan 13 22:04:59 2013 +0200

    block: row: Add some debug information on ROW queues
    
    1. Add a counter for number of requests on queue.
    2. Add function to print queues status (number requests
       currently on queue and number of already dispatched requests
       in current dispatch cycle).
    
    Change-Id: I1e98b9ca33853e6e6a8ddc53240f6cd6981e6024
    Signed-off-by: Tatyana Brokhman <tlinder@codeaurora.org>

commit 65a1b681b89028c4197b2c80b38f1f93914b9066
Author: Tatyana Brokhman <tlinder@codeaurora.org>
Date:   Thu Dec 20 19:23:58 2012 +0200

    row: Add support for urgent request handling
    
    This patch adds support for handling urgent requests.
    ROW queue can be marked as "urgent" so if it was un-served in last
    dispatch cycle and a request was added to it - it will trigger
    issuing an urgent-request-notification to the block device driver.
    The block device driver may choose at stop the transmission of current
    ongoing request to handle the urgent one. Foe example: long WRITE may
    be stopped to handle an urgent READ. This decreases READ latency.
    
    Change-Id: I84954c13f5e3b1b5caeadc9fe1f9aa21208cb35e
    Signed-off-by: Tatyana Brokhman <tlinder@codeaurora.org>

commit 7028bb02d17b03bf3a81ab93ce0193dc69a3dade
Author: Tatyana Brokhman <tlinder@codeaurora.org>
Date:   Thu Dec 6 13:17:19 2012 +0200

    block:row: fix idling mechanism in ROW
    
    This patch addresses the following issues found in the ROW idling
    mechanism:
    1. Fix the delay passed to queue_delayed_work (pass actual delay
       and not the time when to start the work)
    2. Change the idle time and the idling-trigger frequency to be
       HZ dependent (instead of using msec_to_jiffies())
    3. Destroy idle_workqueue() in queue_exit
    
    Change-Id: If86513ad6b4be44fb7a860f29bd2127197d8d5bf
    Signed-off-by: Tatyana Brokhman <tlinder@codeaurora.org>

commit dc78f100d76639374135afa72789243731ea8b07
Author: Tatyana Brokhman <tlinder@codeaurora.org>
Date:   Tue Oct 30 08:33:06 2012 +0200

    row: Adding support for reinsert already dispatched req
    
    Add support for reinserting already dispatched request back to the
    schedulers internal data structures.
    The request will be reinserted back to the queue (head) it was
    dispatched from as if it was never dispatched.
    
    Change-Id: I70954df300774409c25b5821465fb3aa33d8feb5
    Signed-off-by: Tatyana Brokhman <tlinder@codeaurora.org>

commit 754d7136afeb43de24526199fe8792b767b504c4
Author: Tatyana Brokhman <tlinder@codeaurora.org>
Date:   Mon Oct 15 20:56:02 2012 +0200

    block: ROW: Fix forced dispatch
    
    This patch fixes forced dispatch in the ROW scheduling algorithm.
    When the dispatch function is called with the forced flag on, we
    can't delay the dispatch of the requests that are in scheduler queues.
    Thus, when dispatch is called with forced turned on, we need to cancel
    idling, or not to idle at all.
    
    Change-Id: I3aa0da33ad7b59c0731c696f1392b48525b52ddc
    Signed-off-by: Tatyana Brokhman <tlinder@codeaurora.org>

commit 761aac13d2cd0d42ab50d20f35523849f8b3709e
Author: Tatyana Brokhman <tlinder@codeaurora.org>
Date:   Mon Oct 15 20:50:54 2012 +0200

    block: ROW: Correct minimum values of ROW tunable parameters
    
    The ROW scheduling algorithm exposes several tunable parameters.
    This patch updates the minimum allowed values for those parameters.
    
    Change-Id: I5ec19d54b694e2e83ad5376bd99cc91f084967f5
    Signed-off-by: Tatyana Brokhman <tlinder@codeaurora.org>

commit 08e9170fe9b2aac9a4c03a3fceef976cf2fe2975
Author: Tatyana Brokhman <tlinder@codeaurora.org>
Date:   Thu Sep 20 10:46:10 2012 +0300

    block: Adding ROW scheduling algorithm
    
    This patch adds the implementation of a new scheduling algorithm - ROW.
    The policy of this algorithm is to prioritize READ requests over WRITE
    as much as possible without starving the WRITE requests.
    
    Change-Id: I4ed52ea21d43b0e7c0769b2599779a3d3869c519
    Signed-off-by: Tatyana Brokhman <tlinder@codeaurora.org>

commit 49e23d0749f4b3228663e95df961d299ff0459dc
Author: c_sridur <sridur@codeaurora.org>
Date:   Fri Jan 23 14:49:59 2015 +0530

    msm_vidc: Frame height is not updating for low resolution clip
    
    Fix: Update frame height preoperly for height lesser than 64
    
    Change-Id: I17cf166a40f77fcea74e7d0c19af801e6e6244d5
    Signed-off-by: c_sridur <sridur@codeaurora.org>

commit bbe43a5e15cf529c30aba4958ba53a3317c9f436
Author: Rudolf Tammekivi <rtammekivi@gmail.com>
Date:   Fri Mar 13 20:43:02 2015 +0200

    msm_fb: Enable MDP4 IGC LUT

commit 27de4a5ec18fdb2ecd9c88764cedb6da905ad8a7
Author: Rudolf Tammekivi <rtammekivi@gmail.com>
Date:   Fri Mar 13 20:41:12 2015 +0200

    msm_fb: Fix GC & IGC LUT
    
    Use linear look up tables by default for mixer & video graphics pipe.
    As a plus, do not export these variables and mark them as constant.
    
    After this change, MDP4 IGC can be enabled again.
    
    Solves issues menitoned here:
    https://www.codeaurora.org/cgit/quic/la/kernel/msm/commit/drivers/video/msm/mdp4_overlay.c?h=eclair_chocolate&id=149ad44c9ceaf6c010e44d866ebae43179c8aa16

commit 73d57458e70dadf70005aa43036385da5340d1a2
Author: Ravi Aravamudhan <aravamud@codeaurora.org>
Date:   Wed Feb 11 17:21:11 2015 -0800

    diag: Make fixes to diag_switch_logging
    
    Diag driver holds on to the socket process task structure even
    after signaling the process to exit. This patch clears the internal
    handle after signaling.
    
    Change-Id: I642fb595fc2caebc6f2f5419efed4fb560e4e4db
    Signed-off-by: Ravi Aravamudhan <aravamud@codeaurora.org>

commit 87b0c134fa91de99b05d70d4085f9e375d58eee7
Author: raghavendra ambadas <rambad@codeaurora.org>
Date:   Mon Mar 16 18:10:35 2015 +0530

    msm_fb: display: validate input args of mdp4_argc_process_write_req
    
    A bounds check has to be done for r/g/b stages variables
    to avoid undetermined behaviour.
    
    Change-Id: Ibdc96e79b36cf188d4b5c42d8e2d9ece8e9ace8a
    Signed-off-by: Raghavendra Ambadas <rambad@codeaurora.org>

commit a03c966173143f8763d4ea69ef092e25088adcfd
Author: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
Date:   Fri Dec 20 15:10:03 2013 +0200

    mm: Fix NULL pointer dereference in madvise(MADV_WILLNEED) support
    
    Sasha Levin found a NULL pointer dereference that is due to a missing
    page table lock, which in turn is due to the pmd entry in question being
    a transparent huge-table entry.
    
    The code - introduced in commit 1998cc048901 ("mm: make
    madvise(MADV_WILLNEED) support swap file prefetch") - correctly checks
    for this situation using pmd_none_or_trans_huge_or_clear_bad(), but it
    turns out that that function doesn't work correctly.
    
    pmd_none_or_trans_huge_or_clear_bad() expected that pmd_bad() would
    trigger if the transparent hugepage bit was set, but it doesn't do that
    if pmd_numa() is also set. Note that the NUMA bit only gets set on real
    NUMA machines, so people trying to reproduce this on most normal
    development systems would never actually trigger this.
    
    Fix it by removing the very subtle (and subtly incorrect) expectation,
    and instead just checking pmd_trans_huge() explicitly.
    
    Reported-by: Sasha Levin <sasha.levin@oracle.com>
    Acked-by: Andrea Arcangeli <aarcange@redhat.com>
    [ Additionally remove the now stale test for pmd_trans_huge() inside the
      pmd_bad() case - Linus ]
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    
    Change-Id: I3f3763f236ef102de735297cd175cf514d40d28f

commit c8d2ecf05ced364cd8a8c99ec276d2c6204a958f
Author: D.S. Ljungmark <ljungmark@modio.se>
Date:   Wed Mar 25 09:28:15 2015 +0100

    ipv6: Don't reduce hop limit for an interface
    
    A local route may have a lower hop_limit set than global routes do.
    
    RFC 3756, Section 4.2.7, "Parameter Spoofing"
    
    >   1.  The attacker includes a Current Hop Limit of one or another small
    >       number which the attacker knows will cause legitimate packets to
    >       be dropped before they reach their destination.
    
    >   As an example, one possible approach to mitigate this threat is to
    >   ignore very small hop limits.  The nodes could implement a
    >   configurable minimum hop limit, and ignore attempts to set it below
    >   said limit.
    
    Change-Id: I51ee1778e3d2d5fa1aefbdf1ad8869e4e8dc28b2
    Signed-off-by: D.S. Ljungmark <ljungmark@modio.se>
    Acked-by: Hannes Frederic Sowa <hannes@stressinduktion.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit b5ff6b1583ebda9aa1d5a02510a08106a977ced6
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Tue Apr 28 22:40:42 2015 +0200

    gpu: ion: delete unused file

commit 3bd50b8b368db25f782b2f9584c892561823bfa9
Author: Srinivasarao P <spathi@codeaurora.org>
Date:   Fri Jan 9 17:07:08 2015 +0530

    gpu: ion: add locking to traversal of volatile rb tree
    
    In ion_debug_heap_show we're iterating over an rb tree (dev->clients)
    that could change while we're iterating. Fix this by taking the lock
    that is used to control access to this tree.
    
    Change-Id: I6832e1e98e2d2a69fc653451d3752d43ec3ef269
    Signed-off-by: Mitchel Humpherys <mitchelh@codeaurora.org>
    Signed-off-by: Srinivasarao P <spathi@codeaurora.org>

commit 5f340e444c1152b51c0945c1c83f19def9bca6ac
Author: Mitchel Humpherys <mitchelh@codeaurora.org>
Date:   Mon Nov 4 15:23:05 2013 -0800

    gpu: ion: fix locking issues in debug code
    
    There are a few places in Ion where we are iterating over volatile rb
    trees without proper locking. In some places the proper locking cannot
    be added since it would require us to take locks in a different order
    than they are taken in other places in Ion. Fix this by re-working some
    of the debug code so that we can take locks in an allowed order.
    
    One side-effect of the re-work is that the memory maps will now show
    every client that has a handle to a particular region of memory, rather
    than just showing the first one that we encounter. This will allow for
    more accurate accounting and will give better insight as to who is
    actually using the memory.
    
    Change-Id: Ia43e4dbc412cd480c828173f8c20b5095d87d858
    Signed-off-by: Mitchel Humpherys <mitchelh@codeaurora.org>
    Signed-off-by: Srinivasarao P <spathi@codeaurora.org>

commit 9006d3281e054c518ebab99cdd746f3541d147df
Author: Srinivasarao P <spathi@codeaurora.org>
Date:   Fri Jan 9 17:35:45 2015 +0530

    ion: cma: Add debug heap ops for CMA heap
    
    For tracking CMA allocation by address and fragmentation
    (if any), add debug heap ops which gives buffer allocation
    info.
    
    Change-Id: Ia8bed38034b85b2d4dcf84811a348bbbe50dc16b
    Signed-off-by: Chintan Pandya <cpandya@codeaurora.org>
    Signed-off-by: Srinivasarao P <spathi@codeaurora.org>

commit 84ffcc9b1d3b3b3a14e1fb1b86b3dcb211c5b4db
Author: Srinivasarao P <spathi@codeaurora.org>
Date:   Fri Jan 9 16:29:21 2015 +0530

    gpu: ion: use a list instead of a tree for heap debug memory map
    
    Currently we use an rb tree to store information about the memory map
    which gets passed to the heap print_debug functions. The reason for
    using a tree instead of a simple list is to maintain sortedness as we
    build the memory map. However, it can be necessary to store multiple
    entries for the same address in the memory map since there can be
    multiple clients with handles to the same buffer. This information is
    interesting and useful but we currently can't store and display it since
    the rb tree requires that the key used for sorting (the physical address
    in this case) be unique. Fix this by replacing the rb tree with a linked
    list. In order to maintain sorted output of the print_debug functions,
    sort the list by physical address after fully building the list.
    
    This also has the positive side-effect of simplifying the code and
    making future print_debug methods less error-prone.
    
    Change-Id: I5b129fd809fb53c66042eab10d096238a34c2b20
    Signed-off-by: Mitchel Humpherys <mitchelh@codeaurora.org>
    Signed-off-by: Srinivasarao P <spathi@codeaurora.org>

commit ee14e1c62370084e057374260603d7f52e868fda
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Tue Apr 28 20:36:12 2015 +0200

     gpu: ion: Refactor locking
    
    Removes contention for lock between allocate and free by reducing
    the length of time the lock is held for. Split out a seperate
    lock to protect the list of heaps and replace it with a rwsem since
    the list will most likely only be updated during initialization.
    
    Change-Id: Id10464dfe0d60cdcd64f29edfc94317d8e5ee251
    Signed-off-by: Rebecca Schultz Zavin <rebecca@android.com>
    Git-commit: 675a52aa0d89e8b6c0c05849627381d8a64b2b2b
    Git-repo: https://android.googlesource.com/kernel/common
    [lauraa@codeaurora.org: Context differences due to debugfs
    differences. Need to adjust locking on MSM specific extensions]
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>
    Signed-off-by: Mitchel Humpherys <mitchelh@codeaurora.org>
    Signed-off-by: Srinivasarao P <spathi@codeaurora.org>

commit b574d7db08a0dce4117c9a4b04778623cea1966a
Author: Shubhraprakash Das <sadas@codeaurora.org>
Date:   Tue Apr 23 21:35:00 2013 -0500

    msm: kgsl: Fix the offset of FSYNR1
    
    Define the offset of FSYNR1 IOMMU register correctly for IOMMU-v0.
    
    Change-Id: Ibcbc863e1be02da3be22dd1a9cfe4a41e043d503
    Signed-off-by: Shubhraprakash Das <sadas@codeaurora.org>

commit 069938605b94331d45b6a73bf08a3b71b6d97fd1
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sat Apr 25 19:19:47 2015 +0200

    mfd: sound remove fauxsound 3

commit 834daaec05274283b82c459793b05118ae7100ae
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Apr 23 19:34:49 2015 +0200

    config: release V1.2! and fix sec_gafx

commit 7f036b5391b7f6045823fa35ca785c6a6605a3d2
Author: Antti P Miettinen <amiettinen@nvidia.com>
Date:   Tue Dec 27 12:09:56 2011 +0200

    cpufreq: Export user_policy min/max
    
    Add sysfs nodes for user_policy min and max settings.
    
    Bug 888312
    
    Signed-off-by: Antti P Miettinen <amiettinen@nvidia.com>
    Signed-off-by: Varun Wadekar <vwadekar@nvidia.com>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 75032c45e6574be0c3a871b745e540f97ef9e653
Author: Pranav Vashi <neobuddy89@gmail.com>
Date:   Sun Mar 29 13:33:01 2015 +0530

    cpufreq: Prevent mpdecision changing scaling freq
    
    * We don't require mpdecision to change min-max freq
      since CPU boost takes care of it.
    
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 0ce13205bf6e419d8a2a961686936373529b0f0b
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Apr 23 15:36:47 2015 +0200

    mfd: remove ifdef 0

commit 9c4452b3f5439eea2302f4b9e11f53bc1882eb0c
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Apr 23 15:35:29 2015 +0200

    Revert "Disable sched debug"
    
    This reverts commit 0a021d7ca4c21b4a2ec79c04744724ab97882ff6.

commit 93e1482052ea9f46fb9c15ccc259f5534110201e
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Apr 22 20:37:59 2015 +0200

    badass: remove cpu idle

commit c0af61ccd39c998dfbdaf708160dc699cc07431c
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Apr 22 19:49:12 2015 +0200

    cpufreq: fix my mistakes with darkness and badass governor

commit e53cfcbbb3bc8874a15a345144b3b2e539cd0578
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Apr 22 15:04:17 2015 +0200

    block: add Tripndroid iosched

commit d47cbf93f867ea71294f5f181c57067fd73b0870
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Apr 22 12:12:40 2015 +0200

    config: enable NTFS support

commit 84070ed169beaf4f1649c796656767ab782434e3
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Apr 22 11:52:02 2015 +0200

    arch/arm: moved intelli_plug to mach-msm and change wake_boost to wake_intelli_boost
    cpufreq: removed more get_cpu_time etc. from all governors
    cpufreq: added pegasusq governor
    cpufreq: updated darnkess and nighmare governor

commit 83bf1f52712db3e19afd6b5bf0f352f2ea520781
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Apr 22 10:40:36 2015 +0200

    Remove unused msm_hotplug

commit 1cddb4272f6e1abd3993b546dbbfaf4e895754f8
Author: Viresh Kumar <viresh.kumar@linaro.org>
Date:   Fri May 17 11:26:32 2013 +0000

    cpufreq: Move get_cpu_idle_time() to cpufreq.c
    
    Governors other than ondemand and conservative can also use
    get_cpu_idle_time() and they aren't required to compile
    cpufreq_governor.c. So, move these independent routines to
    cpufreq.c instead.
    
    Signed-off-by: Viresh Kumar <viresh.kumar@linaro.org>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    [junjiew@codeaurora.org: update non-upstream files]
    Signed-off-by: Junjie Wu <junjiew@codeaurora.org>
    [neo: Update local drivers as well.]
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 3f659c13764f971f16d7eab512058212deb79b17
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Apr 22 10:32:31 2015 +0200

    cpufreq: added back cpu_idle_time

commit dcbd66707ae39836cc783e8e28b43c7c9297144b
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Apr 22 09:59:55 2015 +0200

    cpufreq: add smartmax and impulse governor

commit f5fb4d169f56ee88413e40c82cd5b05396f83343
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Apr 22 08:58:38 2015 +0200

    some fixes

commit 2034203700c739210113ec3a77c94dac02660fa1
Author: Colin Cross <ccross@android.com>
Date:   Wed Jun 26 17:26:01 2013 -0700

    mm: add a field to store names for private anonymous memory
    
    Userspace processes often have multiple allocators that each do
    anonymous mmaps to get memory.  When examining memory usage of
    individual processes or systems as a whole, it is useful to be
    able to break down the various heaps that were allocated by
    each layer and examine their size, RSS, and physical memory
    usage.
    
    This patch adds a user pointer to the shared union in
    vm_area_struct that points to a null terminated string inside
    the user process containing a name for the vma.  vmas that
    point to the same address will be merged, but vmas that
    point to equivalent strings at different addresses will
    not be merged.
    
    Userspace can set the name for a region of memory by calling
    prctl(PR_SET_VMA, PR_SET_VMA_ANON_NAME, start, len, (unsigned long)name);
    Setting the name to NULL clears it.
    
    The names of named anonymous vmas are shown in /proc/pid/maps
    as [anon:<name>] and in /proc/pid/smaps in a new "Name" field
    that is only present for named vmas.  If the userspace pointer
    is no longer valid all or part of the name will be replaced
    with "<fault>".
    
    The idea to store a userspace pointer to reduce the complexity
    within mm (at the expense of the complexity of reading
    /proc/pid/mem) came from Dave Hansen.  This results in no
    runtime overhead in the mm subsystem other than comparing
    the anon_name pointers when considering vma merging.  The pointer
    is stored in a union with fieds that are only used on file-backed
    mappings, so it does not increase memory usage.
    
    Change-Id: Ie2ffc0967d4ffe7ee4c70781313c7b00cf7e3092
    Signed-off-by: Colin Cross <ccross@android.com>
    
    Conflicts:
    	fs/proc/task_mmu.c
    	include/linux/prctl.h
    	kernel/sys.c

commit f3ae1dc024d734fb2d8e948e69a3db2e249dd441
Author: Steve Muckle <smuckle@codeaurora.org>
Date:   Mon Jul 14 03:19:29 2014 -0700

    sched: fix race between try_to_wake_up() and move_task()
    
    Until a task's state has been seen as interruptible/uninterruptible
    and it is no longer on_cpu, it is possible that the task may move
    to another CPU (load balancing may cause this). Here is an example
    where the race condition results in incorrect operation:
    
    - cpu 0 calls put_prev_task on task A, task A's state is TASK_RUNNING
    - cpu 0 runs task B, which attempts to wake up A
    - cpu 0 begins try_to_wake_up(), recording src_cpu for task A as cpu 0
    - cpu 1 then pulls task A (perhaps due to idle balance)
    - cpu 1 runs task A, which then sleeps, becoming INTERRUPTIBLE
    - cpu 0 continues in try_to_wake_up(), thinking task A's previous
      cpu is 0, where it is actually 1
    - if select_task_rq returns cpu 0, task A will be woken up on cpu 0
      without properly updating its cpu to 0 in set_task_cpu()
    
    Signed-off-by: Steve Muckle <smuckle@codeaurora.org>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit e98c66723354cebeeef73a100d825b5369dbdb0f
Author: Srivatsa Vaddagiri <vatsa@codeaurora.org>
Date:   Tue Nov 25 12:13:58 2014 +0530

    sched: Fix reference to stale task_struct in try_to_wake_up()
    
    try_to_wake_up() currently drops p->pi_lock and later checks for need
    to notify cpufreq governor on task migrations or wakeups. However the
    woken task could exit between the time p->pi_lock is released and the
    time the test for notification is run. As a result, the test for
    notification could refer to an exited task. task_notify_on_migrate(p)
    could thus lead to invalid memory reference.
    
    Fix this by running the test for notification with task's pi_lock
    held.
    
    Signed-off-by: Srivatsa Vaddagiri <vatsa@codeaurora.org>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 4a7888b4d03c3f9f1722aac214f9413099915cd4
Author: Mike Galbraith <bitbucket@online.de>
Date:   Mon Jan 28 12:19:25 2013 +0100

    sched: Fix select_idle_sibling() bouncing cow syndrome
    
    If the previous CPU is cache affine and idle, select it.
    
    The current implementation simply traverses the sd_llc domain,
    taking the first idle CPU encountered, which walks buddy pairs
    hand in hand over the package, inflicting excruciating pain.
    
    1 tbench pair (worst case) in a 10 core + SMT package:
    
      pre   15.22 MB/sec 1 procs
      post 252.01 MB/sec 1 procs
    
    Signed-off-by: Mike Galbraith <bitbucket@online.de>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/1359371965.5783.127.camel@marge.simpson.net
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 1ec47b3543a4112d12179ef09b0ba7fcb8530881
Author: Sai Gurrappadi <sgurrappadi@nvidia.com>
Date:   Tue Jan 21 16:41:37 2014 -0800

    sched: Force sleep on consecutive sched_yields
    
    If a task sched_yields to itself continuously, force the task to
    sleep in sched_yield. This will lower the CPU load of this task
    thereby lowering the cpu frequency and improving power.
    
    Added a stat variable to track how many times we sleep due these
    consecutive sched_yields. Also added sysctl knobs to control the number
    of consecutive sched_yields before which the sleep kicks in and the
    duration fo the sleep in us.
    
    Bug 1424617
    
    Signed-off-by: Sai Gurrappadi <sgurrappadi@nvidia.com>
    Reviewed-by: Wen Yi <wyi@nvidia.com>
    Reviewed-by: Peter Zu <pzu@nvidia.com>
    Reviewed-by: Diwakar Tundlam <dtundlam@nvidia.com>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>
    
    Conflicts:
    	kernel/sched/core.c

commit 765f549aa5c52613b31877b94985d96f838cd8ce
Author: Viresh Kumar <viresh.kumar@linaro.org>
Date:   Tue Jun 11 16:32:44 2013 +0530

    sched: Fix memory leakage in build_sched_groups()
    
    In build_sched_groups() we don't need to call get_group() for cpus
    which are already covered in previous iterations. Calling get_group()
    would mark the group used and eventually leak it since we wouldn't
    connect it and not find it again to free it.
    
    This will happen only in cases where sg->cpumask contained more than
    one cpu (For any topology level). This patch would free sg's memory
    for all cpus leaving the group leader as the group isn't marked used
    now.
    
    Signed-off-by: Viresh Kumar <viresh.kumar@linaro.org>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit dedba002e69c04a4ae7243d31043bd71cc96bc3d
Author: Srivatsa Vaddagiri <vatsa@codeaurora.org>
Date:   Fri May 16 16:15:50 2014 -0700

    sched: Skip load update for idle task
    
    Load statistics for idle tasks is not useful in any manner. Skip load
    update for such idle tasks.
    
    Signed-off-by: Srivatsa Vaddagiri <vatsa@codeaurora.org>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit b7ddfdefb1d29c77436fe0a21c96fc4e037495c0
Author: Pranav Vashi <neobuddy89@gmail.com>
Date:   Thu Apr 2 03:11:35 2015 -0700

    cpufreq: cpu-boost: Upgrade to enhanced version
    
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit c3d222cad45184a7d27abd97274cc7217c01c6e0
Author: Girish S Ghongdemath <girishsg@codeaurora.org>
Date:   Thu Apr 3 12:54:37 2014 -0700

    cpufreq: cpu-boost: Consider only task load to decide on sync frequency
    
    Currently we take the maximum between the source CPU frequency and
    the calculated frequency based on the migrating task load to decide on
    the frequency to sync the destination CPU to. This was done to handle
    short bursts in workloads of tasks which migrated immediately after
    causing a ramp up on the source CPU. Since their load history wasn't
    high enough, the destination CPU synced to a lower frequency which
    wasn't sufficient for the spike in workload.
    
    But as such cases are rare, taking the higher of source and calculated
    frequency can lead to destination CPU unnecessarily spending a
    considerable amount of time at higher frequencies which in turn can
    hurt power.
    
    With this change we make sure only the migrating task load is used
    to calculate the sync frequency for destination CPU when load based
    syncing is enabled.
    
    Signed-off-by: Girish Ghongdemath <girishsg@codeaurora.org>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit d06f8090ae85c77d0a4334ce9712e139b3430907
Author: Rohit Gupta <rohgup@codeaurora.org>
Date:   Thu Mar 13 20:57:32 2014 -0700

    cpufreq: cpu-boost: Handle wakeup hints received for foreground tasks
    
    A previous change modifies the notification conditions in the scheduler
    to call the notifier chain even on foreground_thread wakeups for tasks
    having load more than a threshold value.
    
    If load_based_syncs is turned OFF then we do not need to perform
    cpu boost for wakeup hints for foreground tasks from scheduler
    
    Signed-off-by: Rohit Gupta <rohgup@codeaurora.org>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 15d0a3f61a2c6abff9ace21e918f236e96f79eb0
Author: Waiman Long <Waiman.Long@hp.com>
Date:   Wed Apr 17 15:23:11 2013 -0400

    mutex: Move mutex spinning code from sched/core.c back to mutex.c
    
    As mentioned by Ingo, the SCHED_FEAT_OWNER_SPIN scheduler
    feature bit was really just an early hack to make with/without
    mutex-spinning testable. So it is no longer necessary.
    
    This patch removes the SCHED_FEAT_OWNER_SPIN feature bit and
    move the mutex spinning code from kernel/sched/core.c back to
    kernel/mutex.c which is where they should belong.
    
    Signed-off-by: Waiman Long <Waiman.Long@hp.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Chandramouleeswaran Aswin <aswin@hp.com>
    Cc: Davidlohr Bueso <davidlohr.bueso@hp.com>
    Cc: Norton Scott J <scott.norton@hp.com>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: David Howells <dhowells@redhat.com>
    Cc: Dave Jones <davej@redhat.com>
    Cc: Clark Williams <williams@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit bbc2cd0168ada40de799c9e38977d5bd7c82b60e
Author: Srivatsa Vaddagiri <vatsa@codeaurora.org>
Date:   Thu May 15 19:06:56 2014 -0700

    sched: window-stats: Fix overflow bug
    
    Multiplication over-flow possibility exists in update_task_ravg() when
    updating task's window_start. That would lead to incorrect accounting
    of task load. Fix the issue by using 64-bit arithmetic.
    
    Signed-off-by: Srivatsa Vaddagiri <vatsa@codeaurora.org>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit a845f5604f7b2b1e35b4fba99adb40a94e9943d8
Author: Srivatsa Vaddagiri <vatsa@codeaurora.org>
Date:   Sat Mar 29 11:40:16 2014 -0700

    sched: Window-based load stat improvements
    
    Some tasks can have a sporadic load pattern such that they can suddenly
    start running for longer intervals of time after running for shorter
    durations. To recognize such sharp increase in tasks' demands, max
    between the average of 5 window load samples and the most recent sample
    is chosen as the task demand.
    
    Make the window size (sched_ravg_window) configurable at boot up
    time. To prevent users from setting inappropriate values for window
    size, min and max limits are defined. As 'ravg' struct tracks load for
    both real-time and non real-time tasks it is moved out of sched_entity
    struct.
    
    In order to prevent changing function signatures for move_tasks() and
    move_one_task() per-cpu variables are defined to track the total load
    moved. In case multiple tasks are selected to migrate in one load
    balance operation, loads > 100 could be sent through migration notifiers.
    Prevent this scenario by setting mnd.load to 100 in such cases.
    
    Define wrapper functions to compute cpu demands for tasks and to change
    rq->cumulative_runnable_avg.
    
    Signed-off-by: Srivatsa Vaddagiri <vatsa@codeaurora.org>
    Signed-off-by: Rohit Gupta <rohgup@codeaurora.org>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>
    
    Conflicts:
    	include/linux/sched.h

commit 480e0d4ea33ebb469f07564959a5dd3e3a42e385
Author: Rohit Gupta <rohgup@codeaurora.org>
Date:   Tue Apr 15 19:30:53 2014 -0700

    sched: Disable wakeup hints for foreground tasks by default
    
    By default sched_wakeup_load_threshold is set to 60 and therefore
    wakeup hints are sent out for those tasks whose loads are higher
    that value. This might cause unnecessary wakeup boosts to happen
    when load based syncing is turned ON for cpu-boost.
    Disable the wake up hints by setting the sched_wakeup_load_threshold
    to a value higher than 100 so that wakeup boost doesnt happen unless
    it is explicitly turned ON from adb shell.
    
    Signed-off-by: Rohit Gupta <rohgup@codeaurora.org>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 74cc36faf699dd87f16134c959955ec507a8e9a0
Author: Rohit Gupta <rohgup@codeaurora.org>
Date:   Thu Mar 13 20:54:47 2014 -0700

    sched: Call the notify_on_migrate notifier chain for wakeups as well
    
    Add a change to send notify_on_migrate hints on wakeups of
    foreground tasks from scheduler if their load is above
    wakeup_load_thresholds (default value is 60).
    These hints can be used to choose an appropriate CPU frequency
    corresponding to the load of the task being woken up.
    
    Signed-off-by: Rohit Gupta <rohgup@codeaurora.org>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 1931fca866ba51724c249b3903131320766d171c
Author: Rohit Gupta <rohgup@codeaurora.org>
Date:   Fri Mar 14 18:56:14 2014 -0700

    cpufreq: cpu-boost: Introduce scheduler assisted load based syncs
    
    Previously, on getting a migration notification cpu-boost changed
    the scaling min of the destination frequency to match that of the
    source frequency or sync_threshold whichever was minimum.
    
    If the scheduler migration notification is extended with task load
    (cpu demand) information, the cpu boost driver can use this load to
    compute a suitable frequency for the migrating task. The required
    frequency for the task is calculated by taking the load percentage
    of the max frequency and no sync is performed if the load is less
    than a particular value (migration_load_threshold).This change is
    beneficial for both perf and power as demand of a task is taken into
    consideration while making cpufreq decisions and unnecessary syncs
    for lightweight tasks are avoided.
    
    The task load information provided by scheduler comes from a
    window-based load collection mechanism which also normalizes the
    load collected by the scheduler to the max possible frequency
    across all CPUs.
    
    Signed-off-by: Rohit Gupta <rohgup@codeaurora.org>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 1eaf34d70bb9fbe16ee499844f26dc1bbd5ab13e
Author: Srivatsa Vaddagiri <vatsa@codeaurora.org>
Date:   Mon Jan 6 16:24:48 2014 -0800

    sched: window-based load stats for tasks
    
    Provide a metric per task that specifies how cpu bound a task is. Task
    execution is monitored over several time windows and the fraction of
    the window for which task was found to be executing or wanting to run
    is recorded as task's demand. Windows over which task was sleeping are
    ignored. We track last 5 recent windows for every task and the maximum
    demand seen in any of the previous 5 windows (where task had some
    activity) drives freq demand for every task.
    
    A per-cpu metric (rq->cumulative_runnable_avg) is also provided which
    is an aggregation of cpu demand of all tasks currently enqueued on it.
    rq->cumulative_runnable_avg will be useful to know if cpu frequency
    will need to be changed to match task demand.
    
    Signed-off-by: Srivatsa Vaddagiri <vatsa@codeaurora.org>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 59e80cdcc37a1131e2a2245122bfc7ffdd89675a
Author: Srivatsa Vaddagiri <vatsa@codeaurora.org>
Date:   Thu Dec 12 17:06:11 2013 -0800

    sched: Make scheduler aware of cpu frequency state
    
    Capacity of a cpu (how much performance it can deliver) is partly
    determined by its frequency (P) state, both current frequency as well
    as max frequency it can reach.  Knowing frequency state of cpus will
    help scheduler optimize various functions such as tracking every
    task's cpu demand and placing tasks on various cpus.
    
    This patch has scheduler registering for cpufreq notifications to
    become aware of cpu's frequency state. Subsequent patches will make
    use of derived information for various purposes, such as task's scaled
    load (cpu demand) accounting and task placement.
    
    Signed-off-by: Srivatsa Vaddagiri <vatsa@codeaurora.org>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 753296a163d3135d1887ab58a0566bf7d4da9c7b
Author: Steve Muckle <smuckle@codeaurora.org>
Date:   Thu May 23 15:24:57 2013 -0700

    sched: remove migration notification from RT class
    
    Commit 88a7e37d265 (sched: provide per cpu-cgroup option to
    notify on migrations) added a notifier call when a task is moved
    to a different CPU. Unfortunately the two call sites in the RT
    sched class where this occurs happens with a runqueue lock held.
    This can result in a deadlock if the notifier call attempts to do
    something like wake up a task.
    
    Fortunately the benefit of 88a7e37d265 comes mainly from notifying
    on migration of non-RT tasks, so we can simply ignore the movements
    of RT tasks.
    
    CRs-Fixed: 491370
    Change-Id: I8849d826bf1eeaf85a6f6ad872acb475247c5926
    Signed-off-by: Steve Muckle <smuckle@codeaurora.org>

commit 9499ad04faf5b835ab9ad6cdf69bfe55181232ee
Author: Steve Muckle <smuckle@codeaurora.org>
Date:   Mon Mar 11 16:33:42 2013 -0700

    sched: provide per cpu-cgroup option to notify on migrations
    
    On systems where CPUs may run asynchronously, task migrations
    between CPUs running at grossly different speeds can cause
    problems.
    
    This change provides a mechanism to notify a subsystem
    in the kernel if a task in a particular cgroup migrates to a
    different CPU. Other subsystems (such as cpufreq) may then
    register for this notifier to take appropriate action when
    such a task is migrated.
    
    The cgroup attribute to set for this behavior is
    "notify_on_migrate" .
    
    Change-Id: Ie1868249e53ef901b89c837fdc33b0ad0c0a4590
    Signed-off-by: Steve Muckle <smuckle@codeaurora.org>

commit 9fa4e636c26850a20b55223b5d2ba3a52be3bfcc
Author: Andy Lutomirski <luto@amacapital.net>
Date:   Mon Jan 30 08:17:26 2012 -0800

    Add PR_{GET,SET}_NO_NEW_PRIVS to prevent execve from granting privs
    
    With this change, calling
      prctl(PR_SET_NO_NEW_PRIVS, 1, 0, 0, 0)
    disables privilege granting operations at execve-time.  For example, a
    process will not be able to execute a setuid binary to change their uid
    or gid if this bit is set.  The same is true for file capabilities.
    
    Additionally, LSM_UNSAFE_NO_NEW_PRIVS is defined to ensure that
    LSMs respect the requested behavior.
    
    To determine if the NO_NEW_PRIVS bit is set, a task may call
      prctl(PR_GET_NO_NEW_PRIVS, 0, 0, 0, 0);
    It returns 1 if set and 0 if it is not set. If any of the arguments are
    non-zero, it will return -1 and set errno to -EINVAL.
    (PR_SET_NO_NEW_PRIVS behaves similarly.)
    
    This functionality is desired for the proposed seccomp filter patch
    series.  By using PR_SET_NO_NEW_PRIVS, it allows a task to modify the
    system call behavior for itself and its child tasks without being
    able to impact the behavior of a more privileged task.
    
    Another potential use is making certain privileged operations
    unprivileged.  For example, chroot may be considered "safe" if it cannot
    affect privileged tasks.
    
    Note, this patch causes execve to fail when PR_SET_NO_NEW_PRIVS is
    set and AppArmor is in use.  It is fixed in a subsequent patch.
    
    Signed-off-by: Andy Lutomirski <luto@amacapital.net>
    Signed-off-by: Will Drewry <wad@chromium.org>
    Acked-by: Eric Paris <eparis@redhat.com>
    
    v18: updated change desc
    v17: using new define values as per 3.4
    
    Conflicts:
    	include/linux/prctl.h
    	kernel/sys.c
    
    Conflicts:
    	kernel/sys.c

commit 27f8a2857626f36c50550c1f9f0dd31e69c8e1be
Author: Rohit Gupta <rohgup@codeaurora.org>
Date:   Wed Jul 23 14:08:42 2014 -0700

    cpufreq: cpu-boost: Use one work to remove input boost for all CPUs
    
    Currently each CPU queues up its own work for removing input boost.
    This is not really required as boost removal for all the CPUs can
    be done at the same time. So this patch uses a single work to
    remove input boost for all the CPUs and updates the policy for
    the online ones.
    
    Change-Id: I37c809f2f155548b1d8c1b9aa7626c8852b3acc6
    Signed-off-by: Rohit Gupta <rohgup@codeaurora.org>

commit a09397dd8c9ff662955a1c143a5f2c47bf5aae56
Author: Junjie Wu <junjiew@codeaurora.org>
Date:   Wed May 28 16:36:25 2014 -0700

    cpufreq: cpu-boost: Support separate input_boost_freq for different CPUs
    
    Different types of CPUs could have different frequency to satisfy same
    input workload. Add support for using different input_boost_freq on
    different CPUs.
    
    input_boost_freq now either takes a single number which applies to all
    CPUs, or cpuid:freq pairs separated by space for different CPUs.
    
    Change-Id: I20506a9fbdb4d532d94168bbd61744595bebc8e5
    Signed-off-by: Junjie Wu <junjiew@codeaurora.org>

commit 1977c6526d4a2808922d15392da50dadc04f31ac
Author: Swetha Chikkaboraiah <schikk@codeaurora.org>
Date:   Tue Jun 10 17:31:23 2014 +0530

    cpufreq: cpu-boost: Use interruptible wait to not affect load average
    
    Using the function wait_event in cpu_boost puts the
    process enter to 'D' state which contribute to the
    high load average. This change will put the process
    boost_sync in the 'S' state (interruptible sleep)
    
    Change-Id: Ie121adbe1fac1d2862ac5342bb97c7c926f7d7a8
    CRs-Fixed: 655484
    Signed-off-by: Swetha Chikkaboraiah <schikk@codeaurora.org>
    Signed-off-by: Raghavendra Ambadas <rambad@codeaurora.org>

commit 6e7da06a47d86362f30d70de28003749af5843ab
Author: Patrick Cain <pcain@codeaurora.org>
Date:   Wed Jan 8 11:54:36 2014 -0800

    cpufreq: Sync on thread migration optimizations
    
    When threads migrate between CPUs the destination CPU will temporarily
    have a boosted frequency and the source CPU will have a residual higher
    frequency even though the load could have just been reduced. To avoid
    leaving CPUs at a high frequencies notify the source CPU after a thread
    migration and make the interactive governor's timer reschedule
    conditionally.
    
    Change-Id: Id55055f2a116ca7ffd2a34dc3d0ec519c888a111
    Signed-off-by: Patrick Cain <pcain@codeaurora.org>

commit 1037ab25cf10d0061f605bee5a619969b4128925
Author: Patrick Cain <pcain@codeaurora.org>
Date:   Wed Mar 12 16:56:16 2014 -0700

    cpufreq: cpu-boost: Re-issue boosts above minimum frequency
    
    Frequency boosts where the source CPU frequency is greater than CPU's
    minimum frequency should always go through regardless of the destination
    CPU's current frequency. This fixes a performance issue where the governor
    lowers the CPU frequency shortly after a thread is migrated to it because
    the boost wasn't re-issued.
    
    Change-Id: I449545a688d84b0a6e834f5a51dcb499caa84d29
    Signed-off-by: Patrick Cain <pcain@codeaurora.org>

commit 25bc696e63b309f21de12d6e5647bbbfb4c30d9c
Author: Srivatsa Vaddagiri <vatsa@codeaurora.org>
Date:   Fri Dec 20 18:56:07 2013 -0800

    cpufreq: cpu-boost: Resolve deadlock when waking up sync thread
    
    CPU boost driver receives notification from scheduler when threads
    migrate towards a cpu and in turn wakes up a sync thread associated
    with that cpu to handle any required frequency transitions. The wakeup
    call however can lead to a deadlock inside scheduler under some
    circumstance. The deadlock is seen when sync thread is the only thread
    running on a cpu and goes to sleep (say by calling wait_event() ->
    schedule()). Midway through this sleep (schedule()) call, while cpu is
    still running in context of sync thread, scheduler attempts a load
    balance (realizing that cpu is about to become idle) which can result
    in tasks being migrated towards the cpu going idle. This will cause
    migration notification to be issued and in turn a wakeup on sync
    thread. The wakeup call however gets stuck in below while() loop
    inside scheduler:
    
    try_to_wake_up(struct task_struct *p, ...)
    {
    
            /*
             * If the owning (remote) cpu is still in the middle of
    	 * schedule() with this task as prev, wait until its done
    	 * referencing the task.
             */
    	while (p->on_cpu)
    		cpu_relax();
    
    }
    
    A possible fix could be to teach try_to_wake_up() about this
    special case. Another fix, implemented in this patch and that helps
    minimize scheduler changes, is to have cpu boost driver not issue a
    wakeup under this special circumstance, which was found to occur very
    infrequently.
    
    Change-Id: I92bc68a22d51595a208673fe2a1eedfa97004f9e
    Signed-off-by: Srivatsa Vaddagiri <vatsa@codeaurora.org>

commit ecbbf4063d68097bb0385068eb39e2642f25f9f3
Author: Saravana Kannan <skannan@codeaurora.org>
Date:   Thu Feb 13 19:08:21 2014 -0800

    cpufreq: cpu-boost: Don't register for cpufreq notifiers too early
    
    The cpufreq notifiers should be registered only after all the data
    structures used in the notifier callbacks have been initialized. So, move
    the cpufreq notifier registration to a later point in the init function.
    
    Change-Id: I043ab5bc0ebb98164c40549fe151a8d801c8c186
    Signed-off-by: Saravana Kannan <skannan@codeaurora.org>

commit 30cceef6e44994fff733bbb49a0aa083fb186f69
Author: Saravana Kannan <skannan@codeaurora.org>
Date:   Tue Jan 28 19:40:32 2014 -0800

    cpufreq: cpu-boost: Fix deadlock in wake_up of sync threads
    
    If wake_up() is called on the current task on a CPU, the call will wait
    until the current task is switched out before it wakes it up again and
    returns.
    
    The sync notifier for a CPU always runs on that CPU.
    
    These two together can result in a deadlock if the sync notifier on CPU A
    tries to wake up the sync thread of CPU A as it goes to sleep (is the
    current task). A previous commit fixed this by adding a check to the sync
    notifier to not wake up the sync thread of CPU A if it's the current task.
    
    But this is still not sufficient to prevent deadlocks.
    
    Sync thread of CPU A could be the current task on CPU B and sync thread of
    CPU B could be the current task on CPU A.  At this point, if sync notifier
    of CPU A and B try to wake up the sync threads of CPU A and B, it will
    result in CPU A waiting for the current task in CPU B to get switched out
    and CPU B waiting for the current task in CPU A to get switched out.  This
    will result in a deadlock.
    
    Prevent this scenario from happening by pinning the sync threads of each
    CPU to run on that CPU. By doing this, we guarantee that sync notifiers
    will only try to wake up sync threads running on that CPU. The fix added by
    "cpufreq: cpu-boost: Resolve deadlock when waking up sync thread" ensures a
    deadlock doesn't happen when a sync notifier tries to wake up a sync thread
    running on that CPU.
    
    Change-Id: I864e545529722a23886dd5a82f66089155d2d193
    Signed-off-by: Saravana Kannan <skannan@codeaurora.org>

commit 59c9be16a0048804cdd439b802bf5002f24c0232
Author: Saravana Kannan <skannan@codeaurora.org>
Date:   Tue Jan 28 19:14:57 2014 -0800

    cpufreq: cpu-boost: Fix queue_delayed_work_on() race with hotplug
    
    Calling queue_delayed_work_on() on a CPU that's in the process of getting
    hotplugged out can result in that CPU infinitely looping in
    msm_pm_wait_cpu_shutdown(). If queue_delayed_work_on() is called after the
    CPU is hotplugged out, it could wake up the CPU without going through the
    hotplug path and cause instability. To avoid this, make sure the CPU is and
    stays online while queuing a work on it.
    
    Change-Id: I1b4aae3db803e476b1a7676d08f495c1f38bb154
    Signed-off-by: Saravana Kannan <skannan@codeaurora.org>

commit 72885d817e14a375b7ef2866ca14b9e6bc992dc8
Author: Rohit Gupta <rohgup@codeaurora.org>
Date:   Wed Dec 4 14:46:18 2013 -0800

    cpufreq: Disable cpu-boost by default
    
    Disable cpu-boost by default, so that it can be turned ON only by
    setting the module parameter boost_ms through command prompt when
    required.
    
    Change-Id: Ia9bc280892f65ed1d2e8051d1951e51922ad13db
    Signed-off-by: Rohit Gupta <rohgup@codeaurora.org>

commit f2e6fcc1600155bf1a7fda4e229e0dd431502c96
Author: Rohit Gupta <rohgup@codeaurora.org>
Date:   Tue Nov 26 18:20:57 2013 -0800

    cpufreq: Add Input Boost feature to the cpu-boost driver
    
    On incoming input events boost the frequency of all online cpus
    for at least input_boost_ms ms. This is accomplished by changing
    the policy->min of all the online cpus to input_boost_freq.
    
    Change-Id: Idb0ab75d68ae4ceff259cbbaaec1a9bb3bc871d3
    Signed-off-by: Rohit Gupta <rohgup@codeaurora.org>

commit ddc4f2a6258e0761d85695beae6739156ac65646
Author: Rohit Gupta <rohgup@codeaurora.org>
Date:   Thu Nov 21 14:51:07 2013 -0800

    cpufreq: Add a sync limit to cpu-boost
    
    Perform frequency synchronization only when source CPU's frequency
    is less than sync_threshold, else sync to the sync_threshold.
    
    Change-Id: I544c414568d4e015b80ce5891dd215275bac95da
    Signed-off-by: Rohit Gupta <rohgup@codeaurora.org>

commit e0c7ae3f37f0dcdf171d7f1a5b396037790d5612
Author: Saravana Kannan <skannan@codeaurora.org>
Date:   Sat Jul 13 01:49:09 2013 -0700

    cpufreq: Add cpu-boost driver
    
    When certain bursty and important events take place, it might take a while
    for the current cpufreq governor to notice the new load and react to it.
    That would result in poor user experience. To alleviate this, the cpu-boost
    driver boosts the frequency of a CPU for a short duration to maintain good
    user experience while the governor catches up.
    
    Specifically, this commit deals with ensuring that when "important" tasks
    migrate from a fast CPU to a slow CPU, the frequency of the slow CPU is
    boosted to be at least as high as the fast CPU for a short duration.
    
    Since this driver enforces the boost by hooking into standard cpufreq
    ADJUST notifiers, it has several advantages:
    - More portable across kernel versions where the cpufreq internals might
      have been rewritten.
    - Governor agnostic and hence works with multiple governors like
      conservative, ondemand, interactive, etc.
    - Does not affect the sampling period/logic of existing governors.
    - Can have the boost period adjusted independent of governor sampling
      period.
    
    Change-Id: Ibd814a20043d0aba64ee7637a4a79b9ffa1b0991
    Signed-off-by: Saravana Kannan <skannan@codeaurora.org>

commit 3e467f98f83ea66b8ff719b239b708369a8a228a
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Tue Apr 21 22:33:41 2015 +0200

    Revert "ARM: asm: Add ARM_BE8() assembly helper"
    
    This reverts commit 1fac60dba4d8e0c66f9239d44ed4b4021b8b0d73.

commit 679a7ce231632e86bafd772fcbe01c9d714dbca2
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Tue Apr 21 22:27:34 2015 +0200

    Linux 3.4.107

commit 5a19d7033c66fc2cfe08c5f48acb8447939c52c3
Author: Will Deacon <will.deacon@arm.com>
Date:   Fri May 10 18:07:19 2013 +0100

    ARM: barrier: allow options to be passed to memory barrier instructions
    
    On ARMv7, the memory barrier instructions take an optional `option'
    field which can be used to constrain the effects of a memory barrier
    based on shareability and access type.
    
    This patch allows the caller to pass these options if required, and
    updates the smp_*() barriers to request inner-shareable barriers,
    affecting only stores for the _wmb variant. wmb() is also changed to
    use the -st version of dsb.
    
    Reported-by: Albin Tonnerre <albin.tonnerre@arm.com>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

commit 07ce151f00767ade7eb366ef11f45ea0b159310b
Author: Shawn Guo <shawn.guo@linaro.org>
Date:   Thu Jul 3 09:56:59 2014 +0100

    ARM: 8089/1: cpu_pj4b_suspend_size should base on cpu_v7_suspend_size
    
    Since pj4b suspend/resume routines are implemented based on generic
    ARMv7 ones, instead of hard-coding cpu_pj4b_suspend_size, we should have
    it be cpu_v7_suspend_size plus pj4b specific bytes.  Otherwise, if
    cpu_v7_suspend_size gets updated alone, the pj4b suspend/resume will
    likely be broken.
    
    While at it, fix the comments in cpu_pj4b_do_resume, as we're restoring
    CP15 registers rather than saving in there.
    
    Signed-off-by: Shawn Guo <shawn.guo@freescale.com>
    Acked-by: Gregory CLEMENT <gregory.clement@free-electrons.com>
    Tested-by: Gregory CLEMENT <gregory.clement@free-electrons.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

commit 0394bbaa28d30a6917bd631b73246a9d53235df2
Author: Gregory CLEMENT <gregory.clement@free-electrons.com>
Date:   Fri Mar 28 12:21:16 2014 +0100

    ARM: 8013/1: PJ4B: Add cpu_suspend/cpu_resume hooks for PJ4B
    
    PJ4B needs extra instructions for suspend and resume, so instead of
    using the armv7 version, this commit introduces specific versions for
    PJ4B.
    
    Signed-off-by: Gregory CLEMENT <gregory.clement@free-electrons.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

commit 1fac60dba4d8e0c66f9239d44ed4b4021b8b0d73
Author: Ben Dooks <ben.dooks@codethink.co.uk>
Date:   Tue Feb 12 18:59:57 2013 +0000

    ARM: asm: Add ARM_BE8() assembly helper
    
    Add ARM_BE8() helper to wrap any code conditional on being
    compile when CONFIG_ARM_ENDIAN_BE8 is selected and convert
    existing places where this is to use it.
    
    Acked-by: Nicolas Pitre <nico@linaro.org>
    Reviewed-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Ben Dooks <ben.dooks@codethink.co.uk>

commit 4fce340238bb1af45d66b5e8063ff45c7c768657
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Fri Apr 3 11:32:34 2015 +0100

    ARM: cache-v7: optimise test for Cortex A9 r0pX devices
    
    Eliminate one unnecessary instruction from this test by pre-shifting
    the Cortex A9 ID - we can shift the actual ID in the teq instruction
    thereby losing the pX bit of the ID at no cost.
    
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

commit 3e77de8c1990a1572aeac6af28aa29d9fa741058
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Fri Apr 3 11:25:39 2015 +0100

    ARM: cache-v7: optimise branches in v7_flush_cache_louis
    
    Optimise the branches such that for the majority of unaffected devices,
    we avoid needing to execute the errata work-around code path by
    branching to start_flush_levels early.
    
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

commit aaf9a1ac8095f8098c7a8a0ac5a2759f21788fa3
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Fri Apr 3 11:21:42 2015 +0100

    ARM: cache-v7: consolidate initialisation of cache level index
    
    Both v7_flush_cache_louis and v7_flush_dcache_all both begin the
    flush_levels loop with r10 initialised to zero.  In each case, this
    is done immediately prior to entering the loop.  Branch to this
    instruction in v7_flush_dcache_all from v7_flush_cache_louis and
    eliminate the unnecessary initialisation in v7_flush_cache_louis.
    
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

commit 4bc28b9dc2552827eff25f766caf81f837c44ebb
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Fri Apr 3 11:15:53 2015 +0100

    ARM: cache-v7: shift CLIDR to extract appropriate field before masking
    
    Rather than have code which masks and then shifts, such as:
    
    	mrc     p15, 1, r0, c0, c0, 1
    ALT_SMP(ands	r3, r0, #7 << 21)
    ALT_UP( ands	r3, r0, #7 << 27)
    ALT_SMP(mov	r3, r3, lsr #20)
    ALT_UP(	mov	r3, r3, lsr #26)
    
    re-arrange this as a shift and then mask.  The masking is the same for
    each field which we want to extract, so this allows the mask to be
    shared amongst code paths:
    
    	mrc     p15, 1, r0, c0, c0, 1
    ALT_SMP(mov	r3, r0, lsr #20)
    ALT_UP(	mov	r3, r0, lsr #26)
    	ands	r3, r3, #7 << 1
    
    Use this method for the LoUIS, LoUU and LoC fields.
    
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

commit 019a5e9d49e258a72d862402d761ea79685e805a
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Tue Apr 21 19:06:18 2015 +0200

     ARM: cache-v7: use movw/movt instructions
    
    We always build cache-v7.S for ARMv7, so we can use the ARMv7 16-bit
    move instructions to load large constants, rather than using constants
    in a literal pool.
    
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

commit e5fd0bce5db34b9d898de53bd3c01d2e43d558f2
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Tue Apr 21 17:54:32 2015 +0200

    sound: wcd9310 go back to stock

commit b6f26505527afdacdd625e68e230837dd89e8c06
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Tue Apr 21 17:52:34 2015 +0200

    fauxsound: go back to first working version

commit a7637968d22325d33608798986a909f3daf7c8d1
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Fri Apr 17 18:19:10 2015 +0200

    update gitgnore

commit b0e57777121d0a92d39c646731bfded681510ce4
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Apr 15 14:38:33 2015 +0200

    fauxsound: update to latest verion

commit a405cf8011f94c7607ab7af96be72da59e7958ee
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Apr 15 14:07:47 2015 +0200

    config: release V1.1

commit 91f268d04698bfa15e06c804189e6ab78e286fbe
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Apr 15 13:40:30 2015 +0200

     cgroup: remove synchronize_rcu() from cgroup_attach_{task|proc}()
    
    These 2 syncronize_rcu()s make attaching a task to a cgroup
    quite slow, and it can't be ignored in some situations.
    
    A real case from Colin Cross: Android uses cgroups heavily to
    manage thread priorities, putting threads in a background group
    with reduced cpu.shares when they are not visible to the user,
    and in a foreground group when they are. Some RPCs from foreground
    threads to background threads will temporarily move the background
    thread into the foreground group for the duration of the RPC.
    This results in many calls to cgroup_attach_task.
    
    In cgroup_attach_task() it's task->cgroups that is protected by RCU,
    and put_css_set() calls kfree_rcu() to free it.
    
    If we remove this synchronize_rcu(), there can be threads in RCU-read
    sections accessing their old cgroup via current->cgroups with
    concurrent rmdir operation, but this is safe.
    
     # time for ((i=0; i<50; i++)) { echo $$ > /mnt/sub/tasks; echo $$ > /mnt/tasks; }
    
    real    0m2.524s
    user    0m0.008s
    sys     0m0.004s
    
    With this patch:
    
    real    0m0.004s
    user    0m0.004s
    sys     0m0.000s
    
    tj: These synchronize_rcu()s are utterly confused.  synchornize_rcu()
        necessarily has to come between two operations to guarantee that
        the changes made by the former operation are visible to all rcu
        readers before proceeding to the latter operation.  Here,
        synchornize_rcu() are at the end of attach operations with nothing
        beyond it.  Its only effect would be delaying completion of
        write(2) to sysfs tasks/procs files until all rcu readers see the
        change, which doesn't mean anything.
    
    cherry-picked from:
    https://android.googlesource.com/kernel/common/+/5d65bc0ca1bceb73204dab943922ba3c83276a8c
    
    Bug: 17709419
    Change-Id: I98dacd6c13da27cb3496fe4a24a24084e46bdd9c
    Signed-off-by: Li Zefan <lizefan@huawei.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Reported-by: Colin Cross <ccross@google.com>
    Signed-off-by: Devin Kim <dojip.kim@lge.com>

commit 5ed972e8d8846fb8f3fac24d6f99854fb80833a1
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Apr 15 13:40:07 2015 +0200

    sound: add soundcontrol v2.1 again

commit 29d05f2616438e7ed5d3b6073aa1c3d082d0c525
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Apr 15 12:58:09 2015 +0200

    Revert "Revert "asm-generic: rwsem: de-PPCify rwsem.h""
    
    This reverts commit d66c510446c7df248fa96da0b50fa07b061b9b37.

commit d2412e572c14a8412531ad2b3575e7986d10a28d
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Apr 15 05:01:54 2015 +0200

    Revert "ARM: rwsem: use asm-generic rwsem implementation"
    
    This reverts commit 07fa1728f7e7dde7bc719e5a3c85ff73316896a2.

commit d66c510446c7df248fa96da0b50fa07b061b9b37
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Apr 15 05:01:14 2015 +0200

    Revert "asm-generic: rwsem: de-PPCify rwsem.h"
    
    This reverts commit 4000cee15ee0d9652dd62312dd44843093d55d31.

commit 072635ef91058d89e90b6a5970a5f1744860dcb7
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Apr 15 04:59:34 2015 +0200

    Revert "ARM: 8118/1: crypto: sha1/make use of common SHA-1 structures"
    
    This reverts commit d2e2f52f1f7c9e3e46eb269c900f01b1d5ecc1fa.

commit 9422f4e2304f55891daf5af0868e606f15f35175
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Apr 15 04:33:04 2015 +0200

    config: update

commit d3828cc2be9fe44294a7d7eef12cd5a2d3efb6e2
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Apr 15 03:56:51 2015 +0200

     workqueue: Add system wide power_efficient workqueues
    
    Adapted for 3.4 from lsk-v3.10
    
    This patch adds system wide workqueues aligned towards power saving. This is
    done by allocating them with WQ_UNBOUND flag if 'wq_power_efficient' is set to
    'true'.
    
    tj: updated comments a bit.
    
    Signed-off-by: Viresh Kumar <viresh.kumar@linaro.org>
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Mark Brown <broonie@linaro.org>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit bf8ab71082b0ae87a5038bf114928db7c6efc1e1
Author: Viresh Kumar <viresh.kumar@linaro.org>
Date:   Mon Apr 8 16:45:40 2013 +0530

    workqueues: Introduce new flag WQ_POWER_EFFICIENT for power oriented workqueues
    
    Adapted for 3.4 from lsk-v3.10
    
    Workqueues can be performance or power-oriented. Currently, most workqueues are
    bound to the CPU they were created on. This gives good performance (due to cache
    effects) at the cost of potentially waking up otherwise idle cores (Idle from
    scheduler's perspective. Which may or may not be physically idle) just to
    process some work. To save power, we can allow the work to be rescheduled on a
    core that is already awake.
    
    Workqueues created with the WQ_UNBOUND flag will allow some power savings.
    However, we don't change the default behaviour of the system.  To enable
    power-saving behaviour, a new config option CONFIG_WQ_POWER_EFFICIENT needs to
    be turned on. This option can also be overridden by the
    workqueue.power_efficient boot parameter.
    
    tj: Updated config description and comments.  Renamed
        CONFIG_WQ_POWER_EFFICIENT to CONFIG_WQ_POWER_EFFICIENT_DEFAULT.
    
    Signed-off-by: Viresh Kumar <viresh.kumar@linaro.org>
    Reviewed-by: Amit Kucheria <amit.kucheria@linaro.org>
    Signed-off-by: Tejun Heo <tj@kernel.org>
    (cherry picked from commit cee22a15052faa817e3ec8985a28154d3fabc7aa)
    Signed-off-by: Mark Brown <broonie@linaro.org>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 551e4a03d637aea7973de632919ead91ead2bd15
Author: Matt Wagantall <mattw@codeaurora.org>
Date:   Thu Jan 30 23:08:03 2014 -0800

    ARM: smp: BUG() if smp_send_reschedule() is called for an offline cpu
    
    Sending an IPI_RESCHEDULE to an offline CPU is incorrect and potentially
    bad for both power and stability. On some sub-architectures such as MSM,
    if a power-collapsed CPU is unexpectedly woken up by an IPI, it will be
    begin executing without the preparations that would normally happen as
    part of CPU_UP_PREPARE. If clocks, voltage regulators, or other hardware
    configuration are not performed, the booting CPU may cause general
    instability or (at best) poor power performance since the CPU would be
    powered up but not utilized.
    
    One common cause for such issues is misuse of add_timer_on() or APIs
    such as queue_work_on() which call it. If proper precautions are not
    taken to block hotplug while these APIs are called then a race may
    result in IPIs being sent to CPUs that are already offline.
    
    This same argument could be applied to other IPIs (with the exception
    of IPI_WAKEUP), but the others are already restricted to only online
    CPUs by existing mechanisms, so an explicit assertion is not useful.
    
    Signed-off-by: Matt Wagantall <mattw@codeaurora.org>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit e153ed5fac10637c16ebae2c8f018eb704951ed0
Author: Syed Rameez Mustafa <rameezmustafa@codeaurora.org>
Date:   Tue Sep 17 19:57:46 2013 -0700

    ARM: smp: fix incorrect per-cpu definition of regs_before_stop
    
    The commit efd002896a3636a4d1606ac0630aaafbbfdea9aa introduced changes
    to store CPU registers for all CPUs that handle IPI_CPU_STOP. The
    structure to save the registers was intended to be a per-cpu variable.
    However, the patch did not allocate a per-cpu structure and instead only
    ended up providing a compiler per-cpu directive. Fix this bug by actually
    defining a static per-cpu variable.
    
    Signed-off-by: Syed Rameez Mustafa <rameezmustafa@codeaurora.org>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit a20ed4d9239559cfb4fc2263a8ca5f874be0ab9b
Author: Tingting Yang <tingting@codeaurora.org>
Date:   Thu Aug 8 09:57:42 2013 +0800

    ARM: smp: Save CPU registers before IPI_CPU_STOP processing
    
    When a kernel panic occurs on one CPU, other CPUs are instructed to stop
    execution via the IPI_CPU_STOP message. These other CPUs dump their stack,
    which may not be good enough to reconstruct their context to perform
    post-mortem analysis. Dump each CPU's context
    (before it started procesing the IPI) into a globally accessible structure
    to allow for easier post-mortem debugging.
    
    Signed-off-by: Tingting Yang <tingting@codeaurora.org>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 46327b06b0debca1ca759e7ef9de4c9a9feac434
Author: Syed Rameez Mustafa <rameezmustafa@codeaurora.org>
Date:   Tue Jun 11 22:36:08 2013 -0700

    ARM: Flush the caches for non panicking CPUs in case of a kernel panic
    
    In case of a kernel panic, only the panicking CPU does an entire
    cache flush. This means that certain dirty cache lines in the L1
    caches of the other CPUs may never get flushed. This gives us
    improper RAM dumps. Add cache flushing for all the online CPUs.
    The outer domain is not flushed since it is already being done by
    the panicking CPU.
    
    Signed-off-by: Syed Rameez Mustafa <rameezmustafa@codeaurora.org>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 07fa1728f7e7dde7bc719e5a3c85ff73316896a2
Author: Will Deacon <will.deacon@arm.com>
Date:   Mon Feb 17 14:13:28 2014 +0000

    ARM: rwsem: use asm-generic rwsem implementation
    
    asm-generic offers an atomic-add based rwsem implementation, which
    can avoid the need for heavier, spinlock-based synchronisation on the
    fast path.
    
    This patch makes use of the optimised implementation for ARM CPUs.
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit f83bba34e16ab278006b97a0185a6fddd1426be6
Author: Will Deacon <will.deacon@arm.com>
Date:   Mon Feb 17 14:02:06 2014 +0000

    asm-generic: rwsem: ensure sem->cnt is only accessed via atomic_long_*
    
    The asm-generic rwsem implementation directly acceses sem->cnt when
    performing a __down_read_trylock operation. Whilst this is probably safe
    on all architectures, we should stick to the atomic_long_* API and use
    atomic_long_read instead.
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 4000cee15ee0d9652dd62312dd44843093d55d31
Author: Will Deacon <will.deacon@arm.com>
Date:   Mon Feb 17 14:10:20 2014 +0000

    asm-generic: rwsem: de-PPCify rwsem.h
    
    asm-generic/rwsem.h used to live under arch/powerpc. During its
    liberation to common code, a few references to its former home where
    preserved, in particular the definition of RWSEM_ACTIVE_MASK is
    predicated on CONFIG_PPC64.
    
    This patch updates the ifdefs and comments to architecturally neutral
    versions.
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 85882d27510b586b26e8a95e28d0574953bcedb9
Author: Will Deacon <will.deacon@arm.com>
Date:   Fri Jul 6 15:45:39 2012 +0100

    ARM: 7449/1: use generic strnlen_user and strncpy_from_user functions
    
    This patch implements the word-at-a-time interface for ARM using the
    same algorithm as x86. We use the fls macro from ARMv5 onwards, where
    we have a clz instruction available which saves us a mov instruction
    when targetting Thumb-2. For older CPUs, we use the magic 0x0ff0001
    constant. Big-endian configurations make use of the implementation from
    asm-generic.
    
    With this implemented, we can replace our byte-at-a-time strnlen_user
    and strncpy_from_user functions with the optimised generic versions.
    
    Reviewed-by: Nicolas Pitre <nico@linaro.org>
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>
    
    lib: Sparc's strncpy_from_user is generic enough, move under lib/
    
    To use this, an architecture simply needs to:
    
    1) Provide a user_addr_max() implementation via asm/uaccess.h
    
    2) Add "select GENERIC_STRNCPY_FROM_USER" to their arch Kcnfig
    
    3) Remove the existing strncpy_from_user() implementation and symbol
       exports their architecture had.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>
    Acked-by: David Howells <dhowells@redhat.com>
    
    lib: add generic strnlen_user() function
    
    This adds a new generic optimized strnlen_user() function that uses the
    <asm/word-at-a-time.h> infrastructure to portably do efficient string
    handling.
    
    In many ways, strnlen is much simpler than strncpy, and in particular we
    can always pre-align the words we load from memory.  That means that all
    the worries about alignment etc are a non-issue, so this one can easily
    be used on any architecture.  You obviously do have to do the
    appropriate word-at-a-time.h macros.
    
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    
    kernel: Move REPEAT_BYTE definition into linux/kernel.h
    
    And make sure that everything using it explicitly includes
    that header file.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>
    
    word-at-a-time: make the interfaces truly generic
    
    This changes the interfaces in <asm/word-at-a-time.h> to be a bit more
    complicated, but a lot more generic.
    
    In particular, it allows us to really do the operations efficiently on
    both little-endian and big-endian machines, pretty much regardless of
    machine details.  For example, if you can rely on a fast population
    count instruction on your architecture, this will allow you to make your
    optimized <asm/word-at-a-time.h> file with that.
    
    NOTE! The "generic" version in include/asm-generic/word-at-a-time.h is
    not truly generic, it actually only works on big-endian.  Why? Because
    on little-endian the generic algorithms are wasteful, since you can
    inevitably do better. The x86 implementation is an example of that.
    
    (The only truly non-generic part of the asm-generic implementation is
    the "find_zero()" function, and you could make a little-endian version
    of it.  And if the Kbuild infrastructure allowed us to pick a particular
    header file, that would be lovely)
    
    The <asm/word-at-a-time.h> functions are as follows:
    
     - WORD_AT_A_TIME_CONSTANTS: specific constants that the algorithm
       uses.
    
     - has_zero(): take a word, and determine if it has a zero byte in it.
       It gets the word, the pointer to the constant pool, and a pointer to
       an intermediate "data" field it can set.
    
       This is the "quick-and-dirty" zero tester: it's what is run inside
       the hot loops.
    
     - "prep_zero_mask()": take the word, the data that has_zero() produced,
       and the constant pool, and generate an *exact* mask of which byte had
       the first zero.  This is run directly *outside* the loop, and allows
       the "has_zero()" function to answer the "is there a zero byte"
       question without necessarily getting exactly *which* byte is the
       first one to contain a zero.
    
       If you do multiple byte lookups concurrently (eg "hash_name()", which
       looks for both NUL and '/' bytes), after you've done the prep_zero_mask()
       phase, the result of those can be or'ed together to get the "either
       or" case.
    
     - The result from "prep_zero_mask()" can then be fed into "find_zero()"
       (to find the byte offset of the first byte that was zero) or into
       "zero_bytemask()" (to find the bytemask of the bytes preceding the
       zero byte).
    
       The existence of zero_bytemask() is optional, and is not necessary
       for the normal string routines.  But dentry name hashing needs it, so
       if you enable DENTRY_WORD_AT_A_TIME you need to expose it.
    
    This changes the generic strncpy_from_user() function and the dentry
    hashing functions to use these modified word-at-a-time interfaces.  This
    gets us back to the optimized state of the x86 strncpy that we lost in
    the previous commit when moving over to the generic version.
    
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    
    ARM: 7450/1: dcache: select DCACHE_WORD_ACCESS for little-endian ARMv6+ CPUs
    
    DCACHE_WORD_ACCESS uses the word-at-a-time API for optimised string
    comparisons in the vfs layer.
    
    This patch implements support for load_unaligned_zeropad for ARM CPUs
    with native support for unaligned memory accesses (v6+) when running
    little-endian.
    
    Change-Id: Ifdf8207f2581f93870eb0e627f5d12f97c4be7cc
    Reviewed-by: Nicolas Pitre <nico@linaro.org>
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>
    
    Conflicts:
    	arch/arm/include/asm/word-at-a-time.h
    	arch/arm/lib/Makefile
    	arch/openrisc/include/asm/Kbuild
    	arch/sparc/Kconfig
    	arch/sparc/include/asm/Kbuild
    	arch/sparc/lib/usercopy.c

commit fc6f792707d12228e29e16ab79e1e62e4d05b3df
Author: Suresh Siddha <suresh.b.siddha@intel.com>
Date:   Fri Apr 20 17:08:50 2012 -0700

    smp, idle: Allocate idle thread for each possible cpu during boot
    
    percpu areas are already allocated during boot for each possible cpu.
    percpu idle threads can be considered as an extension of the percpu areas,
    and allocate them for each possible cpu during boot.
    
    This will eliminate the need for workqueue based idle thread allocation.
    In future we can move the idle thread area into the percpu area too.
    
    Signed-off-by: Suresh Siddha <suresh.b.siddha@intel.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: David Rientjes <rientjes@google.com>
    Cc: venki@google.com
    Link: http://lkml.kernel.org/r/1334966930.28674.245.camel@sbsiddha-desk.sc.intel.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 00b70970260f6b0ed0d70f8243831e23be1b0e40
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri Apr 20 13:05:42 2012 +0000

    smp: Add task_struct argument to __cpu_up()
    
    Preparatory patch to make the idle thread allocation for secondary
    cpus generic.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Cc: Matt Turner <mattst88@gmail.com>
    Cc: Russell King <linux@arm.linux.org.uk>
    Cc: Mike Frysinger <vapier@gentoo.org>
    Cc: Jesper Nilsson <jesper.nilsson@axis.com>
    Cc: Richard Kuo <rkuo@codeaurora.org>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Hirokazu Takata <takata@linux-m32r.org>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: David Howells <dhowells@redhat.com>
    Cc: James E.J. Bottomley <jejb@parisc-linux.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Paul Mundt <lethal@linux-sh.org>
    Cc: David S. Miller <davem@davemloft.net>
    Cc: Chris Metcalf <cmetcalf@tilera.com>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: x86@kernel.org
    Link: http://lkml.kernel.org/r/20120420124556.964170564@linutronix.de
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>
    
    Conflicts:
    	arch/alpha/kernel/smp.c
    	arch/arm/kernel/smp.c
    	arch/blackfin/mach-common/smp.c
    	arch/cris/arch-v32/kernel/smp.c
    	arch/hexagon/kernel/smp.c
    	arch/ia64/kernel/smpboot.c
    	arch/m32r/kernel/smpboot.c
    	arch/mips/kernel/smp.c
    	arch/mn10300/kernel/smp.c
    	arch/parisc/kernel/smp.c
    	arch/powerpc/kernel/smp.c
    	arch/s390/include/asm/smp.h
    	arch/s390/kernel/smp.c
    	arch/sh/kernel/smp.c
    	arch/sparc/kernel/smp_32.c
    	arch/sparc/kernel/smp_64.c
    	arch/tile/kernel/smpboot.c

commit ce024a6e811120aae1c0dcadcabfbf8531a1d162
Author: Eric Dumazet <edumazet@google.com>
Date:   Thu Jan 10 15:26:34 2013 -0800

    softirq: reduce latencies
    
    In various network workloads, __do_softirq() latencies can be up
    to 20 ms if HZ=1000, and 200 ms if HZ=100.
    
    This is because we iterate 10 times in the softirq dispatcher,
    and some actions can consume a lot of cycles.
    
    This patch changes the fallback to ksoftirqd condition to :
    
    - A time limit of 2 ms.
    - need_resched() being set on current task
    
    When one of this condition is met, we wakeup ksoftirqd for further
    softirq processing if we still have pending softirqs.
    
    Using need_resched() as the only condition can trigger RCU stalls,
    as we can keep BH disabled for too long.
    
    I ran several benchmarks and got no significant difference in
    throughput, but a very significant reduction of latencies (one order
    of magnitude) :
    
    In following bench, 200 antagonist "netperf -t TCP_RR" are started in
    background, using all available cpus.
    
    Then we start one "netperf -t TCP_RR", bound to the cpu handling the NIC
    IRQ (hard+soft)
    
    Before patch :
    
    RT_LATENCY,MIN_LATENCY,MAX_LATENCY,P50_LATENCY,P90_LATENCY,P99_LATENCY,MEAN_LATENCY,STDDEV_LATENCY
    MIGRATED TCP REQUEST/RESPONSE TEST from 0.0.0.0 (0.0.0.0) port 0 AF_INET
    to 7.7.7.84 () port 0 AF_INET : first burst 0 : cpu bind
    RT_LATENCY=550110.424
    MIN_LATENCY=146858
    MAX_LATENCY=997109
    P50_LATENCY=305000
    P90_LATENCY=550000
    P99_LATENCY=710000
    MEAN_LATENCY=376989.12
    STDDEV_LATENCY=184046.92
    
    After patch :
    
    RT_LATENCY,MIN_LATENCY,MAX_LATENCY,P50_LATENCY,P90_LATENCY,P99_LATENCY,MEAN_LATENCY,STDDEV_LATENCY
    MIGRATED TCP REQUEST/RESPONSE TEST from 0.0.0.0 (0.0.0.0) port 0 AF_INET
    to 7.7.7.84 () port 0 AF_INET : first burst 0 : cpu bind
    RT_LATENCY=40545.492
    MIN_LATENCY=9834
    MAX_LATENCY=78366
    P50_LATENCY=33583
    P90_LATENCY=59000
    P99_LATENCY=69000
    MEAN_LATENCY=38364.67
    STDDEV_LATENCY=12865.26
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Cc: David Miller <davem@davemloft.net>
    Cc: Tom Herbert <therbert@google.com>
    Cc: Ben Hutchings <bhutchings@solarflare.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 2ba1aef00ebf9dbcefb1e2d6ca2cc03ae5b54942
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Jul 16 10:42:37 2012 +0000

    softirq: Use hotplug thread infrastructure
    
    [ paulmck: Call rcu_note_context_switch() with interrupts enabled. ]
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Reviewed-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Namhyung Kim <namhyung@kernel.org>
    Link: http://lkml.kernel.org/r/20120716103948.456416747@linutronix.de
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 258cc8c7ae0033c1faa0bdc7bd59abe740f9e805
Author: Stepan Moskovchenko <stepanm@codeaurora.org>
Date:   Mon May 19 19:07:33 2014 -0700

    smp: Allow booting a specific subset of CPUs
    
    In a heterogenous multiprocessor system, specifying the
    'maxcpus' parameter on the kernel command line does not
    provide sufficient control over which CPUs are brought
    online at kernel boot time, since CPUs may have nonuniform
    performance characteristics. Thus, we introduce a
    'boot_cpus' command line argument, allowing the user to
    explicitly specify the list of CPUs that shall be brought
    online during kernel boot.
    
    Signed-off-by: Stepan Moskovchenko <stepanm@codeaurora.org>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit ea7bd8dd5a41b986b85824c9ae32041c0633ba65
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Feb 26 18:44:33 2013 +0100

    stop_machine: Mark per cpu stopper enabled early
    
    commit 14e568e78 (stop_machine: Use smpboot threads) introduced the
    following regression:
    
    Before this commit the stopper enabled bit was set in the online
    notifier.
    
    CPU0				CPU1
    cpu_up
    				cpu online
    hotplug_notifier(ONLINE)
      stopper(CPU1)->enabled = true;
    ...
    stop_machine()
    
    The conversion to smpboot threads moved the enablement to the wakeup
    path of the parked thread. The majority of users seem to have the
    following working order:
    
    CPU0				CPU1
    cpu_up
    				cpu online
    unpark_threads()
      wakeup(stopper[CPU1])
    ....
    				stopper thread runs
    				  stopper(CPU1)->enabled = true;
    stop_machine()
    
    But Konrad and Sander have observed:
    
    CPU0				CPU1
    cpu_up
    				cpu online
    unpark_threads()
      wakeup(stopper[CPU1])
    ....
    stop_machine()
    				stopper thread runs
    				  stopper(CPU1)->enabled = true;
    
    Now the stop machinery kicks CPU0 into the stop loop, where it gets
    stuck forever because the queue code saw stopper(CPU1)->enabled ==
    false, so CPU0 waits for CPU1 to enter stomp_machine, but the CPU1
    stopper work got discarded due to enabled == false.
    
    Add a pre_unpark function to the smpboot thread descriptor and call it
    before waking the thread.
    
    This fixes the problem at hand, but the stop_machine code should be
    more robust. The stopper->enabled flag smells fishy at best.
    
    Thanks to Konrad for going through a loop of debug patches and
    providing the information to decode this issue.
    
    Reported-and-tested-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Reported-and-tested-by: Sander Eikelenboom <linux@eikelenboom.it>
    Cc: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Link: http://lkml.kernel.org/r/alpine.LFD.2.02.1302261843240.22263@ionos
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit f385b64568460ca43411385aa1a244030ac73c06
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Jan 31 12:11:14 2013 +0000

    stop_machine: Use smpboot threads
    
    Use the smpboot thread infrastructure. Mark the stopper thread
    selfparking and park it after it has finished the take_cpu_down()
    work.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Paul McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Cc: Arjan van de Veen <arjan@infradead.org>
    Cc: Paul Turner <pjt@google.com>
    Cc: Richard Weinberger <rw@linutronix.de>
    Cc: Magnus Damm <magnus.damm@gmail.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 0fc9044f9f0f7868b1c194c29ecd3d64982c34fa
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Jan 31 12:11:13 2013 +0000

    stop_machine: Store task reference in a separate per cpu variable
    
    To allow the stopper thread being managed by the smpboot thread
    infrastructure separate out the task storage from the stopper data
    structure.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Paul McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Cc: Arjan van de Veen <arjan@infradead.org>
    Cc: Paul Turner <pjt@google.com>
    Cc: Richard Weinberger <rw@linutronix.de>
    Cc: Magnus Damm <magnus.damm@gmail.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit eba7ad53399c0640d13b7452c9b130a3de79f230
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Apr 9 09:33:34 2013 +0200

    kthread: Prevent unpark race which puts threads on the wrong cpu
    
    The smpboot threads rely on the park/unpark mechanism which binds per
    cpu threads on a particular core. Though the functionality is racy:
    
    CPU0	       	 	CPU1  	     	    CPU2
    unpark(T)				    wake_up_process(T)
      clear(SHOULD_PARK)	T runs
    			leave parkme() due to !SHOULD_PARK
      bind_to(CPU2)		BUG_ON(wrong CPU)
    
    We cannot let the tasks move themself to the target CPU as one of
    those tasks is actually the migration thread itself, which requires
    that it starts running on the target cpu right away.
    
    The solution to this problem is to prevent wakeups in park mode which
    are not from unpark(). That way we can guarantee that the association
    of the task to the target cpu is working correctly.
    
    Add a new task state (TASK_PARKED) which prevents other wakeups and
    use this state explicitly for the unpark wakeup.
    
    Peter noticed: Also, since the task state is visible to userspace and
    all the parked tasks are still in the PID space, its a good hint in ps
    and friends that these tasks aren't really there for the moment.
    
    The migration thread has another related issue.
    
    CPU0	      	     	 CPU1
    Bring up CPU2
    create_thread(T)
    park(T)
     wait_for_completion()
    			 parkme()
    			 complete()
    sched_set_stop_task()
    			 schedule(TASK_PARKED)
    
    The sched_set_stop_task() call is issued while the task is on the
    runqueue of CPU1 and that confuses the hell out of the stop_task class
    on that cpu. So we need the same synchronizaion before
    sched_set_stop_task().
    
    Reported-by: Dave Jones <davej@redhat.com>
    Reported-and-tested-by: Dave Hansen <dave@sr71.net>
    Reported-and-tested-by: Borislav Petkov <bp@alien8.de>
    Acked-by: Peter Ziljstra <peterz@infradead.org>
    Cc: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Cc: dhillf@gmail.com
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: stable@vger.kernel.org
    Link: http://lkml.kernel.org/r/alpine.LFD.2.02.1304091635430.21884@ionos
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 036c291af3fc656abe5279dbe32a1a817b154063
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Jul 16 10:42:36 2012 +0000

    kthread: Implement park/unpark facility
    
    To avoid the full teardown/setup of per cpu kthreads in the case of
    cpu hot(un)plug, provide a facility which allows to put the kthread
    into a park position and unpark it when the cpu comes online again.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Namhyung Kim <namhyung@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Reviewed-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Reviewed-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Link: http://lkml.kernel.org/r/20120716103948.236618824@linutronix.de
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 6c952fd68b5e948f1f89884299be0c6efd058889
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Jan 31 12:11:12 2013 +0000

    smpboot: Allow selfparking per cpu threads
    
    The stop machine threads are still killed when a cpu goes offline. The
    reason is that the thread is used to bring the cpu down, so it can't
    be parked along with the other per cpu threads.
    
    Allow a per cpu thread to be excluded from automatic parking, so it
    can park itself once it's done
    
    Add a create callback function as well.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Paul McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Cc: Arjan van de Veen <arjan@infradead.org>
    Cc: Paul Turner <pjt@google.com>
    Cc: Richard Weinberger <rw@linutronix.de>
    Cc: Magnus Damm <magnus.damm@gmail.com>
    Link: http://lkml.kernel.org/r/20130131120741.553993267@linutronix.de
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 7d381790f31c3c8519201b842aee3c5a2f505adf
Author: Paul E. McKenney <paul.mckenney@linaro.org>
Date:   Thu Jul 12 01:55:54 2012 -0700

    hotplug: Fix UP bug in smpboot hotplug code
    
    Because kernel subsystems need their per-CPU kthreads on UP systems as
    well as on SMP systems, the smpboot hotplug kthread functions must be
    provided in UP builds as well as in SMP builds.  This commit therefore
    adds smpboot.c to UP builds and excludes irrelevant code via #ifdef.
    
    Signed-off-by: Paul E. McKenney <paul.mckenney@linaro.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 34b921af131cbd3cb7c74722ca762e8b511d935c
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Jul 16 10:42:36 2012 +0000

    smpboot: Provide infrastructure for percpu hotplug threads
    
    Provide a generic interface for setting up and tearing down percpu
    threads.
    
    On registration the threads for already online cpus are created and
    started. On deregistration (modules) the threads are stoppped.
    
    During hotplug operations the threads are created, started, parked and
    unparked. The datastructure for registration provides a pointer to
    percpu storage space and optional setup, cleanup, park, unpark
    functions. These functions are called when the thread state changes.
    
    Each implementation has to provide a function which is queried and
    returns whether the thread should run and the thread function itself.
    
    The core code handles all state transitions and avoids duplicated code
    in the call sites.
    
    [ paulmck: Preemption leak fix ]
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Reviewed-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Reviewed-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Namhyung Kim <namhyung@kernel.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 0ebf15af4f4a09cf51723dd3f890f0d2684d39ee
Author: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
Date:   Thu May 24 20:41:00 2012 +0530

    smpboot, idle: Fix comment mismatch over idle_threads_init()
    
    The comment over idle_threads_init() really talks about the functionality
    of idle_init(). Move that comment to idle_init(), and add a suitable
    comment over idle_threads_init().
    
    Signed-off-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Cc: suresh.b.siddha@intel.com
    Cc: venki@google.com
    Cc: nikunj@linux.vnet.ibm.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit aad89d59f4517eb43ae9324f7c2ffcefe99d5324
Author: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
Date:   Thu May 24 20:40:55 2012 +0530

    smpboot, idle: Optimize calls to smp_processor_id() in idle_threads_init()
    
    While trying to initialize idle threads for all cpus, idle_threads_init()
    calls smp_processor_id() in a loop, which is unnecessary. The intent
    is to initialize idle threads for all non-boot cpus. So just use a variable
    to note the boot cpu and use it in the loop.
    
    Signed-off-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Cc: suresh.b.siddha@intel.com
    Cc: venki@google.com
    Cc: nikunj@linux.vnet.ibm.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 0673dd884640d4c1479150365923e02c780e269f
Author: Suresh Siddha <suresh.b.siddha@intel.com>
Date:   Fri Apr 20 17:08:50 2012 -0700

    smp, idle: Allocate idle thread for each possible cpu during boot
    
    percpu areas are already allocated during boot for each possible cpu.
    percpu idle threads can be considered as an extension of the percpu areas,
    and allocate them for each possible cpu during boot.
    
    This will eliminate the need for workqueue based idle thread allocation.
    In future we can move the idle thread area into the percpu area too.
    
    [ tglx: Moved the loop into smpboot.c and added an error check when
      the init code failed to allocate an idle thread for a cpu which
      should be onlined ]
    
    Signed-off-by: Suresh Siddha <suresh.b.siddha@intel.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: David Rientjes <rientjes@google.com>
    Cc: venki@google.com
    Link: http://lkml.kernel.org/r/1334966930.28674.245.camel@sbsiddha-desk.sc.intel.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 3166cf5fa3e1cc53f4f5d810b4e1da5c6a53ceec
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri Apr 20 13:05:45 2012 +0000

    smp: Provide generic idle thread allocation
    
    All SMP architectures have magic to fork the idle task and to store it
    for reusage when cpu hotplug is enabled. Provide a generic
    infrastructure for it.
    
    Create/reinit the idle thread for the cpu which is brought up in the
    generic code and hand the thread pointer to the architecture code via
    __cpu_up().
    
    Note, that fork_idle() is called via a workqueue, because this
    guarantees that the idle thread does not get a reference to a user
    space VM. This can happen when the boot process did not bring up all
    possible cpus and a later cpu_up() is initiated via the sysfs
    interface. In that case fork_idle() would be called in the context of
    the user space task and take a reference on the user space VM.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Cc: Matt Turner <mattst88@gmail.com>
    Cc: Russell King <linux@arm.linux.org.uk>
    Cc: Mike Frysinger <vapier@gentoo.org>
    Cc: Jesper Nilsson <jesper.nilsson@axis.com>
    Cc: Richard Kuo <rkuo@codeaurora.org>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Hirokazu Takata <takata@linux-m32r.org>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: David Howells <dhowells@redhat.com>
    Cc: James E.J. Bottomley <jejb@parisc-linux.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Paul Mundt <lethal@linux-sh.org>
    Cc: David S. Miller <davem@davemloft.net>
    Cc: Chris Metcalf <cmetcalf@tilera.com>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: x86@kernel.org
    Acked-by: Venkatesh Pallipadi <venki@google.com>
    Link: http://lkml.kernel.org/r/20120420124557.102478630@linutronix.de
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit c3fda350b3042c49e7bcb7a5bebabc19692ca6c1
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri Apr 20 13:05:44 2012 +0000

    smp: Add generic smpboot facility
    
    Start a new file, which will hold SMP and CPU hotplug related generic
    infrastructure.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Cc: Matt Turner <mattst88@gmail.com>
    Cc: Russell King <linux@arm.linux.org.uk>
    Cc: Mike Frysinger <vapier@gentoo.org>
    Cc: Jesper Nilsson <jesper.nilsson@axis.com>
    Cc: Richard Kuo <rkuo@codeaurora.org>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Hirokazu Takata <takata@linux-m32r.org>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: David Howells <dhowells@redhat.com>
    Cc: James E.J. Bottomley <jejb@parisc-linux.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Paul Mundt <lethal@linux-sh.org>
    Cc: David S. Miller <davem@davemloft.net>
    Cc: Chris Metcalf <cmetcalf@tilera.com>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: x86@kernel.org
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit d2e2f52f1f7c9e3e46eb269c900f01b1d5ecc1fa
Author: Jussi Kivilinna <jussi.kivilinna@iki.fi>
Date:   Tue Jul 29 17:14:09 2014 +0100

    ARM: 8118/1: crypto: sha1/make use of common SHA-1 structures
    
    Common SHA-1 structures are defined in <crypto/sha.h> for code sharing.
    
    This patch changes SHA-1/ARM glue code to use these structures.
    
    Change-Id: Iedcc2210314d52d7e13bf5d2b535052a18f04e49
    Acked-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Jussi Kivilinna <jussi.kivilinna@iki.fi>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

commit d1fbdb17d0d745ce28981e8a016867ffbaf43621
Author: Mikulas Patocka <mpatocka@redhat.com>
Date:   Fri Jul 25 19:42:30 2014 -0400

    crypto: arm-aes - fix encryption of unaligned data
    
    Fix the same alignment bug as in arm64 - we need to pass residue
    unprocessed bytes as the last argument to blkcipher_walk_done.
    
    Change-Id: I8d49b8a190327b46801a3db4884e2b309138525b
    Signed-off-by: Mikulas Patocka <mpatocka@redhat.com>
    Cc: stable@vger.kernel.org	# 3.13+
    Acked-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>

commit 10e1afb57034e23df9ad31ff4271eac94ad69cac
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Thu Jan 2 17:14:45 2014 +0000

    CRYPTO: Fix more AES build errors
    
    Building a multi-arch kernel results in:
    
    arch/arm/crypto/built-in.o: In function `aesbs_xts_decrypt':
    sha1_glue.c:(.text+0x15c8): undefined reference to `bsaes_xts_decrypt'
    arch/arm/crypto/built-in.o: In function `aesbs_xts_encrypt':
    sha1_glue.c:(.text+0x1664): undefined reference to `bsaes_xts_encrypt'
    arch/arm/crypto/built-in.o: In function `aesbs_ctr_encrypt':
    sha1_glue.c:(.text+0x184c): undefined reference to `bsaes_ctr32_encrypt_blocks'
    arch/arm/crypto/built-in.o: In function `aesbs_cbc_decrypt':
    sha1_glue.c:(.text+0x19b4): undefined reference to `bsaes_cbc_encrypt'
    
    This code is already runtime-conditional on NEON being supported, so
    there's no point compiling it out depending on the minimum build
    architecture.
    
    Change-Id: I219dc496b3ad60754f95a6db2a71ce73d037a6e0
    Acked-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

commit b4ba77d2b69bee51ffa34469e51ac7658c218180
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Sun Sep 15 17:10:43 2013 +0200

    ARM: move AES typedefs and function prototypes to separate header
    
    Put the struct definitions for AES keys and the asm function prototypes in a
    separate header and export the asm functions from the module.
    This allows other drivers to use them directly.
    
    Change-Id: I5ce0cf285e2981755adb55b66a846eb738cedd58
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>

commit 1a9ba83b9a0e8a621d05ddbec6850be7432cc014
Author: Jussi Kivilinna <jussi.kivilinna@iki.fi>
Date:   Tue Jul 29 17:15:24 2014 +0100

    ARM: 8120/1: crypto: sha512: add ARM NEON implementation
    
    This patch adds ARM NEON assembly implementation of SHA-512 and SHA-384
    algorithms.
    
    tcrypt benchmark results on Cortex-A8, sha512-generic vs sha512-neon-asm:
    
    block-size      bytes/update    old-vs-new
    16              16              2.99x
    64              16              2.67x
    64              64              3.00x
    256             16              2.64x
    256             64              3.06x
    256             256             3.33x
    1024            16              2.53x
    1024            256             3.39x
    1024            1024            3.52x
    2048            16              2.50x
    2048            256             3.41x
    2048            1024            3.54x
    2048            2048            3.57x
    4096            16              2.49x
    4096            256             3.42x
    4096            1024            3.56x
    4096            4096            3.59x
    8192            16              2.48x
    8192            256             3.42x
    8192            1024            3.56x
    8192            4096            3.60x
    8192            8192            3.60x
    
    Change-Id: Ibc318f8c9136507f57e2bb8d8f51b4714d8ed70b
    Acked-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Tested-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Jussi Kivilinna <jussi.kivilinna@iki.fi>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>
    Signed-off-by: Iliyan Malchev <malchev@google.com>

commit 4100b03415a47f65759a7748de225d11859196f1
Author: Jussi Kivilinna <jussi.kivilinna@iki.fi>
Date:   Tue Jul 29 17:14:14 2014 +0100

    ARM: 8119/1: crypto: sha1: add ARM NEON implementation
    
    This patch adds ARM NEON assembly implementation of SHA-1 algorithm.
    
    tcrypt benchmark results on Cortex-A8, sha1-arm-asm vs sha1-neon-asm:
    
    block-size      bytes/update    old-vs-new
    16              16              1.04x
    64              16              1.02x
    64              64              1.05x
    256             16              1.03x
    256             64              1.04x
    256             256             1.30x
    1024            16              1.03x
    1024            256             1.36x
    1024            1024            1.52x
    2048            16              1.03x
    2048            256             1.39x
    2048            1024            1.55x
    2048            2048            1.59x
    4096            16              1.03x
    4096            256             1.40x
    4096            1024            1.57x
    4096            4096            1.62x
    8192            16              1.03x
    8192            256             1.40x
    8192            1024            1.58x
    8192            4096            1.63x
    8192            8192            1.63x
    
    Change-Id: I6df3c0a9ba8d450d034cf78785b6ce80a72bef4a
    Acked-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Tested-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Jussi Kivilinna <jussi.kivilinna@iki.fi>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>
    Signed-off-by: Iliyan Malchev <malchev@google.com>

commit 173fbb5d02e5ddc95f659c627ebfcbb7d8ef9143
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Mon Sep 16 18:31:38 2013 +0200

    ARM: add support for bit sliced AES using NEON instructions
    
    Bit sliced AES gives around 45% speedup on Cortex-A15 for encryption
    and around 25% for decryption. This implementation of the AES algorithm
    does not rely on any lookup tables so it is believed to be invulnerable
    to cache timing attacks.
    
    This algorithm processes up to 8 blocks in parallel in constant time. This
    means that it is not usable by chaining modes that are strictly sequential
    in nature, such as CBC encryption. CBC decryption, however, can benefit from
    this implementation and runs about 25% faster. The other chaining modes
    implemented in this module, XTS and CTR, can execute fully in parallel in
    both directions.
    
    The core code has been adopted from the OpenSSL project (in collaboration
    with the original author, on cc). For ease of maintenance, this version is
    identical to the upstream OpenSSL code, i.e., all modifications that were
    required to make it suitable for inclusion into the kernel have been made
    upstream. The original can be found here:
    
        http://git.openssl.org/gitweb/?p=openssl.git;a=commit;h=6f6a6130
    
    Note to integrators:
    While this implementation is significantly faster than the existing table
    based ones (generic or ARM asm), especially in CTR mode, the effects on
    power efficiency are unclear as of yet. This code does fundamentally more
    work, by calculating values that the table based code obtains by a simple
    lookup; only by doing all of that work in a SIMD fashion, it manages to
    perform better.
    
    Change-Id: I936dc7142b91133c55c7cf0af6a565d219d62e11
    Cc: Andy Polyakov <appro@openssl.org>
    Acked-by: Nicolas Pitre <nico@linaro.org>
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>

commit 52a562fd83c999e75f6e4c479333a5e240cded8f
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Apr 15 02:44:06 2015 +0200

    block: fiops bias sync workload

commit a868bce50fd5b67e09ade1a029e33076226be1f8
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Apr 15 02:37:02 2015 +0200

     block: zen: Optimize usage for android
    
    * Source - Dorimanx
    
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit c354e91f9fe575e83c94418eb87c6da625513e5e
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Apr 15 02:20:19 2015 +0200

    fs: fix merge problem

commit 13963c95eefed25b175ccb9182577e07f2f4cc9b
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Mon Apr 13 13:21:11 2015 +0200

    f2fs: fix f2fs

commit d99220c7d97a4ce3317e1bc43a7c25b5b8181236
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Mon Apr 13 19:40:26 2015 +0200

    sound: disable checksum varification

commit cce0d2e35420dc6d2e183cb6faf9f1745c42d928
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Mon Apr 13 14:08:58 2015 +0200

    config: enable glove

commit 492ea5e770dc8b596421802a3479e6cc64b7753e
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Apr 1 12:01:36 2015 +0200

     slimbus: Optimized overclocking of master component
    
    * Change root frequency to optimal value (11 was default. Credits to poondog.)
    * Add 44khz and 48khz to default SLIM RATES
    
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 3b55e06b23c9df454a02c6be716380ad032e38b6
Author: Sagar Dharia <sdharia@codeaurora.org>
Date:   Tue Jan 22 17:51:41 2013 -0700

    slimbus: Add slimbus device driver unregistration function
    
    slim_driver_unregister is added to reverse effects of
    slim_driver_register. That way a slimbus device driver can be
    registered when driver module is loaded, and unregistered when
    the driver module is unloaded.
    
    CRs-Fixed: 441540
    Change-Id: Iae64f84c99d5027c9b55c3fde5fe71c6a550354e
    Signed-off-by: Sagar Dharia <sdharia@codeaurora.org>

commit 4b248eff19e562c239f3fb85882d14744d7c5ef1
Author: Joonwoo Park <joonwoop@codeaurora.org>
Date:   Tue Aug 28 15:26:11 2012 -0700

    slimbus: Defer probing slimbus until ADSP is ready
    
    If slimbus master resides in ADSP, slimbus driver has to wait until it's
    up and running.  Do probe deferral until ADSP is ready.
    
    Change-Id: I31ba4e270331c3b5250758f604d0d0cde9537427
    Signed-off-by: Joonwoo Park <joonwoop@codeaurora.org>
    
    Conflicts:
    	drivers/slimbus/slim-msm-ctrl.c

commit 0a7fe99a535ed0434fd92c35b953285ae8c76ea0
Author: Sagar Dharia <sdharia@codeaurora.org>
Date:   Tue Aug 28 12:20:49 2012 -0600

    slimbus: Add slave devices to controller after controller HW is up
    
    Register controller is only responsible for making sure slimbus
    controller structure is initialized with framework. This should not
    assume that slimbus controller's hardware programming is done (e.g. if
    slave's probe is trying to get logical address, then that will fail if
    controller is registered, but slimbus hardware programming is not
    complete, since probe is called synchrounously when device is added.
    So this API separates register controller from slave device
    registration. It should be called by controller when its hardware
    programming is complete.
    
    Change-Id: I52fa5567fc072951c89f99c70507aa3b1e4e22af
    Signed-off-by: Sagar Dharia <sdharia@codeaurora.org>

commit 2fa32324aab81d4e06b30239cd6b071b5032e35e
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Mon Mar 30 17:32:26 2015 +0200

    Revert everything back to stock

commit 5f10593b14f1c38c39f16c708042300eaec40947
Merge: d9ce667 d96ee6d
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Mon Mar 30 16:45:09 2015 +0200

    Merge branch 'master' of https://github.com/Tkkg1994/Hulk-Kernel-V2
    
    Conflicts:
    	arch/arm/configs/jf_defconfig

commit d9ce667df9b8d3e76e32cd33ab0d281f135990e7
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Mon Mar 30 16:44:20 2015 +0200

    config: changes

commit d96ee6d93af50f4320558e2aa8a828706095681b
Author: Hybridmax <hybridmax95@gmail.com>
Date:   Mon Mar 30 15:37:45 2015 +0200

    Revert "drivers/thermal: Updated intelli_thermal to v2.0 [faux123]"

commit a6274bb19cb91962af9e623a67392e04b12ebbed
Author: Hybridmax <hybridmax95@gmail.com>
Date:   Mon Mar 30 15:03:13 2015 +0200

    drivers/thermal: Updated intelli_thermal to v2.0 [faux123]

commit 3889a9420e70dc1f17681ab54727c1c6b9fa2d2e
Author: Luca Grifo <luca.grifo@outlook.com>
Date:   Mon Mar 30 08:54:54 2015 +0200

    config: fix all other doubles of definitions

commit 5c955ea2925f536fbc1d9ef2c2a92e388677e498
Author: Luca Grifo <luca.grifo@outlook.com>
Date:   Mon Mar 30 08:40:18 2015 +0200

    config: Fix intelli-thermal set two times

commit f909f5c24fb7c0f4ffd99c11aa1ba2ce3abad924
Author: Luca Grifo <luca.grifo@outlook.com>
Date:   Mon Mar 30 08:05:52 2015 +0200

    Update 0hulk_jf_defconfig

commit 7a0256f5342ecb240517c1da6867cdddc1e75b7d
Author: Hybridmax <hybridmax95@gmail.com>
Date:   Mon Mar 30 02:44:06 2015 +0200

    Revert "sound: Updated Faux Sound Control to v3.6"

commit 56ed23ac973a038bbaa39a6a288a587264f1c9f0
Author: Hybridmax <hybridmax95@gmail.com>
Date:   Mon Mar 30 02:41:51 2015 +0200

    sound: Updated Faux Sound Control to v3.6

commit c88663becc67a4885194e46a6c512877b7a841e8
Author: Hybridmax <hybridmax95@gmail.com>
Date:   Mon Mar 30 01:56:10 2015 +0200

    config: Update to get TCP Congestion Controllers working

commit f82b5225936dcb0de7c73e9df655e1f60db536e4
Author: Hybridmax <hybridmax95@gmail.com>
Date:   Mon Mar 30 01:35:58 2015 +0200

    build.sh: Updated Kernel Name

commit c7a39ec5cf5894af48e400f87e1a232abfc92b66
Author: Hybridmax <hybridmax95@gmail.com>
Date:   Mon Mar 30 01:33:31 2015 +0200

    Added new Kernel Build Script

commit c489be3bc1df64438f3c675de4c8325e0d775e79
Author: Hybridmax <hybridmax95@gmail.com>
Date:   Mon Mar 30 01:31:44 2015 +0200

    Added .gitignore

commit a7b78baf98a2effd158473fc020512ce8bbc7760
Author: Hybridmax <hybridmax95@gmail.com>
Date:   Mon Mar 30 01:31:19 2015 +0200

    arch/arm/configs: Added hulk_jf_defconfig

commit c0b4b6b143bea6c2b0fefbeb97ed07ea1769b79b
Author: Hybridmax <hybridmax95@gmail.com>
Date:   Mon Mar 30 00:08:21 2015 +0200

    arch/arm/configs: Switch back to stock jf_defconfig

commit 85465d04d3c81fd160f570dc54e46bc7a4537a7d
Author: Hybridmax <hybridmax95@gmail.com>
Date:   Sun Mar 29 23:38:36 2015 +0200

    Revert "block: Updated BFQ Iosched to v7r6 for 3.4.0"

commit 52a5ec7b8c2bc27ed710706cbe7327361d021162
Author: Hybridmax <hybridmax95@gmail.com>
Date:   Sun Mar 29 23:37:11 2015 +0200

    block: Updated BFQ Iosched to v7r6 for 3.4.0

commit d2122ae5efc96d555b6ab48290192ec23c890a61
Author: Hybridmax <hybridmax95@gmail.com>
Date:   Sun Mar 29 22:27:17 2015 +0200

    cpufreq: Updated zzmoove governor to v1.0-beta6a (official version)

commit 1b5251c2b92ec1d261f81f52464fb1f449b3a379
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun Mar 29 18:39:23 2015 +0200

    Revert "net: ipv6: autoconf routes into per-device tables"
    
    This reverts commit 354c06713ecd618158a4533a29e94209239f7d3d.

commit 354c06713ecd618158a4533a29e94209239f7d3d
Author: Lorenzo Colitti <lorenzo@google.com>
Date:   Wed Mar 26 19:35:41 2014 +0900

    net: ipv6: autoconf routes into per-device tables
    
    Currently, IPv6 router discovery always puts routes into
    RT6_TABLE_MAIN. This causes problems for connection managers
    that want to support multiple simultaneous network connections
    and want control over which one is used by default (e.g., wifi
    and wired).
    
    To work around this connection managers typically take the routes
    they prefer and copy them to static routes with low metrics in
    the main table. This puts the burden on the connection manager
    to watch netlink to see if the routes have changed, delete the
    routes when their lifetime expires, etc.
    
    Instead, this patch adds a per-interface sysctl to have the
    kernel put autoconf routes into different tables. This allows
    each interface to have its own autoconf table, and choosing the
    default interface (or using different interfaces at the same
    time for different types of traffic) can be done using
    appropriate ip rules.
    
    The sysctl behaves as follows:
    
    - = 0: default. Put routes into RT6_TABLE_MAIN as before.
    - > 0: manual. Put routes into the specified table.
    - < 0: automatic. Add the absolute value of the sysctl to the
           device's ifindex, and use that table.
    
    The automatic mode is most useful in conjunction with
    net.ipv6.conf.default.accept_ra_rt_table. A connection manager
    or distribution could set it to, say, -100 on boot, and
    thereafter just use IP rules.
    
    Change-Id: I82d16e3737d9cdfa6489e649e247894d0d60cbb1
    Signed-off-by: Lorenzo Colitti <lorenzo@google.com>
    
    Conflicts:
    	include/uapi/linux/ipv6.h
    
    Conflicts:
    	include/uapi/linux/ipv6.h
    	net/ipv6/route.c

commit 5479a275f17f9df4b84b1b42d38850da6ebaa55f
Author: Lorenzo Colitti <lorenzo@google.com>
Date:   Thu Nov 20 10:46:11 2014 +0900

    net: ipv6: Add a sysctl to make optimistic addresses useful candidates
    
    Add a sysctl that causes an interface's optimistic addresses
    to be considered equivalent to other non-deprecated addresses
    for source address selection purposes.  Preferred addresses
    will still take precedence over optimistic addresses, subject
    to other ranking in the source address selection algorithm.
    
    This is useful where different interfaces are connected to
    different networks from different ISPs (e.g., a cell network
    and a home wifi network).
    
    The current behaviour complies with RFC 3484/6724, and it
    makes sense if the host has only one interface, or has
    multiple interfaces on the same network (same or cooperating
    administrative domain(s), but not in the multiple distinct
    networks case.
    
    For example, if a mobile device has an IPv6 address on an LTE
    network and then connects to IPv6-enabled wifi, while the wifi
    IPv6 address is undergoing DAD, IPv6 connections will try use
    the wifi default route with the LTE IPv6 address, and will get
    stuck until they time out.
    
    Also, because optimistic nodes can receive frames, issue
    an RTM_NEWADDR as soon as DAD starts (with the IFA_F_OPTIMSTIC
    flag appropriately set).  A second RTM_NEWADDR is sent if DAD
    completes (the address flags have changed), otherwise an
    RTM_DELADDR is sent.
    
    Also: add an entry in ip-sysctl.txt for optimistic_dad.
    
    [cherry-pick of net-next 7fd2561e4ebdd070ebba6d3326c4c5b13942323f]
    
    Signed-off-by: Erik Kline <ek@google.com>
    Acked-by: Lorenzo Colitti <lorenzo@google.com>
    Acked-by: Hannes Frederic Sowa <hannes@stressinduktion.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>
    Bug: 17769720
    Bug: 18180674
    Change-Id: Ic7e50781c607e1f3a492d9ce7395946efb95c533
    
    Conflicts:
    	Documentation/networking/ip-sysctl.txt
    	include/uapi/linux/ipv6.h

commit 689b0f50639270d842e426cfd4df0ae327841042
Author: Erik Kline <ek@google.com>
Date:   Fri Dec 5 19:45:10 2014 +0900

    net: ipv6: allow choosing optimistic addresses with use_optimistic
    
    The use_optimistic sysctl makes optimistic IPv6 addresses
    equivalent to preferred addresses for source address selection
    (e.g., when calling connect()), but it does not allow an
    application to bind to optimistic addresses. This behaviour is
    inconsistent - for example, it doesn't make sense for bind() to
    an optimistic address fail with EADDRNOTAVAIL, but connect() to
    choose that address outgoing address on the same socket.
    
    Bug: 17769720
    Bug: 18609055
    Change-Id: I9de0d6c92ac45e29d28e318ac626c71806666f13
    Signed-off-by: Erik Kline <ek@google.com>
    Signed-off-by: Lorenzo Colitti <lorenzo@google.com>

commit f791d85bd1df690ecab80d7213617da956616fd0
Author: Paul Reioux <reioux@gmail.com>
Date:   Thu Jan 15 03:01:02 2015 -0600

    Simple GPU Algorithm: Initial coding for devfreq based Adreno Drivers
    
    This is an open source user configurable simple GPU Control Algorithm used to
    replace the closed sourced Qualcomm TrustZone GPU controller
    
    Copyright 2011~2015
    
    Signed-off-by: Paul Reioux <reioux@gmail.com>
    
    Conflicts:
    	drivers/devfreq/Kconfig
    	drivers/devfreq/Makefile
    	drivers/devfreq/governor_msm_adreno_tz.c

commit 068ebc90215a12660ff9971ea47be5cc8d8b6771
Author: Jeremy Gebben <jgebben@codeaurora.org>
Date:   Wed Mar 13 13:54:15 2013 -0600

    PM / devfreq: add msm-adreno-tz governor
    
    This governor can be used to control adreno GPUs.
    It is unlikely to be useful for other devices.
    
    Change-Id: Icf481322454b814d2f41019f2f01286062409952
    Signed-off-by: Jeremy Gebben <jgebben@codeaurora.org>
    Signed-off-by: Vladimir Razgulin <vrazguli@codeaurora.org>
    
    Conflicts:
    	drivers/devfreq/governor_msm_adreno_tz.c

commit 2ef225ae209a1ca5cadc8bc17622e627be6ca769
Author: Lucille Sylvester <lsylvest@codeaurora.org>
Date:   Thu Jun 27 10:56:18 2013 -0600

    PM / devfreq: Allow the governors to set the target flag
    
    The devfreq framework calls a frequency targeting function with
    a flag parameter.  Allow the governors to influence that parameter.
    
    Change-Id: I4058bd9dcd027dd246ccdb90d25c68f1dc055901
    Signed-off-by: Lucille Sylvester <lsylvest@codeaurora.org>
    
    Conflicts:
    	drivers/devfreq/governor_msm_adreno_tz.c
    	include/linux/devfreq.h

commit b757e49c9ff25e84d2c5c3ea7b8f5e3d4ea5a9f9
Merge: 980ebb5 9eabad1
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sat Mar 28 18:05:54 2015 +0100

    Merge branch 'master' of https://github.com/Tkkg1994/Hulk-Kernel-V2

commit 980ebb57b4fde8dd9f001bf84f10705d34a0bd58
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sat Mar 28 17:58:39 2015 +0100

     PM / devfreq: Add new flag to do simple clock scaling
    
    Add new flag "simple_scaling" to on demand governor so that
    the clocks can be scaled up only when the load is more than
    up threshold and can be scaled down only when the load is less
    than down differential data as provided within
    struct devfreq_simple_ondemand_data.
    
    Change-Id: Ibc6ab6297c1b64b6e6eaaa76d735d0b9ae0f6477
    Signed-off-by: Sahitya Tummala <stummala@codeaurora.org>

commit 65d25bd3360368915a454119f419f84b37d06b5c
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sat Mar 28 17:51:43 2015 +0100

     PM: devfreq: Fix simple_ondemand crashing on startup
    
    simple_ondemands private data must be set to NULL, otherwise we would
    run into a NULL pointer in kgsl_devfreq_get_dev_status().
    
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit e15a226d5ef197c2b502d204dff897b4cd680e42
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sat Mar 28 14:17:01 2015 +0100

    Add new kernel build script

commit 9eabad129af16d43c70632b9cf91fdf12755ab2e
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sat Mar 28 14:17:01 2015 +0100

    Add new kernel build script

commit 4eded90614d4f92e48bc64b32a72f60f3f3b6ea1
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Fri Mar 27 20:41:59 2015 +0100

    cpufreq: add missing default freqs

commit 846635d690231503d875f06977a781abfa4f1e62
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Fri Mar 27 20:24:02 2015 +0100

    Fix zzmoove compiling problem (thanks to neobuddy) and lionheart problem

commit 3290f917a9dbb4d2bf82d4b4ed272f6d4896f81e
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Fri Mar 27 17:09:51 2015 +0100

    Makefile: switch to latest Cortex A15 optimized toolchain

commit c11e00c7dadd323debc439e4653126c3318f98ae
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Mar 26 23:43:35 2015 +0100

    Fixed problem with fauxsound 3.x

commit 4ec8294b75f99cba148a550d4a2023f90157ab24
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Mar 26 14:27:24 2015 +0100

    Fixed a compiling error. Added defconfigs for TW 4.4.2 Kernel. Fixed intelli-thermal problem while compiling

commit d1d10380ea5705ad77329463ded2851317a1c2ec
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Mar 25 20:37:30 2015 +0100

     vfs: allow custom EOF in generic_file_llseek code
    
    For ext3/4 htree directories, using the vfs llseek function with
    SEEK_END goes to i_size like for any other file, but in reality
    we want the maximum possible hash value.  Recent changes
    in ext4 have cut & pasted generic_file_llseek() back into fs/ext4/dir.c,
    but replicating this core code seems like a bad idea, especially
    since the copy has already diverged from the vfs.
    
    This patch updates generic_file_llseek_size to accept
    both a custom maximum offset, and a custom EOF position.  With this
    in place, ext4_dir_llseek can pass in the appropriate maximum hash
    position for both maxsize and eof, and get what it wants.
    
    As far as I know, this does not fix any bugs - nfs in the kernel
    doesn't use SEEK_END, and I don't know of any user who does.  But
    some ext4 folks seem keen on doing the right thing here, and I can't
    really argue.
    
    (Patch also fixes up some comments slightly)
    
    Signed-off-by: Eric Sandeen <sandeen@redhat.com>
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

commit 1ab90dbd50977610549dd761eb02ceff2255392c
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Mar 25 19:26:21 2015 +0100

    Revert "nfs_lookup_verify_inode() - nd is *always* non-NULL here"
    
    This reverts commit e5592fe42f60a690d27d4f86ef46fb89efe8ad1f.
    
    Conflicts:
    	fs/nfs/dir.c

commit e69f65de040b625683b054e2bdb51437a691acc8
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Mar 25 19:23:54 2015 +0100

    Revert "namei.c: let follow_link() do put_link() on failure"
    
    This reverts commit 895577d2f837bef6437255c26553f0a05f7a0fab.

commit b909012b625aa12ffa7602edaa2fee94520f71d9
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Mar 25 19:23:26 2015 +0100

    Revert "coda: use list_for_each_entry"
    
    This reverts commit fde5b4e8fe838253ee1e099334a12469d1f561d8.

commit 53f6943b3e392df8f019442df04238cf3f95a67d
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Mar 25 19:22:53 2015 +0100

    Revert "ext4: get rid of open-coded d_find_any_alias()"
    
    This reverts commit 0d87e3de709201aac31e4d16e9e9d69dc434bdd4.

commit eba8d31931b245483d89f1455862a0a1dbf37567
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Mar 25 19:22:28 2015 +0100

    Revert "ocfs2: use list_for_each_entry in ocfs2_find_local_alias()"
    
    This reverts commit 9f55296e144ba92c2a0083f6338d6ed7e1aabe86.

commit d3a01f91a4b21ed62b6afe006732c0d573f63354
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Mar 25 19:21:33 2015 +0100

    Revert "affs: get rid of open-coded list_for_each_entry()"
    
    This reverts commit 73eef687f188ccdadd400c72d5604667087092b6.
    
    Conflicts:
    	fs/affs/amigaffs.c
    
    ALSO reverts: " affs: unobfuscate affs_fix_dcache()"
    
    and add a comment on what it's doing Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

commit bf7a613fb89c17be460a95e241899a83ca85f36c
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Mar 25 19:18:34 2015 +0100

    Revert "vfs: update documentation on ->i_dentry handling"
    
    This reverts commit c38e12ed95b6843db3d69710c220853b1d22f068.

commit 2356c3ddbe90bf1a9a3cb0ac038c0db1ec301709
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Mar 25 19:18:16 2015 +0100

    Revert "cifs: don't bother with ->i_dentry in ->destroy_inode()"
    
    This reverts commit a621430c0faaa5a4e29141fef0dd5be03dce5772.

commit b199b344a01ee058cbca06eb5ef448e02a2a3899
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Mar 25 19:18:01 2015 +0100

    Revert "adfs: don't bother with ->i_dentry in ->destroy_inode()"
    
    This reverts commit 6733cde74866a051d59d627b55293f2b17325615.

commit 5262398257a5e05e1f977edd12e673273a362957
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Mar 25 19:17:51 2015 +0100

    Revert "qnx6: don't bother with ->i_dentry in inode-freeing callback"
    
    This reverts commit b1f7c2c06cf1ca8aaa76da2a0c83afa6c82fbaf9.

commit b50b757cc58d3e009026599dc89ca8ebaa1b3fbc
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Mar 25 19:17:22 2015 +0100

    Revert "get rid of magic in proc_namespace.c"
    
    This reverts commit 7048a9180db4fd4a416b9b93bd019a774e65d836.

commit 9e6f6855abeb1576e6dd5fdf3f8382c0fda7f927
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Mar 25 17:53:28 2015 +0100

     vfs: read file_handle only once in handle_to_path
    
    We used to read file_handle twice. Once to get the amount of extra bytes, and
    once to fetch the entire structure.
    
    This may be problematic since we do size verifications only after the first
    read, so if the number of extra bytes changes in userspace between the first
    and second calls, we'll have an incoherent view of file_handle.
    
    Instead, read the constant size once, and copy that over to the final
    structure without having to re-read it again.
    
    Signed-off-by: Sasha Levin <sasha.levin@oracle.com>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 18a23e689ae0b9d0e9f71c77d879e4adab14caaa
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Mar 25 17:27:19 2015 +0100

    FS: fixes to fs/ext4/extents.c && fs/f2fs/file.c after commits.

commit 3cc8cd8fb7bf455cc1d57e8d7c13b40dca2e6825
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Mar 25 15:45:54 2015 +0100

    config: disable dm_req_crypt

commit b0ca2b1c39f3993a2f6c1ab60836a8048731ee3b
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Mar 25 14:46:16 2015 +0100

     crypto: msm: Add support for multiple qcrypto devices
    
    Expose APIs to enable qcrypto client to select any specific qcrypto
    instance.
    
    Change-Id: Ia96f7fa0f15216c0656aa6dc495db350b3c574a8
    Signed-off-by: Mona Hossain <mhossain@codeaurora.org>
    Signed-off-by: Zhen Kong <zkong@codeaurora.org>

commit 74994421d8406028e29a295c5036b0871068d7e1
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Mar 25 14:35:43 2015 +0100

     crypto: msm: Add support for setting ctx flags
    
    Currently there is no way of instructing the crypto driver
    to use HW specific capabilities. The kernel crypto framework
    does not expose any API to set any specific HW feature flags:
    ex: use of HW key,  use of PIPE keys, setting XTS data unit
    size.
    
    The driver currently uses a specific key pattern to decide
    when to instruct HW to use HW_KEY (uses key pattern of all
    zeros) and when to use PIPE_KEY (uses key pattern of all 0xF's).
    This limits the use of the specific key pattern from ever being
    used for any crypto operation.
    
    Adding support for crypto client to set flags to indicate when
    to use HW KEY, PIPE KEY, size removes the limitation on the key
    patterns that can be used by the driver.
    
    Adding XTS data unit size flags allows the client to take
    advantage of the hardware capability to set data unit size
    for XTS operations.
    
    Change-Id: I0d1417a6d3c990e0750dbd90e3816f45fec8d693
    Signed-off-by: Mona Hossain <mhossain@codeaurora.org>

commit 641669e41e5042f4c2492c96fea8c4d805c9aad4
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Mar 25 13:36:29 2015 +0100

    Revert "vfs: switch i_dentry/d_alias to hlist"
    
    This reverts commit 447f64eb56189f92c134958f4e799e55ebd72bfa.

commit 237a5fffb6343e6bde0f7662f86b60560461270f
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Mar 25 12:56:49 2015 +0100

     list: introduce list_first_entry_or_null and fixing some stuff
    
    non-rcu variant of list_first_or_null_rcu
    
    Signed-off-by: Jiri Pirko <jiri@resnulli.us>
    Signed-off-by: David S. Miller <davem@davemloft.net>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 057f6bfaa4506793b06c0c4abd1a61d8c1b9323a
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Mar 25 12:07:15 2015 +0100

    Revert "switch nfs_lookup_check_intent() away from nameidata"
    
    This reverts commit 921e394258ad50d23bbbcdaa363dc610a12fc2fe.
    
    Conflicts:
    	fs/nfs/dir.c

commit ce461b9f33418635e1f7d9cad942aa4213f971c5
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Mar 25 12:05:24 2015 +0100

    Revert "fs/nfs/dir.c: switch to passing nd->flags instead of nd wherever possible"
    
    This reverts commit 7e6d61648dedc08a5e4691f2023f4ab47f172132.

commit 2ab6381578f30e92daee8f522a3554579675b82b
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Mar 25 12:04:56 2015 +0100

    Revert the new helper "replace_fd()" back

commit fa0f416dfabb194a0bfb021d4df44bd2a1a799f6
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Mar 25 11:53:31 2015 +0100

    Revert "stop passing nameidata to ->lookup()"
    
    This reverts commit ccac6a29d794fd584ff7fe13378e73f77261da8f.

commit b79f7481efe34a346f532f4d6e8de287a84b8709
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Mar 25 11:53:22 2015 +0100

    Fix merge problem

commit a79ce8e3479466153c3525f55097035c4fce1acc
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Mar 25 11:50:59 2015 +0100

    Revert "fs/namei.c: don't pass nameidata to d_revalidate()"
    
    This reverts commit 714ddaecfe1563bd3cd9c72488619824d87978b3.
    
    Conflicts:
    	fs/namei.c

commit 58bb6d6bccaf189fe37501f20884bcbbe7c48cb2
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Mar 25 11:50:27 2015 +0100

    Revert "fs/namei.c: don't pass nameidata to d_revalidate()"
    
    This reverts commit 714ddaecfe1563bd3cd9c72488619824d87978b3.
    
    Conflicts:
    	fs/namei.c

commit 65e66fe561253749db34a417628cb94c5df6e7e5
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Mar 25 11:50:09 2015 +0100

    Revert "fs/namei.c: don't pass namedata to lookup_dcache()"
    
    This reverts commit da2c91a48680d28ebf9d861cbc6695b94a2bfbeb.

commit 301ea3beaa1b358b6f5d3fea3818b8c7b401797d
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Mar 25 11:49:58 2015 +0100

    FIx compiling errors

commit be64fd243c29cf843f951b5e6af90ca4d9d41344
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Mar 25 11:33:21 2015 +0100

    Revert "don't pass nameidata to ->create()"
    
    This reverts commit 86dd71011640cf1f612a447da0d63e3cccc89c61.
    
    Conflicts:
    	Documentation/filesystems/porting

commit b6eb6fb0d2fc90dc9841c2386e767cee167862f9
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Mar 25 11:32:41 2015 +0100

    Revert "fs/namei.c: don't pass nameidata to __lookup_hash() and lookup_real()"
    
    This reverts commit 059f17ef587ce03d05c2ec7e6c2246ff8b425a20.

commit 4f94ccb2f549dc973157eb363d1bbdeb4d919fd9
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Mar 25 11:32:23 2015 +0100

    Revert "don't pass nameidata * to vfs_create()"
    
    This reverts commit 80e9025e23b4958a1f3dc3e17ccb6d50ad087398.

commit 1e9f139d4df4f8a1ac3c8a2d5d252c4b51e93f66
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Mar 25 11:30:09 2015 +0100

    Revert "stop passing nameidata * to ->d_revalidate()"
    
    This reverts commit b8feb9bda61a506c3e014a591ddfca8de5786387.
    
    Conflicts:
    	Documentation/filesystems/porting
    	fs/namei.c

commit cea2d1c27da33a158f870393daccc2c6a6ecfaae
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Mar 25 11:13:34 2015 +0100

    Revert "get rid of ->mnt_longterm"
    
    This reverts commit 377d7164ef6d7e017d83bfc57eff6f6e3ba2abee.

commit 80e9025e23b4958a1f3dc3e17ccb6d50ad087398
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Mar 25 11:04:08 2015 +0100

    don't pass nameidata * to vfs_create()
    all we want is a boolean flag, same as the method gets now Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

commit 86dd71011640cf1f612a447da0d63e3cccc89c61
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Mar 25 11:02:12 2015 +0100

    don't pass nameidata to ->create()
    boolean "does it have to be exclusive?" flag is passed instead; Local filesystem should just ignore it - the object is guaranteed not to be there yet. Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

commit 059f17ef587ce03d05c2ec7e6c2246ff8b425a20
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Mar 25 10:58:07 2015 +0100

    fs/namei.c: don't pass nameidata to __lookup_hash() and lookup_real()
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

commit ccac6a29d794fd584ff7fe13378e73f77261da8f
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Mar 25 10:56:01 2015 +0100

    stop passing nameidata to ->lookup()
    Just the flags; only NFS cares even about that, but there are legitimate uses for such argument. And getting rid of that completely would require splitting ->lookup() into a couple of methods (at least), so let's leave that alone for now... Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

commit da2c91a48680d28ebf9d861cbc6695b94a2bfbeb
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Mar 25 10:50:29 2015 +0100

    fs/namei.c: don't pass namedata to lookup_dcache()
    just the flags... Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

commit 714ddaecfe1563bd3cd9c72488619824d87978b3
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Mar 25 10:48:04 2015 +0100

    fs/namei.c: don't pass nameidata to d_revalidate()
    since the method wrapped by it doesn't need that anymore... Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

commit b8feb9bda61a506c3e014a591ddfca8de5786387
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Mar 25 10:47:22 2015 +0100

    stop passing nameidata * to ->d_revalidate()
    Just the lookup flags. Die, bastard, die... Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

commit 7e6d61648dedc08a5e4691f2023f4ab47f172132
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Mar 25 10:44:34 2015 +0100

    fs/nfs/dir.c: switch to passing nd->flags instead of nd wherever possible
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

commit e5592fe42f60a690d27d4f86ef46fb89efe8ad1f
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Mar 25 10:36:54 2015 +0100

    nfs_lookup_verify_inode() - nd is *always* non-NULL here
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

commit 921e394258ad50d23bbbcdaa363dc610a12fc2fe
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Mar 25 10:36:20 2015 +0100

    switch nfs_lookup_check_intent() away from nameidata
    just pass the flags Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

commit 895577d2f837bef6437255c26553f0a05f7a0fab
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Mar 25 10:02:15 2015 +0100

    namei.c: let follow_link() do put_link() on failure
    no need for kludgy "set cookie to ERR_PTR(...) because we failed before we did actual ->follow_link() and want to suppress put_link()", no pointless check in put_link() itself. Callers checked if follow_link() has failed anyway; might as well break out of their loops if that happened, without bothering to call put_link() first. [AV: folded fixes from hch] Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

commit fde5b4e8fe838253ee1e099334a12469d1f561d8
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Mar 25 10:01:45 2015 +0100

    coda: use list_for_each_entry
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

commit 447f64eb56189f92c134958f4e799e55ebd72bfa
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Mar 25 10:00:52 2015 +0100

    vfs: switch i_dentry/d_alias to hlist
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

commit 0d87e3de709201aac31e4d16e9e9d69dc434bdd4
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Mar 25 09:58:59 2015 +0100

    ext4: get rid of open-coded d_find_any_alias()
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

commit 9f55296e144ba92c2a0083f6338d6ed7e1aabe86
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Mar 25 09:58:25 2015 +0100

    ocfs2: use list_for_each_entry in ocfs2_find_local_alias()
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

commit 94f48ef1390f43ecba3bdfac5a1f547415114c2a
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Mar 25 09:57:25 2015 +0100

    affs: unobfuscate affs_fix_dcache()
    and add a comment on what it's doing Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

commit 73eef687f188ccdadd400c72d5604667087092b6
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Mar 25 09:56:30 2015 +0100

    affs: get rid of open-coded list_for_each_entry()
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

commit c38e12ed95b6843db3d69710c220853b1d22f068
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Mar 25 00:26:05 2015 +0100

    vfs: update documentation on ->i_dentry handling
    we used to need to clean it in RCU callback freeing an inode; in 3.2 that requirement went away. Unfortunately, it hadn't been reflected in Documentation/filesystems/porting. Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

commit a621430c0faaa5a4e29141fef0dd5be03dce5772
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Mar 25 00:25:32 2015 +0100

    cifs: don't bother with ->i_dentry in ->destroy_inode()
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

commit 6733cde74866a051d59d627b55293f2b17325615
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Mar 25 00:24:33 2015 +0100

    adfs: don't bother with ->i_dentry in ->destroy_inode()
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

commit b1f7c2c06cf1ca8aaa76da2a0c83afa6c82fbaf9
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Mar 25 00:23:07 2015 +0100

    qnx6: don't bother with ->i_dentry in inode-freeing callback
    we'll initialize it in inode_init_always() when we allocate that object again. Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

commit 7048a9180db4fd4a416b9b93bd019a774e65d836
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Mar 25 00:21:37 2015 +0100

    get rid of magic in proc_namespace.c
    don't rely on proc_mounts->m being the first field; container_of() is there for purpose. No need to bother with ->private, while we are at it - the same container_of will do nicely. Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

commit 377d7164ef6d7e017d83bfc57eff6f6e3ba2abee
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Mar 25 00:19:43 2015 +0100

    get rid of ->mnt_longterm
    it's enough to set ->mnt_ns of internal vfsmounts to something distinct from all struct mnt_namespace out there; then we can just use the check for ->mnt_ns != NULL in the fast path of mntput_no_expire() Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

commit ec760dab7bee34aca1baeff0f8ae1694052a73e1
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Tue Mar 24 23:04:52 2015 +0100

    Hyb-Stub file

commit 3e9b63e51c02f424ae71a3c67c946a6a37623cf0
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Tue Mar 24 22:57:41 2015 +0100

    Revert:"make expand_files() and alloc_fd() static "

commit fc72930874599adf37b584c5bff25e5e6f6f14f9
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Tue Mar 24 22:54:02 2015 +0100

     staging: android: lowmemorykiller: implement task's adj rbtree
    
    Based on the current LMK implementation, LMK has to scan all processes to
    select the correct task to kill during low memory.
    The basic idea for the optimization is to :
    queue all tasks with oom_score_adj priority, and then LMK just selects the
    proper task from the queue(rbtree) to kill.
    
    performance improvement:
    the current implementation: average time to find a task to kill : 1004us
    the optimized implementation: average time to find a task to kill: 43us
    
    Change-Id: I4dbbdd5673314dbbdabb71c3eff0dc229ce4ea91
    Signed-off-by: Hong-Mei Li <a21834@motorola.com>
    Reviewed-on: http://gerrit.pcs.mot.com/548917
    SLT-Approved: Slta Waiver <sltawvr@motorola.com>
    Tested-by: Jira Key <jirakey@motorola.com>
    Reviewed-by: Yi-Wei Zhao <gbjc64@motorola.com>
    Submit-Approved: Jira Key <jirakey@motorola.com>

commit e1c730a2704a7c71dbf5fc49bbde8bde52ecde88
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Tue Mar 24 22:37:08 2015 +0100

    Revert: "make expand_files() and alloc_fd() static"

commit 5774231523eb9f32579f407d36285206fdd839f8
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Tue Mar 24 22:13:28 2015 +0100

     make expand_files() and alloc_fd() static
    
    no callers outside of fs/file.c left
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

commit d13907de294b29f653a06d7126f0cbfdb98f4175
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Tue Mar 24 22:10:01 2015 +0100

     new helper: replace_fd()
    
    analog of dup2(), except that it takes struct file * as source.
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

commit ab9091b5d0a64e27be184e1ae472d54bc4697c2f
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Tue Mar 24 22:05:37 2015 +0100

     take purely descriptor-related stuff from fcntl.c to file.c
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

commit 7b816004c2b46ec1e9611ff0fbee8bf87cb45f57
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Tue Mar 24 21:59:27 2015 +0100

     make get_unused_fd_flags() a function
    
    ... and get_unused_fd() a macro around it
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

commit eb4bddea8dc4e92115677065be1a4669e6a4c697
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Tue Mar 24 21:57:26 2015 +0100

    Merged legit fixes found by arm-eabi-5.0 compiler. thanks @Alucard

commit 403c517477ca5af11164b318487411b2d8c4ab99
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Tue Mar 24 21:56:35 2015 +0100

    selinux: conditionally reschedule in hashtab_insert while loading sel
    
    inux policy
    
    After silencing the sleeping warning in mls_convert_context() I started
    seeing similar traces from hashtab_insert. Do a cond_resched there too.
    
    Signed-off-by: Dave Jones <davej@redhat.com>
    Acked-by: Stephen Smalley <sds@tycho.nsa.gov>
    Signed-off-by: Paul Moore <pmoore@redhat.com>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 4bdbe797da7d0b34deae89c347c46ebdf10d464c
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Tue Mar 24 21:55:49 2015 +0100

     selinux: conditionally reschedule in mls_convert_context while loadin
    
    g selinux policy
    
    On a slow machine (with debugging enabled), upgrading selinux policy may take
    a considerable amount of time. Long enough that the softlockup detector
    gets triggered.
    
    The backtrace looks like this..
    
     > BUG: soft lockup - CPU#2 stuck for 23s! [load_policy:19045]
     > Call Trace:
     >  [<ffffffff81221ddf>] symcmp+0xf/0x20
     >  [<ffffffff81221c27>] hashtab_search+0x47/0x80
     >  [<ffffffff8122e96c>] mls_convert_context+0xdc/0x1c0
     >  [<ffffffff812294e8>] convert_context+0x378/0x460
     >  [<ffffffff81229170>] ? security_context_to_sid_core+0x240/0x240
     >  [<ffffffff812221b5>] sidtab_map+0x45/0x80
     >  [<ffffffff8122bb9f>] security_load_policy+0x3ff/0x580
     >  [<ffffffff810788a8>] ? sched_clock_cpu+0xa8/0x100
     >  [<ffffffff810786dd>] ? sched_clock_local+0x1d/0x80
     >  [<ffffffff810788a8>] ? sched_clock_cpu+0xa8/0x100
     >  [<ffffffff8103096a>] ? __change_page_attr_set_clr+0x82a/0xa50
     >  [<ffffffff810786dd>] ? sched_clock_local+0x1d/0x80
     >  [<ffffffff810788a8>] ? sched_clock_cpu+0xa8/0x100
     >  [<ffffffff8103096a>] ? __change_page_attr_set_clr+0x82a/0xa50
     >  [<ffffffff810788a8>] ? sched_clock_cpu+0xa8/0x100
     >  [<ffffffff81534ddc>] ? retint_restore_args+0xe/0xe
     >  [<ffffffff8109c82d>] ? trace_hardirqs_on_caller+0xfd/0x1c0
     >  [<ffffffff81279a2e>] ? trace_hardirqs_on_thunk+0x3a/0x3f
     >  [<ffffffff810d28a8>] ? rcu_irq_exit+0x68/0xb0
     >  [<ffffffff81534ddc>] ? retint_restore_args+0xe/0xe
     >  [<ffffffff8121e947>] sel_write_load+0xa7/0x770
     >  [<ffffffff81139633>] ? vfs_write+0x1c3/0x200
     >  [<ffffffff81210e8e>] ? security_file_permission+0x1e/0xa0
     >  [<ffffffff8113952b>] vfs_write+0xbb/0x200
     >  [<ffffffff811581c7>] ? fget_light+0x397/0x4b0
     >  [<ffffffff81139c27>] SyS_write+0x47/0xa0
     >  [<ffffffff8153bde4>] tracesys+0xdd/0xe2
    
    Stephen Smalley suggested:
    
     > Maybe put a cond_resched() within the ebitmap_for_each_positive_bit()
     > loop in mls_convert_context()?
    
    That seems to do the trick. Tested by downgrading and re-upgrading selinux-policy-targeted.
    
    Signed-off-by: Dave Jones <davej@redhat.com>
    Acked-by: Stephen Smalley <sds@tycho.nsa.gov>
    Signed-off-by: Paul Moore <pmoore@redhat.com>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 0b7d5b9a633edfa5ef93fc6a514765daa09ecc90
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Tue Mar 24 21:55:09 2015 +0100

     selinux: no recursive read_lock of policy_rwlock in security_genfs_sid()
    
    With the introduction of fair queued rwlock, recursive read_lock()
    may hang the offending process if there is a write_lock() somewhere
    in between.
    
    With recursive read_lock checking enabled, the following error was
    reported:
    
    =============================================
    [ INFO: possible recursive locking detected ]
    3.16.0-rc1 #2 Tainted: G            E
    ---------------------------------------------
    load_policy/708 is trying to acquire lock:
     (policy_rwlock){.+.+..}, at: [<ffffffff8125b32a>]
    security_genfs_sid+0x3a/0x170
    
    but task is already holding lock:
     (policy_rwlock){.+.+..}, at: [<ffffffff8125b48c>]
    security_fs_use+0x2c/0x110
    
    other info that might help us debug this:
     Possible unsafe locking scenario:
    
           CPU0
           ----
      lock(policy_rwlock);
      lock(policy_rwlock);
    
    This patch fixes the occurrence of recursive read_lock() of
    policy_rwlock by adding a helper function __security_genfs_sid()
    which requires caller to take the lock before calling it. The
    security_fs_use() was then modified to call the new helper function.
    
    Signed-off-by: Waiman Long <Waiman.Long@hp.com>
    Acked-by:  Stephen Smalley <sds@tycho.nsa.gov>
    Signed-off-by: Paul Moore <pmoore@redhat.com>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 10c00ce6e28477b493499aedfac81a0e57520e7a
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Tue Mar 24 21:52:07 2015 +0100

     selinux: fix a possible memory leak in cond_read_node()
    
    The cond_read_node() should free the given node on error path as it's
    not linked to p->cond_list yet.  This is done via cond_node_destroy()
    but it's not called when next_entry() fails before the expr loop.
    
    Signed-off-by: Namhyung Kim <namhyung@kernel.org>
    Signed-off-by: Paul Moore <pmoore@redhat.com>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 037530a7bd2d64d170714d648b7393657655e244
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Tue Mar 24 21:51:27 2015 +0100

     selinux: simple cleanup for cond_read_node()
    
    The node->cur_state and len can be read in a single call of next_entry().
    And setting len before reading is a dead write so can be eliminated.
    
    Signed-off-by: Namhyung Kim <namhyung@kernel.org>
    (Minor tweak to the length parameter in the call to next_entry())
    Signed-off-by: Paul Moore <pmoore@redhat.com>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 29d756f22631a658a26644b366e687d1f2a8b06e
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Tue Mar 24 21:50:15 2015 +0100

     selinux: Add option to force audit
    
    * To help debugging selinux
    * echo Y > /sys/module/selinux/parameters/force_audit
    
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 8081a0ae7386fae81418d0361a56642d2b2adc3c
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Tue Mar 24 21:49:06 2015 +0100

     SELINUX HOOKS: Disable Special handling for sysfs, debugfs, rootfs
    
    It's brake Selinux permissions with LS/VS980 ROMS and possible others.
    
    No need this for real. so just leave it OFF. [Dorimanx]

commit 175e92fd8d3eecc0b0b7131a194391feb1ee4fa0
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Tue Mar 24 21:46:55 2015 +0100

     pstore: selinux: add security in-core xattr support for pstore and de
    
    bugfs
    
    - add "pstore" and "debugfs" to list of in-core exceptions
    - change fstype checks to boolean equation
    - change from strncmp to strcmp for checking
    
    Signed-off-by: Mark Salyzyn <salyzyn@google.com>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit e0c52054995784c31eeafa9bc3fa0b3f5e7cc10d
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Tue Mar 24 21:45:27 2015 +0100

     Enable setting security contexts on rootfs inodes.
    
    rootfs (ramfs) can support setting of security contexts
    by userspace due to the vfs fallback behavior of calling
    the security module to set the in-core inode state
    for security.* attributes when the filesystem does not
    provide an xattr handler.  No xattr handler required
    as the inodes are pinned in memory and have no backing
    store.
    
    This is useful in allowing early userspace to label individual
    files within a rootfs while still providing a policy-defined
    default via genfs.
    
    Signed-off-by: Stephen Smalley <sds@tycho.nsa.gov>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 632ca840257b2e95932a44d5856a5946be70dccb
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Tue Mar 24 21:39:23 2015 +0100

     dm-req-crypt: Free resources in destructor
    
    Signed-off-by: Ajay Dudani <adudani@codeaurora.org>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>
    
    AND Added qcrypto.h file

commit 18a7017539991d4c0984b921c7f3068b2f66bc29
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Tue Mar 24 21:31:48 2015 +0100

     RESTORE FS/KERNEL/BLOCK/SECURITY Updates: now that SOD bug found! I c
    
    an restore!
    
    SOD bug was with MMC change, now that it's fixed, i can restore all good
    updates/fixes that i lost during SOD bug hunt.

commit d228fd500bf4abfbad6bb7d49f050cb48b9151c6
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Tue Mar 24 20:42:59 2015 +0100

     dm: dm-req-crypt: add support for Per-File-Encryption
    
    Support Per-File-Encryption (PFE) based on file tagging.
    The Per-File-Tagger (PFT) reports if a file should be encrypted or not.
    The Per-File-Encryption can be used after Full-Disk-Encryption (FDE) was
    completed or without FDE.
    The PFE and FDE uses different keys, which are managed by the Trust-Zone.
    
    Change-Id: I727ef11e252649f895a5e3f8a49ca848cea50795
    Signed-off-by: Amir Samuelov <amirs@codeaurora.org>

commit f28489074a8c5a6eca00daf6c76341200bc0f60e
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Tue Mar 24 20:42:33 2015 +0100

     Setting correct HW crypto driver instance for FDE
    
    HW crypto provides different instance for different use cases.
    Dm-req-crypt should use the one for full disk encryption use case.
    
    Change-Id: I22d3f6ab1dd6f83b0d5b83a681236719e2340934
    Signed-off-by: Dinesh K Garg <dineshg@codeaurora.org>

commit 2831e1cbd6aaaf329934f56ee075e1068324269e
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Tue Mar 24 20:42:06 2015 +0100

     dm: Clean up dm-req-crypt
    
    dm-req-crypt defines optional functions from device mapper. These
    functions are not required for dm-req-crypt. dm-req-crypt has few
    memory de-allocation issues. This change removes unnecessary
    functions and fixes memory deallocation issues as well.
    
    Change-Id: Id36bd13989940ce41571a8a10277730048df6f6d
    Signed-off-by: Dinesh K Garg <dineshg@codeaurora.org>

commit 430bc6940ed2a7c31f01e3c3e7b9f91d888fc4cf
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Tue Mar 24 20:41:38 2015 +0100

     dm: updating crypto algorithm used by dm-req-crypt
    
    dm-req-crypt uses AES-XTS algorithm implemented by HW crypto
    engine. Crypto driver renamed the aes-xts algorithm to avoid
    conflict with SW based implementation of AES-XTS. dm-req-crypt
    must use the new name for AES-XTS provided by crypto driver.
    
    Change-Id: I286b6435af33fd64673c7c1af4bb2447f3115868
    Signed-off-by: Dinesh K Garg <dineshg@codeaurora.org>

commit cfc9f58f664c44fc21a0d8752b2fc48b3962fda2
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Tue Mar 24 20:40:54 2015 +0100

     Request based dm-crypt
    
    dm-crypt provides bios based device mapper module. dm-crypt
    operates on packets with 512 bytes size which is not effiicent
    way for HW based crypto blocks. dm-req-crypt is developed to
    address this. dm-req-crypt works on requests which carry upto
    512KB of data for unmerged requests.
    
    Change-Id: I479230b2f0c1ab2e2a1ddb84947ebaf0629dff8c
    Signed-off-by: Dinesh K Garg <dineshg@codeaurora.org>

commit 032bd6ace582a3de86c2cfd9a360c5c92683b0b4
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Tue Mar 24 20:32:56 2015 +0100

     c/r: fcntl: add F_GETOWNER_UIDS option
    
    When we restore file descriptors we would like them to look exactly as
    they were at dumping time.
    
    With help of fcntl it's almost possible, the missing snippet is file
    owners UIDs.
    
    To be able to read their values the F_GETOWNER_UIDS is introduced.
    
    This option is valid iif CONFIG_CHECKPOINT_RESTORE is turned on, otherwise
    returning -EINVAL.
    
    Signed-off-by: Cyrill Gorcunov <gorcunov@openvz.org>
    Acked-by: "Eric W. Biederman" <ebiederm@xmission.com>
    Cc: "Serge E. Hallyn" <serge@hallyn.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Pavel Emelyanov <xemul@parallels.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit 2b1e1a1c967d16bcdb41ab5333dcc843a519bca9
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Tue Mar 24 20:20:51 2015 +0100

     switch flush_unauthorized_files() to replace_fd()
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

commit 9ccd3f9a273a6ce2d74428cf083902dfce5f90c7
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Tue Mar 24 20:17:07 2015 +0100

     SELinux: possible NULL deref in context_struct_to_string
    
    It's possible that the caller passed a NULL for scontext.  However if this
    is a defered mapping we might still attempt to call *scontext=kstrdup().
    This is bad.  Instead just return the len.
    
    Signed-off-by: Eric Paris <eparis@redhat.com>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 88feba4eb6386b3684e85ac005b1a5fcf66562fe
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Tue Mar 24 20:15:52 2015 +0100

     SELinux: use ARRAY_SIZE
    
    ARRAY_SIZE is more concise to use when the size of an array is divided
    by the size of its type or the size of its first element.
    
    The Coccinelle semantic patch that makes this change is as follows:
    
    // <smpl>
    @@
    type T;
    T[] E;
    @@
    
    - (sizeof(E)/sizeof(E[...]))
    + ARRAY_SIZE(E)
    // </smpl>
    
    Signed-off-by: Himangi Saraogi <himangi774@gmail.com>
    Signed-off-by: Paul Moore <pmoore@redhat.com>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 738b276a4811121e4779371c8f886c8d37bed060
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Tue Mar 24 20:13:53 2015 +0100

     platform: msm: Fix compile when CONFIG_PFT is not set
    
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 6ececa2ace159a018e6c184ae734be8179daaf1b
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Tue Mar 24 20:11:31 2015 +0100

     msm: cleanup and minor fixes for PFT driver
    
    remove unused symlink hook.
    free open-file-list on driver exit.
    check return value of pft_add_file().
    
    Change-Id: Ida1e4351742ae62cafb2ffa49727f490709351e2
    Signed-off-by: Amir Samuelov <amirs@codeaurora.org>

commit a7974181235af9b9282dedbb47eac3d56ecb3b89
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Tue Mar 24 20:10:58 2015 +0100

     platform: msm: pft: fix close all encrypted files
    
    filp_close() eventually calls pft_file_close().
    Simplify pft_close_opened_enc_files() and remove having mutex
    in both places that cause a deadlock.
    
    Change-Id: I0afa2d2f0806f18d430c027601fb9974855ef185
    Signed-off-by: Amir Samuelov <amirs@codeaurora.org>

commit d8aaf82a349cafb4bdcdc6a2d1c7b657f15d77c9
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Tue Mar 24 20:10:12 2015 +0100

     platform: msm: fix PFT char device major number
    
    Dynamically allocate the major number
    of the PFT character device.
    
    Change-Id: I7877579781353203753f86a3993bc5731307db2a
    Signed-off-by: Amir Samuelov <amirs@codeaurora.org>

commit 7ef2db53d2e8c96233f8a53dc1389ad8d74cc100
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Tue Mar 24 20:08:32 2015 +0100

     platform: msm: add Per-File-Tagger (PFT) driver
    
    This driver is part of the Per-File-Encryption (PFE) feature.
    It allows to tag enterprise files and encrypt them
    while keeping the user private files untagged and plain text.
    
    Change-Id: I2ba8bffb2a8815991dc3994a1f94a0c52b937a25
    Signed-off-by: Nir Ofry <nofry@codeaurora.org>
    Signed-off-by: Amir Samuelov <amirs@codeaurora.org>

commit fec3afbc296b86d98f4a51cd28e47a8e8b6c97d3
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Tue Mar 24 19:54:46 2015 +0100

     selinux: Report permissive mode in avc: denied messages.
    
    We cannot presently tell from an avc: denied message whether access was in
    fact denied or was allowed due to global or per-domain permissive mode.
    Add a permissive= field to the avc message to reflect this information.
    
    Signed-off-by: Stephen Smalley <sds@tycho.nsa.gov>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit c50684eb8712a091ce05c1e95c2fee46a2dd1e54
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Tue Mar 24 19:31:13 2015 +0100

    Fix linux 3.4.81 patch doubling code

commit 5a8e1f570b43da67519757df3e70d78d6fbb7a47
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Tue Mar 24 19:28:16 2015 +0100

    thermal: downgraded to version 1 intellithermal

commit 2e93dcc6bb2be37527ab6d68d01423e4e63d80e8
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Tue Mar 24 17:35:57 2015 +0100

     msm: thermal: Add IOCTL interface support to Kernel Thermal Monitor
    
    Kernel Thermal Monitor exposes an IOCTL dev interface, which can
    be used for user space cpu frequency mitigation.
    
    CRs-Fixed: 538981
    Change-Id: I48ab6be5a9bebee5219c94bdff81badf09576b2a
    Signed-off-by: Ram Chandrasekar <rkumbako@codeaurora.org>

commit 877b470f760e692bcc2be0bf67b0bac7f42b2559
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Tue Mar 24 17:25:21 2015 +0100

    thermal: fixes

commit 6f01c3908205153f3134c2af544d82456f5e31d6
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Tue Mar 24 15:47:45 2015 +0100

    f2fs: add missing files

commit 89c382fc7cf5526a3a634a42bef2f071561b792e
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Tue Mar 24 15:47:04 2015 +0100

     SELinux: Reduce overhead of mls_level_isvalid() function call
    
    Date	Mon, 10 Jun 2013 13:55:08 -0400
    
    v4->v5:
      - Fix scripts/checkpatch.pl warning.
    
    v3->v4:
      - Merge the 2 separate while loops in ebitmap_contains() into
        a single one.
    
    v2->v3:
      - Remove unused local variables i, node from mls_level_isvalid().
    
    v1->v2:
     - Move the new ebitmap comparison logic from mls_level_isvalid()
       into the ebitmap_contains() helper function.
     - Rerun perf and performance tests on the latest v3.10-rc4 kernel.
    
    While running the high_systime workload of the AIM7 benchmark on
    a 2-socket 12-core Westmere x86-64 machine running 3.10-rc4 kernel
    (with HT on), it was found that a pretty sizable amount of time was
    spent in the SELinux code. Below was the perf trace of the "perf
    record -a -s" of a test run at 1500 users:
    
      5.04%            ls  [kernel.kallsyms]     [k] ebitmap_get_bit
      1.96%            ls  [kernel.kallsyms]     [k] mls_level_isvalid
      1.95%            ls  [kernel.kallsyms]     [k] find_next_bit
    
    The ebitmap_get_bit() was the hottest function in the perf-report
    output.  Both the ebitmap_get_bit() and find_next_bit() functions
    were, in fact, called by mls_level_isvalid(). As a result, the
    mls_level_isvalid() call consumed 8.95% of the total CPU time of
    all the 24 virtual CPUs which is quite a lot. The majority of the
    mls_level_isvalid() function invocations come from the socket creation
    system call.
    
    Looking at the mls_level_isvalid() function, it is checking to see
    if all the bits set in one of the ebitmap structure are also set in
    another one as well as the highest set bit is no bigger than the one
    specified by the given policydb data structure. It is doing it in
    a bit-by-bit manner. So if the ebitmap structure has many bits set,
    the iteration loop will be done many times.
    
    The current code can be rewritten to use a similar algorithm as the
    ebitmap_contains() function with an additional check for the
    highest set bit. The ebitmap_contains() function was extended to
    cover an optional additional check for the highest set bit, and the
    mls_level_isvalid() function was modified to call ebitmap_contains().
    
    With that change, the perf trace showed that the used CPU time drop
    down to just 0.08% (ebitmap_contains + mls_level_isvalid) of the
    total which is about 100X less than before.
    
      0.07%            ls  [kernel.kallsyms]     [k] ebitmap_contains
      0.05%            ls  [kernel.kallsyms]     [k] ebitmap_get_bit
      0.01%            ls  [kernel.kallsyms]     [k] mls_level_isvalid
      0.01%            ls  [kernel.kallsyms]     [k] find_next_bit
    
    The remaining ebitmap_get_bit() and find_next_bit() functions calls
    are made by other kernel routines as the new mls_level_isvalid()
    function will not call them anymore.
    
    This patch also improves the high_systime AIM7 benchmark result,
    though the improvement is not as impressive as is suggested by the
    reduction in CPU time spent in the ebitmap functions. The table below
    shows the performance change on the 2-socket x86-64 system (with HT
    on) mentioned above.
    
    +--------------+---------------+----------------+-----------------+
    |   Workload   | mean % change | mean % change  | mean % change   |
    |              | 10-100 users  | 200-1000 users | 1100-2000 users |
    +--------------+---------------+----------------+-----------------+
    | high_systime |     +0.1%     |     +0.9%      |     +2.6%       |
    +--------------+---------------+----------------+-----------------+
    Signed-off-by: Waiman Long <Waiman.Long@hp.com>
    Signed-off-by: Paul Reioux <reioux@gmail.com>
    
        LG-STOCK-KK
    
    commit dcb9e18a7c3079323cfa4b9d7492fe3846a849f0 1 parent 30afb30
    @longman88 longman88 authored on 19 Jun 2013
    dorimanx committed on 22 Apr 2014
    Showing
    with 27 additions and 19 deletions.
    20  security/selinux/ss/ebitmap.c
    @@ -213,7 +213,12 @@ int ebitmap_netlbl_import(struct ebitmap *ebmap,
    }
    -int ebitmap_contains(struct ebitmap *e1, struct ebitmap *e2)
    +/*
    + * Check to see if all the bits set in e2 are also set in e1. Optionally,
    + * if last_e2bit is non-zero, the highest set bit in e2 cannot exceed
    + * last_e2bit.
    + */
    +int ebitmap_contains(struct ebitmap *e1, struct ebitmap *e2, u32 last_e2bit)
    {
    struct ebitmap_node *n1, *n2;
    int i;
    @@ -223,14 +228,25 @@ int ebitmap_contains(struct ebitmap *e1, struct ebitmap *e2)
    n1 = e1->node;
    n2 = e2->node;
    +
    while (n1 && n2 && (n1->startbit <= n2->startbit)) {
    if (n1->startbit < n2->startbit) {
    n1 = n1->next;
    continue;
    }
    - for (i = 0; i < EBITMAP_UNIT_NUMS; i++) {
    + for (i = EBITMAP_UNIT_NUMS - 1; (i >= 0) && !n2->maps[i]; )
    + i--; /* Skip trailing NULL map entries */
    + if (last_e2bit && (i >= 0)) {
    + u32 lastsetbit = n2->startbit + i * EBITMAP_UNIT_SIZE +
    + __fls(n2->maps[i]);
    + if (lastsetbit > last_e2bit)
    + return 0;
    + }
    +
    + while (i >= 0) {
    if ((n1->maps[i] & n2->maps[i]) != n2->maps[i])
    return 0;
    + i--;
    }
    n1 = n1->next;
    2  security/selinux/ss/ebitmap.h
    @@ -117,7 +117,7 @@ static inline void ebitmap_node_clr_bit(struct ebitmap_node *n,
    int ebitmap_cmp(struct ebitmap *e1, struct ebitmap *e2);
    int ebitmap_cpy(struct ebitmap *dst, struct ebitmap *src);
    -int ebitmap_contains(struct ebitmap *e1, struct ebitmap *e2);
    +int ebitmap_contains(struct ebitmap *e1, struct ebitmap *e2, u32 last_e2bit);
    int ebitmap_get_bit(struct ebitmap *e, unsigned long bit);
    int ebitmap_set_bit(struct ebitmap *e, unsigned long bit, int value);
    void ebitmap_destroy(struct ebitmap *e);
    22  security/selinux/ss/mls.c
    @@ -160,8 +160,6 @@ void mls_sid_to_context(struct context *context,
    int mls_level_isvalid(struct policydb *p, struct mls_level *l)
    {
    struct level_datum *levdatum;
    - struct ebitmap_node *node;
    - int i;
    if (!l->sens || l->sens > p->p_levels.nprim)
    return 0;
    levdatum = hashtab_search(p->p_levels.table,
    sym_name(p, SYM_LEVELS, l->sens - 1));
    if (!levdatum)
    return 0;
    - ebitmap_for_each_positive_bit(&l->cat, node, i) {
    - if (i > p->p_cats.nprim)
    - return 0;
    - if (!ebitmap_get_bit(&levdatum->level->cat, i)) {
    - /*
    -	* Category may not be associated with
    -	* sensitivity.
    -	*/
    - return 0;
    - }
    - }
    -
    - return 1;
    + /*
    +	* Return 1 iff all the bits set in l->cat are also be set in
    +	* levdatum->level->cat and no bit in l->cat is larger than
    +	* p->p_cats.nprim.
    +	*/
    + return ebitmap_contains(&levdatum->level->cat, &l->cat,
    + p->p_cats.nprim);
    }
    int mls_range_isvalid(struct policydb *p, struct mls_range *r)
    2  security/selinux/ss/mls_types.h
    @@ -35,7 +35,7 @@ static inline int mls_level_eq(struct mls_level *l1, struct mls_level *l2)
    static inline int mls_level_dom(struct mls_level *l1, struct mls_level *l2)
    {
    return ((l1->sens >= l2->sens) &&
    - ebitmap_contains(&l1->cat, &l2->cat));
    + ebitmap_contains(&l1->cat, &l2->cat, 0));
    }
    @Tkkg1994
    Markdown supported
    Edit in fullscreen
    Write Preview
    
    Attach images by dragging & dropping or selecting them.

commit 2c53282e402800d4b08db896317e2ab982a02916
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Tue Mar 24 12:51:09 2015 +0100

    fs: f2fs fix problems while compiling

commit 3cdef8d8ef923ac1036bce2029945d982acb820c
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Mon Mar 23 23:24:08 2015 +0100

    config: enable intellithermal

commit 37437df8430c7401abbb99bfbb0112b125b76393
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Mon Mar 23 23:23:24 2015 +0100

     intelli_thermal v3.0: initial coding for Linux 3.10 Qualcomm kernels
    
    Intelli_thermal is a replacement for the existing Qualcomm msm_thermal engine.
    It allows users greater controls over the stock Qualcomm thermal engine while
    maintaining full backwards compatibilty.
    
    Copyright 2014~2015 Paul Reioux (aka Faux123)
    
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit c5028825719d1c4af38b5b914d4ccbcd64e66e41
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Mon Mar 23 23:15:01 2015 +0100

    fs: f2fs: First try to get f2fs probably working

commit a7542ecbe9206d8e2c4d7f7a5fe1e78d7cbff735
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Mon Mar 23 22:33:58 2015 +0100

    mach-msm: fixed overclock problem and added f2fs to config file

commit 1eb75235da51b8a77ac0cef631122cb9191bbc43
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Mon Mar 23 20:50:09 2015 +0100

     page_alloc: Make watermarks tunable separately
    
    This patch introduces three new sysctls to /proc/sys/vm:
    wmark_min_kbytes, wmark_low_kbytes and wmark_high_kbytes.
    
    Each entry is used to compute watermark[min], watermark[low]
    and watermark[high] for each zone.
    
    These parameters are also updated when min_free_kbytes are
    changed because originally they are set based on min_free_kbytes.
    On the other hand, min_free_kbytes is updated when wmark_free_kbytes
    changes.
    
    By using the parameters one can adjust the difference among
    watermark[min], watermark[low] and watermark[high] and as a result
    one can tune the kernel reclaim behaviour to fit their requirement.
    
    Signed-off-by: Satoru Moriya <satoru.moriya@hds.com>
    
    modified and tuned for Hammerhead
    
    Signed-off-by: Paul Reioux <reioux@gmail.com>
    
    Conflicts:
    	kernel/sysctl.c
    	mm/page_alloc.c

commit 3579ba7de8f56c033890ec24f72180793ba28041
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Mon Mar 23 20:27:09 2015 +0100

    mach-msm: add OC and UC support, and enable it in config

commit 5cd4bdf4119bce82bfced625932d615120cc8b19
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Mon Mar 23 20:09:37 2015 +0100

     kgsl combined changes
    
    msm: kgsl: Align ION buffer mappings from userspace to 4k
    When userspace allocates an ION buffer and maps it to GPU virtual space,
    adjust the buffer size to be 4k aligned before mapping it to GPU virtual
    space. GPU virtual space map routine expects the buffer to be 4k
    aligned.
    
    Change-Id: Ibdb95597c24e2b5d0c3390d2c8550abb6b4b730b
    Acked-by: Rama Vaddula <rvaddula@qualcomm.com>
    Signed-off-by: Tarun Karra <tkarra@codeaurora.org>
    
    msm: kgsl: Avoid turning on GPU clock if already off
    In adreno_idle(), check if the GPU clock is already
    turned off. If so, the GPU is already idle,
    so don't turn on the clocks again.
    
    Change-Id: I27ae939253ee71830466e3117bed0230d7ff67d5
    Signed-off-by: Ananta Kishore K <akollipa@codeaurora.org>

commit fa8f18040a65a3563633ad4bf2ab9dc1d48e4842
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Mon Mar 23 17:47:56 2015 +0100

     intelli_plug: add performance boost option
    
    also fixed dependency on __cpuinit
    
    bumped to version 3.9
    
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit cb58be1c77f4e4e9b3e854ff1e511f476a4a0ba1
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun Mar 22 20:21:27 2015 +0100

     cpufreq: Optimize cpufreq_frequency_table_verify()
    
    cpufreq_frequency_table_verify() is rewritten here to make it more logical
    and efficient.
     - merge multiple lines for variable declarations together.
     - quit early if any frequency between min/max is found.
     - don't call cpufreq_verify_within_limits() in case any valid freq is
       found as it is of no use.
     - rename the count variable as found and change its type to boolean.
    
    Signed-off-by: Viresh Kumar <viresh.kumar@linaro.org>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Git-commit: 77db50c4eb1991d6e88254390ec368e1d23a8fa5
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Junjie Wu <junjiew@codeaurora.org>

commit ae38863ff4e4d3f9f1f646a1e90497979396f99f
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun Mar 22 20:20:02 2015 +0100

     cpufreq: Persist cpufreq time in state data across hotplug
    
    Cpufreq time_in_state data for all CPUs is made persistent across
    hotplug and exposed to userspace via sysfs file
    /sys/devices/system/cpu/cpufreq/all_time_in_state
    
    Change-Id: I97cb5de24b6de16189bf8b5df9592d0a6e6ddf32
    Signed-off-by: Ruchi Kandoi <kandoiruchi@google.com>

commit 71bbbca1af0cc96b0eff7d6ecd8c25326e7dbfcf
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun Mar 22 20:15:26 2015 +0100

     cpufreq: Protect against hotplug in cpufreq_register_driver()
    
    cpufreq_register_driver() could race with CPU hotplug during
    bootup. Since hotplug notification is not registered when
    subsys_interface_register() is being executed, it's possible
    cpufreq's view of online CPUs becomes stale before it registers
    for hotplug notification.
    
    Register for hotplug notification first and protect
    subsys_interface_register() against hotplug using
    get/put_online_cpus().
    
    Signed-off-by: Junjie Wu <junjiew@codeaurora.org>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 56cc0ce0d7743e5475d6e70a7a1148b3e846438e
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun Mar 22 20:09:33 2015 +0100

    Remove permissive mode from kernel

commit d36d60fba8ea4089db77f3944d112bad5f8e0d44
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun Mar 22 04:32:49 2015 +0100

    config: chose new compression for kernel

commit 3b7438c7bec03893fca3feec3a240a83e25c5f2c
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun Mar 22 03:52:01 2015 +0100

     jbd2: optimize jbd2_journal_force_commit
    
    Current implementation of jbd2_journal_force_commit() is suboptimal because
    result in empty and useless commits. But callers just want to force and wait
    any unfinished commits. We already have jbd2_journal_force_commit_nested()
    which does exactly what we want, except we are guaranteed that we do not hold
    journal transaction open.
    
    Signed-off-by: Dmitry Monakhov <dmonakhov@openvz.org>
    Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>

commit 509809618932e43fb240e23d022279a91440099d
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun Mar 22 03:46:15 2015 +0100

     netfilter: conntrack: disable generic tracking for known protocols
    
    Given following iptables ruleset:
    
    -P FORWARD DROP
    -A FORWARD -m sctp --dport 9 -j ACCEPT
    -A FORWARD -p tcp --dport 80 -j ACCEPT
    -A FORWARD -p tcp -m conntrack -m state ESTABLISHED,RELATED -j ACCEPT
    
    One would assume that this allows SCTP on port 9 and TCP on port 80.
    Unfortunately, if the SCTP conntrack module is not loaded, this allows
    *all* SCTP communication, to pass though, i.e. -p sctp -j ACCEPT,
    which we think is a security issue.
    
    This is because on the first SCTP packet on port 9, we create a dummy
    "generic l4" conntrack entry without any port information (since
    conntrack doesn't know how to extract this information).
    
    All subsequent packets that are unknown will then be in established
    state since they will fallback to proto_generic and will match the
    'generic' entry.
    
    Our originally proposed version [1] completely disabled generic protocol
    tracking, but Jozsef suggests to not track protocols for which a more
    suitable helper is available, hence we now mitigate the issue for in
    tree known ct protocol helpers only, so that at least NAT and direction
    information will still be preserved for others.
    
    [1] http://www.spinics.net/lists/netfilter-devel/msg33430.html
    
    Joint work with Daniel Borkmann.
    
    Signed-off-by: Florian Westphal <fw@strlen.de>
    Signed-off-by: Daniel Borkmann <dborkman@redhat.com>
    Acked-by: Jozsef Kadlecsik <kadlec@blackhole.kfki.hu>
    Signed-off-by: Pablo Neira Ayuso <pablo@netfilter.org>

commit 44a43b497f92200fc2f446eb0d18603644338758
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun Mar 22 03:44:39 2015 +0100

     block: recursive merge requests
    
    In a workload, thread 1 accesses a, a+2, ..., thread 2 accesses a+1, a+3,....
    When the requests are flushed to queue, a and a+1 are merged to (a, a+1), a+2
    and a+3 too to (a+2, a+3), but (a, a+1) and (a+2, a+3) aren't merged.
    
    If we do recursive merge for such interleave access, some workloads throughput
    get improvement. A recent worload I'm checking on is swap, below change
    boostes the throughput around 5% ~ 10%.
    
    Signed-off-by: Shaohua Li <shli@fusionio.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

commit 59ed0b4d230081f90b0e347e74511ac35ca0ea49
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun Mar 22 03:42:25 2015 +0100

     fs/super.c: sync ro remount after blocking writers
    
    Move sync_filesystem() after sb_prepare_remount_readonly().  If writers
    sneak in anywhere from sync_filesystem() to sb_prepare_remount_readonly()
    it can cause inodes to be dirtied and writeback to occur well after
    sys_mount() has completely successfully.
    
    This was spotted by corrupted ubifs filesystems on reboot, but appears
    that it can cause issues with any filesystem using writeback.
    
    Cc: Artem Bityutskiy <dedekind1@gmail.com>
    Cc: Christoph Hellwig <hch@infradead.org>
    Cc: Alexander Viro <viro@zeniv.linux.org.uk>
    CC: Richard Weinberger <richard@nod.at>
    Co-authored-by: Richard Weinberger <richard@nod.at>
    Signed-off-by: Andrew Ruder <andrew.ruder@elecsyscorp.com>
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

commit 4d4db5c765e1358118a322c5e4f5ec85ce8df4fd
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun Mar 22 03:40:48 2015 +0100

     kernel: cgroup: push rcu read locking from css_is_ancestor() to callsite
    
    Library functions should not grab locks when the callsites can do it,
    even if the lock nests like the rcu read-side lock does.
    
    Push the rcu_read_lock() from css_is_ancestor() to its single user,
    mem_cgroup_same_or_subtree() in preparation for another user that may
    already hold the rcu read-side lock.
    
    Change-Id: I0ec7ec0059ed588d8f85bc9be8fdc42ce0ca7f5d
    Signed-off-by: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Konstantin Khlebnikov <khlebnikov@openvz.org>
    Acked-by: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Acked-by: Michal Hocko <mhocko@suse.cz>
    Acked-by: Li Zefan <lizf@cn.fujitsu.com>
    Cc: Li Zefan <lizf@cn.fujitsu.com>
    Cc: Tejun Heo <tj@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Git-commit: 91c63734f6908425903aed69c04035592f18d398
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit ffb7ad365535d9526fcaa8753f945b5324a0ef2f
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun Mar 22 03:36:47 2015 +0100

     ARM: 7565/1: sched: stop sched_clock() during suspend
    
    The scheduler imposes a requirement to sched_clock()
    which is to stop the clock during suspend, if we don't
    do that any RT thread will be rescheduled in the future
    which might cause any sort of problems.
    
    This became an issue on OMAP when we converted omap-i2c.c
    to use threaded IRQs, it turned out that depending on how
    much time we spent on suspend, the I2C IRQ thread would
    end up being rescheduled so far in the future that I2C
    transfers would timeout and, because omap_hsmmc depends
    on an I2C-connected device to detect if an MMC card is
    inserted in the slot, our rootfs would just vanish.
    
    arch/arm/kernel/sched_clock.c already had an optional
    implementation (sched_clock_needs_suspend()) which would
    handle scheduler's requirement properly, what this patch
    does is simply to make that implementation non-optional.
    
    Note that this has the side-effect that printk timings
    won't reflect the actual time spent on suspend so other
    methods to measure that will have to be used.
    
    This has been tested with beagleboard XM (OMAP3630) and
    pandaboard rev A3 (OMAP4430). Suspend to RAM is now working
    after this patch.
    
    Thanks to Kevin Hilman for helping out with debugging.
    
    Change-Id: Ie2f9e3b22eb3d1f3806cf8c598f22e2fa1b8651f
    Acked-by: Kevin Hilman <khilman@ti.com>
    Acked-by: Linus Walleij <linus.walleij@linaro.org>
    Signed-off-by: Felipe Balbi <balbi@ti.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>
    CRs-Fixed: 497236
    Git-commit: 6a4dae5e138a32b45ca5218cc2b81802f9d378c3
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Srivatsa Vaddagiri <vatsa@codeaurora.org>

commit aa8163081bad4ceacf6f31ac152f071c67b106f3
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun Mar 22 03:35:09 2015 +0100

     ARM: 7486/1: sched_clock: update epoch_cyc on resume
    
    Many clocks that are used to provide sched_clock will reset during
    suspend.  If read_sched_clock returns 0 after suspend, sched_clock will
    appear to jump forward.  This patch resets cd.epoch_cyc to the current
    value of read_sched_clock during resume, which causes sched_clock() just
    after suspend to return the same value as sched_clock() just before
    suspend.
    
    In addition, during the window where epoch_ns has been updated before
    suspend, but epoch_cyc has not been updated after suspend, it is unknown
    whether the clock has reset or not, and sched_clock() could return a
    bogus value.  Add a suspended flag, and return the pre-suspend epoch_ns
    value during this period.
    
    The new behavior is triggered by calling setup_sched_clock_needs_suspend
    instead of setup_sched_clock.
    
    Change-Id: I7441ef74dc6802c00eea61f3b8c0a25ac00a724d
    Signed-off-by: Colin Cross <ccross@android.com>
    Reviewed-by: Linus Walleij <linus.walleij@linaro.org>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>
    CRs-Fixed: 497236
    Git-commit: 237ec6f2e51d2fc2ff37c7c5f1ccc9264d09c85b
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Srivatsa Vaddagiri <vatsa@codeaurora.org>

commit bc21a924d6efe6fa451517d03afb2830d3d3d374
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun Mar 22 03:31:40 2015 +0100

     lib/int_sqrt.c: optimize square root algorithm
    
    Optimize the current version of the shift-and-subtract (hardware)
    algorithm, described by John von Newmann[1] and Guy L Steele.
    
    Iterating 1,000,000 times, perf shows for the current version:
    
     Performance counter stats for './sqrt-curr' (10 runs):
    
             27.170996 task-clock                #    0.979 CPUs utilized            ( +-  3.19% )
                     3 context-switches          #    0.103 K/sec                    ( +-  4.76% )
                     0 cpu-migrations            #    0.004 K/sec                    ( +-100.00% )
                   104 page-faults               #    0.004 M/sec                    ( +-  0.16% )
            64,921,199 cycles                    #    2.389 GHz                      ( +-  0.03% )
            28,967,789 stalled-cycles-frontend   #   44.62% frontend cycles idle     ( +-  0.18% )
       <not supported> stalled-cycles-backend
           104,502,623 instructions              #    1.61  insns per cycle
                                                 #    0.28  stalled cycles per insn  ( +-  0.00% )
            34,088,368 branches                  # 1254.587 M/sec                    ( +-  0.00% )
                 4,901 branch-misses             #    0.01% of all branches          ( +-  1.32% )
    
           0.027763015 seconds time elapsed                                          ( +-  3.22% )
    
    And for the new version:
    
    Performance counter stats for './sqrt-new' (10 runs):
    
              0.496869 task-clock                #    0.519 CPUs utilized            ( +-  2.38% )
                     0 context-switches          #    0.000 K/sec
                     0 cpu-migrations            #    0.403 K/sec                    ( +-100.00% )
                   104 page-faults               #    0.209 M/sec                    ( +-  0.15% )
               590,760 cycles                    #    1.189 GHz                      ( +-  2.35% )
               395,053 stalled-cycles-frontend   #   66.87% frontend cycles idle     ( +-  3.67% )
       <not supported> stalled-cycles-backend
               398,963 instructions              #    0.68  insns per cycle
                                                 #    0.99  stalled cycles per insn  ( +-  0.39% )
                70,228 branches                  #  141.341 M/sec                    ( +-  0.36% )
                 3,364 branch-misses             #    4.79% of all branches          ( +-  5.45% )
    
           0.000957440 seconds time elapsed                                          ( +-  2.42% )
    
    Furthermore, this saves space in instruction text:
    
       text    data     bss     dec     hex filename
        111       0       0     111      6f lib/int_sqrt-baseline.o
         89       0       0      89      59 lib/int_sqrt.o
    
    [1] http://en.wikipedia.org/wiki/First_Draft_of_a_Report_on_the_EDVAC
    
    Signed-off-by: Davidlohr Bueso <davidlohr.bueso@hp.com>
    Reviewed-by: Jonathan Gonzalez <jgonzlez@linets.cl>
    Tested-by: Jonathan Gonzalez <jgonzlez@linets.cl>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit 48f5a498a43c4cff962952f7379f4e5044df00fc
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun Mar 22 03:29:44 2015 +0100

     sched: convert WARN_ON() to printk_sched() in try_to_wake_up_local()
    
    try_to_wake_up_local() is called with the rq lock held. Printing to
    console in this context can result in a deadlock if klogd needs to
    be woken up. Print to the kernel log buffer via printk_sched()
    instead which avoids the wakeup.
    
    Change-Id: Ia07baea3cb7e0b158545207fdbbb866203256d3c
    Signed-off-by: Syed Rameez Mustafa <rameezmustafa@codeaurora.org>

commit e6508a887e37afea0f3dca99000a2ec566f1f558
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun Mar 22 03:28:27 2015 +0100

     sched: change WARN_ON_ONCE to WARN_ON in try_to_wake_up_local()
    
    The WARN_ON_ONCE() calls at the beginning of try_to_wake_up_local()
    were recently converted from BUG_ON() calls. If these hit it indicates
    something is wrong and that may contribute to other system instability.
    To eliminate the risk of an instance of one of these errors going
    un-noticed because there was an earlier instance that occured long ago,
    change to WARN_ON(). If there ever is a flood of these there are bigger
    problems.
    
    Change-Id: I392832e2b6ec24b3569b001b1af9ecd4ed6828e7
    Signed-off-by: Steve Muckle <smuckle@codeaurora.org>

commit 1701615f6a44cc4546184711dd35e9f9735a2a42
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun Mar 22 03:26:45 2015 +0100

     sched: fix reference to wrong cfs_rq
    
    Commit 7db16c8c (sched: Fix SCHED_HRTICK bug leading to late preemption
    of tasks) introduced a bug in sched_slice() calculation by using wrong
    cfs_rq for tasks. rq->cfs was incorrectly used as task's cfs_rq, rather
    than the correct one to which they belonged.
    
    Fix the bug by using correct cfs_rq for tasks.
    
    Signed-off-by: Srivatsa Vaddagiri <vatsa@codeaurora.org>

commit bf323a8c600d556ff1407e99eb1cfa2615e57206
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun Mar 22 03:25:50 2015 +0100

     sched: Fix SCHED_HRTICK bug leading to late preemption of tasks
    
    SCHED_HRTICK feature is useful to preempt SCHED_FAIR tasks on-the-dot
    (just when they would have exceeded their ideal_runtime). It makes use
    of a a per-cpu hrtimer resource and hence alarming that hrtimer should
    be based on total SCHED_FAIR tasks a cpu has across its various cfs_rqs,
    rather than being based on number of tasks in a particular cfs_rq (as
    implemented currently). As a result, with current code, its possible for
    a running task (which is the sole task in its cfs_rq) to be preempted
    much after its ideal_runtime has elapsed, resulting in increased latency
    for tasks in other cfs_rq on same cpu.
    
    Fix this by alarming sched hrtimer based on total number of SCHED_FAIR
    tasks a CPU has across its various cfs_rqs.
    
    Change-Id: I1f23680a64872f8ce0f451ac4bcae28e8967918f
    Signed-off-by: Srivatsa Vaddagiri <vatsa@codeaurora.org>

commit f92dff1308e4562dd0b2de2d2fe21d45e818ff7b
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun Mar 22 03:24:10 2015 +0100

     sched: fix rq->lock recursion
    
    Enabling SCHED_HRTICK currently results in rq->lock recursion and a hard
    hang at bootup.  Essentially try_to_wakeup() grabs rq->lock and tries
    arming a hrtimer via hrtimer_restart(), which deep down tries waking up
    ksoftirqd, which leads to a recursive call to try_to_wakeup() and thus
    attempt to take rq->lock recursively!!
    
    This is fixed by having scheduler queue hrtimer via
    __hrtimer_start_range_ns() which avoids waking up ksoftirqd.
    
    Signed-off-by: Srivatsa Vaddagiri <vatsa@codeaurora.org>
    Change-Id: I11a13be1d9db3a749614ccf3d4f5fb7bf6f18fa1

commit 52c60b6f52572615516681f3bcf3849b04741344
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun Mar 22 03:23:05 2015 +0100

     sched: Micro-optimize the smart wake-affine logic
    
    Smart wake-affine is using node-size as the factor currently, but the overhead
    of the mask operation is high.
    
    Thus, this patch introduce the 'sd_llc_size' percpu variable, which will record
    the highest cache-share domain size, and make it to be the new factor, in order
    to reduce the overhead and make it more reasonable.
    
    Tested-by: Davidlohr Bueso <davidlohr.bueso@hp.com>
    Tested-by: Michael Wang <wangyun@linux.vnet.ibm.com>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Acked-by: Michael Wang <wangyun@linux.vnet.ibm.com>
    Cc: Mike Galbraith <efault@gmx.de>
    Link: http://lkml.kernel.org/r/51D5008E.6030102@linux.vnet.ibm.com
    [ Tidied up the changelog. ]
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 3f24165339673aea358d09d4d99763d2ce7da960
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun Mar 22 03:19:57 2015 +0100

     sched: Implement smarter wake-affine logic
    
    The wake-affine scheduler feature is currently always trying to pull
    the wakee close to the waker. In theory this should be beneficial if
    the waker's CPU caches hot data for the wakee, and it's also beneficial
    in the extreme ping-pong high context switch rate case.
    
    Testing shows it can benefit hackbench up to 15%.
    
    However, the feature is somewhat blind, from which some workloads
    such as pgbench suffer. It's also time-consuming algorithmically.
    
    Testing shows it can damage pgbench up to 50% - far more than the
    benefit it brings in the best case.
    
    So wake-affine should be smarter and it should realize when to
    stop its thankless effort at trying to find a suitable CPU to wake on.
    
    This patch introduces 'wakee_flips', which will be increased each
    time the task flips (switches) its wakee target.
    
    So a high 'wakee_flips' value means the task has more than one
    wakee, and the bigger the number, the higher the wakeup frequency.
    
    Now when making the decision on whether to pull or not, pay attention to
    the wakee with a high 'wakee_flips', pulling such a task may benefit
    the wakee. Also imply that the waker will face cruel competition later,
    it could be very cruel or very fast depends on the story behind
    'wakee_flips', waker therefore suffers.
    
    Furthermore, if waker also has a high 'wakee_flips', that implies that
    multiple tasks rely on it, then waker's higher latency will damage all
    of them, so pulling wakee seems to be a bad deal.
    
    Thus, when 'waker->wakee_flips / wakee->wakee_flips' becomes
    higher and higher, the cost of pulling seems to be worse and worse.
    
    The patch therefore helps the wake-affine feature to stop its pulling
    work when:
    
    	wakee->wakee_flips > factor &&
    	waker->wakee_flips > (factor * wakee->wakee_flips)
    
    The 'factor' here is the number of CPUs in the current CPU's NUMA node,
    so a bigger node will lead to more pulling since the trial becomes more
    severe.
    
    After applying the patch, pgbench shows up to 40% improvements and no regressions.
    
    Tested with 12 cpu x86 server and tip 3.10.0-rc7.
    
    The percentages in the final column highlight the areas with the biggest wins,
    all other areas improved as well:
    
    	pgbench		    base	smart
    
    	| db_size | clients |  tps  |	|  tps  |
    	+---------+---------+-------+   +-------+
    	| 22 MB   |       1 | 10598 |   | 10796 |
    	| 22 MB   |       2 | 21257 |   | 21336 |
    	| 22 MB   |       4 | 41386 |   | 41622 |
    	| 22 MB   |       8 | 51253 |   | 57932 |
    	| 22 MB   |      12 | 48570 |   | 54000 |
    	| 22 MB   |      16 | 46748 |   | 55982 | +19.75%
    	| 22 MB   |      24 | 44346 |   | 55847 | +25.93%
    	| 22 MB   |      32 | 43460 |   | 54614 | +25.66%
    	| 7484 MB |       1 |  8951 |   |  9193 |
    	| 7484 MB |       2 | 19233 |   | 19240 |
    	| 7484 MB |       4 | 37239 |   | 37302 |
    	| 7484 MB |       8 | 46087 |   | 50018 |
    	| 7484 MB |      12 | 42054 |   | 48763 |
    	| 7484 MB |      16 | 40765 |   | 51633 | +26.66%
    	| 7484 MB |      24 | 37651 |   | 52377 | +39.11%
    	| 7484 MB |      32 | 37056 |   | 51108 | +37.92%
    	| 15 GB   |       1 |  8845 |   |  9104 |
    	| 15 GB   |       2 | 19094 |   | 19162 |
    	| 15 GB   |       4 | 36979 |   | 36983 |
    	| 15 GB   |       8 | 46087 |   | 49977 |
    	| 15 GB   |      12 | 41901 |   | 48591 |
    	| 15 GB   |      16 | 40147 |   | 50651 | +26.16%
    	| 15 GB   |      24 | 37250 |   | 52365 | +40.58%
    	| 15 GB   |      32 | 36470 |   | 50015 | +37.14%
    
    Signed-off-by: Michael Wang <wangyun@linux.vnet.ibm.com>
    Cc: Mike Galbraith <efault@gmx.de>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/51D50057.9000809@linux.vnet.ibm.com
    [ Improved the changelog. ]
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    
        master
    
    commit 46898611ad21b103e22f7dd1468f3db97c2c71bb 1 parent cc13c95
    Michael Wang authored on 4 Jul 2013
    googyanas committed 23 days ago
    Showing
    with 50 additions and 0 deletions.
    3  include/linux/sched.h
    @@ -1235,6 +1235,9 @@ struct task_struct {
    struct llist_node wake_entry;
    int on_cpu;
    + struct task_struct *last_wakee;
    + unsigned long wakee_flips;
    + unsigned long wakee_flip_decay_ts;
    int on_rq;
    47  kernel/sched/fair.c
    @@ -2344,26 +2344,43 @@
    if (type == 0 || !sched_feat(LB_BIAS))
    return total;
    return max(rq->cpu_load[type-1], total);
    }
    static unsigned long power_of(int cpu)
    {
    return cpu_rq(cpu)->cpu_power;
    }
    static unsigned long cpu_avg_load_per_task(int cpu)
    {
    struct rq *rq = cpu_rq(cpu);
    unsigned long nr_running = ACCESS_ONCE(rq->nr_running);
    if (nr_running)
    return rq->load.weight / nr_running;
    return 0;
    }
    +static void record_wakee(struct task_struct *p)
    +{
    + /*
    +	* Rough decay (wiping) for cost saving, don't worry
    +	* about the boundary, really active task won't care
    +	* about the loss.
    +	*/
    + if (jiffies > current->wakee_flip_decay_ts + HZ) {
    + current->wakee_flips = 0;
    + current->wakee_flip_decay_ts = jiffies;
    + }
    +
    + if (current->last_wakee != p) {
    + current->last_wakee = p;
    + current->wakee_flips++;
    + }
    +}
    static void task_waking_fair(struct task_struct *p)
    {
    @@ -2384,6 +2401,7 @@ static void task_waking_fair(struct task_struct *p)
    se->vruntime -= min_vruntime;
    + record_wakee(p);
    }
    @@ -2482,26 +2500,48 @@
    /*
    * Recursively apply this logic to all parent groups to compute
    * the final effective load change on the root group. Since
    * only the @tg group gets extra weight, all parent groups can
    * only redistribute existing shares. @wl is the shift in shares
    * resulting from this level per the above.
    */
    wg = 0;
    }
    return wl;
    }
    static inline unsigned long effective_load(struct task_group *tg, int cpu,
    unsigned long wl, unsigned long wg)
    {
    return wl;
    }
    +static int wake_wide(struct task_struct *p)
    +{
    + int factor = nr_cpus_node(cpu_to_node(smp_processor_id()));
    +
    + /*
    +	* Yeah, it's the switching-frequency, could means many wakee or
    +	* rapidly switch, use factor here will just help to automatically
    +	* adjust the loose-degree, so bigger node will lead to more pull.
    +	*/
    + if (p->wakee_flips > factor) {
    + /*
    +	* wakee is somewhat hot, it needs certain amount of cpu
    +	* resource, so if waker is far more hot, prefer to leave
    +	* it alone.
    +	*/
    + if (current->wakee_flips > (factor * p->wakee_flips))
    + return 1;
    + }
    +
    + return 0;
    +}
    +
    static int wake_affine(struct sched_domain *sd, struct task_struct *p, int sync)
    {
    s64 this_load, load;
    int idx, this_cpu, prev_cpu;
    unsigned long tl_per_task;
    struct task_group *tg;
    unsigned long weight;
    int balanced;
    + /*
    +	* If we wake multiple tasks be careful to not bounce
    +	* ourselves around too much.
    +	*/
    + if (wake_wide(p))
    + return 0;
    +
    idx = sd->wake_idx;
    this_cpu = smp_processor_id();
    prev_cpu = task_cpu(p);
    @Tkkg1994
    Markdown supported
    Edit in fullscreen
    Write Preview
    
    Attach images by dragging & dropping or selecting them.

commit 08e06ade8452659c7ee49edc040beeba4267d0b0
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun Mar 22 03:14:44 2015 +0100

     kthread: fix return value of kthread_create() upon SIGKILL.
    
    Commit 786235eeba0e ("kthread: make kthread_create() killable") meant
    for allowing kthread_create() to abort as soon as killed by the
    OOM-killer.  But returning -ENOMEM is wrong if killed by SIGKILL from
    userspace.  Change kthread_create() to return -EINTR upon SIGKILL.
    
    Signed-off-by: Tetsuo Handa <penguin-kernel@I-love.SAKURA.ne.jp>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Acked-by: David Rientjes <rientjes@google.com>
    Cc: <stable@vger.kernel.org> [3.13+]
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit 648f49d51285a72a34b1b46530ade445aa6045bc
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun Mar 22 03:13:56 2015 +0100

     kthread: ensure locality of task_struct allocations
    
    In the presence of memoryless nodes, numa_node_id() will return the
    current CPU's NUMA node, but that may not be where we expect to allocate
    from memory from.  Instead, we should rely on the fallback code in the
    memory allocator itself, by using NUMA_NO_NODE.  Also, when calling
    kthread_create_on_node(), use the nearest node with memory to the cpu in
    question, rather than the node it is running on.
    
    Signed-off-by: Nishanth Aravamudan <nacc@linux.vnet.ibm.com>
    Reviewed-by: Christoph Lameter <cl@linux.com>
    Acked-by: David Rientjes <rientjes@google.com>
    Cc: Anton Blanchard <anton@samba.org>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Jan Kara <jack@suse.cz>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tetsuo Handa <penguin-kernel@I-love.SAKURA.ne.jp>
    Cc: Wanpeng Li <liwanp@linux.vnet.ibm.com>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Cc: Ben Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit e4daef85f908b9f2158be7bee73a4a252320f6bc
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun Mar 22 03:13:18 2015 +0100

     kthread: make kthread_create() killable
    
    Any user process callers of wait_for_completion() except global init
    process might be chosen by the OOM killer while waiting for completion()
    call by some other process which does memory allocation.  See
    CVE-2012-4398 "kernel: request_module() OOM local DoS" can happen.
    
    When such users are chosen by the OOM killer when they are waiting for
    completion() in TASK_UNINTERRUPTIBLE, the system will be kept stressed
    due to memory starvation because the OOM killer cannot kill such users.
    
    kthread_create() is one of such users and this patch fixes the problem
    for kthreadd by making kthread_create() killable - the same approach
    used for fixing CVE-2012-4398.
    
    Signed-off-by: Tetsuo Handa <penguin-kernel@I-love.SAKURA.ne.jp>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Acked-by: David Rientjes <rientjes@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit 5989612d101af7d06ccf9b2385509f2fdea125e8
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun Mar 22 03:11:53 2015 +0100

     ext4: optimize starting extent in ext4_ext_rm_leaf()
    
    Both hole punch and truncate use ext4_ext_rm_leaf() for removing
    blocks.  Currently we choose the last extent as the starting
    point for removing blocks:
    
    	ex = EXT_LAST_EXTENT(eh);
    
    This is OK for truncate but for hole punch we can optimize the extent
    selection as the path is already initialized.  We could use this
    information to select proper starting extent.  The code change in this
    patch will not affect truncate as for truncate path[depth].p_ext will
    always be NULL.
    
    Signed-off-by: Ashish Sangwan <a.sangwan@samsung.com>
    Signed-off-by: Namjae Jeon <namjae.jeon@samsung.com>
    Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>

commit 2f7fde1cf997a5df4508d524dac1d2a4b2bd5f12
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun Mar 22 03:10:36 2015 +0100

     ext4: get rid of code duplication
    
    Reviewed-by: Jan Kara <jack@suse.cz>
    Signed-off-by: Dmitry Monakhov <dmonakhov@openvz.org>
    Signed-off-by: Theodore Ts'o <tytso@mit.edu>

commit 3dcc62dcdfcb2f6e27c56e8f78ab474abd5dd90a
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun Mar 22 03:09:35 2015 +0100

     ext4: optimize test_root()
    
    The test_root() function could potentially loop forever due to
    overflow issues.  So rewrite test_root() to avoid this issue; as a
    bonus, it is 38% faster when benchmarked via a test loop:
    
    int main(int argc, char **argv)
    {
    	int  i;
    
    	for (i = 0; i < 1 << 24; i++) {
    		if (test_root(i, 7))
    			printf("%d\n", i);
    	}
    }
    
    Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>

commit f1c77874e2f6043b54755c21d226d4f8c46a15af
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun Mar 22 03:07:40 2015 +0100

     timer: Generalize timer->base flags handling
    
    To prepare for addition of another flag, generalize timer->base flags
    handling.
    
    * Rename from TBASE_*_FLAG to TIMER_* and make them LU constants.
    
    * Define and use TIMER_FLAG_MASK for flags masking so that multiple
      flags can be handled correctly.
    
    * Don't dereference timer->base directly even if
      !tbase_get_deferrable().  All two such places are already passed in
      @base, so use it instead.
    
    * Make sure tvec_base's alignment is large enough for timer->base
      flags using BUILD_BUG_ON().
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: torvalds@linux-foundation.org
    Cc: peterz@infradead.org
    Link: http://lkml.kernel.org/r/1344449428-24962-2-git-send-email-tj@kernel.org
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

commit a09c26f4b5a939a4ab0cfcf473e7f3c37eb9cf27
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun Mar 22 03:01:38 2015 +0100

     timers: Add accounting of non deferrable timers
    
    The code in get_next_timer_interrupt() is suboptimal as it has to run
    through the cascade to find the next expiring timer. On a completely
    idle core we should only do that when there is an active timer
    enqueued and base->next_timer does not give us a fast answer.
    
    Add accounting of the active timers to the now consolidated
    attach/detach code. I deliberately avoided sanity checks because the
    code is fully symetric and any fiddling with timers w/o using the API
    functions will lead to cute explosions anyway. ulong is big enough
    even on 32bit and if we really run into the situation to have more
    than 1<<32 timers enqueued there, then we are definitely not in a
    state to go idle and run through that code.
    
    This allows us to fix another shortcoming of get_next_timer_interrupt().
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Gilad Ben-Yossef <gilad@benyossef.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Link: http://lkml.kernel.org/r/20120525214819.236377028@linutronix.de

commit 9615045356c820cb69d04cb66dcfbe5002225c85
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun Mar 22 02:59:24 2015 +0100

     timers: Create detach_if_pending() and use it
    
    Most callers of detach_timer() have the same pattern around
    them. Check whether the timer is pending and eventually updating
    base->next_timer.
    
    Create detach_if_pending() and replace the duplicated code.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Gilad Ben-Yossef <gilad@benyossef.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Link: http://lkml.kernel.org/r/20120525214819.131246037@linutronix.de
    
    Conflicts:
    	kernel/timer.c

commit 8b3518fae34bdf3dc872708a86a42892cf04d5c7
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun Mar 22 02:57:11 2015 +0100

     net: rps: fix cpu unplug
    
    softnet_data.input_pkt_queue is protected by a spinlock that we must
    hold when transferring packets from victim queue to an active one.
    This is because other cpus could still be trying to enqueue packets
    into victim queue.
    
    A second problem is that when we transfert the NAPI poll_list from
    victim to current cpu, we absolutely need to special case the percpu
    backlog, because we do not want to add complex locking to protect
    process_queue : Only owner cpu is allowed to manipulate it, unless
    cpu is offline.
    
    Based on initial patch from Prasad Sodagudi &
    Subash Abhinov Kasiviswanathan.
    
    This version is better because we do not slow down packet processing,
    only make migration safer.
    
    Reported-by: Prasad Sodagudi <psodagud@codeaurora.org>
    Reported-by: Subash Abhinov Kasiviswanathan <subashab@codeaurora.org>
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Cc: Tom Herbert <therbert@google.com>
    Signed-off-by: Trilok Soni <tsoni@codeaurora.org>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit d28e8cf06cb934ba4f20ec6f4b7c5333eefb1500
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun Mar 22 02:55:26 2015 +0100

    regulator: core: fix race condition in regulator_put() and enable rcu_fast_no_hz
    
    commit 83b0302d347a49f951e904184afe57ac3723476e upstream.
    
    The regulator framework maintains a list of consumer regulators
    for a regulator device and protects it from concurrent access using
    the regulator device's mutex lock.
    
    In the case of regulator_put() the consumer is removed and regulator
    device's parameters are updated without holding the regulator device's
    mutex. This would lead to a race condition between the regulator_put()
    and any function which traverses the consumer list or modifies regulator
    device's parameters.
    Fix this race condition by holding the regulator device's mutex in case
    of regulator_put.
    
    Signed-off-by: Ashay Jaiswal <ashayj@codeaurora.org>
    Signed-off-by: Mark Brown <broonie@kernel.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 3da2f5dc2ccaa9a9f005ee5f1be8e41ac9a6e02c
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun Mar 22 02:53:25 2015 +0100

     rcu: Stop rcu_do_batch() from multiplexing the "count" variable
    
    Commit b1420f1c (Make rcu_barrier() less disruptive) rearranged the
    code in rcu_do_batch(), moving the ->qlen manipulation to follow
    the requeueing of the callbacks.  Unfortunately, this rearrangement
    clobbered the value of the "count" local variable before the value
    of rdp->qlen was adjusted, resulting in the value of rdp->qlen being
    inaccurate.  This commit therefore introduces an index variable "i",
    avoiding the inadvertent multiplexing.
    
    Signed-off-by: Paul E. McKenney <paul.mckenney@linaro.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Reviewed-by: Josh Triplett <josh@joshtriplett.org>
    
    Conflicts:
    	kernel/rcutree.c

commit 22fe38fdca8c9a6eedef3dec4b33d63ee047dae6
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun Mar 22 02:50:53 2015 +0100

     rcu: Precompute RCU_FAST_NO_HZ timer offsets
    
    When a CPU is entering dyntick-idle mode, tick_nohz_stop_sched_tick()
    calls rcu_needs_cpu() see if RCU needs that CPU, and, if not, computes the
    next wakeup time based on the timer wheels.  Only later, when actually
    entering the idle loop, rcu_prepare_for_idle() will be invoked.  In some
    cases, rcu_prepare_for_idle() will post timers to wake the CPU back up.
    But all for naught: The next wakeup time for the CPU has already been
    computed, and posting a timer afterwards does not force that wakeup
    time to be recomputed.  This means that rcu_prepare_for_idle()'s have
    no effect.
    
    This is not a problem on a busy system because something else will wake
    up the CPU soon enough.  However, on lightly loaded systems, the CPU
    might stay asleep for a considerable length of time.  If that CPU has
    a callback that the rest of the system is waiting on, the system might
    run very slowly or (in theory) even hang.
    
    This commit avoids this problem by having rcu_needs_cpu() give
    tick_nohz_stop_sched_tick() an estimate of when RCU will need the CPU
    to wake back up, which tick_nohz_stop_sched_tick() takes into account
    when programming the CPU's wakeup time.  An alternative approach is
    for rcu_prepare_for_idle() to use hrtimers instead of normal timers,
    but timers are much more efficient than are hrtimers for frequently
    and repeatedly posting and cancelling a given timer, which is exactly
    what RCU_FAST_NO_HZ does.
    
    Reported-by: Pascal Chapperon <pascal.chapperon@wanadoo.fr>
    Reported-by: Heiko Carstens <heiko.carstens@de.ibm.com>
    Signed-off-by: Paul E. McKenney <paul.mckenney@linaro.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Tested-by: Heiko Carstens <heiko.carstens@de.ibm.com>
    Tested-by: Pascal Chapperon <pascal.chapperon@wanadoo.fr>

commit a53e2ff531963df49fcd0c1a2760d6e521e2fffc
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun Mar 22 02:43:30 2015 +0100

     rcu: Move RCU_FAST_NO_HZ per-CPU variables to rcu_dynticks structure
    
    The RCU_FAST_NO_HZ code relies on a number of per-CPU variables.
    This works, but is hidden from someone scanning the data structures
    in rcutree.h.  This commit therefore converts these per-CPU variables
    to fields in the per-CPU rcu_dynticks structures.
    
    Suggested-by: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Paul E. McKenney <paul.mckenney@linaro.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Tested-by: Heiko Carstens <heiko.carstens@de.ibm.com>
    Tested-by: Pascal Chapperon <pascal.chapperon@wanadoo.fr>

commit 19fbc462737e4facf6313b1790947fa7dfa0730f
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun Mar 22 02:36:07 2015 +0100

     rcu: Update RCU_FAST_NO_HZ tracing for lazy callbacks
    
    In the current code, a short dyntick-idle interval (where there is
    at least one non-lazy callback on the CPU) and a long dyntick-idle
    interval (where there are only lazy callbacks on the CPU) are traced
    identically, which can be less than helpful.  This commit therefore
    emits different event traces in these two cases.
    
    Signed-off-by: Paul E. McKenney <paul.mckenney@linaro.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Tested-by: Heiko Carstens <heiko.carstens@de.ibm.com>
    Tested-by: Pascal Chapperon <pascal.chapperon@wanadoo.fr>

commit eb9c1cf9ddf1d4dde187fd4f82e424faac9fb29a
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun Mar 22 02:34:05 2015 +0100

     timer: Fix mod_timer_pinned() header comment
    
    The mod_timer_pinned() header comment states that it prevents timers
    from being migrated to a different CPU.  This is not the case, instead,
    it ensures that the timer is posted to the current CPU, but does nothing
    to prevent CPU-hotplug operations from migrating the timer.
    
    This commit therefore brings the comment header into alignment with
    reality.
    
    Signed-off-by: Paul E. McKenney <paul.mckenney@linaro.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Acked-by: Steven Rostedt <rostedt@goodmis.org>

commit 3008d17adc6b9fbcad931217d40804205c365769
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun Mar 22 02:32:35 2015 +0100

     rcu: Make rcu_barrier() less disruptive
    
    The rcu_barrier() primitive interrupts each and every CPU, registering
    a callback on every CPU.  Once all of these callbacks have been invoked,
    rcu_barrier() knows that every callback that was registered before
    the call to rcu_barrier() has also been invoked.
    
    However, there is no point in registering a callback on a CPU that
    currently has no callbacks, most especially if that CPU is in a
    deep idle state.  This commit therefore makes rcu_barrier() avoid
    interrupting CPUs that have no callbacks.  Doing this requires reworking
    the handling of orphaned callbacks, otherwise callbacks could slip through
    rcu_barrier()'s net by being orphaned from a CPU that rcu_barrier() had
    not yet interrupted to a CPU that rcu_barrier() had already interrupted.
    This reworking was needed anyway to take a first step towards weaning
    RCU from the CPU_DYING notifier's use of stop_cpu().
    
    Signed-off-by: Paul E. McKenney <paul.mckenney@linaro.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

commit 46c08f9480452bd020585fe2e0fd6719fc40ced6
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun Mar 22 02:28:39 2015 +0100

     rcu: Explicitly initialize RCU_FAST_NO_HZ per-CPU variables
    
    The current initialization of the RCU_FAST_NO_HZ per-CPU variables makes
    needless and fragile assumptions about the initial value of things like
    the jiffies counter.  This commit therefore explicitly initializes all of
    them that are better started with a non-zero value.  It also adds some
    comments describing the per-CPU state variables.
    
    Signed-off-by: Paul E. McKenney <paul.mckenney@linaro.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

commit f00e5496bcbdb3a8029298d97780d0e2c1f11143
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun Mar 22 02:27:18 2015 +0100

     rcu: Make RCU_FAST_NO_HZ handle timer migration
    
    The current RCU_FAST_NO_HZ assumes that timers do not migrate unless a
    CPU goes offline, in which case it assumes that the CPU will have to come
    out of dyntick-idle mode (cancelling the timer) in order to go offline.
    This is important because when RCU_FAST_NO_HZ permits a CPU to enter
    dyntick-idle mode despite having RCU callbacks pending, it posts a timer
    on that CPU to force a wakeup on that CPU.  This wakeup ensures that the
    CPU will eventually handle the end of the grace period, including invoking
    its RCU callbacks.
    
    However, Pascal Chapperon's test setup shows that the timer handler
    rcu_idle_gp_timer_func() really does get invoked in some cases.  This is
    problematic because this can cause the CPU that entered dyntick-idle
    mode despite still having RCU callbacks pending to remain in
    dyntick-idle mode indefinitely, which means that its RCU callbacks might
    never be invoked.  This situation can result in grace-period delays or
    even system hangs, which matches Pascal's observations of slow boot-up
    and shutdown (https://lkml.org/lkml/2012/4/5/142).  See also the bugzilla:
    
    	https://bugzilla.redhat.com/show_bug.cgi?id=806548
    
    This commit therefore causes the "should never be invoked" timer handler
    rcu_idle_gp_timer_func() to use smp_call_function_single() to wake up
    the CPU for which the timer was intended, allowing that CPU to invoke
    its RCU callbacks in a timely manner.
    
    Reported-by: Pascal Chapperon <pascal.chapperon@wanadoo.fr>
    Signed-off-by: Paul E. McKenney <paul.mckenney@linaro.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

commit f137606f3e688c08a82f3d1bd4b7be9deef414c4
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun Mar 22 02:24:33 2015 +0100

     rcu: Make exit_rcu() more precise and consolidate
    
    When running preemptible RCU, if a task exits in an RCU read-side
    critical section having blocked within that same RCU read-side critical
    section, the task must be removed from the list of tasks blocking a
    grace period (perhaps the current grace period, perhaps the next grace
    period, depending on timing).  The exit() path invokes exit_rcu() to
    do this cleanup.
    
    However, the current implementation of exit_rcu() needlessly does the
    cleanup even if the task did not block within the current RCU read-side
    critical section, which wastes time and needlessly increases the size
    of the state space.  Fix this by only doing the cleanup if the current
    task is actually on the list of tasks blocking some grace period.
    
    While we are at it, consolidate the two identical exit_rcu() functions
    into a single function.
    
    Signed-off-by: Paul E. McKenney <paul.mckenney@linaro.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Tested-by: Linus Torvalds <torvalds@linux-foundation.org>
    
    Conflicts:
    
    	kernel/rcupdate.c

commit 02fd016ed18e575326c97c40d904a1e336236250
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun Mar 22 02:20:13 2015 +0100

     rcu: Ensure that RCU_FAST_NO_HZ timers expire on correct CPU
    
    Timers are subject to migration, which can lead to the following
    system-hang scenario when CONFIG_RCU_FAST_NO_HZ=y:
    
    1.	CPU 0 executes synchronize_rcu(), which posts an RCU callback.
    
    2.	CPU 0 then goes idle.  It cannot immediately invoke the callback,
    	but there is nothing RCU needs from ti, so it enters dyntick-idle
    	mode after posting a timer.
    
    3.	The timer gets migrated to CPU 1.
    
    4.	CPU 0 never wakes up, so the synchronize_rcu() never returns, so
    	the system hangs.
    
    This commit fixes this problem by using mod_timer_pinned(), as suggested
    by Peter Zijlstra, to ensure that the timer is actually posted on the
    running CPU.
    
    Reported-by: Dipankar Sarma <dipankar@in.ibm.com>
    Signed-off-by: Paul E. McKenney <paul.mckenney@linaro.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

commit 97933184e398a00fc727b69d10ec1727bc274b45
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun Mar 22 02:19:34 2015 +0100

     rcu: Add warning for RCU_FAST_NO_HZ timer firing
    
    RCU_FAST_NO_HZ uses a timer to limit the time that a CPU with callbacks
    can remain in dyntick-idle mode.  This timer is cancelled when the CPU
    exits idle, and therefore should never fire.  However, if the timer
    were migrated to some other CPU for whatever reason (1) the timer could
    actually fire and (2) firing on some other CPU would fail to wake up the
    CPU with callbacks, possibly resulting in sluggishness or a system hang.
    
    This commit therfore adds a WARN_ON_ONCE() to the timer handler in order
    to detect this condition.
    
    Signed-off-by: Paul E. McKenney <paul.mckenney@linaro.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

commit 511e2526e765691e06b40abc22ee184cd60e0efb
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun Mar 22 02:18:26 2015 +0100

     rcu: Make RCU_FAST_NO_HZ account for pauses out of idle
    
    Both Steven Rostedt's new idle-capable trace macros and the RCU_NONIDLE()
    macro can cause RCU to momentarily pause out of idle without the rest
    of the system being involved.  This can cause rcu_prepare_for_idle()
    to run through its state machine too quickly, which can in turn result
    in needless scheduling-clock interrupts.
    
    This commit therefore adds code to enable rcu_prepare_for_idle() to
    distinguish between an initial entry to idle on the one hand (which needs
    to advance the rcu_prepare_for_idle() state machine) and an idle reentry
    due to idle-capable trace macros and RCU_NONIDLE() on the other hand
    (which should avoid advancing the rcu_prepare_for_idle() state machine).
    Additional state is maintained to allow the timer to be correctly reposted
    when returning after a momentary pause out of idle, and even more state
    is maintained to detect when new non-lazy callbacks have been enqueued
    (which may require re-evaluation of the approach to idleness).
    
    Signed-off-by: Paul E. McKenney <paul.mckenney@linaro.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

commit ba2b0294b1e7eabca7d9975d85cda6fea8662147
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun Mar 22 02:16:58 2015 +0100

     rcu: Make RCU_FAST_NO_HZ use timer rather than hrtimer
    
    The RCU_FAST_NO_HZ facility uses an hrtimer to wake up a CPU when
    it is allowed to go into dyntick-idle mode, which is almost always
    cancelled soon after.  This is not what hrtimers are good at, so
    this commit switches to the timer wheel.
    
    Reported-by: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Paul E. McKenney <paul.mckenney@linaro.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

commit ef6026630513e7e8e29b49c96343add64d662855
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun Mar 22 02:15:25 2015 +0100

     rcu: Add RCU_FAST_NO_HZ tracing for idle exit
    
    Traces of rcu_prep_idle events can be confusing because
    rcu_cleanup_after_idle() does no tracing.  This commit therefore adds
    this tracing.
    
    Signed-off-by: Paul E. McKenney <paul.mckenney@linaro.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

commit e4b5aa4d390a50351307de761a05912cdf702568
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun Mar 22 02:10:47 2015 +0100

     tick: Don't invoke tick_nohz_stop_sched_tick() if the cpu is offline
    
    commit 5b39939a4 (nohz: Move ts->idle_calls incrementation into strict
    idle logic) moved code out of tick_nohz_stop_sched_tick() and missed
    to bail out when the cpu is offline. That's causing subsequent
    failures as an offline CPU is supposed to die and not to fiddle with
    nohz magic.
    
    Return false in can_stop_idle_tick() if the cpu is offline.
    
    Reported-and-tested-by: Jiri Kosina <jkosina@suse.cz>
    Reported-and-tested-by: Prarit Bhargava <prarit@redhat.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: x86@kernel.org
    Link: http://lkml.kernel.org/r/alpine.LFD.2.02.1305132138160.2863@ionos
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    [zwliew: Backport to 3.4]
    Signed-off-by: Zhao Wei Liew <zhaoweiliew@gmail.com>
    
    Conflicts:
    	kernel/time/tick-sched.c

commit 480bef7cc03f92b9893d9ccf3ca8c5eb9d6abf42
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun Mar 22 02:09:55 2015 +0100

    timers: Improve get_next_timer_interrupt()
    
    Gilad reported at
    
     http://lkml.kernel.org/r/1336056962-10465-2-git-send-email-gilad@benyossef.com
    
    "Current timer code fails to correctly return a value meaning that
     there is no future timer event, with the result that the timer keeps
     getting re-armed in HZ one shot mode even when we could turn it off,
     generating unneeded interrupts.
    
     What is happening is that when __next_timer_interrupt() wishes
     to return a value that signifies "there is no future timer
     event", it returns (base->timer_jiffies + NEXT_TIMER_MAX_DELTA).
    
     However, the code in tick_nohz_stop_sched_tick(), which called
     __next_timer_interrupt() via get_next_timer_interrupt(),
     compares the return value to (last_jiffies + NEXT_TIMER_MAX_DELTA)
     to see if the timer needs to be re-armed.
    
     base->timer_jiffies != last_jiffies and so tick_nohz_stop_sched_tick()
     interperts the return value as indication that there is a distant
     future event 12 days from now and programs the timer to fire next
     after KTIME_MAX nsecs instead of avoiding to arm it. This ends up
     causing a needless interrupt once every KTIME_MAX nsecs."
    
    Fix this by using the new active timer accounting. This avoids scans
    when no active timer is enqueued completely, so we don't have to rely
    on base->timer_next and base->timer_jiffies anymore.
    
    Reported-by: Gilad Ben-Yossef <gilad@benyossef.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Link: http://lkml.kernel.org/r/20120525214819.317535385@linutronix.de

commit 0eaae5f9dfc55d99fcc384a5eca4b9670db44925
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun Mar 22 02:07:21 2015 +0100

     timers: Add accounting of non deferrable timers
    
    The code in get_next_timer_interrupt() is suboptimal as it has to run
    through the cascade to find the next expiring timer. On a completely
    idle core we should only do that when there is an active timer
    enqueued and base->next_timer does not give us a fast answer.
    
    Add accounting of the active timers to the now consolidated
    attach/detach code. I deliberately avoided sanity checks because the
    code is fully symetric and any fiddling with timers w/o using the API
    functions will lead to cute explosions anyway. ulong is big enough
    even on 32bit and if we really run into the situation to have more
    than 1<<32 timers enqueued there, then we are definitely not in a
    state to go idle and run through that code.
    
    This allows us to fix another shortcoming of get_next_timer_interrupt().
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Gilad Ben-Yossef <gilad@benyossef.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Link: http://lkml.kernel.org/r/20120525214819.236377028@linutronix.de

commit 3c01d613636493143464f5834ddfcc5692d92aa5
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun Mar 22 02:02:38 2015 +0100

     timers: Consolidate base->next_timer update
    
    Another bunch of mindlessly copied code. All callers of
    internal_add_timer() except the recascading code updates
    base->next_timer.
    
    Move this into internal_add_timer() and let the cascading code call
    __internal_add_timer().
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Gilad Ben-Yossef <gilad@benyossef.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Link: http://lkml.kernel.org/r/20120525214819.189946224@linutronix.de

commit 9c972d2e66a0a804060cddfa13de9ce81e128508
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun Mar 22 02:00:01 2015 +0100

     timers: Create detach_if_pending() and use it
    
    Most callers of detach_timer() have the same pattern around
    them. Check whether the timer is pending and eventually updating
    base->next_timer.
    
    Create detach_if_pending() and replace the duplicated code.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Gilad Ben-Yossef <gilad@benyossef.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Link: http://lkml.kernel.org/r/20120525214819.131246037@linutronix.de

commit ff512f8ff880e0dfccb06f07f0ecb6ce90131ba4
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun Mar 22 01:55:16 2015 +0100

     sched: Fix clear NOHZ_BALANCE_KICK
    
    I have faced a sequence where the Idle Load Balance was sometime not
    triggered for a while on my platform, in the following scenario:
    
     CPU 0 and CPU 1 are running tasks and CPU 2 is idle
    
     CPU 1 kicks the Idle Load Balance
     CPU 1 selects CPU 2 as the new Idle Load Balancer
     CPU 2 sets NOHZ_BALANCE_KICK for CPU 2
     CPU 2 sends a reschedule IPI to CPU 2
    
     While CPU 3 wakes up, CPU 0 or CPU 1 migrates a waking up task A on CPU 2
    
     CPU 2 finally wakes up, runs task A and discards the Idle Load Balance
           task A quickly goes back to sleep (before a tick occurs on CPU 2)
     CPU 2 goes back to idle with NOHZ_BALANCE_KICK set
    
    Whenever CPU 2 will be selected as the ILB, no reschedule IPI will be sent
    because NOHZ_BALANCE_KICK is already set and no Idle Load Balance will be
    performed.
    
    We must wait for the sched softirq to be raised on CPU 2 thanks to another
    part the kernel to come back to clear NOHZ_BALANCE_KICK.
    
    The proposed solution clears NOHZ_BALANCE_KICK in schedule_ipi if
    we can't raise the sched_softirq for the Idle Load Balance.
    
    Change since V1:
    
    - move the clear of NOHZ_BALANCE_KICK in got_nohz_idle_kick if the ILB
      can't run on this CPU (as suggested by Peter)
    
    Signed-off-by: Vincent Guittot <vincent.guittot@linaro.org>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/1370419991-13870-1-git-send-email-vincent.guittot@linaro.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Git-commit: 873b4c65b519fd769940eb281f77848227d4e5c1
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    [smuckle@codeaurora.org: minor merge resolution for 3.4 in scheduler_ipi()]
    Signed-off-by: Steve Muckle <smuckle@codeaurora.org>
    Change-Id: I3548612057cccc2ecc29429c129c44183083831f

commit f244211c31ede5da855ed6828d645ffb67bdd6b1
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun Mar 22 01:53:00 2015 +0100

     cfg80211: Fix memory leak
    
    When a driver requests a specific regulatory domain after cfg80211 already
    has one, a struct ieee80211_regdomain is leaked.
    
    Change-Id: Id28fc9861b9c911a97bd242439eabca097d76258
    Reported-by: Larry Finger <Larry.Finger@lwfinger.net>
    Tested-by: Larry Finger <Larry.Finger@lwfinger.net>
    Signed-off-by: Larry Finger <Larry.Finger@lwfinger.net>
    Signed-off-by: Johannes Berg <johannes.berg@intel.com>
    Git-commit: b7566fc363e23f0efd3fa1e1460f9421cdc0d77e
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    [mattw@codeaurora.org: trivially backport to the msm-3.4 kernel]
    Signed-off-by: Matt Wagantall <mattw@codeaurora.org>

commit c1d44e8ee056a4d6076e1b17b0d9551f8c2bb97c
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun Mar 22 01:50:33 2015 +0100

     cpufreq: Manage only online cpus
    
    cpufreq core doesn't manage offline cpus and if driver->init() has
    returned
    mask including offline cpus, it may result in unwanted behavior by
    cpufreq core
    or governors.
    
    We need to get only online cpus in this mask. There are two places to
    fix this
    mask, cpufreq core and cpufreq driver. It makes sense to do this at
    common place
    and hence is done in core.
    
    Signed-off-by: Viresh Kumar <viresh.kumar@linaro.org>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit cfbf1b33c33dd19ebe111be455b76cb27d8c54c8
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun Mar 22 01:49:40 2015 +0100

     sched: Set MC (multi-core) sched domain's busy_factor attribute to 1
    
    busy_factor attribute of a scheduler domain causes busy CPUs (CPUs
    that are not idle) to load balance less frequently in that domain,
    which could impact performance by increasing scheduling latency for
    tasks.
    
    As an example, consider MC scheduler domain's attribute values of
    max_interval = 4ms and busy_factor = 64. Further consider
    max_load_balance_interval = 100 (HZ/10). In this case, a non-idle CPU
    could put off load balance check in MC domain by 100ms. This
    effectively means that a CPU running a single task in its queue could
    fail to notice increased load on another CPU (that is in same MC
    domain) for upto 100ms, before picking up load from the overloaded
    CPU. Needless to say, this leads to increased scheduling latency for
    tasks, affecting performance adversely.
    
    By setting MC domain's busy_factor value to 1, we limit maximum
    interval that busy CPU can put off load balance checks to 4ms
    (effectively 10ms, given HZ value of 100).
    
    Change-Id: Id45869d06f5556ea8eec602b65c2ffd2143fe060
    Signed-off-by: Srivatsa Vaddagiri <vatsa@codeaurora.org>

commit a3f265bdc5ebc42b670185337ca9cbafbdb39717
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun Mar 22 01:48:00 2015 +0100

    workqeue: fix compiling errors (thanks googy anas)

commit 8931534b9b3e588a53c3a6c23432f0b2e17676b9
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun Mar 22 01:45:32 2015 +0100

     workqueue: convert BUG_ON()s in __queue_delayed_work() to WARN_ON_ONC
    
    E()s
    
    8852aac25e ("workqueue: mod_delayed_work_on() shouldn't queue timer on
    0 delay") unexpectedly uncovered a very nasty abuse of delayed_work in
    megaraid - it allocated work_struct, casted it to delayed_work and
    then pass that into queue_delayed_work().
    
    Previously, this was okay because 0 @delay short-circuited to
    queue_work() before doing anything with delayed_work.  8852aac25e
    moved 0 @delay test into __queue_delayed_work() after sanity check on
    delayed_work making megaraid trigger BUG_ON().
    
    Although megaraid is already fixed by c1d390d8e6 ("megaraid: fix
    BUG_ON() from incorrect use of delayed work"), this patch converts
    BUG_ON()s in __queue_delayed_work() to WARN_ON_ONCE()s so that such
    abusers, if there are more, trigger warning but don't crash the
    machine.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Xiaotian Feng <xtfeng@gmail.com>

commit eac6627e35ff4ae566b548f93d269f500d5a44bc
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun Mar 22 01:44:53 2015 +0100

     workqueue: add WARN_ON_ONCE() on CPU number to wq_worker_waking_up()
    
    Recently, workqueue code has gone through some changes and we found
    some bugs related to concurrency management operations happening on
    the wrong CPU.  When a worker is concurrency managed
    (!WORKER_NOT_RUNNIG), it should be bound to its associated cpu and
    woken up to that cpu.  Add WARN_ON_ONCE() to verify this.
    
    Signed-off-by: Joonsoo Kim <js1304@gmail.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

commit 527d2969a53e6bf7c2a57b1f46e8eb6de888183d
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun Mar 22 01:43:47 2015 +0100

     workqueue: trivial fix for return statement in work_busy()
    
    Return type of work_busy() is unsigned int.
    There is return statement returning boolean value, 'false' in work_busy().
    It is not problem, because 'false' may be treated '0'.
    However, fixing it would make code robust.
    
    Signed-off-by: Joonsoo Kim <js1304@gmail.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

commit 0f28677db1faedb2fee24a0ae870364450f3feb4
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun Mar 22 01:43:04 2015 +0100

     workqueue: mod_delayed_work_on() shouldn't queue timer on 0 delay
    
    8376fe22c7 ("workqueue: implement mod_delayed_work[_on]()")
    implemented mod_delayed_work[_on]() using the improved
    try_to_grab_pending().  The function is later used, among others, to
    replace [__]candel_delayed_work() + queue_delayed_work() combinations.
    
    Unfortunately, a delayed_work item w/ zero @delay is handled slightly
    differently by mod_delayed_work_on() compared to
    queue_delayed_work_on().  The latter skips timer altogether and
    directly queues it using queue_work_on() while the former schedules
    timer which will expire on the closest tick.  This means, when @delay
    is zero, that [__]cancel_delayed_work() + queue_delayed_work_on()
    makes the target item immediately executable while
    mod_delayed_work_on() may induce delay of upto a full tick.
    
    This somewhat subtle difference breaks some of the converted users.
    e.g. block queue plugging uses delayed_work for deferred processing
    and uses mod_delayed_work_on() when the queue needs to be immediately
    unplugged.  The above problem manifested as noticeably higher number
    of context switches under certain circumstances.
    
    The difference in behavior was caused by missing special case handling
    for 0 delay in mod_delayed_work_on() compared to
    queue_delayed_work_on().  Joonsoo Kim posted a patch to add it -
    ("workqueue: optimize mod_delayed_work_on() when @delay == 0")[1].
    The patch was queued for 3.8 but it was described as optimization and
    I missed that it was a correctness issue.
    
    As both queue_delayed_work_on() and mod_delayed_work_on() use
    __queue_delayed_work() for queueing, it seems that the better approach
    is to move the 0 delay special handling to the function instead of
    duplicating it in mod_delayed_work_on().
    
    Fix the problem by moving 0 delay special case handling from
    queue_delayed_work_on() to __queue_delayed_work().  This replaces
    Joonsoo's patch.
    
    [1] http://thread.gmane.org/gmane.linux.kernel/1379011/focus=1379012
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Reported-and-tested-by: Anders Kaseorg <andersk@MIT.EDU>
    Reported-and-tested-by: Zlatko Calusic <zlatko.calusic@iskon.hr>
    LKML-Reference: <alpine.DEB.2.00.1211280953350.26602@dr-wily.mit.edu>
    LKML-Reference: <50A78AA9.5040904@iskon.hr>
    Cc: Joonsoo Kim <js1304@gmail.com>

commit 769fcfa858b2577c5cc15c8ff890ba916b5db599
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun Mar 22 01:42:07 2015 +0100

     workqueue: exit rescuer_thread() as TASK_RUNNING
    
    A rescue thread exiting TASK_INTERRUPTIBLE can lead to a task scheduling
    off, never to be seen again.  In the case where this occurred, an exiting
    thread hit reiserfs homebrew conditional resched while holding a mutex,
    bringing the box to its knees.
    
    PID: 18105  TASK: ffff8807fd412180  CPU: 5   COMMAND: "kdmflush"
     #0 [ffff8808157e7670] schedule at ffffffff8143f489
     #1 [ffff8808157e77b8] reiserfs_get_block at ffffffffa038ab2d [reiserfs]
     #2 [ffff8808157e79a8] __block_write_begin at ffffffff8117fb14
     #3 [ffff8808157e7a98] reiserfs_write_begin at ffffffffa0388695 [reiserfs]
     #4 [ffff8808157e7ad8] generic_perform_write at ffffffff810ee9e2
     #5 [ffff8808157e7b58] generic_file_buffered_write at ffffffff810eeb41
     #6 [ffff8808157e7ba8] __generic_file_aio_write at ffffffff810f1a3a
     #7 [ffff8808157e7c58] generic_file_aio_write at ffffffff810f1c88
     #8 [ffff8808157e7cc8] do_sync_write at ffffffff8114f850
     #9 [ffff8808157e7dd8] do_acct_process at ffffffff810a268f
        [exception RIP: kernel_thread_helper]
        RIP: ffffffff8144a5c0  RSP: ffff8808157e7f58  RFLAGS: 00000202
        RAX: 0000000000000000  RBX: 0000000000000000  RCX: 0000000000000000
        RDX: 0000000000000000  RSI: ffffffff8107af60  RDI: ffff8803ee491d18
        RBP: 0000000000000000   R8: 0000000000000000   R9: 0000000000000000
        R10: 0000000000000000  R11: 0000000000000000  R12: 0000000000000000
        R13: 0000000000000000  R14: 0000000000000000  R15: 0000000000000000
        ORIG_RAX: ffffffffffffffff  CS: 0010  SS: 0018
    
    Signed-off-by: Mike Galbraith <mgalbraith@suse.de>
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: stable@vger.kernel.org

commit 1c8220512acfbc5c34845e1589be9a46434c06a8
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun Mar 22 01:41:24 2015 +0100

     workqueue: cancel_delayed_work() should return %false if work item is
    
     idle
    
    57b30ae77b ("workqueue: reimplement cancel_delayed_work() using
    try_to_grab_pending()") made cancel_delayed_work() always return %true
    unless someone else is also trying to cancel the work item, which is
    broken - if the target work item is idle, the return value should be
    %false.
    
    try_to_grab_pending() indicates that the target work item was idle by
    zero return value.  Use it for return.  Note that this brings
    cancel_delayed_work() in line with __cancel_work_timer() in return
    value handling.
    
    Signed-off-by: Dan Magenheimer <dan.magenheimer@oracle.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>
    LKML-Reference: <444a6439-b1a4-4740-9e7e-bc37267cfe73@default>

commit 948176ac7777e259fed94e74cc41fcd893c5c878
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun Mar 22 01:40:53 2015 +0100

     workqueue: remove spurious WARN_ON_ONCE(in_irq()) from try_to_grab_pe
    
    nding()
    
    e0aecdd874 ("workqueue: use irqsafe timer for delayed_work") made
    try_to_grab_pending() safe to use from irq context but forgot to
    remove WARN_ON_ONCE(in_irq()).  Remove it.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Reported-by: Fengguang Wu <fengguang.wu@intel.com>

commit 242aa40508fc144e23a7f5cff7c79e6047cc00e2
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun Mar 22 01:40:21 2015 +0100

     workqueue: use cwq_set_max_active() helper for workqueue_set_max_acti
    
    ve()
    
    workqueue_set_max_active() may increase ->max_active without
    activating delayed works and may make the activation order differ from
    the queueing order.  Both aren't strictly bugs but the resulting
    behavior could be a bit odd.
    
    To make things more consistent, use cwq_set_max_active() helper which
    immediately makes use of the newly increased max_mactive if there are
    delayed work items and also keeps the activation order.
    
    tj: Slight update to description.
    
    Signed-off-by: Lai Jiangshan <laijs@cn.fujitsu.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

commit 55bf15e1c60d01b38d89c0a373ee7b1bf05b8146
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun Mar 22 01:39:52 2015 +0100

     workqueue: introduce cwq_set_max_active() helper for thaw_workqueues()
    
    Using a helper instead of open code makes thaw_workqueues() clearer.
    The helper will also be used by the next patch.
    
    tj: Slight update to comment and description.
    
    Signed-off-by: Lai Jiangshan <laijs@cn.fujitsu.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

commit 29789baf7686eb406922ff06a26dda6ead214f3d
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun Mar 22 01:38:47 2015 +0100

     workqueue: reimplement work_on_cpu() using system_wq
    
    The existing work_on_cpu() implementation is hugely inefficient.  It
    creates a new kthread, execute that single function and then let the
    kthread die on each invocation.
    
    Now that system_wq can handle concurrent executions, there's no
    advantage of doing this.  Reimplement work_on_cpu() using system_wq
    which makes it simpler and way more efficient.
    
    stable: While this isn't a fix in itself, it's needed to fix a
            workqueue related bug in cpufreq/powernow-k8.  AFAICS, this
            shouldn't break other existing users.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Acked-by: Jiri Kosina <jkosina@suse.cz>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Bjorn Helgaas <bhelgaas@google.com>
    Cc: Len Brown <lenb@kernel.org>
    Cc: Rafael J. Wysocki <rjw@sisk.pl>
    Cc: stable@vger.kernel.org

commit 8d3c5eb1c3c9eae34dcebdd99ed8858d58fa37cf
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun Mar 22 01:37:39 2015 +0100

     workqueue: fix possible stall on try_to_grab_pending() of a delayed w
    
    ork item
    
    Currently, when try_to_grab_pending() grabs a delayed work item, it
    leaves its linked work items alone on the delayed_works.  The linked
    work items are always NO_COLOR and will cause future
    cwq_activate_first_delayed() increase cwq->nr_active incorrectly, and
    may cause the whole cwq to stall.  For example,
    
    state: cwq->max_active = 1, cwq->nr_active = 1
           one work in cwq->pool, many in cwq->delayed_works.
    
    step1: try_to_grab_pending() removes a work item from delayed_works
           but leaves its NO_COLOR linked work items on it.
    
    step2: Later on, cwq_activate_first_delayed() activates the linked
           work item increasing ->nr_active.
    
    step3: cwq->nr_active = 1, but all activated work items of the cwq are
           NO_COLOR.  When they finish, cwq->nr_active will not be
           decreased due to NO_COLOR, and no further work items will be
           activated from cwq->delayed_works. the cwq stalls.
    
    Fix it by ensuring the target work item is activated before stealing
    PENDING in try_to_grab_pending().  This ensures that all the linked
    work items are activated without incorrectly bumping cwq->nr_active.
    
    tj: Updated comment and description.
    
    Signed-off-by: Lai Jiangshan <laijs@cn.fujitsu.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: stable@kernel.org

commit feeb4286fe06cbc9082cdc927a443dd9849befed
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun Mar 22 01:36:14 2015 +0100

     workqueue: use __cpuinit instead of __devinit for cpu callbacks
    
    For workqueue hotplug callbacks, it makes less sense to use __devinit
    which discards the memory after boot if !HOTPLUG.  __cpuinit, which
    discards the memory after boot if !HOTPLUG_CPU fits better.
    
    tj: Updated description.
    
    Signed-off-by: Lai Jiangshan <laijs@cn.fujitsu.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

commit f2cc83eb2e797b6ddea7601bc37ad5ef749ba7fe
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun Mar 22 01:35:04 2015 +0100

     workqueue: rename manager_mutex to assoc_mutex
    
    Now that manager_mutex's role has changed from synchronizing manager
    role to excluding hotplug against manager, the name is misleading.
    
    As it is protecting the CPU-association of the gcwq now, rename it to
    assoc_mutex.
    
    This patch is pure rename and doesn't introduce any functional change.
    
    tj: Updated comments and description.
    
    Signed-off-by: Lai Jiangshan <laijs@cn.fujitsu.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

commit 7dee2d780f85b8c87ec848297df49f31f2accd14
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sat Mar 21 19:30:56 2015 +0100

     workqueue: WORKER_REBIND is no longer necessary for idle rebinding
    
    Now both worker destruction and idle rebinding remove the worker from
    idle list while it's still idle, so list_empty(&worker->entry) can be
    used to test whether either is pending and WORKER_DIE to distinguish
    between the two instead making WORKER_REBIND unnecessary.
    
    Use list_empty(&worker->entry) to determine whether destruction or
    rebinding is pending.  This simplifies worker state transitions.
    
    WORKER_REBIND is not needed anymore.  Remove it.
    
    tj: Updated comments and description.
    
    Signed-off-by: Lai Jiangshan <laijs@cn.fujitsu.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

commit 2b7c6e4f969a1244c933da0b36be29fea0188703
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sat Mar 21 19:28:28 2015 +0100

     workqueue: WORKER_REBIND is no longer necessary for busy rebinding
    
    Because the old unbind/rebinding implementation wasn't atomic w.r.t.
    GCWQ_DISASSOCIATED manipulation which is protected by
    global_cwq->lock, we had to use two flags, WORKER_UNBOUND and
    WORKER_REBIND, to avoid incorrectly losing all NOT_RUNNING bits with
    back-to-back CPU hotplug operations; otherwise, completion of
    rebinding while another unbinding is in progress could clear UNBIND
    prematurely.
    
    Now that both unbind/rebinding are atomic w.r.t. GCWQ_DISASSOCIATED,
    there's no need to use two flags.  Just one is enough.  Don't use
    WORKER_REBIND for busy rebinding.
    
    tj: Updated description.
    
    Signed-off-by: Lai Jiangshan <laijs@cn.fujitsu.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

commit ae548fe392a25a1430bf18162c95dc2afa585fba
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sat Mar 21 19:27:10 2015 +0100

     workqueue: reimplement idle worker rebinding
    
    Currently rebind_workers() uses rebinds idle workers synchronously
    before proceeding to requesting busy workers to rebind.  This is
    necessary because all workers on @worker_pool->idle_list must be bound
    before concurrency management local wake-ups from the busy workers
    take place.
    
    Unfortunately, the synchronous idle rebinding is quite complicated.
    This patch reimplements idle rebinding to simplify the code path.
    
    Rather than trying to make all idle workers bound before rebinding
    busy workers, we simply remove all to-be-bound idle workers from the
    idle list and let them add themselves back after completing rebinding
    (successful or not).
    
    As only workers which finished rebinding can on on the idle worker
    list, the idle worker list is guaranteed to have only bound workers
    unless CPU went down again and local wake-ups are safe.
    
    After the change, @worker_pool->nr_idle may deviate than the actual
    number of idle workers on @worker_pool->idle_list.  More specifically,
    nr_idle may be non-zero while ->idle_list is empty.  All users of
    ->nr_idle and ->idle_list are audited.  The only affected one is
    too_many_workers() which is updated to check %false if ->idle_list is
    empty regardless of ->nr_idle.
    
    After this patch, rebind_workers() no longer performs the nasty
    idle-rebind retries which require temporary release of gcwq->lock, and
    both unbinding and rebinding are atomic w.r.t. global_cwq->lock.
    
    worker->idle_rebind and global_cwq->rebind_hold are now unnecessary
    and removed along with the definition of struct idle_rebind.
    
    Changed from V1:
    	1) remove unlikely from too_many_workers(), ->idle_list can be empty
    	   anytime, even before this patch, no reason to use unlikely.
    	2) fix a small rebasing mistake.
    	   (which is from rebasing the orignal fixing patch to for-next)
    	3) add a lot of comments.
    	4) clear WORKER_REBIND unconditionaly in idle_worker_rebind()
    
    tj: Updated comments and description.
    
    Signed-off-by: Lai Jiangshan <laijs@cn.fujitsu.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

commit 791b41f8888c7d4c23124e5a08d34fc86ec96260
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sat Mar 21 19:23:09 2015 +0100

     workqueue: always clear WORKER_REBIND in busy_worker_rebind_fn()
    
    busy_worker_rebind_fn() didn't clear WORKER_REBIND if rebinding failed
    (CPU is down again).  This used to be okay because the flag wasn't
    used for anything else.
    
    However, after 25511a477 "workqueue: reimplement CPU online rebinding
    to handle idle workers", WORKER_REBIND is also used to command idle
    workers to rebind.  If not cleared, the worker may confuse the next
    CPU_UP cycle by having REBIND spuriously set or oops / get stuck by
    prematurely calling idle_worker_rebind().
    
      WARNING: at /work/os/wq/kernel/workqueue.c:1323 worker_thread+0x4cd/0x5
     00()
      Hardware name: Bochs
      Modules linked in: test_wq(O-)
      Pid: 33, comm: kworker/1:1 Tainted: G           O 3.6.0-rc1-work+ #3
      Call Trace:
       [<ffffffff8109039f>] warn_slowpath_common+0x7f/0xc0
       [<ffffffff810903fa>] warn_slowpath_null+0x1a/0x20
       [<ffffffff810b3f1d>] worker_thread+0x4cd/0x500
       [<ffffffff810bc16e>] kthread+0xbe/0xd0
       [<ffffffff81bd2664>] kernel_thread_helper+0x4/0x10
      ---[ end trace e977cf20f4661968 ]---
      BUG: unable to handle kernel NULL pointer dereference at           (null)
      IP: [<ffffffff810b3db0>] worker_thread+0x360/0x500
      PGD 0
      Oops: 0000 [#1] PREEMPT SMP DEBUG_PAGEALLOC
      Modules linked in: test_wq(O-)
      CPU 0
      Pid: 33, comm: kworker/1:1 Tainted: G        W  O 3.6.0-rc1-work+ #3 Bochs Bochs
      RIP: 0010:[<ffffffff810b3db0>]  [<ffffffff810b3db0>] worker_thread+0x360/0x500
      RSP: 0018:ffff88001e1c9de0  EFLAGS: 00010086
      RAX: 0000000000000000 RBX: ffff88001e633e00 RCX: 0000000000004140
      RDX: 0000000000000000 RSI: 0000000000000000 RDI: 0000000000000009
      RBP: ffff88001e1c9ea0 R08: 0000000000000000 R09: 0000000000000001
      R10: 0000000000000002 R11: 0000000000000000 R12: ffff88001fc8d580
      R13: ffff88001fc8d590 R14: ffff88001e633e20 R15: ffff88001e1c6900
      FS:  0000000000000000(0000) GS:ffff88001fc00000(0000) knlGS:0000000000000000
      CS:  0010 DS: 0000 ES: 0000 CR0: 000000008005003b
      CR2: 0000000000000000 CR3: 00000000130e8000 CR4: 00000000000006f0
      DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000
      DR3: 0000000000000000 DR6: 00000000ffff0ff0 DR7: 0000000000000400
      Process kworker/1:1 (pid: 33, threadinfo ffff88001e1c8000, task ffff88001e1c6900)
      Stack:
       ffff880000000000 ffff88001e1c9e40 0000000000000001 ffff88001e1c8010
       ffff88001e519c78 ffff88001e1c9e58 ffff88001e1c6900 ffff88001e1c6900
       ffff88001e1c6900 ffff88001e1c6900 ffff88001fc8d340 ffff88001fc8d340
      Call Trace:
       [<ffffffff810bc16e>] kthread+0xbe/0xd0
       [<ffffffff81bd2664>] kernel_thread_helper+0x4/0x10
      Code: b1 00 f6 43 48 02 0f 85 91 01 00 00 48 8b 43 38 48 89 df 48 8b 00 48 89 45 90 e8 ac f0 ff ff 3c 01 0f 85 60 01 00 00 48 8b 53 50 <8b> 02 83 e8 01 85 c0 89 02 0f 84 3b 01 00 00 48 8b 43 38 48 8b
      RIP  [<ffffffff810b3db0>] worker_thread+0x360/0x500
       RSP <ffff88001e1c9de0>
      CR2: 0000000000000000
    
    There was no reason to keep WORKER_REBIND on failure in the first
    place - WORKER_UNBOUND is guaranteed to be set in such cases
    preventing incorrectly activating concurrency management.  Always
    clear WORKER_REBIND.
    
    tj: Updated comment and description.
    
    Signed-off-by: Lai Jiangshan <laijs@cn.fujitsu.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

commit e2bca533da1fad9447462a67d0f1854b4b5f2929
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sat Mar 21 19:21:55 2015 +0100

     workqueue: fix possible idle worker depletion across CPU hotplug
    
    To simplify both normal and CPU hotplug paths, worker management is
    prevented while CPU hoplug is in progress.  This is achieved by CPU
    hotplug holding the same exclusion mechanism used by workers to ensure
    there's only one manager per pool.
    
    If someone else seems to be performing the manager role, workers
    proceed to execute work items.  CPU hotplug using the same mechanism
    can lead to idle worker depletion because all workers could proceed to
    execute work items while CPU hotplug is in progress and CPU hotplug
    itself wouldn't actually perform the worker management duty - it
    doesn't guarantee that there's an idle worker left when it releases
    management.
    
    This idle worker depletion, under extreme circumstances, can break
    forward-progress guarantee and thus lead to deadlock.
    
    This patch fixes the bug by using separate mechanisms for manager
    exclusion among workers and hotplug exclusion.  For manager exclusion,
    POOL_MANAGING_WORKERS which was restored by the previous patch is
    used.  pool->manager_mutex is now only used for exclusion between the
    elected manager and CPU hotplug.  The elected manager won't proceed
    without holding pool->manager_mutex.
    
    This ensures that the worker which won the manager position can't skip
    managing while CPU hotplug is in progress.  It will block on
    manager_mutex and perform management after CPU hotplug is complete.
    
    Note that hotplug may happen while waiting for manager_mutex.  A
    manager isn't either on idle or busy list and thus the hoplug code
    can't unbind/rebind it.  Make the manager handle its own un/rebinding.
    
    tj: Updated comment and description.
    
    Signed-off-by: Lai Jiangshan <laijs@cn.fujitsu.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

commit 1d753f62b890ba0139369920c368ed206c993a70
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sat Mar 21 19:20:31 2015 +0100

     workqueue: restore POOL_MANAGING_WORKERS
    
    This patch restores POOL_MANAGING_WORKERS which was replaced by
    pool->manager_mutex by 6037315269 "workqueue: use mutex for global_cwq
    manager exclusion".
    
    There's a subtle idle worker depletion bug across CPU hotplug events
    and we need to distinguish an actual manager and CPU hotplug
    preventing management.  POOL_MANAGING_WORKERS will be used for the
    former and manager_mutex the later.
    
    This patch just lays POOL_MANAGING_WORKERS on top of the existing
    manager_mutex and doesn't introduce any synchronization changes.  The
    next patch will update it.
    
    Note that this patch fixes a non-critical anomaly where
    too_many_workers() may return %true spuriously while CPU hotplug is in
    progress.  While the issue could schedule idle timer spuriously, it
    didn't trigger any actual misbehavior.
    
    tj: Rewrote patch description.
    
    Signed-off-by: Lai Jiangshan <laijs@cn.fujitsu.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

commit d94770b8e5bd5369bc28e16ec2e97889391f18e2
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sat Mar 21 19:19:01 2015 +0100

     workqueue: fix possible deadlock in idle worker rebinding
    
    Currently, rebind_workers() and idle_worker_rebind() are two-way
    interlocked.  rebind_workers() waits for idle workers to finish
    rebinding and rebound idle workers wait for rebind_workers() to finish
    rebinding busy workers before proceeding.
    
    Unfortunately, this isn't enough.  The second wait from idle workers
    is implemented as follows.
    
    	wait_event(gcwq->rebind_hold, !(worker->flags & WORKER_REBIND));
    
    rebind_workers() clears WORKER_REBIND, wakes up the idle workers and
    then returns.  If CPU hotplug cycle happens again before one of the
    idle workers finishes the above wait_event(), rebind_workers() will
    repeat the first part of the handshake - set WORKER_REBIND again and
    wait for the idle worker to finish rebinding - and this leads to
    deadlock because the idle worker would be waiting for WORKER_REBIND to
    clear.
    
    This is fixed by adding another interlocking step at the end -
    rebind_workers() now waits for all the idle workers to finish the
    above WORKER_REBIND wait before returning.  This ensures that all
    rebinding steps are complete on all idle workers before the next
    hotplug cycle can happen.
    
    This problem was diagnosed by Lai Jiangshan who also posted a patch to
    fix the issue, upon which this patch is based.
    
    This is the minimal fix and further patches are scheduled for the next
    merge window to simplify the CPU hotplug path.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Original-patch-by: Lai Jiangshan <laijs@cn.fujitsu.com>
    LKML-Reference: <1346516916-1991-3-git-send-email-laijs@cn.fujitsu.com>

commit a60fc053c8a9453710d1d50076420fb20227bb58
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sat Mar 21 19:17:40 2015 +0100

     workqueue: move WORKER_REBIND clearing in rebind_workers() to the end
    
     of the function
    
    This doesn't make any functional difference and is purely to help the
    next patch to be simpler.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Lai Jiangshan <laijs@cn.fujitsu.com>

commit 2326e4227485599ae5ef0f2781d9df7acc1a7527
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sat Mar 21 19:15:36 2015 +0100

     workqueue: UNBOUND -> REBIND morphing in rebind_workers() should be a
    
    tomic
    
    The compiler may compile the following code into TWO write/modify
    instructions.
    
    	worker->flags &= ~WORKER_UNBOUND;
    	worker->flags |= WORKER_REBIND;
    
    so the other CPU may temporarily see worker->flags which doesn't have
    either WORKER_UNBOUND or WORKER_REBIND set and perform local wakeup
    prematurely.
    
    Fix it by using single explicit assignment via ACCESS_ONCE().
    
    Because idle workers have another WORKER_NOT_RUNNING flag, this bug
    doesn't exist for them; however, update it to use the same pattern for
    consistency.
    
    tj: Applied the change to idle workers too and updated comments and
        patch description a bit.
    
    Signed-off-by: Lai Jiangshan <laijs@cn.fujitsu.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: stable@vger.kernel.org
    
    Conflicts:
    	kernel/workqueue.c

commit 297a19942e7b710a2d3af76d2fc4e95eeba4c0a7
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sat Mar 21 19:14:20 2015 +0100

     workqueue: reimplement cancel_delayed_work() using try_to_grab_pending()
    
    cancel_delayed_work() can't be called from IRQ handlers due to its use
    of del_timer_sync() and can't cancel work items which are already
    transferred from timer to worklist.
    
    Also, unlike other flush and cancel functions, a canceled delayed_work
    would still point to the last associated cpu_workqueue.  If the
    workqueue is destroyed afterwards and the work item is re-used on a
    different workqueue, the queueing code can oops trying to dereference
    already freed cpu_workqueue.
    
    This patch reimplements cancel_delayed_work() using
    try_to_grab_pending() and set_work_cpu_and_clear_pending().  This
    allows the function to be called from IRQ handlers and makes its
    behavior consistent with other flush / cancel functions.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>

commit 85d9a3c31587c97bae078b383bbe78ea2a7d7e17
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sat Mar 21 19:12:05 2015 +0100

     workqueue: cosmetic whitespace updates for macro definitions
    
    Consistently use the last tab position for '\' line continuation in
    complex macro definitions.  This is to help the following patches.
    
    This patch is cosmetic.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>

commit 0783cb3a73d9e37ad4bcc5439a69199d1955c468
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sat Mar 21 19:11:08 2015 +0100

     workqueue: use hotcpu_notifier() for workqueue_cpu_down_callback()
    
    workqueue_cpu_down_callback() is used only if HOTPLUG_CPU=y, so
    hotcpu_notifier() fits better than cpu_notifier().
    
    When HOTPLUG_CPU=y, hotcpu_notifier() and cpu_notifier() are the same.
    
    When HOTPLUG_CPU=n, if we use cpu_notifier(),
    workqueue_cpu_down_callback() will be called during boot to do
    nothing, and the memory of workqueue_cpu_down_callback() and
    gcwq_unbind_fn() will be discarded after boot.
    
    If we use hotcpu_notifier(), we can avoid the no-op call of
    workqueue_cpu_down_callback() and the memory of
    workqueue_cpu_down_callback() and gcwq_unbind_fn() will be discard at
    build time:
    
    $ ls -l kernel/workqueue.o.cpu_notifier kernel/workqueue.o.hotcpu_notifier
    -rw-rw-r-- 1 laijs laijs 484080 Sep 15 11:31 kernel/workqueue.o.cpu_notifier
    -rw-rw-r-- 1 laijs laijs 478240 Sep 15 11:31 kernel/workqueue.o.hotcpu_notifier
    
    $ size kernel/workqueue.o.cpu_notifier kernel/workqueue.o.hotcpu_notifier
       text	   data	    bss	    dec	    hex	filename
      18513	   2387	   1221	  22121	   5669	kernel/workqueue.o.cpu_notifier
      18082	   2355	   1221	  21658	   549a	kernel/workqueue.o.hotcpu_notifier
    
    tj: Updated description.
    
    Signed-off-by: Lai Jiangshan <laijs@cn.fujitsu.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

commit 1dcae1f1258b233222f8df36b467e955e65711cd
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sat Mar 21 19:09:24 2015 +0100

     workqueue: gut system_nrt[_freezable]_wq()
    
    Now that all workqueues are non-reentrant, system[_freezable]_wq() are
    equivalent to system_nrt[_freezable]_wq().  Replace the latter with
    wrappers around system[_freezable]_wq().  The wrapping goes through
    inline functions so that __deprecated can be added easily.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>

commit eb7d288dcc6bed7bf17f02d7586b1d1bda59197e
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sat Mar 21 19:06:42 2015 +0100

     workqueue: gut flush[_delayed]_work_sync()
    
    Now that all workqueues are non-reentrant, flush[_delayed]_work_sync()
    are equivalent to flush[_delayed]_work().  Drop the separate
    implementation and make them thin wrappers around
    flush[_delayed]_work().
    
    * start_flush_work() no longer takes @wait_executing as the only left
      user - flush_work() - always sets it to %true.
    
    * __cancel_work_timer() uses flush_work() instead of wait_on_work().
    
    Signed-off-by: Tejun Heo <tj@kernel.org>

commit bbbb6a8cd84e26e1a61dff7b1f66cdbe264d3b9d
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sat Mar 21 19:02:36 2015 +0100

     workqueue: make all workqueues non-reentrant
    
    By default, each per-cpu part of a bound workqueue operates separately
    and a work item may be executing concurrently on different CPUs.  The
    behavior avoids some cross-cpu traffic but leads to subtle weirdities
    and not-so-subtle contortions in the API.
    
    * There's no sane usefulness in allowing a single work item to be
      executed concurrently on multiple CPUs.  People just get the
      behavior unintentionally and get surprised after learning about it.
      Most either explicitly synchronize or use non-reentrant/ordered
      workqueue but this is error-prone.
    
    * flush_work() can't wait for multiple instances of the same work item
      on different CPUs.  If a work item is executing on cpu0 and then
      queued on cpu1, flush_work() can only wait for the one on cpu1.
    
      Unfortunately, work items can easily cross CPU boundaries
      unintentionally when the queueing thread gets migrated.  This means
      that if multiple queuers compete, flush_work() can't even guarantee
      that the instance queued right before it is finished before
      returning.
    
    * flush_work_sync() was added to work around some of the deficiencies
      of flush_work().  In addition to the usual flushing, it ensures that
      all currently executing instances are finished before returning.
      This operation is expensive as it has to walk all CPUs and at the
      same time fails to address competing queuer case.
    
      Incorrectly using flush_work() when flush_work_sync() is necessary
      is an easy error to make and can lead to bugs which are difficult to
      reproduce.
    
    * Similar problems exist for flush_delayed_work[_sync]().
    
    Other than the cross-cpu access concern, there's no benefit in
    allowing parallel execution and it's plain silly to have this level of
    contortion for workqueue which is widely used from core code to
    extremely obscure drivers.
    
    This patch makes all workqueues non-reentrant.  If a work item is
    executing on a different CPU when queueing is requested, it is always
    queued to that CPU.  This guarantees that any given work item can be
    executing on one CPU at maximum and if a work item is queued and
    executing, both are on the same CPU.
    
    The only behavior change which may affect workqueue users negatively
    is that non-reentrancy overrides the affinity specified by
    queue_work_on().  On a reentrant workqueue, the affinity specified by
    queue_work_on() is always followed.  Now, if the work item is
    executing on one of the CPUs, the work item will be queued there
    regardless of the requested affinity.  I've reviewed all workqueue
    users which request explicit affinity, and, fortunately, none seems to
    be crazy enough to exploit parallel execution of the same work item.
    
    This adds an additional busy_hash lookup if the work item was
    previously queued on a different CPU.  This shouldn't be noticeable
    under any sane workload.  Work item queueing isn't a very
    high-frequency operation and they don't jump across CPUs all the time.
    In a micro benchmark to exaggerate this difference - measuring the
    time it takes for two work items to repeatedly jump between two CPUs a
    number (10M) of times with busy_hash table densely populated, the
    difference was around 3%.
    
    While the overhead is measureable, it is only visible in pathological
    cases and the difference isn't huge.  This change brings much needed
    sanity to workqueue and makes its behavior consistent with timer.  I
    think this is the right tradeoff to make.
    
    This enables significant simplification of workqueue API.
    Simplification patches will follow.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>

commit 29562448dd78956b120ef1ff8bc3e7d548149e65
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sat Mar 21 19:01:04 2015 +0100

     workqueue: fix checkpatch issues
    
    Fixed some checkpatch warnings.
    
    tj: adapted to wq/for-3.7 and massaged pr_xxx() format strings a bit.
    
    Signed-off-by: Valentin Ilie <valentin.ilie@gmail.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>
    LKML-Reference: <1345326762-21747-1-git-send-email-valentin.ilie@gmail.com>
    
    Conflicts:
    	kernel/workqueue.c

commit c060000af7dee87c2983995ab79e7544828fe253
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sat Mar 21 18:59:39 2015 +0100

     workqueue: use system_highpri_wq for unbind_work
    
    To speed cpu down processing up, use system_highpri_wq.
    As scheduling priority of workers on it is higher than system_wq and
    it is not contended by other normal works on this cpu, work on it
    is processed faster than system_wq.
    
    tj: CPU up/downs care quite a bit about latency these days.  This
        shouldn't hurt anything and makes sense.
    
    Signed-off-by: Joonsoo Kim <js1304@gmail.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

commit d6189d81bb7775e5ccc84184388be78b454a3593
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sat Mar 21 18:58:59 2015 +0100

     workqueue: use system_highpri_wq for highpri workers in rebind_workers()
    
    In rebind_workers(), we do inserting a work to rebind to cpu for busy workers.
    Currently, in this case, we use only system_wq. This makes a possible
    error situation as there is mismatch between cwq->pool and worker->pool.
    
    To prevent this, we should use system_highpri_wq for highpri worker
    to match theses. This implements it.
    
    tj: Rephrased comment a bit.
    
    Signed-off-by: Joonsoo Kim <js1304@gmail.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

commit f97702698d80549a815060bd8ce221ccd8d26154
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sat Mar 21 18:56:28 2015 +0100

     workqueue: introduce system_highpri_wq
    
    Commit 3270476a6c0ce322354df8679652f060d66526dc ('workqueue: reimplement
    WQ_HIGHPRI using a separate worker_pool') introduce separate worker pool
    for HIGHPRI. When we handle busyworkers for gcwq, it can be normal worker
    or highpri worker. But, we don't consider this difference in rebind_workers(),
    we use just system_wq for highpri worker. It makes mismatch between
    cwq->pool and worker->pool.
    
    It doesn't make error in current implementation, but possible in the future.
    Now, we introduce system_highpri_wq to use proper cwq for highpri workers
    in rebind_workers(). Following patch fix this issue properly.
    
    tj: Even apart from rebinding, having system_highpri_wq generally
        makes sense.
    
    Signed-off-by: Joonsoo Kim <js1304@gmail.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

commit 9aaed63fc57b373655065ff8805217ae09289240
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sat Mar 21 18:55:09 2015 +0100

     workqueue: change value of lcpu in __queue_delayed_work_on()
    
    We assign cpu id into work struct's data field in __queue_delayed_work_on().
    In current implementation, when work is come in first time,
    current running cpu id is assigned.
    If we do __queue_delayed_work_on() with CPU A on CPU B,
    __queue_work() invoked in delayed_work_timer_fn() go into
    the following sub-optimal path in case of WQ_NON_REENTRANT.
    
    	gcwq = get_gcwq(cpu);
    	if (wq->flags & WQ_NON_REENTRANT &&
    		(last_gcwq = get_work_gcwq(work)) && last_gcwq != gcwq) {
    
    Change lcpu to @cpu and rechange lcpu to local cpu if lcpu is WORK_CPU_UNBOUND.
    It is sufficient to prevent to go into sub-optimal path.
    
    tj: Slightly rephrased the comment.
    
    Signed-off-by: Joonsoo Kim <js1304@gmail.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

commit c75723752d5b083f1f3d1a62ba553eecd1f50dd5
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sat Mar 21 18:53:42 2015 +0100

     workqueue: correct req_cpu in trace_workqueue_queue_work()
    
    When we do tracing workqueue_queue_work(), it records requested cpu.
    But, if !(@wq->flag & WQ_UNBOUND) and @cpu is WORK_CPU_UNBOUND,
    requested cpu is changed as local cpu.
    In case of @wq->flag & WQ_UNBOUND, above change is not occured,
    therefore it is reasonable to correct it.
    
    Use temporary local variable for storing requested cpu.
    
    Signed-off-by: Joonsoo Kim <js1304@gmail.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

commit ccdfc1dfb8caa06536392957f809d161965478e4
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sat Mar 21 18:52:21 2015 +0100

     workqueue: use enum value to set array size of pools in gcwq
    
    Commit 3270476a6c0ce322354df8679652f060d66526dc ('workqueue: reimplement
    WQ_HIGHPRI using a separate worker_pool') introduce separate worker_pool
    for HIGHPRI. Although there is NR_WORKER_POOLS enum value which represent
    size of pools, definition of worker_pool in gcwq doesn't use it.
    Using it makes code robust and prevent future mistakes.
    So change code to use this enum value.
    
    Signed-off-by: Joonsoo Kim <js1304@gmail.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

commit 69c2175f563f9a1c140e0a0d687cd0ac5ad87aa8
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sat Mar 21 18:50:41 2015 +0100

     workqueue: add missing wmb() in clear_work_data()
    
    Any operation which clears PENDING should be preceded by a wmb to
    guarantee that the next PENDING owner sees all the changes made before
    PENDING release.
    
    There are only two places where PENDING is cleared -
    set_work_cpu_and_clear_pending() and clear_work_data().  The caller of
    the former already does smp_wmb() but the latter doesn't have any.
    
    Move the wmb above set_work_cpu_and_clear_pending() into it and add
    one to clear_work_data().
    
    There hasn't been any report related to this issue, and, given how
    clear_work_data() is used, it is extremely unlikely to have caused any
    actual problems on any architecture.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Oleg Nesterov <oleg@redhat.com>

commit 7a7ab9af2e0b7c983a5da050f598d5de7e984e5b
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sat Mar 21 18:48:56 2015 +0100

     workqueue: fix CPU binding of flush_delayed_work[_sync]()
    
    delayed_work encodes the workqueue to use and the last CPU in
    delayed_work->work.data while it's on timer.  The target CPU is
    implicitly recorded as the CPU the timer is queued on and
    delayed_work_timer_fn() queues delayed_work->work to the CPU it is
    running on.
    
    Unfortunately, this leaves flush_delayed_work[_sync]() no way to find
    out which CPU the delayed_work was queued for when they try to
    re-queue after killing the timer.  Currently, it chooses the local CPU
    flush is running on.  This can unexpectedly move a delayed_work queued
    on a specific CPU to another CPU and lead to subtle errors.
    
    There isn't much point in trying to save several bytes in struct
    delayed_work, which is already close to a hundred bytes on 64bit with
    all debug options turned off.  This patch adds delayed_work->cpu to
    remember the CPU it's queued for.
    
    Note that if the timer is migrated during CPU down, the work item
    could be queued to the downed global_cwq after this change.  As a
    detached global_cwq behaves like an unbound one, this doesn't change
    much for the delayed_work.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>

commit 0179ab9acc7e71052e298fa178446bddf673f4ab
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sat Mar 21 18:47:03 2015 +0100

     workqueue: implement mod_delayed_work[_on]()
    
    Workqueue was lacking a mechanism to modify the timeout of an already
    pending delayed_work.  delayed_work users have been working around
    this using several methods - using an explicit timer + work item,
    messing directly with delayed_work->timer, and canceling before
    re-queueing, all of which are error-prone and/or ugly.
    
    This patch implements mod_delayed_work[_on]() which behaves similarly
    to mod_timer() - if the delayed_work is idle, it's queued with the
    given delay; otherwise, its timeout is modified to the new value.
    Zero @delay guarantees immediate execution.
    
    v2: Updated to reflect try_to_grab_pending() changes.  Now safe to be
        called from bh context.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Ingo Molnar <mingo@redhat.com>

commit 71a2d11000628fa4fa57d6b0c33b53eefda401c5
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sat Mar 21 18:44:48 2015 +0100

     workqueue: mark a work item being canceled as such
    
    There can be two reasons try_to_grab_pending() can fail with -EAGAIN.
    One is when someone else is queueing or deqeueing the work item.  With
    the previous patches, it is guaranteed that PENDING and queued state
    will soon agree making it safe to busy-retry in this case.
    
    The other is if multiple __cancel_work_timer() invocations are racing
    one another.  __cancel_work_timer() grabs PENDING and then waits for
    running instances of the target work item on all CPUs while holding
    PENDING and !queued.  try_to_grab_pending() invoked from another task
    will keep returning -EAGAIN while the current owner is waiting.
    
    Not distinguishing the two cases is okay because __cancel_work_timer()
    is the only user of try_to_grab_pending() and it invokes
    wait_on_work() whenever grabbing fails.  For the first case, busy
    looping should be fine but wait_on_work() doesn't cause any critical
    problem.  For the latter case, the new contender usually waits for the
    same condition as the current owner, so no unnecessarily extended
    busy-looping happens.  Combined, these make __cancel_work_timer()
    technically correct even without irq protection while grabbing PENDING
    or distinguishing the two different cases.
    
    While the current code is technically correct, not distinguishing the
    two cases makes it difficult to use try_to_grab_pending() for other
    purposes than canceling because it's impossible to tell whether it's
    safe to busy-retry grabbing.
    
    This patch adds a mechanism to mark a work item being canceled.
    try_to_grab_pending() now disables irq on success and returns -EAGAIN
    to indicate that grabbing failed but PENDING and queued states are
    gonna agree soon and it's safe to busy-loop.  It returns -ENOENT if
    the work item is being canceled and it may stay PENDING && !queued for
    arbitrary amount of time.
    
    __cancel_work_timer() is modified to mark the work canceling with
    WORK_OFFQ_CANCELING after grabbing PENDING, thus making
    try_to_grab_pending() fail with -ENOENT instead of -EAGAIN.  Also, it
    invokes wait_on_work() iff grabbing failed with -ENOENT.  This isn't
    necessary for correctness but makes it consistent with other future
    users of try_to_grab_pending().
    
    v2: try_to_grab_pending() was testing preempt_count() to ensure that
        the caller has disabled preemption.  This triggers spuriously if
        !CONFIG_PREEMPT_COUNT.  Use preemptible() instead.  Reported by
        Fengguang Wu.
    
    v3: Updated so that try_to_grab_pending() disables irq on success
        rather than requiring preemption disabled by the caller.  This
        makes busy-looping easier and will allow try_to_grap_pending() to
        be used from bh/irq contexts.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Fengguang Wu <fengguang.wu@intel.com>

commit 09dc289a2665b399cf39c54334a9a61d1287f86b
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sat Mar 21 18:41:21 2015 +0100

     workqueue: reorganize try_to_grab_pending() and __cancel_timer_work()
    
    * Use bool @is_dwork instead of @timer and let try_to_grab_pending()
      use to_delayed_work() to determine the delayed_work address.
    
    * Move timer handling from __cancel_work_timer() to
      try_to_grab_pending().
    
    * Make try_to_grab_pending() use -EAGAIN instead of -1 for
      busy-looping and drop the ret local variable.
    
    * Add proper function comment to try_to_grab_pending().
    
    This makes the code a bit easier to understand and will ease further
    changes.  This patch doesn't make any functional change.
    
    v2: Use @is_dwork instead of @timer.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>

commit e6da5bf5a06b3c8ce7f2d71e5623f01481b69c43
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sat Mar 21 18:38:13 2015 +0100

     workqueue: factor out __queue_delayed_work() from queue_delayed_work_
    
    on()
    
    This is to prepare for mod_delayed_work[_on]() and doesn't cause any
    functional difference.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>

commit db9d59336e4b0cb9fa9add9a7b7196cffdbebb35
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sat Mar 21 18:36:51 2015 +0100

     workqueue: introduce WORK_OFFQ_FLAG_*
    
    Low WORK_STRUCT_FLAG_BITS bits of work_struct->data contain
    WORK_STRUCT_FLAG_* and flush color.  If the work item is queued, the
    rest point to the cpu_workqueue with WORK_STRUCT_CWQ set; otherwise,
    WORK_STRUCT_CWQ is clear and the bits contain the last CPU number -
    either a real CPU number or one of WORK_CPU_*.
    
    Scheduled addition of mod_delayed_work[_on]() requires an additional
    flag, which is used only while a work item is off queue.  There are
    more than enough bits to represent off-queue CPU number on both 32 and
    64bits.  This patch introduces WORK_OFFQ_FLAG_* which occupy the lower
    part of the @work->data high bits while off queue.  This patch doesn't
    define any actual OFFQ flag yet.
    
    Off-queue CPU number is now shifted by WORK_OFFQ_CPU_SHIFT, which adds
    the number of bits used by OFFQ flags to WORK_STRUCT_FLAG_SHIFT, to
    make room for OFFQ flags.
    
    To avoid shift width warning with large WORK_OFFQ_FLAG_BITS, ulong
    cast is added to WORK_STRUCT_NO_CPU and, just in case, BUILD_BUG_ON()
    to check that there are enough bits to accomodate off-queue CPU number
    is added.
    
    This patch doesn't make any functional difference.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>

commit cac62dd2b176f4cffe45f9959b2e4e4db085251a
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sat Mar 21 18:34:33 2015 +0100

     workqueue: move try_to_grab_pending() upwards
    
    try_to_grab_pending() will be used by to-be-implemented
    mod_delayed_work[_on]().  Move try_to_grab_pending() and related
    functions above queueing functions.
    
    This patch only moves functions around.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    
    Conflicts:
    	kernel/workqueue.c

commit e2d8edfe5302d965de7791c58cce7f62cd79887b
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sat Mar 21 18:32:57 2015 +0100

     workqueue: fix zero @delay handling of queue_delayed_work_on()
    
    If @delay is zero and the dealyed_work is idle, queue_delayed_work()
    queues it for immediate execution; however, queue_delayed_work_on()
    lacks this logic and always goes through timer regardless of @delay.
    
    This patch moves 0 @delay handling logic from queue_delayed_work() to
    queue_delayed_work_on() so that both functions behave the same.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>

commit 2ad26c9d15a7ea195e7c6ff2f16f09824df6f85a
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sat Mar 21 18:31:14 2015 +0100

     workqueue: unify local CPU queueing handling
    
    Queueing functions have been using different methods to determine the
    local CPU.
    
    * queue_work() superflously uses get/put_cpu() to acquire and hold the
      local CPU across queue_work_on().
    
    * delayed_work_timer_fn() uses smp_processor_id().
    
    * queue_delayed_work() calls queue_delayed_work_on() with -1 @cpu
      which is interpreted as the local CPU.
    
    * flush_delayed_work[_sync]() were using raw_smp_processor_id().
    
    * __queue_work() interprets %WORK_CPU_UNBOUND as local CPU if the
      target workqueue is bound one but nobody uses this.
    
    This patch converts all functions to uniformly use %WORK_CPU_UNBOUND
    to indicate local CPU and use the local binding feature of
    __queue_work().  unlikely() is dropped from %WORK_CPU_UNBOUND handling
    in __queue_work().
    
    Signed-off-by: Tejun Heo <tj@kernel.org>

commit a16b0c35ac34476cb97c0dd05714019a3a61723c
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sat Mar 21 18:28:50 2015 +0100

     workqueue: set delayed_work->timer function on initialization
    
    delayed_work->timer.function is currently initialized during
    queue_delayed_work_on().  Export delayed_work_timer_fn() and set
    delayed_work timer function during delayed_work initialization
    together with other fields.
    
    This ensures the timer function is always valid on an initialized
    delayed_work.  This is to help mod_delayed_work() implementation.
    
    To detect delayed_work users which diddle with the internal timer,
    trigger WARN if timer function doesn't match on queue.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>

commit d1562e4344654f63cd2c24d618d9cb01e2edb943
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sat Mar 21 18:24:40 2015 +0100

     workqueue: disable irq while manipulating PENDING
    
    Queueing operations use WORK_STRUCT_PENDING_BIT to synchronize access
    to the target work item.  They first try to claim the bit and proceed
    with queueing only after that succeeds and there's a window between
    PENDING being set and the actual queueing where the task can be
    interrupted or preempted.
    
    There's also a similar window in process_one_work() when clearing
    PENDING.  A work item is dequeued, gcwq->lock is released and then
    PENDING is cleared and the worker might get interrupted or preempted
    between releasing gcwq->lock and clearing PENDING.
    
    cancel[_delayed]_work_sync() tries to claim or steal PENDING.  The
    function assumes that a work item with PENDING is either queued or in
    the process of being [de]queued.  In the latter case, it busy-loops
    until either the work item loses PENDING or is queued.  If canceling
    coincides with the above described interrupts or preemptions, the
    canceling task will busy-loop while the queueing or executing task is
    preempted.
    
    This patch keeps irq disabled across claiming PENDING and actual
    queueing and moves PENDING clearing in process_one_work() inside
    gcwq->lock so that busy looping from PENDING && !queued doesn't wait
    for interrupted/preempted tasks.  Note that, in process_one_work(),
    setting last CPU and clearing PENDING got merged into single
    operation.
    
    This removes possible long busy-loops and will allow using
    try_to_grab_pending() from bh and irq contexts.
    
    v2: __queue_work() was testing preempt_count() to ensure that the
        caller has disabled preemption.  This triggers spuriously if
        !CONFIG_PREEMPT_COUNT.  Use preemptible() instead.  Reported by
        Fengguang Wu.
    
    v3: Disable irq instead of preemption.  IRQ will be disabled while
        grabbing gcwq->lock later anyway and this allows using
        try_to_grab_pending() from bh and irq contexts.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Fengguang Wu <fengguang.wu@intel.com>

commit 16f0cf4b1a393627d453504c713a229020d69edb
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sat Mar 21 18:21:13 2015 +0100

     workqueue: add missing smp_wmb() in process_one_work()
    
    WORK_STRUCT_PENDING is used to claim ownership of a work item and
    process_one_work() releases it before starting execution.  When
    someone else grabs PENDING, all pre-release updates to the work item
    should be visible and all updates made by the new owner should happen
    afterwards.
    
    Grabbing PENDING uses test_and_set_bit() and thus has a full barrier;
    however, clearing doesn't have a matching wmb.  Given the preceding
    spin_unlock and use of clear_bit, I don't believe this can be a
    problem on an actual machine and there hasn't been any related report
    but it still is theretically possible for clear_pending to permeate
    upwards and happen before work->entry update.
    
    Add an explicit smp_wmb() before work_clear_pending().
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: stable@vger.kernel.org

commit 6f224855557aafa3e7bde4e6279f9ec1a321104f
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sat Mar 21 18:20:11 2015 +0100

     workqueue: make queueing functions return bool
    
    All queueing functions return 1 on success, 0 if the work item was
    already pending.  Update them to return bool instead.  This signifies
    better that they don't return 0 / -errno.
    
    This is cleanup and doesn't cause any functional difference.
    
    While at it, fix comment opening for schedule_work_on().
    
    Signed-off-by: Tejun Heo <tj@kernel.org>

commit 7557c4def35adc5dce40c4c93404c043a67a2808
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sat Mar 21 18:13:54 2015 +0100

     workqueue: reorder queueing functions so that _on() variants are on top
    
    Currently, queue/schedule[_delayed]_work_on() are located below the
    counterpart without the _on postifx even though the latter is usually
    implemented using the former.  Swap them.
    
    This is cleanup and doesn't cause any functional difference.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>

commit 457f6d63275001b4d291837236059fb370d33c79
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sat Mar 21 18:08:07 2015 +0100

     workqueue: fix spurious CPU locality WARN from process_one_work()
    
    25511a4776 "workqueue: reimplement CPU online rebinding to handle idle
    workers" added CPU locality sanity check in process_one_work().  It
    triggers if a worker is executing on a different CPU without UNBOUND
    or REBIND set.
    
    This works for all normal workers but rescuers can trigger this
    spuriously when they're serving the unbound or a disassociated
    global_cwq - rescuers don't have either flag set and thus its
    gcwq->cpu can be a different value including %WORK_CPU_UNBOUND.
    
    Fix it by additionally testing %GCWQ_DISASSOCIATED.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Reported-by: "Paul E. McKenney" <paulmck@linux.vnet.ibm.com>
    LKML-Refence: <20120721213656.GA7783@linux.vnet.ibm.com>

commit 6cb4f587d363f668cd5cb1ced93b1db245dc7be8
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sat Mar 21 18:06:10 2015 +0100

     workqueue: simplify CPU hotplug code
    
    With trustee gone, CPU hotplug code can be simplified.
    
    * gcwq_claim/release_management() now grab and release gcwq lock too
      respectively and gained _and_lock and _and_unlock postfixes.
    
    * All CPU hotplug logic was implemented in workqueue_cpu_callback()
      which was called by workqueue_cpu_up/down_callback() for the correct
      priority.  This was because up and down paths shared a lot of logic,
      which is no longer true.  Remove workqueue_cpu_callback() and move
      all hotplug logic into the two actual callbacks.
    
    This patch doesn't make any functional changes.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Acked-by: "Rafael J. Wysocki" <rjw@sisk.pl>

commit 8bb7d1b7b65a5a25093e6a8be4059797e69a41f0
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sat Mar 21 18:01:14 2015 +0100

     workqueue: remove CPU offline trustee
    
    With the previous changes, a disassociated global_cwq now can run as
    an unbound one on its own - it can create workers as necessary to
    drain remaining works after the CPU has been brought down and manage
    the number of workers using the usual idle timer mechanism making
    trustee completely redundant except for the actual unbinding
    operation.
    
    This patch removes the trustee and let a disassociated global_cwq
    manage itself.  Unbinding is moved to a work item (for CPU affinity)
    which is scheduled and flushed from CPU_DONW_PREPARE.
    
    This patch moves nr_running clearing outside gcwq and manager locks to
    simplify the code.  As nr_running is unused at the point, this is
    safe.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Acked-by: "Rafael J. Wysocki" <rjw@sisk.pl>

commit 57678c05fd387e93157ab7053018bd64f998d9d8
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sat Mar 21 17:56:06 2015 +0100

     workqueue: don't butcher idle workers on an offline CPU
    
    Currently, during CPU offlining, after all pending work items are
    drained, the trustee butchers all workers.  Also, on CPU onlining
    failure, workqueue_cpu_callback() ensures that the first idle worker
    is destroyed.  Combined, these guarantee that an offline CPU doesn't
    have any worker for it once all the lingering work items are finished.
    
    This guarantee isn't really necessary and makes CPU on/offlining more
    expensive than needs to be, especially for platforms which use CPU
    hotplug for powersaving.
    
    This patch lets offline CPUs removes idle worker butchering from the
    trustee and let a CPU which failed onlining keep the created first
    worker.  The first worker is created if the CPU doesn't have any
    during CPU_DOWN_PREPARE and started right away.  If onlining succeeds,
    the rebind_workers() call in CPU_ONLINE will rebind it like any other
    workers.  If onlining fails, the worker is left alone till the next
    try.
    
    This makes CPU hotplugs cheaper by allowing global_cwqs to keep
    workers across them and simplifies code.
    
    Note that trustee doesn't re-arm idle timer when it's done and thus
    the disassociated global_cwq will keep all workers until it comes back
    online.  This will be improved by further patches.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Acked-by: "Rafael J. Wysocki" <rjw@sisk.pl>

commit 316eed7d592b462d8ab54e24ecd36a6e064024c7
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sat Mar 21 17:53:37 2015 +0100

     workqueue: reimplement CPU online rebinding to handle idle workers
    
    Currently, if there are left workers when a CPU is being brough back
    online, the trustee kills all idle workers and scheduled rebind_work
    so that they re-bind to the CPU after the currently executing work is
    finished.  This works for busy workers because concurrency management
    doesn't try to wake up them from scheduler callbacks, which require
    the target task to be on the local run queue.  The busy worker bumps
    concurrency counter appropriately as it clears WORKER_UNBOUND from the
    rebind work item and it's bound to the CPU before returning to the
    idle state.
    
    To reduce CPU on/offlining overhead (as many embedded systems use it
    for powersaving) and simplify the code path, workqueue is planned to
    be modified to retain idle workers across CPU on/offlining.  This
    patch reimplements CPU online rebinding such that it can also handle
    idle workers.
    
    As noted earlier, due to the local wakeup requirement, rebinding idle
    workers is tricky.  All idle workers must be re-bound before scheduler
    callbacks are enabled.  This is achieved by interlocking idle
    re-binding.  Idle workers are requested to re-bind and then hold until
    all idle re-binding is complete so that no bound worker starts
    executing work item.  Only after all idle workers are re-bound and
    parked, CPU_ONLINE proceeds to release them and queue rebind work item
    to busy workers thus guaranteeing scheduler callbacks aren't invoked
    until all idle workers are ready.
    
    worker_rebind_fn() is renamed to busy_worker_rebind_fn() and
    idle_worker_rebind() for idle workers is added.  Rebinding logic is
    moved to rebind_workers() and now called from CPU_ONLINE after
    flushing trustee.  While at it, add CPU sanity check in
    worker_thread().
    
    Note that now a worker may become idle or the manager between trustee
    release and rebinding during CPU_ONLINE.  As the previous patch
    updated create_worker() so that it can be used by regular manager
    while unbound and this patch implements idle re-binding, this is safe.
    
    This prepares for removal of trustee and keeping idle workers across
    CPU hotplugs.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Acked-by: "Rafael J. Wysocki" <rjw@sisk.pl>

commit aefbda63d41b47abc7e0c0eda32f814cc7361b07
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sat Mar 21 17:48:40 2015 +0100

     workqueue: drop @bind from create_worker()
    
    Currently, create_worker()'s callers are responsible for deciding
    whether the newly created worker should be bound to the associated CPU
    and create_worker() sets WORKER_UNBOUND only for the workers for the
    unbound global_cwq.  Creation during normal operation is always via
    maybe_create_worker() and @bind is true.  For workers created during
    hotplug, @bind is false.
    
    Normal operation path is planned to be used even while the CPU is
    going through hotplug operations or offline and this static decision
    won't work.
    
    Drop @bind from create_worker() and decide whether to bind by looking
    at GCWQ_DISASSOCIATED.  create_worker() will also set WORKER_UNBOUND
    autmatically if disassociated.  To avoid flipping GCWQ_DISASSOCIATED
    while create_worker() is in progress, the flag is now allowed to be
    changed only while holding all manager_mutexes on the global_cwq.
    
    This requires that GCWQ_DISASSOCIATED is not cleared behind trustee's
    back.  CPU_ONLINE no longer clears DISASSOCIATED before flushing
    trustee, which clears DISASSOCIATED before rebinding remaining workers
    if asked to release.  For cases where trustee isn't around, CPU_ONLINE
    clears DISASSOCIATED after flushing trustee.  Also, now, first_idle
    has UNBOUND set on creation which is explicitly cleared by CPU_ONLINE
    while binding it.  These convolutions will soon be removed by further
    simplification of CPU hotplug path.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Acked-by: "Rafael J. Wysocki" <rjw@sisk.pl>

commit b3ad212f9be08826bf3f8cde1caf4f528203b77a
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sat Mar 21 17:43:08 2015 +0100

     workqueue: use mutex for global_cwq manager exclusion
    
    POOL_MANAGING_WORKERS is used to ensure that at most one worker takes
    the manager role at any given time on a given global_cwq.  Trustee
    later hitched on it to assume manager adding blocking wait for the
    bit.  As trustee already needed a custom wait mechanism, waiting for
    MANAGING_WORKERS was rolled into the same mechanism.
    
    Trustee is scheduled to be removed.  This patch separates out
    MANAGING_WORKERS wait into per-pool mutex.  Workers use
    mutex_trylock() to test for manager role and trustee uses mutex_lock()
    to claim manager roles.
    
    gcwq_claim/release_management() helpers are added to grab and release
    manager roles of all pools on a global_cwq.  gcwq_claim_management()
    always grabs pool manager mutexes in ascending pool index order and
    uses pool index as lockdep subclass.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Acked-by: "Rafael J. Wysocki" <rjw@sisk.pl>

commit 5ec56efa3b2d134fa777ab2db7e1d8071a60fb8b
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sat Mar 21 17:37:41 2015 +0100

     workqueue: ROGUE workers are UNBOUND workers
    
    Currently, WORKER_UNBOUND is used to mark workers for the unbound
    global_cwq and WORKER_ROGUE is used to mark workers for disassociated
    per-cpu global_cwqs.  Both are used to make the marked worker skip
    concurrency management and the only place they make any difference is
    in worker_enter_idle() where WORKER_ROGUE is used to skip scheduling
    idle timer, which can easily be replaced with trustee state testing.
    
    This patch replaces WORKER_ROGUE with WORKER_UNBOUND and drops
    WORKER_ROGUE.  This is to prepare for removing trustee and handling
    disassociated global_cwqs as unbound.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Acked-by: "Rafael J. Wysocki" <rjw@sisk.pl>

commit ff11621e3cfaa337fd77daccd2e2337ed2b4dd56
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sat Mar 21 17:34:00 2015 +0100

     workqueue: drop CPU_DYING notifier operation
    
    Workqueue used CPU_DYING notification to mark GCWQ_DISASSOCIATED.
    This was necessary because workqueue's CPU_DOWN_PREPARE happened
    before other DOWN_PREPARE notifiers and workqueue needed to stay
    associated across the rest of DOWN_PREPARE.
    
    After the previous patch, workqueue's DOWN_PREPARE happens after
    others and can set GCWQ_DISASSOCIATED directly.  Drop CPU_DYING and
    let the trustee set GCWQ_DISASSOCIATED after disabling concurrency
    management.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Acked-by: "Rafael J. Wysocki" <rjw@sisk.pl>

commit 353ef63c5ff00957526b915c68c057ac63e708c0
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sat Mar 21 17:31:19 2015 +0100

    workqueue: add missing function

commit a806a451bcfa847ad97e13362e19d9a81d8e75b8
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sat Mar 21 17:30:18 2015 +0100

     workqueue: perform cpu down operations from low priority cpu_notifier()
    
    Currently, all workqueue cpu hotplug operations run off
    CPU_PRI_WORKQUEUE which is higher than normal notifiers.  This is to
    ensure that workqueue is up and running while bringing up a CPU before
    other notifiers try to use workqueue on the CPU.
    
    Per-cpu workqueues are supposed to remain working and bound to the CPU
    for normal CPU_DOWN_PREPARE notifiers.  This holds mostly true even
    with workqueue offlining running with higher priority because
    workqueue CPU_DOWN_PREPARE only creates a bound trustee thread which
    runs the per-cpu workqueue without concurrency management without
    explicitly detaching the existing workers.
    
    However, if the trustee needs to create new workers, it creates
    unbound workers which may wander off to other CPUs while
    CPU_DOWN_PREPARE notifiers are in progress.  Furthermore, if the CPU
    down is cancelled, the per-CPU workqueue may end up with workers which
    aren't bound to the CPU.
    
    While reliably reproducible with a convoluted artificial test-case
    involving scheduling and flushing CPU burning work items from CPU down
    notifiers, this isn't very likely to happen in the wild, and, even
    when it happens, the effects are likely to be hidden by the following
    successful CPU down.
    
    Fix it by using different priorities for up and down notifiers - high
    priority for up operations and low priority for down operations.
    
    Workqueue cpu hotplug operations will soon go through further cleanup.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: stable@vger.kernel.org
    Acked-by: "Rafael J. Wysocki" <rjw@sisk.pl>

commit 11adfc00c2ff73bbee42008ec8dbb8760fc037a3
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sat Mar 21 17:27:43 2015 +0100

     lockdep: fix oops in processing workqueue
    
    Under memory load, on x86_64, with lockdep enabled, the workqueue's
    process_one_work() has been seen to oops in __lock_acquire(), barfing
    on a 0xffffffff00000000 pointer in the lockdep_map's class_cache[].
    
    Because it's permissible to free a work_struct from its callout function,
    the map used is an onstack copy of the map given in the work_struct: and
    that copy is made without any locking.
    
    Surprisingly, gcc (4.5.1 in Hugh's case) uses "rep movsl" rather than
    "rep movsq" for that structure copy: which might race with a workqueue
    user's wait_on_work() doing lock_map_acquire() on the source of the
    copy, putting a pointer into the class_cache[], but only in time for
    the top half of that pointer to be copied to the destination map.
    
    Boom when process_one_work() subsequently does lock_map_acquire()
    on its onstack copy of the lockdep_map.
    
    Fix this, and a similar instance in call_timer_fn(), with a
    lockdep_copy_map() function which additionally NULLs the class_cache[].
    
    Note: this oops was actually seen on 3.4-next, where flush_work() newly
    does the racing lock_map_acquire(); but Tejun points out that 3.4 and
    earlier are already vulnerable to the same through wait_on_work().
    
    * Patch orginally from Peter.  Hugh modified it a bit and wrote the
      description.
    
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Reported-by: Hugh Dickins <hughd@google.com>
    LKML-Reference: <alpine.LSU.2.00.1205070951170.1544@eggly.anvils>
    Signed-off-by: Tejun Heo <tj@kernel.org>

commit 5339adc1c029dedd1e250093c9a2bbf899fd0050
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sat Mar 21 17:24:49 2015 +0100

     workqueue: Catch more locking problems with flush_work()
    
    If a workqueue is flushed with flush_work() lockdep checking can
    be circumvented. For example:
    
     static DEFINE_MUTEX(mutex);
    
     static void my_work(struct work_struct *w)
     {
             mutex_lock(&mutex);
             mutex_unlock(&mutex);
     }
    
     static DECLARE_WORK(work, my_work);
    
     static int __init start_test_module(void)
     {
             schedule_work(&work);
             return 0;
     }
     module_init(start_test_module);
    
     static void __exit stop_test_module(void)
     {
             mutex_lock(&mutex);
             flush_work(&work);
             mutex_unlock(&mutex);
     }
     module_exit(stop_test_module);
    
    would not always print a warning when flush_work() was called.
    In this trivial example nothing could go wrong since we are
    guaranteed module_init() and module_exit() don't run concurrently,
    but if the work item is schedule asynchronously we could have a
    scenario where the work item is running just at the time flush_work()
    is called resulting in a classic ABBA locking problem.
    
    Add a lockdep hint by acquiring and releasing the work item
    lockdep_map in flush_work() so that we always catch this
    potential deadlock scenario.
    
    Signed-off-by: Stephen Boyd <sboyd@codeaurora.org>
    Reviewed-by: Yong Zhang <yong.zhang0@gmail.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

commit fa2debbc636a0862ced594493f27aa896bfe8333
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sat Mar 21 17:23:36 2015 +0100

     workqueue: change BUG_ON() to WARN_ON()
    
    This BUG_ON() can be triggered if you call schedule_work() before
    calling INIT_WORK().  It is a bug definitely, but it's nicer to just
    print a stack trace and return.
    
    Reported-by: Matt Renzelmann <mjr@cs.wisc.edu>
    Signed-off-by: Dan Carpenter <dan.carpenter@oracle.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

commit d89217f72de909295199501b07c87709c1f4f9be
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sat Mar 21 17:21:40 2015 +0100

     workqueue: reimplement WQ_HIGHPRI using a separate worker_pool
    
    WQ_HIGHPRI was implemented by queueing highpri work items at the head
    of the global worklist.  Other than queueing at the head, they weren't
    handled differently; unfortunately, this could lead to execution
    latency of a few seconds on heavily loaded systems.
    
    Now that workqueue code has been updated to deal with multiple
    worker_pools per global_cwq, this patch reimplements WQ_HIGHPRI using
    a separate worker_pool.  NR_WORKER_POOLS is bumped to two and
    gcwq->pools[0] is used for normal pri work items and ->pools[1] for
    highpri.  Highpri workers get -20 nice level and has 'H' suffix in
    their names.  Note that this change increases the number of kworkers
    per cpu.
    
    POOL_HIGHPRI_PENDING, pool_determine_ins_pos() and highpri chain
    wakeup code in process_one_work() are no longer used and removed.
    
    This allows proper prioritization of highpri work items and removes
    high execution latency of highpri work items.
    
    v2: nr_running indexing bug in get_pool_nr_running() fixed.
    
    v3: Refreshed for the get_pool_nr_running() update in the previous
        patch.
    
    Change-Id: Id843c0a425f51f84083786fbf413d999d35771b7
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Reported-by: Josh Hunt <joshhunt00@gmail.com>
    LKML-Reference: <CAKA=qzaHqwZ8eqpLNFjxnO2fX-tgAOjmpvxgBFjv6dJeQaOW1w@mail.gmail.com>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Fengguang Wu <fengguang.wu@intel.com>
    Git-commit: 3270476a6c0ce322354df8679652f060d66526dc
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Matt Wagantall <mattw@codeaurora.org>

commit a0b6903d235946d52f9ad52a8e6239fb5c670772
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sat Mar 21 17:03:13 2015 +0100

     workqueue: introduce NR_WORKER_POOLS and for_each_worker_pool()
    
    Introduce NR_WORKER_POOLS and for_each_worker_pool() and convert code
    paths which need to manipulate all pools in a gcwq to use them.
    NR_WORKER_POOLS is currently one and for_each_worker_pool() iterates
    over only @gcwq->pool.
    
    Note that nr_running is per-pool property and converted to an array
    with NR_WORKER_POOLS elements and renamed to pool_nr_running.  Note
    that get_pool_nr_running() currently assumes 0 index.  The next patch
    will make use of non-zero index.
    
    The changes in this patch are mechanical and don't caues any
    functional difference.  This is to prepare for multiple pools per
    gcwq.
    
    v2: nr_running indexing bug in get_pool_nr_running() fixed.
    
    v3: Pointer to array is stupid.  Don't use it in get_pool_nr_running()
        as suggested by Linus.
    
    Change-Id: I46e9488601d764d25e4a6c707de129ab68f7064c
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Fengguang Wu <fengguang.wu@intel.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Git-commit: 4ce62e9e30cacc26885cab133ad1de358dd79f21
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Matt Wagantall <mattw@codeaurora.org>

commit b46863a69e570d0d2daeda821fad3df06dd2eeaa
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sat Mar 21 16:55:06 2015 +0100

     workqueue: separate out worker_pool flags
    
    GCWQ_MANAGE_WORKERS, GCWQ_MANAGING_WORKERS and GCWQ_HIGHPRI_PENDING
    are per-pool properties.  Add worker_pool->flags and make the above
    three flags per-pool flags.
    
    The changes in this patch are mechanical and don't caues any
    functional difference.  This is to prepare for multiple pools per
    gcwq.
    
    Change-Id: I1824fd1c509d8ac6b0619536621a22b15b316256
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Git-commit: 11ebea50dbc1ade5994b2c838a096078d4c02399
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Matt Wagantall <mattw@codeaurora.org>

commit 88f49a6421fa0027ff44b5caca82821386d6274e
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sat Mar 21 16:50:23 2015 +0100

     workqueue: use @pool instead of @gcwq or @cpu where applicable
    
    Modify all functions which deal with per-pool properties to pass
    around @pool instead of @gcwq or @cpu.
    
    The changes in this patch are mechanical and don't caues any
    functional difference.  This is to prepare for multiple pools per
    gcwq.
    
    Change-Id: I4be6727e1cce6f9aa2a0057b96bdc725c84f1ea8
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Git-commit: 63d95a9150ee3bbd4117fcd609dee40313b454d9
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Matt Wagantall <mattw@codeaurora.org>

commit f8e60227e83cb8abfbf5ca7e467fb28db2afbda6
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sat Mar 21 16:19:33 2015 +0100

     workqueue: factor out worker_pool from global_cwq
    
    Move worklist and all worker management fields from global_cwq into
    the new struct worker_pool.  worker_pool points back to the containing
    gcwq.  worker and cpu_workqueue_struct are updated to point to
    worker_pool instead of gcwq too.
    
    This change is mechanical and doesn't introduce any functional
    difference other than rearranging of fields and an added level of
    indirection in some places.  This is to prepare for multiple pools per
    gcwq.
    
    v2: Comment typo fixes as suggested by Namhyung.
    
    Change-Id: Iefae84798c2af580f425b92ed79117935d99f21f
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Namhyung Kim <namhyung@kernel.org>
    Git-commit: bd7bdd43dcb81bb08240b9401b36a104f77dc135
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Matt Wagantall <mattw@codeaurora.org>
    
    Conflicts:
    	kernel/workqueue.c

commit 96c046d3d563e356b30e1d68b6b1dbabe8707b90
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Mar 19 22:21:06 2015 +0100

     workqueue: don't use WQ_HIGHPRI for unbound workqueues
    
    Unbound wqs aren't concurrency-managed and try to execute work items
    as soon as possible.  This is currently achieved by implicitly setting
    %WQ_HIGHPRI on all unbound workqueues; however, WQ_HIGHPRI
    implementation is about to be restructured and this usage won't be
    valid anymore.
    
    Add an explicit chain-wakeup path for unbound workqueues in
    process_one_work() instead of piggy backing on %WQ_HIGHPRI.
    
    Change-Id: Iecd17a9935ee28f856d8b726bb4c296762922bed
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Git-commit: 974271c485a4d8bb801decc616748f90aafb07ec
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Matt Wagantall <mattw@codeaurora.org>

commit 6ac4a3076f569d0cdebb0ce669cdd8c514cc5bf3
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Mar 19 22:19:35 2015 +0100

     workqueue: Reset to 3.4 branch
    
    Signed-off-by: Zhao Wei Liew <zhaoweiliew@gmail.com>

commit 0347de0a055b832ddf07ef6fbd51402aa36be4fb
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Mar 19 22:11:43 2015 +0100

    Some optimisations and bugfixes

commit 0a021d7ca4c21b4a2ec79c04744724ab97882ff6
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Mar 19 21:09:20 2015 +0100

    Disable sched debug

commit fbfa5201c96b8defed0bb5eee35cca1f1921680d
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Mar 19 21:05:52 2015 +0100

    Try to fix not booting problem

commit 6c1970130aa23dd0b3225efaf4b7c3702acbc6e0
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Mar 19 20:50:59 2015 +0100

     workqueue: skip nr_running sanity check in worker_enter_idle() if tru
    
    stee is active
    
    worker_enter_idle() has WARN_ON_ONCE() which triggers if nr_running
    isn't zero when every worker is idle.  This can trigger spuriously
    while a cpu is going down due to the way trustee sets %WORKER_ROGUE
    and zaps nr_running.
    
    It first sets %WORKER_ROGUE on all workers without updating
    nr_running, releases gcwq->lock, schedules, regrabs gcwq->lock and
    then zaps nr_running.  If the last running worker enters idle
    inbetween, it would see stale nr_running which hasn't been zapped yet
    and trigger the WARN_ON_ONCE().
    
    Fix it by performing the sanity check iff the trustee is idle.
    
    Change-Id: I54465b67d76bfaff3138963153256cde48add673
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Reported-by: "Paul E. McKenney" <paulmck@linux.vnet.ibm.com>
    Cc: stable@vger.kernel.org
    Git-commit: 544ecf310f0e7f51fa057ac2a295fc1b3b35a9d3
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Matt Wagantall <mattw@codeaurora.org>

commit 0cf21b038bfb9320f7716037f813d7fdcbaad2f4
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Mar 19 20:34:42 2015 +0100

     mm: pagewalk: call pte_hole() for VM_PFNMAP during walk_page_range
    
    commit 23aaed6659df9adfabe9c583e67a36b54e21df46 upstream.
    
    walk_page_range() silently skips vma having VM_PFNMAP set, which leads
    to undesirable behaviour at client end (who called walk_page_range).
    Userspace applications get the wrong data, so the effect is like just
    confusing users (if the applications just display the data) or sometimes
    killing the processes (if the applications do something with
    misunderstanding virtual addresses due to the wrong data.)
    
    For example for pagemap_read, when no callbacks are called against
    VM_PFNMAP vma, pagemap_read may prepare pagemap data for next virtual
    address range at wrong index.
    
    Eventually userspace may get wrong pagemap data for a task.
    Corresponding to a VM_PFNMAP marked vma region, kernel may report
    mappings from subsequent vma regions.  User space in turn may account
    more pages (than really are) to the task.
    
    In my case I was using procmem, procrack (Android utility) which uses
    pagemap interface to account RSS pages of a task.  Due to this bug it
    was giving a wrong picture for vmas (with VM_PFNMAP set).
    
    Fixes: a9ff785e4437 ("mm/pagewalk.c: walk_page_range should avoid VM_PFNMAP areas")
    Signed-off-by: Shiraz Hashim <shashim@codeaurora.org>
    Acked-by: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit f3a36b45d3937bb93d93ba99eacff09c0b0a818c
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Mar 19 20:33:36 2015 +0100

     gpio: sysfs: fix memory leak in gpiod_sysfs_set_active_low
    
    commit 49d2ca84e433dab854c7a866bc6add09cfab682d upstream.
    
    Fix memory leak in the gpio sysfs interface due to failure to drop
    reference to device returned by class_find_device when setting the
    gpio-line polarity.
    
    Fixes: 0769746183ca ("gpiolib: add support for changing value polarity in sysfs")
    Signed-off-by: Johan Hovold <johan@kernel.org>
    Signed-off-by: Linus Walleij <linus.walleij@linaro.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit e670b00dd247ef5ff296ee62be2ff73d4e398387
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Mar 19 20:32:54 2015 +0100

     gpio: sysfs: fix memory leak in gpiod_export_link
    
    commit 0f303db08df0df9bd0966443ad6001e63960af16 upstream.
    
    Fix memory leak in the gpio sysfs interface due to failure to drop
    reference to device returned by class_find_device when creating a link.
    
    Fixes: a4177ee7f1a8 ("gpiolib: allow exported GPIO nodes to be named using sysfs links")
    Signed-off-by: Johan Hovold <johan@kernel.org>
    Signed-off-by: Linus Walleij <linus.walleij@linaro.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 2746790da06a9408da27d3e549d898b9b82ff1f0
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Mar 19 20:31:48 2015 +0100

    Fixing some compiling errors

commit e5fe8d1746f76d3baa939663f1f4855e92cb1f9d
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Mar 19 20:29:20 2015 +0100

     nl80211: fix per-station group key get/del and memory leak
    
    commit 0fa7b39131576dd1baa6ca17fca53c65d7f62249 upstream.
    
    In case userspace attempts to obtain key information for or delete a
    unicast key, this is currently erroneously rejected unless the driver
    sets the WIPHY_FLAG_IBSS_RSN flag. Apparently enough drivers do so it
    was never noticed.
    
    Fix that, and while at it fix a potential memory leak: the error path
    in the get_key() function was placed after allocating a message but
    didn't free it - move it to a better place. Luckily admin permissions
    are needed to call this operation.
    
    Fixes: e31b82136d1ad ("cfg80211/mac80211: allow per-station GTKs")
    Signed-off-by: Johannes Berg <johannes.berg@intel.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 9e6d102334c4450caf157fc57dc6ba60eba10bc5
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Mar 19 20:27:36 2015 +0100

     ARM: 8109/1: mm: Modify pte_write and pmd_write logic for LPAE
    
    commit ded9477984690d026e46dd75e8157392cea3f13f upstream.
    
    For LPAE, we have the following means for encoding writable or dirty
    ptes:
                                  L_PTE_DIRTY       L_PTE_RDONLY
        !pte_dirty && !pte_write        0               1
        !pte_dirty && pte_write         0               1
        pte_dirty && !pte_write         1               1
        pte_dirty && pte_write          1               0
    
    So we can't distinguish between writeable clean ptes and read only
    ptes. This can cause problems with ptes being incorrectly flagged as
    read only when they are writeable but not dirty.
    
    This patch renumbers L_PTE_RDONLY from AP[2] to a software bit #58,
    and adds additional logic to set AP[2] whenever the pte is read only
    or not dirty. That way we can distinguish between clean writeable ptes
    and read only ptes.
    
    HugeTLB pages will use this new logic automatically.
    
    We need to add some logic to Transparent HugePages to ensure that they
    correctly interpret the revised pgprot permissions (L_PTE_RDONLY has
    moved and no longer matches PMD_SECT_AP2). In the process of revising
    THP, the names of the PMD software bits have been prefixed with L_ to
    make them easier to distinguish from their hardware bit counterparts.
    
    Signed-off-by: Steve Capper <steve.capper@linaro.org>
    Reviewed-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>
    [hpy: Backported to 3.10
    - adjust the context
    - ignore change related to pmd, because 3.10 does not support HugePage
    ]
    Signed-off-by: Hou Pengyang <houpengyang@huawei.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit bdbdc3dd572ecc2fc6cadee25f384ce84120245a
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Mar 19 20:24:50 2015 +0100

     ARM: 8108/1: mm: Introduce {pte,pmd}_isset and {pte,pmd}_isclear
    
    commit f2950706871c4b6e8c0f0d7c3f62d35930b8de63 upstream.
    
    Long descriptors on ARM are 64 bits, and some pte functions such as
    pte_dirty return a bitwise-and of a flag with the pte value. If the
    flag to be tested resides in the upper 32 bits of the pte, then we run
    into the danger of the result being dropped if downcast.
    
    For example:
    	gather_stats(page, md, pte_dirty(*pte), 1);
    where pte_dirty(*pte) is downcast to an int.
    
    This patch introduces a new macro pte_isset which performs the bitwise
    and, then performs a double logical invert (where needed) to ensure
    predictable downcasting. The logical inverse pte_isclear is also
    introduced.
    
    Equivalent pmd functions for Transparent HugePages have also been
    added.
    
    Signed-off-by: Steve Capper <steve.capper@linaro.org>
    Reviewed-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>
    [hpy: Backported to 3.10:
    - adjust the context
    - ignore change to pmd, because 3.10 does not support HugePage.]
    Signed-off-by: Hou Pengyang <houpengyang@huawei.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 638cc636b8fc7e8fe20c16f50e30ce0bf0719511
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Mar 19 20:22:58 2015 +0100

     ARM: DMA: ensure that old section mappings are flushed from the TLB
    
    commit 6b076991dca9817e75c37e2f0db6d52611ea42fa upstream.
    
    When setting up the CMA region, we must ensure that the old section
    mappings are flushed from the TLB before replacing them with page
    tables, otherwise we can suffer from mismatched aliases if the CPU
    speculatively prefetches from these mappings at an inopportune time.
    
    A mismatched alias can occur when the TLB contains a section mapping,
    but a subsequent prefetch causes it to load a page table mapping,
    resulting in the possibility of the TLB containing two matching
    mappings for the same virtual address region.
    
    Acked-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>
    Cc: Hou Pengyang <houpengyang@huawei.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit f0c38bda9f54ebb42c10d8b07bb508f658f2dbd6
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Mar 19 20:20:34 2015 +0100

     ARM: fix asm/memory.h build error
    
    commit b713aa0b15015a65ad5421543b80df86de043d62 upstream.
    
    Jason Gunthorpe reports a build failure when ARM_PATCH_PHYS_VIRT is
    not defined:
    
    In file included from arch/arm/include/asm/page.h:163:0,
                     from include/linux/mm_types.h:16,
                     from include/linux/sched.h:24,
                     from arch/arm/kernel/asm-offsets.c:13:
    arch/arm/include/asm/memory.h: In function '__virt_to_phys':
    arch/arm/include/asm/memory.h:244:40: error: 'PHYS_OFFSET' undeclared
    (first use in this function)
    arch/arm/include/asm/memory.h:244:40: note: each undeclared identifier
    is reported only once for each function it appears in
    arch/arm/include/asm/memory.h: In function '__phys_to_virt':
    arch/arm/include/asm/memory.h:249:13: error: 'PHYS_OFFSET' undeclared
    (first use in this function)
    
    Fixes: ca5a45c06cd4 ("ARM: mm: use phys_addr_t appropriately in p2v and v2p conversions")
    Tested-By: Jason Gunthorpe <jgunthorpe@obsidianresearch.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>
    [hpy: Backported to 3.10:
    - adjust the context
    - MPU is not supported by 3.10, so ignore fix to MPU compared with the
    original patch.]
    Signed-off-by: Hou Pengyang <houpengyang@huawei.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit bb3965dd4fc66eb45efda43042c39599e293ca69
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Mar 19 20:17:41 2015 +0100

     ARM: 7931/1: Correct virt_addr_valid
    
    commit efea3403d4b7c6d1dd5d5ac3234c161e8b314d66 upstream.
    
    The definition of virt_addr_valid is that virt_addr_valid should
    return true if and only if virt_to_page returns a valid pointer.
    The current definition of virt_addr_valid only checks against the
    virtual address range. There's no guarantee that just because a
    virtual address falls bewteen PAGE_OFFSET and high_memory the
    associated physical memory has a valid backing struct page. Follow
    the example of other architectures and convert to pfn_valid to
    verify that the virtual address is actually valid. The check for
    an address between PAGE_OFFSET and high_memory is still necessary
    as vmalloc/highmem addresses are not valid with virt_to_page.
    
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Nicolas Pitre <nico@linaro.org>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>
    Cc: Hou Pengyang <houpengyang@huawei.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 242aa1a8dddab45fb866443fbde3fb6522df006d
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Mar 19 20:16:06 2015 +0100

     ARM: 7867/1: include: asm: use 'int' instead of 'unsigned long' for '
    
    oldval' in atomic_cmpxchg().
    
    commit 4dcc1cf7316a26e112f5c9fcca531ff98ef44700 upstream.
    
    For atomic_cmpxchg(), the type of 'oldval' need be 'int' to match the
    type of "*ptr" (used by 'ldrex' instruction) and 'old' (used by 'teq'
    instruction).
    
    Reviewed-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Chen Gang <gang.chen@asianux.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>
    Cc: Hou Pengyang <houpengyang@huawei.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 50e5335efc205de140cf5b443a0bf5ca63092845
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Mar 19 20:14:10 2015 +0100

     ARM: 7866/1: include: asm: use 'long long' instead of 'u64' within at
    
    omic.h
    
    commit 237f12337cfa2175474e4dd015bc07a25eb9080d upstream.
    
    atomic* value is signed value, and atomic* functions need also process
    signed value (parameter value, and return value), so 32-bit arm need
    use 'long long' instead of 'u64'.
    
    After replacement, it will also fix a bug for atomic64_add_negative():
    "u64 is never less than 0".
    
    The modifications are:
    
    in vim, use "1,% s/\<u64\>/long long/g" command.
    remove '__aligned(8)' which is useless for 64-bit.
    be sure of 80 column limitation after replacement.
    
    Acked-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Chen Gang <gang.chen@asianux.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>
    Cc: Hou Pengyang <houpengyang@huawei.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 6bb5c1961c2e604619f1aaf2326c4382ae77b90c
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Mar 19 20:13:32 2015 +0100

    Fix my compilation errors

commit 6e7ec9dd8112f54aaa4cc16c97c57257c5a9dda4
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Mar 19 19:31:18 2015 +0100

     ARM: lpae: fix definition of PTE_HWTABLE_PTRS
    
    commit e38a517578d6c0f764b0d0f6e26dcdf9f70c69d7 upstream.
    
    For 2-level page tables, PTE_HWTABLE_PTRS describes the offset between
    Linux PTEs and hardware PTEs. On LPAE, there is no distinction (since
    we have 64-bit descriptors with plenty of space) so PTE_HWTABLE_PTRS
    should be 0. Unfortunately, it is wrongly defined as PTRS_PER_PTE,
    meaning that current pte table flushing is off by a page. Luckily,
    all current LPAE implementations are SMP, so the hardware walker can
    snoop L1.
    
    This patch fixes the broken definition.
    
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Cc: Hou Pengyang <houpengyang@huawei.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 4d5951c31b787d4e4348db6d5efc59520a25aa46
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Mar 19 19:27:46 2015 +0100

     ARM: fix type of PHYS_PFN_OFFSET to unsigned long
    
    commit 5b20c5b2f014ecc0a6310988af69cd7ede9e7c67 upstream.
    
    On LPAE machines, PHYS_OFFSET evaluates to a phys_addr_t and this type
    is
    inherited by the PHYS_PFN_OFFSET definition as well.  Consequently, the
    kernel
    build emits warnings of the form:
    
    init/main.c: In function 'start_kernel':
    init/main.c:588:7: warning: format '%lx' expects argument of type 'long
    unsigned int', but argument 2 has type 'phys_addr_t' [-Wformat]
    
    This patch fixes this warning by pinning down the PFN type to unsigned
    long.
    
    Signed-off-by: Cyril Chemparathy <cyril@ti.com>
    Acked-by: Nicolas Pitre <nico@linaro.org>
    Tested-by: Santosh Shilimkar <santosh.shilimkar@ti.com>
    Tested-by: Subash Patel <subash.rp@samsung.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Cc: Hou Pengyang <houpengyang@huawei.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 713f590be693428ccfa45858c8e480bbfbe00aa7
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Mar 19 19:23:28 2015 +0100

     ARM: LPAE: use signed arithmetic for mask definitions
    
    commit 926edcc747e2efb3c9add7ed4dbc4e7a3a959d02 upstream.
    
    This patch applies to PAGE_MASK, PMD_MASK, and PGDIR_MASK, where forcing
    unsigned long math truncates the mask at the 32-bits.  This clearly does
    bad
    things on PAE systems.
    
    This patch fixes this problem by defining these masks as signed
    quantities.
    We then rely on sign extension to do the right thing.
    
    Signed-off-by: Cyril Chemparathy <cyril@ti.com>
    Signed-off-by: Vitaly Andrianov <vitalya@ti.com>
    Reviewed-by: Nicolas Pitre <nico@linaro.org>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Tested-by: Santosh Shilimkar <santosh.shilimkar@ti.com>
    Tested-by: Subash Patel <subash.rp@samsung.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Cc: Hou Pengyang <houpengyang@huawei.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 6ef11142261c097854c40b414d94b90d7cadc8c0
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Mar 19 19:22:01 2015 +0100

     ARM: mm: correct pte_same behaviour for LPAE.
    
    commit dde1b65110353517816bcbc58539463396202244 upstream.
    
    For 3 levels of paging the PTE_EXT_NG bit will be set for user
    address ptes that are written to a page table but not for ptes
    created with mk_pte.
    
    This can cause some comparison tests made by pte_same to fail
    spuriously and lead to other problems.
    
    To correct this behaviour, we mask off PTE_EXT_NG for any pte that
    is present before running the comparison.
    
    Signed-off-by: Steve Capper <steve.capper@linaro.org>
    Reviewed-by: Will Deacon <will.deacon@arm.com>
    Cc: Hou Pengyang <houpengyang@huawei.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 20f53433e60f0d421c5a8ccf1ce386e6b2f0d73c
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Mar 19 19:20:17 2015 +0100

     ARM: 7829/1: Add ".text.unlikely" and ".text.hot" to arm unwind tables
    
    commit 849b882b52df0f276d9ffded01d85654aa0da422 upstream.
    
    It appears that gcc may put some code in ".text.unlikely" or
    ".text.hot" sections.  Right now those aren't accounted for in unwind
    tables.  Add them.
    
    I found some docs about this at:
    http://gcc.gnu.org/onlinedocs/gcc-4.6.2/gcc.pdf
    
    Without this, if you have slub_debug turned on, you can get messages
    that look like this:
    unwind: Index not found 7f008c50
    
    Signed-off-by: Doug Anderson <dianders@chromium.org>
    Acked-by: Mike Frysinger <vapier@gentoo.org>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>
    [wangkai: backport to 3.10
    	- adjust context
    ]
    Signed-off-by: Wang Kai <morgan.wang@huawei.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit a63f182a00fc7013dc80da4b35547ebced674ed1
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Mar 19 19:12:59 2015 +0100

     cpufreq: Break out early when frequency equals target_freq
    
    Many drivers keep frequencies in frequency table in ascending
    or descending order. When governor tries to change to policy->min
    or policy->max respectively then the cpufreq_frequency_table_target
    could return on first iteration. This will save some iteration cycles.
    
    So, break out early when a frequency in cpufreq_frequency_table
    equals to target one.
    
    Testing this during kernel compilation using ondemand governor
    with a frequency table in ascending order, the
    cpufreq_frequency_table_target returned early on the first
    iteration at about 30% of times called.
    
    Signed-off-by: Stratos Karafotis <stratosk@semaphore.gr>
    Acked-by: Viresh Kumar <viresh.kumar@linaro.org>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit e9dc6416241dce6291248b8d886859a84a9b196f
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Mar 19 19:09:55 2015 +0100

     cpufreq: Simplify cpufreq_add_dev()
    
    Currently cpufreq_add_dev() firsts allocates policy, calls
    driver->init() and then checks if this CPU is already managed or not.
    And if it is already managed, its policy is freed.
    
    We can save all this if we somehow know that CPU is managed or not in
    advance.  policy->related_cpus contains the list of all valid sibling
    CPUs of policy->cpu. We can check this to see if the current CPU is
    already managed.
    
    From now on, platforms don't really need to set related_cpus from
    their init() routines, as the same work is done by core too.
    
    If a platform driver needs to set the related_cpus mask with some
    additional CPUs, other than CPUs present in policy->cpus, they are
    free to do it, though, as we don't override anything.
    
    Signed-off-by: Viresh Kumar <viresh.kumar@linaro.org>
    Tested-by: Shawn Guo <shawn.guo@linaro.org>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 10e56ba638dc45d1235a8c5ec1aaf1b5f7dce326
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Mar 19 18:59:47 2015 +0100

     qcom: rq_stats: remove the redundant iowait check
    
    Iowait_time check is redundant and it would lead to heavy load
    due to miscaluculation of idle time. Hence remove this check
    as iowait will considered based on io_busy flag.
    
    Change-Id: I4dd9f9c79205eca588daea8cc0fe892394a83391
    CRs-fixed: 662052
    Signed-off-by: Anji Jonnala <anjir@codeaurora.org>

commit d5a4e03a308ba926458647c4330b757768ce8a5b
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Mar 19 18:57:24 2015 +0100

     msm: rq_stats: update related_cpus in the cpu_load runtime
    
    "related_cpu" indicates the list of CPUs that need some
    sort of frequency coordination, whether software or hardware.
    ex:- if all cpus shares the same clock, all 4 cpus belongs to
    related_cpus.
    
    cpufreq driver updates the related_cpus during hotplug offline/online
    events. Hence update related_cpus for all cpus during hotplug notifier.
    
    CRs-fixed: 548631
    Change-Id: I6543ddaeebf73d18334066b090ee7c4a72466e4a
    Signed-off-by: Anji Jonnala <anjir@codeaurora.org>

commit 1c1e28a688d1491244ed0e9ce9c1403c11afd2a2
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Mar 19 18:54:25 2015 +0100

     ksm: Provide support to use deferred timers for scanner thread
    
    KSM thread to scan pages is getting schedule on definite timeout.
    That wakes up CPU from idle state and hence may affect the power
    consumption. Provide an optional support to use deferred timer
    which suites low-power use-cases.
    
    To enable deferred timers,
    $ echo 1 > /sys/kernel/mm/ksm/deferred_timer
    
    Change-Id: I07fe199f97fe1f72f9a9e1b0b757a3ac533719e8
    Signed-off-by: Chintan Pandya <cpandya@codeaurora.org>

commit ae985f79c8f580ebbd839287290f5c0116ede957
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Mar 19 18:51:31 2015 +0100

     mmc: core: add long read time fixup for certain Samsung eMMC
    
    Certain Samsung eMMC meet multi read timeout, and could not
    reponse status CMD anymore after that. Add long read timeout
    fixup to resolve it.
    
    Change-Id: Ibeb0e6ab3d889d48fdee91244bec720a6994b907
    Signed-off-by: Guoping Yu <guopingy@codeaurora.org>

commit 82fa69007b8a63b08fd2cdb1badf764ceb35309f
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Mar 19 18:50:03 2015 +0100

     xt_qtaguid: Use sk_callback_lock read locks before reading sk->sk_socket
    
    It prevents a kernel panic when accessing sk->sk_socket fields due to
    NULLing sk->sk_socket when sock_orphan is called through
    sk_common_release.
    
    Change-Id: I4aa46b4e2d8600e4d4ef8dcdd363aa4e6e5f8433
    Signed-off-by: Mohamad Ayyash <mkayyash@google.com>
    (cherry picked from commit cdea0ebcb8bcfe57688f6cb692b49e550ebd9796)

commit 16a3693dc73c3101e3e2c6e435aba2663b9472e0
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Mar 19 18:47:35 2015 +0100

    arch/arm more mm changes

commit 9767aa9bcec8c6263549958877f5772ee6c8b4a5
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Mar 19 18:45:53 2015 +0100

    arch/arm small changes

commit 65a002453d1633fcbcd70f97296381809cbf4ae9
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Mar 19 18:41:54 2015 +0100

    arm: add intelli_plug

commit f6873dfb2f56fc78474dd4ba3f4ccd5f917848e3
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Mar 19 18:35:23 2015 +0100

    Introduce USE_USER_ACCESSIBLE_TIMERS

commit 728bb469253384b27ca8aa224bcffdb3b1e2c536
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Mar 19 15:54:31 2015 +0100

     sched: Remove get_online_cpus() usage
    
    Remove get_online_cpus() usage from the scheduler; there's 4 sites that
    use it:
    
    - sched_init_smp(); where its completely superfluous since we're in
       'early' boot and there simply cannot be any hotplugging.
    
    - sched_getaffinity(); we already take a raw spinlock to protect the
       task cpus_allowed mask, this disables preemption and therefore
       also stabilizes cpu_online_mask as that's modified using
       stop_machine. However switch to active mask for symmetry with
       sched_setaffinity()/set_cpus_allowed_ptr(). We guarantee active
       mask stability by inserting sync_rcu/sched() into _cpu_down.
    
    - sched_setaffinity(); we don't appear to need get_online_cpus()
       either, there's two sites where hotplug appears relevant:
        * cpuset_cpus_allowed(); for the !cpuset case we use possible_mask,
          for the cpuset case we hold task_lock, which is a spinlock and
          thus for mainline disables preemption (might cause pain on RT).
        * set_cpus_allowed_ptr(); Holds all scheduler locks and thus has
          preemption properly disabled; also it already deals with hotplug
          races explicitly where it releases them.
    
    - migrate_swap(); we can make stop_two_cpus() do the heavy lifting for
       us with a little trickery. By adding a sync_sched/rcu() after the
       CPU_DOWN_PREPARE notifier we can provide preempt/rcu guarantees for
       cpu_active_mask. Use these to validate that both our cpus are active
       when queueing the stop work before we queue the stop_machine works
       for take_cpu_down().
    
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Cc: "Srivatsa S. Bhat" <srivatsa.bhat@linux.vnet.ibm.com>
    Cc: Paul McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Link: http://lkml.kernel.org/r/20131011123820.GV3081@twins.programming
    .kicks-ass.net
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 2d1a934bbf730db6a3c71f225e5895c5d1237096
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Mar 19 15:48:54 2015 +0100

     mm: change initial readahead window size calculation
    
    Change the logic which determines the initial readahead window size
    such that for small requests (one page) the initial window size
    will be x4 the size of the original request, regardless of the
    VM_MAX_READAHEAD value. This prevents a rapid ramp-up
    that could be caused due to increasing VM_MAX_READAHEAD.
    
    Change-Id: I93d59c515d7e6c6d62348790980ff7bd4f434997
    Signed-off-by: Lee Susman <lsusman@codeaurora.org>

commit fdf30edc469367db055d99503c7bc46719e9847e
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Mar 19 15:47:54 2015 +0100

     readahead: bump up the default readahead size
    
    Use 512kb max readahead size, and 32kb min readahead size.
    
    The former helps io performance for common workloads.
    The latter will be used in the thrashing safe context readahead.
    
    -- Rationals on the 512kb size --
    
    I believe it yields more I/O throughput without noticeably increasing
    I/O latency for today's HDD.
    
    For example, for a 100MB/s and 8ms access time HDD, its random IO or
    highly concurrent sequential IO would in theory be:
    
    io_size KB  access_time  transfer_time  io_latency   util%   throughput
    KB/s
    4           8             0.04           8.04        0.49%    497.57
    8           8             0.08           8.08        0.97%    990.33
    16          8             0.16           8.16        1.92%   1961.69
    32          8             0.31           8.31        3.76%   3849.62
    64          8             0.62           8.62        7.25%   7420.29
    128         8             1.25           9.25       13.51%  13837.84
    256         8             2.50          10.50       23.81%  24380.95
    512         8             5.00          13.00       38.46%  39384.62
    1024        8            10.00          18.00       55.56%  56888.89
    2048        8            20.00          28.00       71.43%  73142.86
    4096        8            40.00          48.00       83.33%  85333.33
    
    The 128KB => 512KB readahead size boosts IO throughput from ~13MB/s to
    ~39MB/s, while merely increases (minimal) IO latency from 9.25ms to
    13ms.
    
    As for SSD, I find that Intel X25-M SSD desires large readahead size
    even for sequential reads:
    
    	rasize	1st run		2nd run
    	----------------------------------
    	  4k	123 MB/s	122 MB/s
    	 16k  	153 MB/s	153 MB/s
    	 32k	161 MB/s	162 MB/s
    	 64k	167 MB/s	168 MB/s
    	128k	197 MB/s	197 MB/s
    	256k	217 MB/s	217 MB/s
    	512k	238 MB/s	234 MB/s
    	  1M	251 MB/s	248 MB/s
    	  2M	259 MB/s	257 MB/s
       	  4M	269 MB/s	264 MB/s
    	  8M	266 MB/s	266 MB/s
    The two other impacts of an enlarged readahead size are
    
    - memory footprint (caused by readahead miss)
    	Sequential readahead hit ratio is pretty high regardless of max
    	readahead size; the extra memory footprint is mainly caused by
    	enlarged mmap read-around.
    	I measured my desktop:
    	- under Xwindow:
    		128KB readahead hit ratio = 143MB/230MB = 62%
    		512KB readahead hit ratio = 138MB/248MB = 55%
    		  1MB readahead hit ratio = 130MB/253MB = 51%
    	- under console: (seems more stable than the Xwindow data)
    		128KB readahead hit ratio = 30MB/56MB   = 53%
    		  1MB readahead hit ratio = 30MB/59MB   = 51%
    	So the impact to memory footprint looks acceptable.
    
    - readahead thrashing
    	It will now cost 1MB readahead buffer per stream.  Memory tight
    	systems typically do not run multiple streams; but if they do
    	so, it should help I/O performance as long as we can avoid
    	thrashing, which can be achieved with the following patches.
    
    -- Benchmarks by Vivek Goyal --
    
    I have got two paths to the HP EVA and got multipath device setup(dm-3).
    I run increasing number of sequential readers. File system is ext3 and
    filesize is 1G.
    I have run the tests 3 times (3sets) and taken the average of it.
    
    Workload=bsr      iosched=cfq     Filesz=1G   bs=32K
    ======================================================================
                        2.6.33-rc5                2.6.33-rc5-readahead
    job   Set NR  ReadBW(KB/s)   MaxClat(us)    ReadBW(KB/s)   MaxClat(us)
    ---   --- --  ------------   -----------    ------------   -----------
    bsr   3   1   141768         130965         190302         97937.3
    bsr   3   2   131979         135402         185636         223286
    bsr   3   4   132351         420733         185986         363658
    bsr   3   8   133152         455434         184352         428478
    bsr   3   16  130316         674499         185646         594311
    I ran same test on a different piece of hardware. There are few SATA
    disks
    (5-6) in striped configuration behind a hardware RAID controller.
    
    Workload=bsr      iosched=cfq     Filesz=1G   bs=32K
    ======================================================================
                        2.6.33-rc5                2.6.33-rc5-readahead
    job   Set NR  ReadBW(KB/s)   MaxClat(us)    ReadBW(KB/s)   MaxClat(us)
    ---   --- --  ------------   -----------    ------------   -----------
    bsr   3   1   147569         14369.7        160191         22752
    bsr   3   2   124716         243932         149343         184698
    bsr   3   4   123451         327665         147183         430875
    bsr   3   8   122486         455102         144568         484045
    bsr   3   16  117645         1.03957e+06    137485         1.06257e+06
    Tested-by: Vivek Goyal <vgoyal@redhat.com>
    CC: Jens Axboe <jens.axboe@oracle.com>
    CC: Chris Mason <chris.mason@oracle.com>
    CC: Peter Zijlstra <a.p.zijlstra@chello.nl>
    CC: Martin Schwidefsky <schwidefsky@de.ibm.com>
    CC: Christian Ehrhardt <ehrhardt@linux.vnet.ibm.com>
    Signed-off-by: Wu Fengguang <fengguang.wu@intel.com>
    
    Conflicts:
    	include/linux/mm.h

commit 81073bce8e3853a13319001ca2829f6f1a6ad6ed
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Mar 19 15:47:06 2015 +0100

     ARM: 8160/1: drop warning about return_address not using unwind tables
    
    The warning was introduced in 2009 (commit 4bf1fa5a34aa ([ARM] 5613/1:
    implement CALLER_ADDRESSx)). The only "problem" here is that
    CALLER_ADDRESSx for x > 1 returns NULL which doesn't do much harm.
    
    The drawback of implementing a fix (i.e. use unwind tables to implement CALLER_ADDRESSx) is that much of the unwinder code would need to be marked as not
    traceable.
    
    Signed-off-by: Uwe Kleine-Knig <u.kleine-koenig@pengutronix.de>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>
    
    Conflicts:
    	arch/arm/kernel/return_address.c

commit 12e45e39b9ce3db2fec82d45b8ba6d37deb0e1c4
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Mar 19 15:46:11 2015 +0100

     [SCSI] Remove scsi_wait_scan module
    
    scsi_wait_scan was introduced with asynchronous host scanning as a hack
    for distributions that weren't using proper udev based wait for root to
    appear in their initramfs scripts.  In 2.6.30 Commit
    
    c751085943362143f84346d274e0011419c84202
    Author: Rafael J. Wysocki <rjw@sisk.pl>
    Date:   Sun Apr 12 20:06:56 2009 +0200
    
        PM/Hibernate: Wait for SCSI devices scan to complete during resume
    
    Actually broke scsi_wait_scan because it renders
    scsi_complete_async_scans() a nop for modular SCSI if you include
    scsi_scans.h (which this module does).
    
    The lack of bug reports is sufficient proof that this module is no
    longer used.
    
    Cc: Jeff Mahoney <jeffm@suse.de>
    Cc: Dave Jones <davej@redhat.com>
    Cc: maximilian attems <max@stro.at>
    Signed-off-by: James Bottomley <JBottomley@Parallels.com>
    
    Conflicts:
    	drivers/scsi/scsi_wait_scan.c

commit 3a806a21c62dbdf23c2099b4de3beaddfecc97f6
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Mar 19 15:18:12 2015 +0100

     net: netfilter: Force no unsafe math optimizations
    
    GCC spouts some weird segmentation fault.
    
    Signed-off-by: Zhao Wei Liew <zhaoweiliew@gmail.com>

commit f6b7293440a5107d49c115bcfb48107b44e778fe
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Mar 19 15:14:10 2015 +0100

     msm: kgsl: Fix policy sysfs implementation
    
    When switching to another governor, the pwrscale kobject will be deleted
    and sysfs_create_group fails due to the uninitialized kobj. This patch
    deletes the kobject before attaching the file group.
    
    [   36.270044] [<c0134304>] (__kmalloc_track_caller+0x80/0x194) from [<c010a6d0>] (kstrdup+0x2c/0x54)
    [   36.270166] [<c010a6d0>] (kstrdup+0x2c/0x54) from [<c019cc98>] (create_dir+0x24/0x204)
    [   36.270257] [<c019cc98>] (create_dir+0x24/0x204) from [<c019d154>] (sysfs_create_subdir+0x24/0x2c)
    [   36.270318] [<c019d154>] (sysfs_create_subdir+0x24/0x2c) from [<c019e878>] (sysfs_create_group+0x40/0x1d8)
    [   36.270441] [<c019e878>] (sysfs_create_group+0x40/0x1d8) from [<c0389e40>] (kgsl_pwrscale_policy_add_files+0x44/0x68)
    [   36.270563] [<c0389e40>] (kgsl_pwrscale_policy_add_files+0x44/0x68) from [<c038e714>] (conservative_init+0xc/0x18)
    [   36.270654] [<c038e714>] (conservative_init+0xc/0x18) from [<c0389c6c>] (pwrscale_policy_store+0xcc/0x154)
    [   36.270776] [<c0389c6c>] (pwrscale_policy_store+0xcc/0x154) from [<c0389a90>] (pwrscale_sysfs_store+0x20/0x2c)
    [   36.270868] [<c0389a90>] (pwrscale_sysfs_store+0x20/0x2c) from [<c019b2a4>] (sysfs_write_file+0x168/0x198)
    [   36.270990] [<c019b2a4>] (sysfs_write_file+0x168/0x198) from [<c01382c4>] (vfs_write+0x9c/0x170)
    [   36.271112] [<c01382c4>] (vfs_write+0x9c/0x170) from [<c0138440>] (sys_write+0x38/0x70)
    [   36.271173] [<c0138440>] (sys_write+0x38/0x70) from [<c000f000>] (ret_fast_syscall+0x0/0x30)
    [   36.271265] Code: e7925001 e3550000 0a000033 e5942014 (e795a002)
    [   36.271661] ---[ end trace ef0d8b9c8acc7bbb ]---

commit ad2295c65942fb39bdcf8e06b2fca7184b20e52c
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Mar 19 15:12:32 2015 +0100

     msm: kgsl: Implement conservative GPU governor
    
    Signed-off-by: Zhao Wei Liew <zhaoweiliew@gmail.com>

commit adf7a8399e98c3030d351051eff272941ff9ae8b
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Mar 19 15:08:33 2015 +0100

    msm: kgsl: Report GPU frequency correctly in sysfs

commit e16fe76cb6fb5e7191804dc04316545441284a44
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Mar 19 14:58:43 2015 +0100

     KEYS: close race between key lookup and freeing
    
    When a key is being garbage collected, it's key->user would get put
    before
    the ->destroy() callback is called, where the key is removed from it's
    respective tracking structures.
    
    This leaves a key hanging in a semi-invalid state which leaves a window
    open
    for a different task to try an access key->user. An example is
    find_keyring_by_name() which would dereference key->user for a key that
    is
    in the process of being garbage collected (where key->user was freed but
    ->destroy() wasn't called yet - so it's still present in the linked
    list).
    
    This would cause either a panic, or corrupt memory.
    
    Change-Id: I01a5ec17916864929458caa9d0fbefea2ca2c5e2
    Signed-off-by: Sasha Levin <sasha.levin@oracle.com>

commit 8e5b8d964b94575c4d6cacd3aaebf8700957a890
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Mar 19 14:56:16 2015 +0100

     cpufreq: Return directly in __cpufreq_get if policy is NULL
    
    __cpufreq_get() checks whether policy->cur matches frequency returned
    by cpufreq device driver and acts accordingly. However, policy could
    be NULL if the CPU is being hotplugged. Current implementation crashes
    when trying to dereference the NULL policy.
    
    Return the frequency provided by cpufreq device driver directly if
    policy is not available.
    
    Signed-off-by: Junjie Wu <junjiew@codeaurora.org>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit c65b43984afd1645c1f0b0faaee2c13bf8d9e08b
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Mar 19 14:39:32 2015 +0100

    cpufreq: New relation C for userspace

commit eb59d7d9d50e14e0d00b715603655633a7717b7d
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Mar 19 14:37:26 2015 +0100

     ARM: convert all "mov.* pc, reg" to "bx reg" for ARMv6+
    
    ARMv6 and greater introduced a new instruction ("bx") which can be used
    to return from function calls.  Recent CPUs perform better when the
    "bx lr" instruction is used rather than the "mov pc, lr" instruction,
    and this sequence is strongly recommended to be used by the ARM
    architecture manual (section A.4.1.1).
    
    We provide a new macro "ret" with all its variants for the condition
    code which will resolve to the appropriate instruction.
    
    Rather than doing this piecemeal, and miss some instances, change all
    the "mov pc" instances to use the new macro, with the exception of
    the "movs" instruction and the kprobes code.  This allows us to detect
    the "mov pc, lr" case and fix it up - and also gives us the possibility
    of deploying this for other registers depending on the CPU selection.
    
    Reported-by: Will Deacon <will.deacon@arm.com>
    Tested-by: Stephen Warren <swarren@nvidia.com> # Tegra Jetson TK1
    Tested-by: Robert Jarzmik <robert.jarzmik@free.fr> # mioa701_bootresume.S
    Tested-by: Andrew Lunn <andrew@lunn.ch> # Kirkwood
    Tested-by: Shawn Guo <shawn.guo@freescale.com>
    Tested-by: Tony Lindgren <tony@atomide.com> # OMAPs
    Tested-by: Gregory CLEMENT <gregory.clement@free-electrons.com> # Armada XP, 375, 385
    Acked-by: Sekhar Nori <nsekhar@ti.com> # DaVinci
    Acked-by: Christoffer Dall <christoffer.dall@linaro.org> # kvm/hyp
    Acked-by: Haojian Zhuang <haojian.zhuang@gmail.com> # PXA3xx
    Acked-by: Stefano Stabellini <stefano.stabellini@eu.citrix.com> # Xen
    Tested-by: Uwe Kleine-Knig <u.kleine-koenig@pengutronix.de> # ARMv7M
    Tested-by: Simon Horman <horms+renesas@verge.net.au> # Shmobile
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

commit 19583debe8c55b479f2b6ee2d8416a879e004e70
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Mar 19 12:09:23 2015 +0100

     staging: binder: Change binder mutex to rtmutex
    
    Surfaceflinger uses binder heavily to receive/send frames from
    applications
    while compositing the screen. Change the binder mutex to an rt mutex to
    minimize
    instances where high priority surfaceflinger binder work is blocked by
    lower
    priority binder ipc.
    
    Signed-off-by: Riley Andrews <riandrews@google.com>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit e6d9033405cc3b73024d28efe111db13e2e9cd57
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Mar 19 12:05:18 2015 +0100

    msm_rq_stats changes

commit 5f95877c331f99178b204c3c3d5980f364be7262
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Mar 19 12:02:13 2015 +0100

    cpufreq_ondemand: use the new freq relation

commit 6f5edd84521800e476bffdcf68e3654c77965b8c
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Mar 19 11:58:53 2015 +0100

    bcmdhd: disable fw_roam_suspend

commit 0e785e84ef25daa400deaf7c95f75ea83723139c
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Mar 19 11:56:00 2015 +0100

     sched: Fix inaccurate accounting for real-time task
    
    It is possible that rq->clock_task was not updated in put_prev_task()
    in which case we can potentially overcharge a real-time task for time
    it did not run. This is because clock_task could be stale and not
    represent the exact time real-time task started running.
    
    Fix this by forcing update of rq->clock_task when real-time task
    starts running.
    
    Signed-off-by: Srivatsa Vaddagiri <vatsa@codeaurora.org>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit e73ea92e8186b94e5e6ea5965dc0244a4349cc05
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Mar 19 11:55:06 2015 +0100

     ARM: 8226/1: cacheflush: get rid of restarting block
    
    We cannot restart cacheflush safely if a process provides user-defined
    signal handler and signal is pending. In this case -EINTR is returned
    and it is expected that process re-invokes syscall. However, there are
    a few problems with that:
    * looks like nobody bothers checking return value from cacheflush
    * but if it did, we don't provide the restart address for that, so the
       process has to use the same range again
    * ...and again, what might lead to looping forever
    
    So, remove cacheflush restarting code and terminate cache flushing
    as early as fatal signal is pending.
    
    Cc: stable@vger.kernel.org # 3.12+
    Reported-by: Chanho Min <chanho.min@lge.com>
    Signed-off-by: Vladimir Murzin <vladimir.murzin@arm.com>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

commit e7b0626a3e2b988703321a474106bfab2574f855
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Mar 19 11:51:25 2015 +0100

     ARM: 8222/1: mvebu: enable strex backoff delay
    
    Under extremely rare conditions, in an MPCore node consisting of at
    least 3 CPUs, two CPUs trying to perform a STREX to data on the same
    shared cache line can enter a livelock situation.
    
    This patch enables the HW mechanism that overcomes the bug. This fixes
    the incorrect setup of the STREX backoff delay bit due to a wrong
    description in the specification.
    
    Note that enabling the STREX backoff delay mechanism is done by
    leaving the bit *cleared*, while the bit was currently being set by
    the proc-v7.S code.
    
    [Thomas: adapt to latest mainline, slightly reword the commit log, add
    stable markers.]
    
    Fixes: de4901933f6d ("arm: mm: Add support for PJ4B cpu and init
    routines")
    
    Cc: <stable@vger.kernel.org> # v3.8+
    Signed-off-by: Nadav Haklai <nadavh@marvell.com>
    Signed-off-by: Thomas Petazzoni <thomas.petazzoni@free-electrons.com>
    Acked-by: Gregory CLEMENT <gregory.clement@free-electrons.com>
    Acked-by: Jason Cooper <jason@lakedaemon.net>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

commit 34d272fa098e3c5de0194ec7e3b93a709cc08760
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Mar 19 11:48:05 2015 +0100

     writeback: Avoid iput() from flusher thread
    
    Doing iput() from flusher thread (writeback_sb_inodes()) can create
    problems
    because iput() can do a lot of work - for example truncate the inode if
    it's
    the last iput on unlinked file. Some filesystems depend on flusher
    thread
    progressing (e.g. because they need to flush delay allocated blocks to
    reduce
    allocation uncertainty) and so flusher thread doing truncate creates
    interesting dependencies and possibilities for deadlocks.
    
    We get rid of iput() in flusher thread by using the fact that I_SYNC
    inode
    flag effectively pins the inode in memory. So if we take care to either
    hold
    i_lock or have I_SYNC set, we can get away without taking inode
    reference
    in writeback_sb_inodes().
    
    As a side effect of these changes, we also fix possible use-after-free
    in
    wb_writeback() because inode_wait_for_writeback() call could try to
    reacquire
    i_lock on the inode that was already free.
    
    Signed-off-by: Jan Kara <jack@suse.cz>
    Signed-off-by: Fengguang Wu <fengguang.wu@intel.com>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit ed5be14743410c4812c1daa1c29355e55c02ff12
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Mar 19 00:06:04 2015 +0100

     vfs: Move waiting for inode writeback from end_writeback() to evict_i
    
    node()
    
    Currently, I_SYNC can never be set when evict_inode() (and thus
    end_writeback()) is called because flusher thread holds inode reference
    while
    inode is under writeback. As a result inode_sync_wait() in those places
    currently does nothing. However that is going to change and unveils
    problems
    with calling inode_sync_wait() from end_writeback(). Several filesystems
    call
    end_writeback() after they have deleted the inode (btrfs, gfs2, ...) and
    other
    filesystems (ext3, ext4, reiserfs, ...) can deadlock when waiting for
    I_SYNC
    because they call end_writeback() from within a transaction.
    
    To avoid these issues, we move inode_sync_wait() into evict_inode()
    before
    calling ->evict_inode(). That way we preserve the current property that
    ->evict_inode() and writeback never run in parallel and all filesystems
    are
    safe.
    
    Signed-off-by: Jan Kara <jack@suse.cz>
    Signed-off-by: Fengguang Wu <fengguang.wu@intel.com>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 1e7f024bba3a8755d3d5799526b3c892d7efa12b
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Mar 19 00:04:17 2015 +0100

     writeback: Refactor writeback_single_inode()
    
    The code in writeback_single_inode() is relatively complex. The list
    requeing
    logic makes sense only for flusher thread but not really for
    sync_inode() or
    write_inode_now() callers. Also when we want to get rid of inode
    references
    held by flusher thread, we will need a special I_SYNC handling there.
    
    So separate part of writeback_single_inode() which does the real
    writeback work
    into __writeback_single_inode() and make writeback_single_inode() do
    only stuff
    necessary for callers writing only one inode, moving the special list
    handling
    into writeback_sb_inodes(). As a sideeffect this fixes a possible race
    where we
    could skip some inode during sync(2) because other writer refiled it
    from b_io
    to b_dirty list. Also I_SYNC handling is moved into the callers of
    __writeback_single_inode() to make locking easier.
    
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jan Kara <jack@suse.cz>
    Signed-off-by: Fengguang Wu <fengguang.wu@intel.com>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 880bd3d9f1c585fa11fd344c247611fe31742490
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Mar 18 23:53:39 2015 +0100

     writeback: Remove wb->list_lock from writeback_single_inode()
    
    writeback_single_inode() doesn't need wb->list_lock for anything on
    entry now.
    So remove the requirement. This makes locking of
    writeback_single_inode()
    temporarily awkward (entering with i_lock, returning with i_lock and
    wb->list_lock) but it will be sanitized in the next patch.
    
    Also inode_wait_for_writeback() doesn't need wb->list_lock for anything.
    It was
    just taking it to make usage convenient for callers but with
    writeback_single_inode() changing it's not very convenient anymore. So
    remove
    the lock from that function.
    
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jan Kara <jack@suse.cz>
    Signed-off-by: Fengguang Wu <fengguang.wu@intel.com>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit c823a26798dc11decbeadcc294218e4159877ea7
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Mar 18 23:49:06 2015 +0100

     writeback: Separate inode requeueing after writeback
    
    Move inode requeueing after inode has been written out into a separate
    function.
    
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jan Kara <jack@suse.cz>
    Signed-off-by: Fengguang Wu <fengguang.wu@intel.com>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 1e79a0fd56aedd8d17bd4f4f3f351b07cbff51c8
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Mar 18 23:47:17 2015 +0100

     writeback: Move I_DIRTY_PAGES handling
    
    Instead of clearing I_DIRTY_PAGES and resetting it when we didn't
    succeed in
    writing them all, just clear the bit only when we succeeded writing all
    the
    pages. We also move the clearing of the bit close to other i_state
    handling to
    separate it from writeback list handling. This is desirable because list
    handling will differ for flusher thread and other
    writeback_single_inode()
    callers in future. No filesystem plays any tricks with I_DIRTY_PAGES
    (like
    checking it in ->writepages or ->write_inode implementation) so this
    movement
    is safe.
    
    Signed-off-by: Jan Kara <jack@suse.cz>
    Signed-off-by: Fengguang Wu <fengguang.wu@intel.com>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 9558ffa76ae613e6591fe7e515f183fcee5e0fef
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Mar 18 23:46:00 2015 +0100

     writeback: Move requeueing when I_SYNC set to writeback_sb_inodes()
    
    When writeback_single_inode() is called on inode which has I_SYNC
    already
    set while doing WB_SYNC_NONE, inode is moved to b_more_io list. However
    this makes sense only if the caller is flusher thread. For other callers
    of
    writeback_single_inode() it doesn't really make sense and may be even
    wrong
    - flusher thread may be doing WB_SYNC_ALL writeback in parallel.
    
    So we move requeueing from writeback_single_inode() to
    writeback_sb_inodes().
    
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jan Kara <jack@suse.cz>
    Signed-off-by: Fengguang Wu <fengguang.wu@intel.com>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 4f10257c9c28b1159a8a621e6380b15b9d4a441e
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Mar 18 23:42:50 2015 +0100

    drivers: thermal: renamed msm_thermal to mako_msm_thermal to fix the compilation problem

commit 729af0c5622338e1e3ffe473ea60fd35c57dda93
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Mar 18 23:16:01 2015 +0100

     writeback: Move clearing of I_SYNC into inode_sync_complete()
    
    Move clearing of I_SYNC into inode_sync_complete().  It is more logical
    to have
    clearing of I_SYNC bit and waking of waiters in one place. Also later we
    will
    have two places needing to clear I_SYNC and wake up waiters so this
    allows them
    to use the common helper. Moving of I_SYNC clearing to a later stage of
    writeback_single_inode() is safe since we hold i_lock all the time.
    
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jan Kara <jack@suse.cz>
    Signed-off-by: Fengguang Wu <fengguang.wu@intel.com>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 85cc478be87b7c35a804774769525c09e542d267
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Mar 18 23:14:59 2015 +0100

    Fix some of my first compiling errors

commit 81305c75c9efb581efd65be05d63309de85e5368
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Mar 18 22:33:02 2015 +0100

     fs: remove 8 bytes of padding from struct writeback_control on 64 bit
    
     builds
    
    Reorder structure writeback_control to remove 8 bytes of padding on 64
    bit builds, this shrinks its size from 48 to 40 bytes.
    
    This structure is always on the stack and uses C99 named initialisation,
    so should be safe and have a small impact on stack usage.
    
    Signed-off-by: Richard Kennedy <richard@rsk.demon.co.uk>
    Signed-off-by: Fengguang Wu <fengguang.wu@intel.com>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 88987ef52320f450ebaf5fb195037304893b03e3
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Mar 18 22:31:56 2015 +0100

     sysfs: Removed dup_name entirely in sysfs_rename
    
    Since no one using "dup_name", removed it completely in sysfs_rename.
    
    Signed-off-by: Sasikantha babu <sasikanth.v19@gmail.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 6a6da11a85ea3fa87f32358fb7452d8e61a75c9a
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Mar 18 22:30:42 2015 +0100

     binfmt_elf.c: use get_random_int() to fix entropy depleting
    
    Changes:
    --------
    v4->v3:
    - s/random_stack_user()/get_atrandom_bytes()/
    - Move this function to ahead of its use to avoid the predeclaration.
    
    v3->v2:
    - Tweak code comments of random_stack_user().
    - Remove redundant bits mask and shift upon the random variable.
    
    v2->v1:
    - Fix random copy to check up buffer length that are not 4-byte
    multiples.
    
    v3 can be found at:
    http://www.spinics.net/lists/linux-fsdevel/msg59597.html
    v2 can be found at:
    http://www.spinics.net/lists/linux-fsdevel/msg59418.html
    v1 can be found at:
    http://www.spinics.net/lists/linux-fsdevel/msg59128.html
    
    Thanks,
    -Jeff
    
    Entropy is quickly depleted under normal operations like ls(1), cat(1),
    etc...  between 2.6.30 to current mainline, for instance:
    
    $ cat /proc/sys/kernel/random/entropy_avail
    3428
    $ cat /proc/sys/kernel/random/entropy_avail
    2911
    $cat /proc/sys/kernel/random/entropy_avail
    2620
    
    We observed this problem has been occurring since 2.6.30 with
    fs/binfmt_elf.c: create_elf_tables()->get_random_bytes(), introduced by
    f06295b ("ELF: implement AT_RANDOM for glibc PRNG seeding").
    
    /*
    * Generate 16 random bytes for userspace PRNG seeding.
    */
    get_random_bytes(k_rand_bytes, sizeof(k_rand_bytes));
    
    The patch introduces a wrapper around get_random_int() which has lower
    overhead than calling get_random_bytes() directly.
    
    With this patch applied:
    $ cat /proc/sys/kernel/random/entropy_avail
    2731
    $ cat /proc/sys/kernel/random/entropy_avail
    2802
    $ cat /proc/sys/kernel/random/entropy_avail
    2878
    
    Analyzed by John Sobecki.
    
    Signed-off-by: Jie Liu <jeff.liu@oracle.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Al Viro <viro@zeniv.linux.org.uk>
    Cc: Andreas Dilger <aedilger@gmail.com>
    Cc: Alan Cox <alan@linux.intel.com>
    Cc: Arnd Bergmann <arnn@arndb.de>
    Cc: John Sobecki <john.sobecki@oracle.com>
    Cc: James Morris <james.l.morris@oracle.com>
    Cc: Jakub Jelinek <jakub@redhat.com>
    Cc: Ted Ts'o <tytso@mit.edu>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Ulrich Drepper <drepper@redhat.com>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 06f3f9c7d257fcdbd5ca1cf67498d8ab9f45fe84
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Mar 18 22:29:02 2015 +0100

     ARM: 7408/1: cacheflush: return error to userspace when flushing sysc
    
    all fails
    
    The cacheflush syscall can fail for two reasons:
    
    (1) The arguments are invalid (nonsensical address range or no VMA)
    
    (2) The region generates a translation fault on a VIPT or PIPT cache
    
    This patch allows do_cache_op to return an error code to userspace in
    the case of the above. The various coherent_user_range implementations
    are modified to return 0 in the case of VIVT caches or -EFAULT in the
    case of an abort on v6/v7 cores.
    
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>
    
    Conflicts:
    	arch/arm/include/asm/cacheflush.h
    	arch/arm/kernel/traps.c

commit 20eaedfeb306396f9b169cb360038d6a3121ded3
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Mar 18 22:19:21 2015 +0100

     ARM: 7784/1: mm: ensure SMP alternates assemble to exactly 4 bytes wi
    
    th Thumb-2
    
    Commit ("ARM: 7691/1: mm: kill unused TLB_CAN_READ_FROM_L1_CACHE
    and use ALT_SMP instead") added early function returns for page table
    cache flushing operations on ARMv7 SMP CPUs.
    
    Unfortunately, when targetting Thumb-2, these `mov pc, lr' sequences
    assemble to 2 bytes which can lead to corruption of the instruction
    stream after code patching.
    
    This patch fixes the alternates to use wide (32-bit) instructions for
    Thumb-2, therefore ensuring that the patching code works correctly.
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>
    
    Conflicts:
    	arch/arm/mm/proc-v7.S

commit 6515a99083d21e9a73a65c0ae256a73f98abaa44
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Mar 18 22:18:12 2015 +0100

     ARM: 7691/1: mm: kill unused TLB_CAN_READ_FROM_L1_CACHE and use ALT_S
    
    MP instead
    
    Many ARMv7 cores have hardware page table walkers that can read the L1
    cache. This is discoverable from the ID_MMFR3 register, although this
    can be expensive to access from the low-level set_pte functions and is a
    pain to cache, particularly with multi-cluster systems.
    
    A useful observation is that the multi-processing extensions for ARMv7
    require coherent table walks, meaning that we can make use of ALT_SMP
    patching in proc-v7-* to patch away the cache flush safely for these
    cores.
    
    Reported-by: Albin Tonnerre <Albin.Tonnerre@arm.com>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>
    
    Conflicts:
    	arch/arm/mm/proc-v7-2level.S
    
    Conflicts:
    	arch/arm/mm/proc-v7.S

commit ee00f81890400f7708323ddddf25ed4813a22bc8
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Mar 18 22:07:44 2015 +0100

    arch/arm: mm mixed changes and optimiations

commit fab8dbaaa89b0a26109399a141af44aac5e79838
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Mar 18 20:42:10 2015 +0100

    Revert: "ARM: 7678/1: Work around faulty ISAR0 register in some Krait CPUs "

commit aeaf40a8450f3beebdec23473438ba4178cc850a
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Mar 18 19:40:08 2015 +0100

     cpufreq: Avoid crash in resume on SMP without OPP
    
    When resuming from s2ram on an SMP system without cpufreq operating
    points (e.g. there's no "operating-points" property for the CPU node in
    DT, or the platform doesn't use DT yet), the kernel crashes when
    bringing CPU 1 online:
    
        Enabling non-boot CPUs ...
        CPU1: Booted secondary processor
        Unable to handle kernel NULL pointer dereference at virtual address
    0000003c
        pgd = ee5e6b00
        [0000003c] *pgd=6e579003, *pmd=6e588003, *pte=00000000
        Internal error: Oops: a07 [#1] SMP ARM
        Modules linked in:
        CPU: 0 PID: 1246 Comm: s2ram Tainted: G        W
    3.18.0-rc3-koelsch-01614-g0377af242bb175c8-dirty #589
        task: eeec5240 ti: ee704000 task.ti: ee704000
        PC is at __cpufreq_add_dev.isra.24+0x24c/0x77c
        LR is at __cpufreq_add_dev.isra.24+0x244/0x77c
        pc : [<c0298efc>]    lr : [<c0298ef4>]    psr: 60000153
        sp : ee705d48  ip : ee705d48  fp : ee705d84
        r10: c04e0450  r9 : 00000000  r8 : 00000001
        r7 : c05426a8  r6 : 00000001  r5 : 00000001  r4 : 00000000
        r3 : 00000000  r2 : 00000000  r1 : 20000153  r0 : c0542734
    
    Verify that policy is not NULL before dereferencing it to fix this.
    
    Signed-off-by: Geert Uytterhoeven <geert+renesas@glider.be>
    Fixes: 8414809c6a1e (cpufreq: Preserve policy structure across
    suspend/resume)
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit f6e2a7766693da22c2da47d4123e89ccbb4000a5
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Mar 18 19:35:46 2015 +0100

     sched/fair: Optimize find_busiest_queue()
    
    Use for_each_cpu_and() and thereby avoid computing the capacity for
    CPUs we know we're not interested in.
    
    Reviewed-by: Paul Turner <pjt@google.com>
    Reviewed-by: Preeti U Murthy <preeti@linux.vnet.ibm.com>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/n/tip-lppceyv6kb3a19g8spmrn20b@git.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Paul Reioux <reioux@gmail.com>
    
    backported for Linux 3.4

commit 6f3abd18b448c198e968884a40d0689ec6d04c7c
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Mar 18 19:32:27 2015 +0100

     sched/cpuidle: reduce IPI store. Backport upstream 3.16 scheduler upd
    
    ates
    
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit 6666f6cc89f395b88e400af51d9c5fe21a1eb6e1
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Mar 18 19:26:46 2015 +0100

     af_unix: speedup /proc/net/unix /proc/net/unix has quadratic behavior
    
    , and can hold unix_table_lock for a while if high number of unix
    sockets are alive. (90 ms for 200k sockets...)
    
    We already have a hash table, so its quite easy to use it.
    
    Problem is unbound sockets are still hashed in a single hash slot
    (unix_socket_table[UNIX_HASH_TABLE])
    
    This patch also spreads unbound sockets to 256 hash slots, to speedup
    both /proc/net/unix and unix_diag.
    
    Time to read /proc/net/unix with 200k unix sockets :
    (time dd if=/proc/net/unix of=/dev/null bs=4k)
    
    before : 520 secs
    after : 2 secs
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Cc: Steven Whitehouse <swhiteho@redhat.com>
    Cc: Pavel Emelyanov <xemul@parallels.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>
    Signed-off-by: flar2 <asegaert@gmail.com>

commit 0c0802670f2d81547e10139e0ed895ed3a3e58ab
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Mar 18 14:30:50 2015 +0100

     sched: Reduce overestimating rq->avg_idle
    
    When updating avg_idle, if the delta exceeds some max value, then
    avg_idle
    gets set to the max, regardless of what the previous avg was. This can
    cause
    avg_idle to often be overestimated.
    
    This patch modifies the way we update avg_idle by always updating it
    with the
    function call to update_avg() first. Then, if avg_idle exceeds the max,
    we set
    it to the max.
    
    Signed-off-by: Jason Low <jason.low2@hp.com>
    Reviewed-by: Rik van Riel <riel@redhat.com>
    
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit 02d75385d11a85e214a2a5b936e6d995bac39e81
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Mar 18 14:27:09 2015 +0100

     zram: avoid kunmap_atomic() of a NULL pointer
    
    zram could kunmap_atomic() a NULL pointer in a rare situation: a zram
    page becomes a full-zeroed page after a partial write io.  The current
    code doesn't handle this case and performs kunmap_atomic() on a NULL
    pointer, which panics the kernel.
    
    This patch fixes this issue.
    
    Signed-off-by: Weijie Yang <weijie.yang@samsung.com>
    Cc: Sergey Senozhatsky <sergey.senozhatsky@gmail.com>
    Cc: Dan Streetman <ddstreet@ieee.org>
    Cc: Nitin Gupta <ngupta@vflare.org>
    Cc: Weijie Yang <weijie.yang.kh@gmail.com>
    Acked-by: Jerome Marchand <jmarchan@redhat.com>
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit 83c05d4902701c1fac822982506f308199bc81bf
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Mar 18 14:24:04 2015 +0100

     ARM: 8191/1: decompressor: ensure I-side picks up relocated code
    
    To speed up decompression, the decompressor sets up a flat, cacheable
    mapping of memory. However, when there is insufficient space to hold
    the page tables for this mapping, we don't bother to enable the caches
    and subsequently skip all the cache maintenance hooks.
    
    Skipping the cache maintenance before jumping to the relocated code
    allows the processor to predict the branch and populate the I-cache
    with stale data before the relocation loop has completed (since a
    bootloader may have SCTLR.I set, which permits normal, cacheable
    instruction fetches regardless of SCTLR.M).
    
    This patch moves the cache maintenance check into the maintenance
    routines themselves, allowing the v6/v7 versions to invalidate the
    I-cache regardless of the MMU state.
    
    Cc: <stable@vger.kernel.org>
    Reported-by: Marc Carino <marc.ceeeee@gmail.com>
    Tested-by: Julien Grall <julien.grall@linaro.org>
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

commit dcaec91fc569ff84e02315ad8ab6a4ccf4355a80
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Mar 18 14:19:25 2015 +0100

     ARM acpuclk: Inline heavy used functions to reduce overhead. by duki9
    
    94@xda

commit d9ffa7c94260445aeb14d31f6536071f0f15f277
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Mar 18 14:10:19 2015 +0100

     cpufreq: Make sure target freq is within limits
    
    __cpufreq_driver_target() must not pass target frequency beyond the
    limits of current policy.
    
    Today most of cpufreq platform drivers are doing this check in their
    target routines. Why not move it to __cpufreq_driver_target()?
    
    Signed-off-by: Viresh Kumar <viresh.kumar@linaro.org>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit ca06e6ee86eda22b66841c3e8b359823fd9d0a3d
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Mar 18 14:08:41 2015 +0100

    CPUFREQ: use bool in __cpufreq_cpu_get sysfs

commit f40e08802fbf892eac70c3a374c6cfef3d231e99
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Mar 18 14:06:12 2015 +0100

     cpufreq: handle cpufreq being disabled for all exported function.
    
    When disable_cpufreq() is called some exported functions are still
    being used that do not have a check for cpufreq being disabled.
    
    Add a disabled check into cpufreq_cpu_get() to return NULL if
    cpufreq is disabled this covers most of the exported functions. For
    the exported functions that do not call cpufreq_cpu_get() add an
    explicit check.
    
    Signed-off-by: Dirk Brandewie <dirk.j.brandewie@intel.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit 4abfc896dd6451de3f35cdcd3e77c5ca6f64f9ca
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Mar 18 14:03:11 2015 +0100

     cpufreq: Add a get_current_driver helper
    
    Add a helper function to return cpufreq_driver->name.
    
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit 6d32711253285060e79dfe729968cf0b0f6cd36c
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Mar 18 13:56:26 2015 +0100

    cpufreq: Create a macro for unlock_policy_rwsem{read,write}
    
    On the lines of macro: lock_policy_rwsem, we can create another macro
    for
    unlock_policy_rwsem. Lets do it.
    
    Signed-off-by: Viresh Kumar <viresh.kumar@linaro.org>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit f0fc660a1f9125d73c1dfb90eb025876080c2045
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Mar 18 13:44:59 2015 +0100

     switch do_fsync() to fget_light()
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>
    Signed-off-by: franciscofranco <franciscofranco.1990@gmail.com>

commit 346043d545add74e08839daa953429378d3f6668
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Mar 18 13:42:54 2015 +0100

     ARM: 8180/1: mm: implement no-highmem fast path in kmap_atomic_pfn()
    
    Since CONFIG_HIGHMEM got enabled on ARMv5 Kirkwood, we have noticed a
    very significant drop in networking performance. The test were
    conducted on an OpenBlocks A7 board. Without this patch, the outgoing
    performance measured with iperf are:
    
    - highmem OFF, TSO OFF   544 Mbit/s
    - highmem OFF, TSO ON	  942 Mbit/s
    - highmem ON,  TSO OFF   306 Mbit/s
    - highmem ON,  TSO ON    246 Mbit/s
    
    On this Kirkwood platform, the L2 cache is a Feroceon cache, and with
    this cache, all the range operations have to be done on virtual
    addresses and not physical addresses. Therefore, whenever
    CONFIG_HIGHMEM is enabled, the cache maintenance operations call
    kmap_atomic_pfn() and kunmap_atomic().
    
    However, kmap_atomic_pfn() does not implement the same fast path for
    non-highmem pages as the one implemented in kmap_atomic(), and this is
    one of the reason for the performance drop. While this patch does not
    fully restore the performances, it clearly improves them a lot:
    
          	      	        without patch  with patch
    
    - highmem ON, TSO OFF   306 Mbit/s     387 Mbit/s
    - highmem ON, TSO ON    246 Mbit/s     434 Mbit/s
    
    We're still far from the !CONFIG_HIGHMEM performances, but it does
    improve a bit the situation.
    
    Thanks a lot to Ezequiel Garcia and Gregory Clement for all the
    testing work around this topic.
    
    Signed-off-by: Thomas Petazzoni <thomas.petazzoni@free-electrons.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

commit f03a53fb1f194664124e42c8c14400ed67aee0f6
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Mar 18 13:38:34 2015 +0100

     ARM: fix some printk formats & Blacklist GCC 4.8.0 to GCC 4.8.2
    
    We fix some printk formats for ptrdiff_t quantities which cause GCC
    4.9 to complain, and we also blacklist known buggy GCC 4.8.x compilers
    as their miscompilation is serious enough to cause filesystem
    corruption, even through many distros have fixed their versions"
    
    * 'fixes' of git://ftp.arm.linux.org.uk/~rmk/linux-arm:
    ARM: fix some printk formats
    ARM: Blacklist GCC 4.8.0 to GCC 4.8.2 - PR58854

commit 67108b7e42cd5d1110fbcc81d5ba0b917e53d349
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Mar 18 13:35:26 2015 +0100

    cpufreq: remove prevention of managing offline cpus

commit 01e9f5471806d6e3f3cfe15bbbf68b40b1b2b60e
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Mar 18 13:29:02 2015 +0100

     tracing/syscalls: combined patches from cyanogenmod
    
    tracing/syscalls: Fix perf syscall tracing when syscall_nr == -1
    syscall_get_nr can return -1 in the case that the task is not executing
    a system call.
    
    This patch fixes perf_syscall_{enter,exit} to check that the syscall
    number is valid before using it as an index into a bitmap.
    
    Link: http://lkml.kernel.org/r/1345137254-7377-1-git-send-email-
    will.deacon@arm.com
    
    Change-Id: Ie730f03d82ed3e44b53a6fe42b97172b435f04eb
    Cc: Jason Baron <jbaron@redhat.com>
    Cc: Wade Farnsworth <wade_farnsworth@mentor.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>
    
    tracing/syscalls: Ignore numbers outside NR_syscalls' range
    ARM has some private syscalls (for example, set_tls(2)) which lie
    outside the range of NR_syscalls.  If any of these are called while
    syscall tracing is being performed, out-of-bounds array access will
    occur in the ftrace and perf sys_{enter,exit} handlers.
    
    ...
    true-653   [000]   384.675777: sys_enter:            NR 192 (0, 1000,
    3, 4000022, ffffffff, 0)
    true-653   [000]   384.675812: sys_exit:             NR 192 =
    1995915264
    true-653   [000]   384.675971: sys_enter:            NR 983045
    (76f74480, 76f74000, 76f74b28, 76f74480, 76f76f74, 1)
    true-653   [000]   384.675988: sys_exit:             NR 983045 = 0
    ...
    
    [   17.289329] Unable to handle kernel paging request at virtual
    address aaaaaace
    [   17.289590] pgd = 9e71c000
    [   17.289696] [aaaaaace] *pgd=00000000
    [   17.289985] Internal error: Oops: 5 [#1] PREEMPT SMP ARM
    [   17.290169] Modules linked in:
    [   17.290391] CPU: 0 PID: 704 Comm: true Not tainted 3.18.0-rc2+ #21
    [   17.290585] task: 9f4dab00 ti: 9e710000 task.ti: 9e710000
    [   17.290747] PC is at ftrace_syscall_enter+0x48/0x1f8
    [   17.290866] LR is at syscall_trace_enter+0x124/0x184
    
    Fix this by ignoring out-of-NR_syscalls-bounds syscall numbers.
    
    Commit cd0980f "tracing: Check invalid syscall nr while tracing
    syscalls"
    added the check for less than zero, but it should have also checked
    for greater than NR_syscalls.
    
    Link: http://lkml.kernel.org/p/1414620418-29472-1-git-send-email-
    rabin@rab.in
    
    Fixes: cd0980f "tracing: Check invalid syscall nr while tracing
    syscalls"
    Cc: stable@vger.kernel.org # 2.6.33+
    Signed-off-by: Rabin Vincent <rabin@rab.in>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 1cc70510a1fd70773d7894d667a4e58130267cd4
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Mar 18 13:25:57 2015 +0100

    frandom: enable frandom support

commit c67129f4b0cd11715c451f38c0ee7a29b1538c04
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Mar 18 12:08:45 2015 +0100

     kgsl: Do not return invalid power stats when the device is off
    
    adreno_power_stats() was returning unitialized data when the device
    state was not active. Leave early from the function if the device
    is not active and return 0s instead.
    
    Signed-off-by: Jordan Crouse <jcrouse@codeaurora.org>
    Signed-off-by: Prabhat Awasthi <pawasthi@codeaurora.org>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit a522ac59112484117facf3b67c10730a84b27c7d
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Mar 18 12:05:55 2015 +0100

     ARM: nommu: add stub local_flush_bp_all() for !CONFIG_MMU
    
    Since the merging of Will's tlb-ops branch, specifically 89c7e4b
    (ARM: 7661/1: mm: perform explicit branch predictor maintenance when
    required),
    building SMP without CONFIG_MMU has been broken.
    
    The local_flush_bp_all function is only called for operations related to
    changing the kernel's view of memory and ASID rollover - both of which
    are
    irrelevant to an !MMU kernel.
    
    This patch adds a stub local_flush_bp_all() function to the other tlb
    maintenance stubs and restores the ability to build an SMP !MMU kernel.
    
    Signed-off-by: Jonathan Austin <jonathan.austin@arm.com>
    Acked-by: Will Deacon <will.deacon@arm.com>

commit d723bcd7711ede4c8f44c4a66b45152c5ddd976b
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Mar 18 12:05:00 2015 +0100

     ARM: nommu: define dummy TLB operations for nommu configurations
    
    nommu platforms do not perform address translation and therefore clearly
    don't have TLBs. However, some SMP code assumes the presence of the TLB
    flushing routines and will therefore fail to compile for a nommu system.
    
    This patch defines dummy local_* TLB operations and #defines
    tlb_ops_need_broadcast() as 0, therefore causing the usual ARM SMP TLB
    operations to call the local variants instead.
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    CC: Lorenzo Pieralisi <lorenzo.pieralisi@arm.com>
    CC: Nicolas Pitre <nico@linaro.org>

commit 7973265104546cbc02e95fceb90834471723c364
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Mar 18 12:03:15 2015 +0100

     ARM: 7447/1: rwlocks: remove unused branch labels from trylock routines
    
    The ARM arch_{read,write}_trylock implementations include unused
    backwards branch labels, since we don't retry the locking operation
    if the exclusive store fails.
    
    This patch removes the labels.
    
    Acked-by: Nicolas Pitre <nico@linaro.org>
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

commit f78e9ba2aeb8cedac1578dde6c2a965fc89fe1c0
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Mar 18 12:02:04 2015 +0100

     ARM: 7983/1: atomics: implement a better __atomic_add_unless for v6+
    
    Looking at perf profiles of multi-threaded hackbench runs, a significant
    performance hit appears to manifest from the cmpxchg loop used to
    implement the 32-bit atomic_add_unless function. This can be mitigated
    by writing a direct implementation of __atomic_add_unless which doesn't
    require iteration outside of the atomic operation.
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

commit c5e3999e373963e332250af6c13d4ef76f2042ec
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Mar 18 11:58:37 2015 +0100

     ARM: atomic64: fix endian-ness in atomic.h
    
    Fix inline asm for atomic64_xxx functions in arm atomic.h. Instead of
    %H operand specifiers code should use %Q for least significant part
    of the value, and %R for the most significant part of the value. %H
    always returns the higher of the two register numbers, and therefore
    it is not endian neutral. %H should be used with ldrexd and strexd
    instructions.
    
    Signed-off-by: Victor Kamensky <victor.kamensky@linaro.org>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Ben Dooks <ben.dooks@codethink.co.uk>

commit 8249e4bcbd64e66a63030f3fdb6039b971f47ffd
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Mar 18 11:55:06 2015 +0100

     ARM: atomics: prefetch the destination word for write prior to strex
    
    The cost of changing a cacheline from shared to exclusive state can be
    significant, especially when this is triggered by an exclusive store,
    since it may result in having to retry the transaction.
    
    This patch prefixes our atomic access implementations with pldw
    instructions (on CPUs which support them) to try and grab the line in
    exclusive state from the start. Only the barrier-less functions are
    updated, since memory barriers can limit the usefulness of prefetching
    data.
    
    Acked-by: Nicolas Pitre <nico@linaro.org>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

commit a3798397dea825b4fe63c8874e5572d4aae2cc4b
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Mar 18 11:51:20 2015 +0100

     ARM: prefetch: remove redundant "cc" clobber
    
    The pld instruction does not affect the condition flags, so don't bother
    clobbering them.
    
    Acked-by: Nicolas Pitre <nico@linaro.org>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

commit e5691f8cafe81ff470482d9ebae76f420b874c4c
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Mar 18 11:43:01 2015 +0100

     ARM: mpu: add MPU initialisation for secondary cores
    
    The MPU initialisation on the primary core is performed in two stages,
    one
    minimal stage to ensure the CPU can boot and a second one after
    sanity_check_meminfo. As the memory configuration is known by the time
    we
    boot secondary cores only a single step is necessary, provided the
    values
    for DRSR are passed to secondaries.
    
    This patch implements this arrangement. The configuration generated for
    the
    MPU regions is made available to the secondary core, which can then use
    the
    asm MPU intialisation code to program a complete region configuration.
    
    This is necessary for SMP configurations without an MMU, as the MPU
    initialisation is the only way to ensure that memory is specified as
    'shared'.
    
    Signed-off-by: Jonathan Austin <jonathan.austin@arm.com>
    Reviewed-by: Will Deacon <will.deacon@arm.com>
    CC: Nicolas Pitre <nico@linaro.org>
    Signed-off-by: Jake Weinstein <xboxlover360@gmail.com>

commit 6dde2f85a32d8fae3d20861c6c30e2ba06c845c8
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Mar 18 11:39:45 2015 +0100

    /* Set which MPU region should be programmed */
    .macro set_region_nr tmp, rgnr
    	mov	\tmp, \rgnr			@ Use static region numbers
    	mcr	p15, 0, \tmp, c6, c2, 0		@ Write RGNR
    .endm
    
    /* Setup a single MPU region, either D or I side (D-side for unified) */
    .macro setup_region bar, acr, sr, side = MPU_DATA_SIDE
    	mcr	p15, 0, \bar, c6, c1, (0 + \side)	@ I/DRBAR
    	mcr	p15, 0, \acr, c6, c1, (4 + \side)	@ I/DRACR
    	mcr	p15, 0, \sr, c6, c1, (2 + \side)		@ I/DRSR
    .endm
    
    /*
     * Setup the MPU and initial MPU Regions. We create the following regions:
     * Region 0: Use this for probing the MPU details, so leave disabled.
     * Region 1: Background region - covers the whole of RAM as strongly ordered
     * Region 2: Normal, Shared, cacheable for RAM. From PHYS_OFFSET, size from r6
     *
     * r6: Value to be written to DRSR (and IRSR if required) for MPU_RAM_REGION
    */
    
    ENTRY(__setup_mpu)
    
    	/* Probe for v7 PMSA compliance */
    	mrc	p15, 0, r0, c0, c1, 4		@ Read ID_MMFR0
    	and	r0, r0, #(MMFR0_PMSA)		@ PMSA field
    	teq	r0, #(MMFR0_PMSAv7)		@ PMSA v7
    	bne	__error_p			@ Fail: ARM_MPU on NOT v7 PMSA
    
    	/* Determine whether the D/I-side memory map is unified. We set the
    	 * flags here and continue to use them for the rest of this function */
    	mrc	p15, 0, r0, c0, c0, 4		@ MPUIR
    	ands	r5, r0, #MPUIR_DREGION_SZMASK	@ 0 size d region => No MPU
    	beq	__error_p			@ Fail: ARM_MPU and no MPU
    	tst	r0, #MPUIR_nU			@ MPUIR_nU = 0 for unified
    
    	/* Setup second region first to free up r6 */
    	set_region_nr r0, #MPU_RAM_REGION
    	isb
    	/* Full access from PL0, PL1, shared for CONFIG_SMP, cacheable */
    	ldr	r0, =PHYS_OFFSET		@ RAM starts at PHYS_OFFSET
    	ldr	r5,=(MPU_AP_PL1RW_PL0RW | MPU_RGN_NORMAL)
    
    	setup_region r0, r5, r6, MPU_DATA_SIDE	@ PHYS_OFFSET, shared, enabled
    	beq	1f				@ Memory-map not unified
    	setup_region r0, r5, r6, MPU_INSTR_SIDE @ PHYS_OFFSET, shared, enabled
    1:	isb
    
    	/* First/background region */
    	set_region_nr r0, #MPU_BG_REGION
    	isb
    	/* Execute Never,  strongly ordered, inaccessible to PL0, rw PL1  */
    	mov	r0, #0				@ BG region starts at 0x0
    	ldr	r5,=(MPU_ACR_XN | MPU_RGN_STRONGLY_ORDERED | MPU_AP_PL1RW_PL0NA)
    	mov	r6, #MPU_RSR_ALL_MEM		@ 4GB region, enabled
    
    	setup_region r0, r5, r6, MPU_DATA_SIDE	@ 0x0, BG region, enabled
    	beq	2f				@ Memory-map not unified
    	setup_region r0, r5, r6, MPU_INSTR_SIDE @ 0x0, BG region, enabled
    2:	isb
    
    	/* Enable the MPU */
    	mrc	p15, 0, r0, c1, c0, 0		@ Read SCTLR
    	bic     r0, r0, #CR_BR			@ Disable the 'default mem-map'
    	orr	r0, r0, #CR_M			@ Set SCTRL.M (MPU on)
    	mcr	p15, 0, r0, c1, c0, 0		@ Enable MPU
    	isb
    	mov pc,lr
    ENDPROC(__setup_mpu)

commit e4e5f92fb59bbc271a04adf26baedd2a6acbf008
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Mar 18 11:09:47 2015 +0100

     ARM: mpu: add header for MPU register layouts and region data
    
    This commit adds definitions relevant to the ARM v7 PMSA compliant MPU.
    
    The register layouts and region configuration data is made accessible to
    asm
    as well as C-code so that it can be used in early bring-up of the MPU.
    
    The mpu region information structs assume that the properties for the
    I/D side
    are the same, though the implementation could be trivially extended for
    future
    platforms where this is no-longer true.
    
    The MPU_*_REGION defines are used for the basic, static MPU region
    setup.
    
    Signed-off-by: Jonathan Austin <jonathan.austin@arm.com>
    Reviewed-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Jake Weinstein <xboxlover360@gmail.com>

commit caddda4cac3f0f97b4deecadc37de5c5e6c531fc
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Mar 18 11:08:33 2015 +0100

     ARM: mpu: add PMSA related registers and bitfields to existing headers
    
    This patch adds the following definitions relevant to the PMSA:
    
    Add SCTLR bit 17, (CR_BR - Background Region bit) to the list of CR_*
    bitfields. This bit determines whether to use the architecturally
    defined
    memory map
    
    Add the MPUIR to the available registers when using read_cpuid macro.
    The
    MPUIR is the MPU type register.
    
    Signed-off-by: Jonathan Austin <jonathan.austin@arm.com>
    Reviewed-by: Will Deacon <will.deacon@arm.com>
    CC:"Uwe Kleine-Knig" <u.kleine-koenig@pengutronix.de>
    Signed-off-by: Jake Weinstein <xboxlover360@gmail.com>

commit e471ba7883f3d11248e6ce05eb3aa9b2920b8af6
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Mar 18 11:06:47 2015 +0100

     asm-generic: io: don't perform swab during {in,out} string functions
    
    The {in,out}s{b,w,l} functions are designed to operate on a stream of
    bytes and therefore should not perform any byte-swapping, regardless of
    the CPU byte order.
    
    This patch fixes the generic IO header so that {in,out}s{b,w,l} call the
    __raw_{read,write} functions directly rather than going via the
    endian-correcting accessors.
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Cc: Mike Frysinger <vapier@gentoo.org>
    Acked-by: Arnd Bergmann <arnd@arndb.de>
    Acked-by: Ben Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit 11f9f308facd9774f62cc4179bbafca20a1b2d76
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Mar 18 11:03:48 2015 +0100

    mach-msm: idle: some changes

commit f647646a7dcde4b4bce9a1b994fef64b59fc5bdb
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Mar 18 10:53:40 2015 +0100

    add ported mako msm_thermal control (thx to showp1984 & Alucard24)

commit f203b756e00778e90763247988e63693d5900a5c
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Mar 18 10:46:03 2015 +0100

     sched/deadline: Fix a precision problem in the microseconds range
    
    An overrun could happen in function start_hrtick_dl()
    when a task with SCHED_DEADLINE runs in the microseconds
    range.
    
    For example, if a task with SCHED_DEADLINE has the following parameters:
    
    Task  runtime  deadline  period
       P1   200us     500us    500us
    
    The deadline and period from task P1 are less than 1ms.
    
    In order to achieve microsecond precision, we need to enable HRTICK
    feature
    by the next command:
    
    PC#echo "HRTICK" > /sys/kernel/debug/sched_features
    PC#trace-cmd record -e sched_switch &
    PC#./schedtool -E -t 200000:500000:500000 -e ./test
    
    The binary test is in an endless while(1) loop here.
    Some pieces of trace.dat are as follows:
    
    <idle>-0   157.603157: sched_switch: :R ==> 2481:4294967295: test
    test-2481  157.603203: sched_switch:  2481:R ==> 0:120: swapper/2
    <idle>-0   157.605657: sched_switch:  :R ==> 2481:4294967295: test
    test-2481  157.608183: sched_switch:  2481:R ==> 2483:120: trace-cmd
    trace-cmd-2483 157.609656: sched_switch:2483:R==>2481:4294967295: test
    
    We can get the runtime of P1 from the information above:
    
    runtime = 157.608183 - 157.605657
    runtime = 0.002526(2.526ms)
    
    The correct runtime should be less than or equal to 200us at some point.
    
    The problem is caused by a conditional judgment "delta > 10000"
    in function start_hrtick_dl().
    
    Because no hrtimer start up to control the rest of runtime
    when the reset of runtime is less than 10us.
    
    So the process will continue to run until tick-period is coming.
    
    Move the code with the limit of the least time slice
    from hrtick_start_fair() to hrtick_start() because the
    EDF schedule class also needs this function in start_hrtick_dl().
    
    To fix this problem, we call hrtimer_start() unconditionally in
    start_hrtick_dl(), and make sure the scheduling slice won't be smaller
    than 10us in hrtimer_start().
    
    Signed-off-by: Xiaofeng Yan <xiaofeng.yan@huawei.com>
    Reviewed-by: Li Zefan <lizefan@huawei.com>
    Acked-by: Juri Lelli <juri.lelli@arm.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Link: http://lkml.kernel.org/r/1409022941-5880-1-git-send-email-
    xiaofeng.yan@huawei.com
    [ Massaged the changelog and the code. ]
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 3252c25597900e4951e192939340d899baba57c0
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Mar 18 10:40:41 2015 +0100

    cpufreq: some cpu changes

commit e5b0a8afe13c8e3b7056a1a439bf840ac081db6d
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Mar 18 10:22:14 2015 +0100

     Add and enable kexec hardboot support
    
    Consists of squashed commits from:
        https://github.com/mkasick/android_kernel_samsung_jfltespr.git
    
        commit 750bb80f2854d6af5273e55ea179a4c60b2d9efc
        Author: Mike Kasick <mike@kasick.org>
        Date:   Sun Jun 2 23:11:24 2013 -0400
    
            Clear download mode flag on kexec hardboot.
    
        commit 138ba851ee949af291eae914f410aea2d85ed9f5
        Author: Mike Kasick <mike@kasick.org>
        Date:   Sun May 12 22:39:26 2013 -0400
    
            Support hard booting to a kexec kernel.
    
            See KEXEC_HARDBOOT config option help for details.
    
        commit 3ab41019a7af08395c233bebb605c2f1ea49c8e0
        Author: Mike Kasick <mike@kasick.org>
        Date:   Sat Jul 7 23:10:24 2012 -0400
    
            Support copying of kernel tagged list (atags) in the
    decompressor.
    
            This is needed to hardboot kexec a kernel with a new tags list
        (including a
            new kernel command line), since the new atags would otherwise be
        lost with
            the limited kernel memory mapping.  Without this patch, a
    hardboot-
        kexec'd
            kernel uses the atags provided by the bootloader.
    
        commit 446c3d7a6631877f7dc9142359cf0eae300e9a08
        Author: Mike Kasick <mike@kasick.org>
        Date:   Sat Jul 7 23:09:39 2012 -0400
    
            Enable caching and buffering for all of physical RAM in the
        decompressor.
    
            Old method is to enable caching and buffering only for the 256
    MB at
        the
            start of the decompressor image.  This makes a hardboot-kexec'd
        kernel very
            slow to decompress since the decompressor is located far above
    the
        kernel
            destination.  This patch reduces boot time from 35 to 8 seconds
    on
        epicmtd.
    
        commit d3646be88f5b1f9a8ae714c922ec50c681b7157f
        Author: Mike Kasick <mike@kasick.org>
        Date:   Sat Jul 7 19:00:47 2012 -0400
    
            Enable and fix kexec syscall support.
    
            Use mem_text_write_kernel_word when assigning reboot_code_buffer
        parameters
            to avoid protection faults (writes to read-only kernel memory)
    when
            CONFIG_STRICT_MEMORY_RWX is enabled.

commit cd442009416bf750fca2a41d470b6917b9526deb
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Tue Mar 17 23:50:06 2015 +0100

     ARM: 8168/1: extend __init_end to a page align address
    
    This patch changes the __init_end address to a
    page align address, so that free_initmem() can
    free the whole .init section, because if the end
    address is not page aligned, it will round down to
    a page align address, then the tail unligned page
    will not be freed.
    
    Signed-off-by: wang <yalin.wang2010@gmail.com>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit b6a83c053d2ab76e9c030f419e8712416a0e4ab2
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Tue Mar 17 23:49:28 2015 +0100

     ARM: 7605/1: vmlinux.lds: Move .notes section next to the rodata
    
    The .notes, being read-only data by nature, were placed between
    read-write .data and .bss. This was harmful in case of the XIP
    kernel, as being placed in the RAM range, most likely far
    from the ROM address, was inflating the XIP images.
    
    Moving the .notes at the end of the read-only section
    (consisting of .text, .rodata and unwind info) fixes the problem.
    
    Reported-by: Marc Kleine-Budde <mkl@pengutronix.de>
    Signed-off-by: Pawel Moll <pawel.moll@arm.com>
    Tested-by: Marc Kleine-Budde <mkl@pengutronix.de>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit d6fab219ee6366513414eaad9ea209364a508f23
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Tue Mar 17 23:44:45 2015 +0100

     ARM: Honor udelay convention for timer-based delay
    
    Replaced strict "below" comparison in timer delay function with "below
    or equal". The former may result in actual delay time below requested
    (if the delay happens to start just before cycle counter advance). The
    latter guarantees actual delay at/above requested.
    
    Signed-off-by: Alex Frid <afrid@nvidia.com>
    Reviewed-by: Seshendra Gadagottu <sgadagottu@nvidia.com>
    Reviewed-by: Bo Yan <byan@nvidia.com>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 465a1eeb40da6dd4a5aa1910f7184f2f8f134b3e
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Tue Mar 17 23:43:12 2015 +0100

     ARM: kgdb: ignore breakpoint instructions from user mode
    
    Avoid conflicts with user mode usage of the same instructions, as with
    Clang -ftrapv.
    
    Signed-off-by: Todd Poynor <toddpoynor@google.com>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 5f3528b9f7d6bd68d24beffcb893a7f2ae578050
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Tue Mar 17 23:40:47 2015 +0100

     ARM: Export LoUIS flush
    
    Export flushing to LoUIS.
    
    Signed-off-by: Antti P Miettinen <amiettinen@nvidia.com>
    Reviewed-by: Juha Tukkinen <jtukkinen@nvidia.com>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit e5abfaba1e20ebf6a5a92b0fc83d52371cee036d
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Tue Mar 17 23:40:08 2015 +0100

     ARM: mm: adding a check for slab objects in flush_dcache_page
    
    A page struct obtained via virt_to_page from a slab object may be passed
    to
    the flush_dcache_page function. However, slab allocation is a kernel
    feature
    (can only be done in kernel space) and thus slab objects are never
    mapped into
    user space. However, slab allocators may use the mapping field for their
    own
    purposes and as a result mapping may not be NULL although the page is
    not
    mapped. For this purpose there is a BUG_ON inside page_mapping() to
    check for
    slab objects. So, using NULL for this special case and avoiding calling
    page_mapping() on slab objects to avoid the BUG_ON.
    
    Bug 845618.
    Bug 1154527.
    
    Signed-off-by: Vishal Singh <vissingh@nvidia.com>
    Reviewed-by: Bharat Nihalani <bnihalani@nvidia.com>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 47d4fa883a8d26d255617feb3238b19553ff2469
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Tue Mar 17 23:37:56 2015 +0100

     ARM: mm: Enable NCSE feature for A15 only
    
    Signed-off-by: Bo Yan <byan@nvidia.com>
    Reviewed-by: Krishna Reddy <vdumpa@nvidia.com>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 81e94ec328cd828366bc8418ccb82c3671f7daaa
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Tue Mar 17 23:36:16 2015 +0100

     ARM: mm: enable non-cacheable streaming enhancement
    
    This is cortex-a15 specific
    
    bug 1178938
    
    Signed-off-by: Bo Yan <byan@nvidia.com>
    Reviewed-by: Mrutyunjay Sawant <msawant@nvidia.com>
    Tested-by: Mrutyunjay Sawant <msawant@nvidia.com>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 53d0809f3374ea01515609c2ce22210337b32fc2
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Tue Mar 17 23:33:09 2015 +0100

     ARM: 7650/1: mm: replace direct access to mm->context.id with new macro
    
    The mmid macro is meant to be used to get the mm->context.id data
    from the mm structure, but it seems to have been missed in a cuple
    of files.
    
    Acked-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Ben Dooks <ben.dooks@codethink.co.uk>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>o

commit 1bc7213b9df843e82e00d49033f8704ec4d11bf9
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Tue Mar 17 23:27:44 2015 +0100

     ARM: cache-l2x0: get size of outer cache
    
    Implement interface for getting size of outer cache.
    
    bug 983964
    
    Signed-off-by: Kirill Artamonov <kartamonov@nvidia.com>
    Reviewed-by: Peter De Schrijver <pdeschrijver@nvidia.com>
    Reviewed-by: Sachin Nikam <snikam@nvidia.com>
    Reviewed-by: Juha Tukkinen <jtukkinen@nvidia.com>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit ebda7f29ce304f573172e70e64bf2ceabf5dd7bf
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Tue Mar 17 23:26:15 2015 +0100

     ARM: dma: Drop GFP_COMP for DMA-IOMMU memory allocations
    
    dma_alloc_coherent wants to split pages after allocation in order to
    reduce the memory footprint. This does not work well with GFP_COMP
    pages, so drop this flag before allocation.
    
    This patch is ported from arch/avr32
    
    Signed-off-by: Hiroshi Doyu <hdoyu@nvidia.com>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 4e9f8561402ca2d93f90a70ab87adf247ff1315c
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Tue Mar 17 23:25:11 2015 +0100

     ARM: mm: cache-l2x0: Implement outer_clean_all()
    
    There is already implemented full outer clean routine in
    arch/arm/mm/cache-l2x0.c.
    
    Make it possible to use it through outer_cache interface,
    like other outer maintenance functions.
    
    bug 983964
    
    Reviewed-by: Kirill Artamonov <kartamonov@nvidia.com>
    Tested-by: Kirill Artamonov <kartamonov@nvidia.com>
    Reviewed-by: Krishna Reddy <vdumpa@nvidia.com>
    Reviewed-by: Justin Paver <jpaver@nvidia.com>
    Reviewed-by: Bharat Nihalani <bnihalani@nvidia.com>
    
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 65ebaefa1895829e3e3322a5521da1afcb6c1a86
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Tue Mar 17 23:19:40 2015 +0100

     ARM: LPAE: Invalidate the TLB for module addresses during translation
    
     fault
    
    During the free_pgtables() call all user and modules/pkmap entries are
    removed. If a translation fault for the modules/pkmap area occurs before
    we switched away from the current pgd, do_translation_fault() would copy
    the init_mm pud into the user pud.
    
    There is a small window between pud_clear() and pmd_free_tlb() in
    free_pmd_range() where the pud entry was cleared but the TLB has not
    been invalidated yet and the CPU may have cached the original (valid)
    pud entry in the TLB. A scenario like below would get stuck in
    continuous prefetch abort:
    
    1. Current process exiting. The modules pmd entries not populated
    2. exit_mmap() -> ... -> pmd_free_tlb()
    3. pud_clear() for the 1GB pud containing user stack and modules (no TLB
       invalidation yet)
    4. Interrupt -> module interrupt routine
    5. Level 2 (pmd) translation fault occurs when executing the module
       interrupt routine. The CPU previously cached (TLB) the old valid pud
       value for the modules area, so we don't get a level 1 translation
       fault
    6. do_translation fault() copies the pud_k into the pud
    7. Linux returns to the faulty instruction. Goes back to 5
    
    At point 7, since the CPU still has the old pud value, it goes back to
    point 5 and never gets out of this loop. With this patch, the stale pud
    TLB entry is invalidated after point 6 and the subsequent prefetch abort
    doesn't occur.
    
    Reported-by: Tony Thompson <Anthony.Thompson@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit ef6689b10156adbd742be5d6ab0b83d00949dbb2
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Tue Mar 17 23:15:45 2015 +0100

     ARM: remove unnecessary dcache_clean_area
    
    Cortex-A9 has PIPT D-cache which do not require clean the cache
    on creating page table.
    
    Reviewed-by: Heechul Yun <hyun@nvidia.com>
    Tested-by: Heechul Yun <hyun@nvidia.com>
    Reviewed-by: Krishna Reddy <vdumpa@nvidia.com>
    Reviewed-by: Scott Williams <scwilliams@nvidia.com>
    
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit f8aa44d863bfa73540335625803c94e538096258
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Tue Mar 17 23:13:55 2015 +0100

     ARM: mm: Skip I-cache invalidate for Cortex-A15 boot
    
    This is not required since cache is invalidated by HW in the reset
    sequence. Bootloader is supposed to do the same before it hands
    over control to kernel.
    
    Signed-off-by: Bo Yan <byan@nvidia.com>
    Reviewed-by: Seshendra Gadagottu <sgadagottu@nvidia.com>
    Reviewed-by: Yu-Huan Hsu <yhsu@nvidia.com>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 3b57407493ee42991815028584f19a62e861ac31
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Tue Mar 17 23:12:35 2015 +0100

     ARM: mm: Remove unnecessary CMO in Cortex A15 startup
    
    Cortex-A15 flush L2 cache after reset, there is no need to do this
    in software, if L2 is already invalidated in bootloader and
    cache is disabled. For secondary startup, there is no reason to
    flush L2 as well.
    
    This change assumes the setup code is always entered as the result
    of CPU reset.
    
    Signed-off-by: Bo Yan <byan@nvidia.com>
    Reviewed-by: Mrutyunjay Sawant <msawant@nvidia.com>
    Tested-by: Mrutyunjay Sawant <msawant@nvidia.com>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 58fa5ac0611a99cad1ccacb12c24b2c54c319067
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Tue Mar 17 23:09:18 2015 +0100

     ARM: cleanup undefined instruction entry code
    
    We don't need to keep reloading the thread into into r10 - we can do
    this once and keep the value cached in the register.  Also, schedule
    some instructions better so that the pipeline doesn't stall after a
    load in the neon code.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 9e4e423510623a2bfc3909cd8a15e8d08f345f14
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Tue Mar 17 23:07:05 2015 +0100

     arch: arm: TLB invalidate if power collapse gets aborted
    
    Invalidate entire local TLB in non sharable mode when
    power collapse is aborted due to pending interrupt.
    
    Signed-off-by: Anji Jonnala <anjir@codeaurora.org>
    Signed-off-by: Venkat Devarasetty <vdevaras@codeaurora.org>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 897b26988630c48afc02ad2a2bf8355fb8890287
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Tue Mar 17 23:05:15 2015 +0100

     ARM: 7553/1: proc-v7: Ensure correct instruction set after cpu_reset
    
    Because mov pc,<Rn> never switches instruction set when executed in
    Thumb code, Thumb-2 kernels will silently execute the target code
    after cpu_reset as Thumb code, even if the passed code pointer
    denotes ARM (bit 0 clear).
    
    This patch uses bx instead, ensuring the correct instruction set
    for the target code.
    
    Thumb code in the kernel is not supported prior to ARMv7, so other
    CPUs are not affected.
    
    Signed-off-by: Dave Martin <dave.martin@linaro.org>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Acked-by: Nicolas Pitre <nico@linaro.org>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 8abf6f326212f0dbc797afc644c6210e4bc3fc44
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Tue Mar 17 23:04:24 2015 +0100

     ARM: 8036/1: Enable IRQs before attempting to read user space in __un
    
    d_usr
    
    The Undef abort handler in the kernel reads the undefined instruction
    from user space. If the page table was modified from another CPU, the
    user access could fail and do_page_fault() will be executed with
    interrupts disabled. This can potentially deadlock on ARM11MPCore or on
    Cortex-A15 with erratum 798181 workaround enabled (both implying IPI for
    TLB maintenance with page table lock held).
    
    This patch enables the IRQs in __und_usr before attempting to read the
    instruction from user space.
    
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Tested-by: Arun KS <getarunks@gmail.com>
    Cc: Hartley Sweeten <hsweeten@visionengravers.com>
    Cc: Ryan Mallon <rmallon@gmail.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 38fbd42d8aff20bc51ad9d7a363dc1001e1d2a68
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Tue Mar 17 22:58:10 2015 +0100

    configs: enable config_fb_msm_triple_buffer

commit 945c5b5c0c4fca8bf82b321cec4edbd7d1440372
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Tue Mar 17 22:52:58 2015 +0100

     arch: arm: lib: optimize memcpy for cortex-A15
    
    The LDRD/STRD instruction is faster than LDM/STM on Cortex-A15.
    Also optimized preload cache size for Cortex-A15.
    
    Added USE_LDRDSTRD_OVER_LDMSTM to turn on LDRD/STRD optimization.
    Added ARM_PLD_SIZE, default 32. Should set to 64 for Cortex-A15.
    
    Bug 1185248
    
    Signed-off-by: Chandler Zhang <chazhang@nvidia.com>
    Reviewed-by: Krishna Reddy <vdumpa@nvidia.com>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit eda6473b07e746b2bc26e0a09b8c8682edf8be6b
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Tue Mar 17 22:34:26 2015 +0100

     arm: Move to upstream udelay via timer implementation
    
    This is a squash of a handful of changes and reverts of the
    Qualcomm specific implementation:
    
    Revert "arm: Implement a timer based __delay() loop"
    
    This reverts commit 976eafa8b18252876e15f861944acf693b07ce7e.
    
    Revert "arm: Allow machines to override __delay()"
    
    This reverts commit bc0ef8ab167272890f1aab62928b04a9aeb87ce9.
    
    Revert "arm: Translate delay.S into (mostly) C"
    
    This reverts commit 8d5868d8205d10a0a8e423f53e9cc9bb3e9d1a34.
    
    ARM: 7451/1: arch timer: implement read_current_timer and get_cycles
    
    This patch implements read_current_timer using the architected timers
    when they are selected via CONFIG_ARM_ARCH_TIMER. If they are detected
    not to be usable at runtime, we return -ENXIO to the caller.
    
    Furthermore, if read_current_timer is exported then we can implement
    get_cycles in terms of it for use as both an entropy source and for
    implementing __udelay and friends.
    
    Tested-by: Shinya Kuribayashi <shinya.kuribayashi.px@renesas.com>
    Reviewed-by: Stephen Boyd <sboyd@codeaurora.org>
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>
    
    ARM: 7452/1: delay: allow timer-based delay implementation to be
    selected
    
    This patch allows a timer-based delay implementation to be selected by
    switching the delay routines over to use get_cycles, which is
    implemented in terms of read_current_timer. This further allows us to
    skip the loop calibration and have a consistent delay function in the
    face of core frequency scaling.
    
    To avoid the pain of dealing with memory-mapped counters, this
    implementation uses the co-processor interface to the architected
    timers
    when they are available. The previous loop-based implementation is
    kept around for CPUs without the architected timers and we retain both
    the maximum delay (2ms) and the corresponding conversion factors for
    determining the number of loops required for a given interval. Since
    the
    indirection of the timer routines will only work when called from C,
    the sa1100 sleep routines are modified to branch to the loop-based
    delay
    functions directly.
    
    Tested-by: Shinya Kuribayashi <shinya.kuribayashi.px@renesas.com>
    Reviewed-by: Stephen Boyd <sboyd@codeaurora.org>
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>
    
    ARM: delay: set loops_per_jiffy when moving to timer-based loop
    
    The delay functions may be called by some platforms between switching
    to
    the timer-based delay loop but before calibration. In this case, the
    initial loops_per_jiffy may not be suitable for the timer (although a
    compromise may be achievable) and delay times may be considered too
    inaccurate.
    
    This patch updates loops_per_jiffy when switching to the timer-based
    delay loop so that delays are consistent prior to calibration.
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    
    ARM: delay: add registration mechanism for delay timer sources
    
    The current timer-based delay loop relies on the architected timer to
    initiate the switch away from the polling-based implementation. This
    is
    unfortunate for platforms without the architected timers but with a
    suitable delay source (that is, constant frequency, always powered-up
    and ticking as long as the CPUs are online).
    
    This patch introduces a registration mechanism for the delay timer
    (which provides an unconditional read_current_timer implementation)
    and
    updates the architected timer code to use the new interface.
    
    Signed-off-by: Jonathan Austin <jonathan.austin@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    
    ARM: export default read_current_timer
    
    read_current_timer is used by get_cycles since "ARM: 7538/1: delay:
    add registration mechanism for delay timer sources", and get_cycles
    can be used by device drivers in loadable modules, so it has to
    be exported.
    
    Without this patch, building imote2_defconfig fails with
    
    ERROR: "read_current_timer" [crypto/tcrypt.ko] undefined!
    
    Signed-off-by: Arnd Bergmann <arnd@arndb.de>
    Cc: Stephen Boyd <sboyd@codeaurora.org>
    Cc: Jonathan Austin <jonathan.austin@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Russell King <rmk+kernel@arm.linux.org.uk>
    
    Change-Id: If1ad095d6852f5966ea995856103e06de6ab2f59
    Signed-off-by: Stephen Boyd <sboyd@codeaurora.org>

commit 37f28a1d17f508cbdf700d7eeb63dd285a4d2291
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Tue Mar 17 22:10:53 2015 +0100

    msm8974: apply two patches at one time:
     msm8974: Adapt memutils optimizations from msm8960pro
    
    Preload farther to take advantage of the memory bus, and assume
    64-byte cache lines.  Unroll some pairs of ldm/stm as well, for
    unexplainable reasons.
    
    Future enhancements should include,
    
    - #define for how far to preload, possibly defined separately for
    memcpy, copy_*_user
    - Tuning for misaligned buffers
    - Tuning for memmove
    - Tuning for small buffers
    - Understanding mechanism behind ldm/stm unroll causing some gains
    in copy_to_user
    
    BASELINE (msm8960pro):
    ======================================================================
    memcpy 1000MB at 5MB       : took 808850 usec, bandwidth 1236.236 MB/s
    copy_to_user 1000MB at 5MB : took 810071 usec, bandwidth 1234.234 MB/s
    copy_from_user 1000MB at 5M: took 942926 usec, bandwidth 1060.060 MB/s
    memmove 1000GB at 5MB      : took 848588 usec, bandwidth 1178.178 MB/s
    copy_to_user 1000GB at 4kB : took 847916 usec, bandwidth 1179.179 MB/s
    copy_from_user 1000GB at 4k: took 935113 usec, bandwidth 1069.069 MB/s
    copy_page 1000GB at 4kB    : took 779459 usec, bandwidth 1282.282 MB/s
    
    THIS PATCH:
    ======================================================================
    memcpy 1000MB at 5MB       : took 346223 usec, bandwidth 2888.888 MB/s
    copy_to_user 1000MB at 5MB : took 348084 usec, bandwidth 2872.872 MB/s
    copy_from_user 1000MB at 5M: took 348176 usec, bandwidth 2872.872 MB/s
    memmove 1000GB at 5MB      : took 348267 usec, bandwidth 2871.871 MB/s
    copy_to_user 1000GB at 4kB : took 377018 usec, bandwidth 2652.652 MB/s
    copy_from_user 1000GB at 4k: took 371829 usec, bandwidth 2689.689 MB/s
    copy_page 1000GB at 4kB    : took 383763 usec, bandwidth 2605.605 MB/s
    
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>
    Signed-off-by: franciscofranco <franciscofranco.1990@gmail.com>
    
    AND:
    
    Apply arch: arm: lib: optimize memcpy for cortex-A15 to memutils

commit a4b7ae91da10f1a52b066fad50fb0ba8ef8af8d6
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Tue Mar 17 21:52:00 2015 +0100

     cpufreq: Notify governors when cpus are hot-[un]plugged
    
    Because cpufreq core and governors worry only about the online cpus, if
    a cpu is
    hot [un]plugged, we must notify governors about it, otherwise be ready
    to expect
    something unexpected.
    
    We already have notifiers in the form of
    CPUFREQ_GOV_START/CPUFREQ_GOV_STOP, we
    just need to call them now.
    
    Signed-off-by: Viresh Kumar <viresh.kumar@linaro.org>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit ca454d80afa4718a69b1fc80287637230a562916
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Tue Mar 17 21:46:48 2015 +0100

     ARM: 7759/1: decouple CPU offlining from reboot/shutdown
    
    Add comments to machine_shutdown()/halt()/power_off()/restart() that
    describe their purpose and/or requirements re: CPUs being active/not.
    
    In machine_shutdown(), replace the call to smp_send_stop() with a call
    to
    disable_nonboot_cpus(). This completely disables all but one CPU, thus
    satisfying the requirement that only a single CPU be active for kexec.
    Adjust Kconfig dependencies for this change.
    
    In machine_halt()/power_off()/restart(), call smp_send_stop() directly,
    rather than via machine_shutdown(); these functions don't need to
    completely de-activate all CPUs using hotplug, but rather just quiesce
    them.
    
    Remove smp_kill_cpus(), and its call from smp_send_stop().
    smp_kill_cpus() was indirectly calling smp_ops.cpu_kill() without
    calling
    smp_ops.cpu_die() on the target CPUs first. At least some
    implementations
    of smp_ops had issues with this; it caused cpu_kill() to hang on Tegra,
    for example. Since smp_send_stop() is only used for shutdown, halt, and
    power-off, there is no need to attempt any kind of CPU hotplug here.
    
    Adjust Kconfig to reflect that machine_shutdown() (and hence kexec)
    relies upon disable_nonboot_cpus(). However, this alone doesn't
    guarantee
    that hotplug will work, or even that hotplug is implemented for a
    particular piece of HW that a multi-platform zImage runs on. Hence, add
    error-checking to machine_kexec() to determine whether it did work.
    
    Suggested-by: Russell King <rmk+kernel@arm.linux.org.uk>
    Signed-off-by: Stephen Warren <swarren@nvidia.com>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Tested-by:  Zhangfei Gao <zhangfei.gao@gmail.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit bae0ca065af21b05227693a6061f6f651272b002
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Tue Mar 17 21:38:23 2015 +0100

     kthread_work: wake up worker only when the worker is idle
    
    If the worker is already executing a work item when another is queued,
    we can safely skip wakeup without worrying about stalling queue thus
    avoiding waking up the busy worker spuriously.  Spurious wakeups
    should be fine but still isn't nice and avoiding it is trivial here.
    
    tj: Updated description.
    
    Signed-off-by: Lai Jiangshan <laijs@cn.fujitsu.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit c7f6b29dcfe89803f33c8e0ecf9da7767e099257
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Tue Mar 17 21:35:18 2015 +0100

    cpufreq: correctly set/limit min & max frequencies

commit 496a249f0dcd05010907dd27c2eb10fb5df3c13d
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Tue Mar 17 18:06:32 2015 +0100

     panel: samsung-octa: flash hot pink for underruns
    
    * while annoying, it'll help debugging

commit e863a902edfd75e3d23e542e29c4ec9b8916fe77
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Tue Mar 17 18:04:47 2015 +0100

    jf: mdp: fix mdp bandwidth

commit e3302490d57cef652e4f7c3fe8bcc46fd7dca821
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Tue Mar 17 18:01:31 2015 +0100

     msm_fb: display: get mdp bandwidth parameters from board file
    
    Underrun issues can be solved by changing mdp bandwidth parameters:
    max_bw, ab_factor and ib_factor. However these parameters are not
    universally same for all targets, and one set of values that works for
    one target could cause underruns or higher power on other
    targets. This change is driver change which makes those parameters
    target dependent.
    
    CRs-Fixed: 464492
    Signed-off-by: Huaibin Yang <huaibiny@codeaurora.org>

commit fa8aa874102e2253652ea112103b6baf49fcd9f1
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Tue Mar 17 17:57:58 2015 +0100

     cpufreq: Always allow update of user policy
    
    In the cases where the system boots up in a constraint with policy min
    and
    max lower than cpuinfo min/max, and user tries to set a higher user
    policy min, the value would be overridden during the verifying the
    limits.
    
    Once user initiates the sysfs write the previous user policy is
    maintained in policy min and max thus changing the limits for
    verification
    of the current policy. Once the verification is completed restore the
    current user policy min/max with the updated values if any. This would
    take
    care of cases uwhere user policy min/max input is higher/lesser than the
    current min and max.
    
    Signed-off-by: Taniya Das <tdas@codeaurora.org>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit bf6c5a7d0db97fdd5884411f78bf168662c7bf7b
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Tue Mar 17 17:55:45 2015 +0100

     msm: Skip building perf debug
    
    * Reduces non-important cruft.
    
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit bcb14ca9de3b8924beabb4aa7330cdff5c94c9dd
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Tue Mar 17 17:54:09 2015 +0100

     mmc: core: Fix NULL pointer dereference issue with mmc_blk_reset()
    
    If the mmc_hw_reset() fails, then host->card might be NULL in some
    cases. Hence, check for reset errors and report it to the caller so
    that the current request can be aborted and also check for host->card
    before accessing it so as to prevent NULL pointer dereference issue.
    
    Signed-off-by: Sahitya Tummala <stummala@codeaurora.org>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit f180a13a2f070fcd9a628d7bc0bb938f9c1ce438
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Tue Mar 17 17:52:55 2015 +0100

     mmc: block: fix race in deferred resume
    
    The actual resume takes place when a request is received. In a
    situation when mmcqd and qseecomd(rpmb) both try to resume the
    device, a race is ensued. This is because mmcqd first resumes
    the device and then claims the host. Ideally, if resume is in
    progress in one context the resume in the other context should
    be cancelled.
    Claim the host before resuming the device in mmcqd context.
    
    Signed-off-by: Asutosh Das <asutoshd@codeaurora.org>
    [neo: Fix code format.]
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 75f5e8d87ce7d6a5955309ebe6a7a1f7d82769e8
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Tue Mar 17 17:51:23 2015 +0100

     mmc: core: remove duplicate code in mmc_rescan
    
    If the card is removed, the wakelock has to be held for
    an extended period of time to enable the user-space to
    respond. The code to extend the wakelock has been
    duplicated. This patch removes this duplicate code.
    
    Signed-off-by: Asutosh Das <asutoshd@codeaurora.org>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit cea9b1fa38d68e2e4c48331b5f36a5ac41a19d21
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Tue Mar 17 17:48:11 2015 +0100

    drivers: add gamma control thanks to CM

commit 8253f588499d257a5a3a735c617ccfdb3daa7193
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Mon Mar 16 22:59:07 2015 +0100

     cpufreq / stats: fix race between stats allocation and first usage
    
    This patch forces complete struct cpufreq_stats allocation for all cpus
    before
    registering CPUFREQ_TRANSITION_NOTIFIER notifier, otherwise in some
    conditions
    cpufreq_stat_notifier_trans() can be called in the middle of stats
    allocation,
    in this case cpufreq_stats_table already exists, but stat->freq_table is
    NULL.
    
    Signed-off-by: Konstantin Khlebnikov <khlebnikov@openvz.org>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 0c4d445db1f7925db970cd2caedf80a1dc0d831b
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Mon Mar 16 22:22:09 2015 +0100

     ARM: 7629/1: mm: Fix missing XN flag for MT_MEMORY_SO
    
    Commit 8fb5428 {ARM: mm: Add strongly ordered descriptor support}
    added XN flag at section level but missed it at PTE level.
    
    Fix it by adding the L_PTE_XN to MT_MEMORY_SO PTE descriptor.
    
    Reported-by: Richard Woodruff <r-woodruff2@ti.com>
    Signed-off-by: Santosh Shilimkar <santosh.shilimkar@ti.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>
    
    ARM: 7928/1: kconfig: select HAVE_EFFICIENT_UNALIGNED_ACCESS for CPUv6+ && MMU
    
    Modern ARM CPUs can perform efficient unaligned memory accesses in
    hardware and this feature is relied up on by code such as the dcache
    word-at-a-time name hashing.
    
    This patch selects HAVE_EFFICIENT_UNALIGNED_ACCESS for these cores and
    reworks the kconfig select logic for DCACHE_WORD_ACCESS to use the new
    symbol.
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>
    
    Revert "ARM: fix user unalignment prefetch abort"
    
    This reverts commit 47e6bdeb9c62b707528ebce7bb1f0238289aaf67.
    
    ARM: cacheflush: split user cache-flushing into interruptible chunks
    
    Flushing a large, non-faulting VMA from userspace can potentially result
    in a long time spent flushing the cache line-by-line without preemption
    occurring (in the case of CONFIG_PREEMPT=n).
    
    Whilst this doesn't affect the stability of the system, it can certainly
    affect the responsiveness and CPU availability for other tasks.
    
    This patch splits up the user cacheflush code so that it flushes in
    chunks of a page. After each chunk has been flushed, we may reschedule
    if appropriate and, before processing the next chunk, we allow any
    pending signals to be handled before resuming from where we left off.
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    
    ARM: cacheflush: don't bother rounding to nearest vma
    
    do_cache_op finds the lowest VMA contained in the specified address
    range and rounds the range to cover only the mapped addresses.
    
    Since commit 4542b6a ("ARM: 7365/1: drop unused parameter from
    flush_cache_user_range") the VMA is not used for anything else in this
    code and seeing as the low-level cache flushing routines return -EFAULT
    if the address is not valid, there is no need for this range truncation.
    
    This patch removes the VMA handling code from the cacheflushing syscall.
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    
    ARM: 7917/1: cacheflush: correctly limit range of memory region being flushed
    
    The __do_cache_op function operates with a 'chunk' size of one page
    but fails to limit the size of the final chunk so as to not exceed
    the specified memory region. Fix this.
    
    Change-Id: Iebc8558947df1fcf8dc5b84123a7df877b67e161
    Cc: <stable@vger.kernel.org>
    Reported-by: Christian Gmeiner <christian.gmeiner@gmail.com>
    Tested-by: Christian Gmeiner <christian.gmeiner@gmail.com>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Jon Medhurst <tixy@linaro.org>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 430f9f6f5d42eb39b622821c30c88d5eae4d7755
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Mon Mar 16 22:13:44 2015 +0100

     ARM: 7691/1: mm: kill unused TLB_CAN_READ_FROM_L1_CACHE and use ALT_S
    
    MP instead
    
    Many ARMv7 cores have hardware page table walkers that can read the L1
    cache. This is discoverable from the ID_MMFR3 register, although this
    can be expensive to access from the low-level set_pte functions and is a
    pain to cache, particularly with multi-cluster systems.
    
    A useful observation is that the multi-processing extensions for ARMv7
    require coherent table walks, meaning that we can make use of ALT_SMP
    patching in proc-v7-* to patch away the cache flush safely for these
    cores.
    
    Reported-by: Albin Tonnerre <Albin.Tonnerre@arm.com>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>
    
    Conflicts:
    	arch/arm/mm/proc-v7-2level.S

commit 8ac78e1b9dd8a0ab2ebc71c9d6cc5f7aa9ce3eb6
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Mon Mar 16 22:10:04 2015 +0100

     ARM: 7660/1: tlb: add branch predictor maintenance operations
    
    The ARM architecture requires explicit branch predictor maintenance
    when updating an instruction stream for a given virtual address. In
    reality, this isn't so much of a burden because the branch predictor
    is flushed during the cache maintenance required to make the new
    instructions visible to the I-side of the processor.
    
    However, there are still some cases where explicit flushing is required,
    so add a local_bp_flush_all operation to deal with this.
    
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit d9e384d6df6b32d9cab71a21d37277dafdec0f7e
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Mon Mar 16 21:58:19 2015 +0100

     ARM: 7678/1: Work around faulty ISAR0 register in some Krait CPUs
    
    Some early versions of the Krait CPU design incorrectly indicate
    that they only support the UDIV and SDIV instructions in Thumb
    mode when they actually support them in ARM and Thumb mode. It
    seems that these CPUs follow the DDI0406B ARM ARM which has two
    possible values for the divide instructions field, instead of the
    DDI0406C document which has three possible values.
    
    Work around this problem by checking the MIDR against Krait CPUs
    with this faulty ISAR0 register and force the hwcaps to indicate
    support in both modes.
    
    [sboyd: Rewrote commit text to reflect real reasoning now that
    	we autodetect udiv/sdiv]
    
    Signed-off-by: Stepan Moskovchenko <stepanm@codeaurora.org>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Stephen Boyd <sboyd@codeaurora.org>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>
    
    Conflicts:
    	arch/arm/mm/proc-v7.S

commit 2cfa7c2fc9be54c5f1d0e750859c25d687b63b1d
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Mon Mar 16 21:52:05 2015 +0100

     ARM: 7747/1: pcpu: ensure __my_cpu_offset cannot be re-ordered across
    
     barrier()
    
    __my_cpu_offset is non-volatile, since we want its value to be cached
    when we access several per-cpu variables in a row with preemption
    disabled. This means that we rely on preempt_{en,dis}able to hazard
    with the operation via the barrier() macro, so that we can't end up
    migrating CPUs without reloading the per-cpu offset.
    
    Unfortunately, GCC doesn't treat a "memory" clobber on a non-volatile
    asm block as a side-effect, and will happily re-order it before other
    memory clobbers (including those in prempt_disable()) and cache the
    value. This has been observed to break the cmpxchg logic in the slub
    allocator, leading to livelock in kmem_cache_alloc in mainline kernels.
    
    This patch adds a dummy memory input operand to __my_cpu_offset,
    forcing it to be ordered with respect to the barrier() macro.
    
    Cc: <stable@vger.kernel.org>
    Cc: Rob Herring <rob.herring@calxeda.com>
    Reviewed-by: Nicolas Pitre <nico@linaro.org>
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit ced7cc90e59d228983380d8fdbf721d651043800
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Mon Mar 16 21:50:10 2015 +0100

     ARM: 7587/1: implement optimized percpu variable access
    
    Use the previously unused TPIDRPRW register to store percpu offsets.
    TPIDRPRW is only accessible in PL1, so it can only be used in the
    kernel.
    
    This replaces 2 loads with a mrc instruction for each percpu variable
    access. With hackbench, the performance improvement is 1.4% on Cortex-A9
    (highbank). Taking an average of 30 runs of "hackbench -l 1000" yields:
    
    Before: 6.2191
    After: 6.1348
    
    Will Deacon reported similar delta on v6 with 11MPCore.
    
    The asm "memory clobber" are needed here to ensure the percpu offset
    gets reloaded. Testing by Will found that this would not happen in
    __schedule() which is a bit of a special case as preemption is disabled
    but the execution can move cores.
    
    Signed-off-by: Rob Herring <rob.herring@calxeda.com>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Acked-by: Nicolas Pitre <nico@linaro.org>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>
    
    Conflicts:
    	arch/arm/kernel/smp.c

commit 46b33cfad8b59ff13eff676b2eac7d41d02604c2
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Mon Mar 16 21:47:03 2015 +0100

     ARM: 7680/1: Detect support for SDIV/UDIV from ISAR0 register
    
    The ISAR0 register indicates support for the SDIV and UDIV
    instructions in both the Thumb and ARM instruction set. Read the
    register to detect the supported instructions and update the
    elf_hwcap mask as appropriate. This is better than adding more
    and more cpuid checks in proc-v7.S for each new cpu variant that
    supports these instructions.
    
    Acked-by: Will Deacon <will.deacon@arm.com>
    Cc: Stepan Moskovchenko <stepanm@codeaurora.org>
    Signed-off-by: Stephen Boyd <sboyd@codeaurora.org>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 542e8eaa10fec9c6fe38fe650c634a1a9bbcccd0
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Mon Mar 16 21:44:18 2015 +0100

     lib: cpu_rmap: avoid flushing all workqueues
    
    In some cases, free_irq_cpu_rmap() is called while holding a lock (eg
    rtnl).  This can lead to deadlocks, because it invokes
    flush_scheduled_work() which ends up waiting for whole system workqueue
    to flush, but some pending works might try to acquire the lock we are
    already holding.
    
    This commit uses reference-counting to replace
    irq_run_affinity_notifiers().  It also removes
    irq_run_affinity_notifiers() altogether.
    
    [akpm@linux-foundation.org: eliminate free_cpu_rmap, rename cpu_rmap_reclaim() to cpu_rmap_release(), propagate kref_put() retval from cpu_rmap_put()]
    Signed-off-by: David Decotigny <decot@googlers.com>
    Reviewed-by: Ben Hutchings <bhutchings@solarflare.com>
    Acked-by: Eric Dumazet <edumazet@google.com>
    Reviewed-by: Josh Triplett <josh@joshtriplett.org>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Or Gerlitz <ogerlitz@mellanox.com>
    Acked-by: Amir Vadai <amirv@mellanox.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit af33c52bfeb6ec6bbd9bf36ca8e7134cb3b1f641
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Mon Mar 16 21:38:03 2015 +0100

     ARM: hotplug: remove majority of cache flushing
    
    Remove the majority of cache flushing calls from the platform
    files.  This is now handled by the core code.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 653f85f65718b305392d3e0834afe0178fb906e5
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Mon Mar 16 21:35:25 2015 +0100

     cpufreq: ondemand: fix timer-related list corruption in store_powersa
    
    ve_bias()
    
    The dbs_timer_init() call in store_powersave_bias() re-initializes the
    dbs_info workqueue, call on dbs_timer_exit() to ensure
    outstanding work is cleared prior to making this call. Also, grab the
    percpu timer_mutex lock to avoid race conditions with respect to the
    dbs timer.
    
    Change-Id: I79f3d43eeb51d2d8e21edd0fe043d6583333951f
    Signed-off-by: Osvaldo Banuelos <osvaldob@codeaurora.org>

commit 9de229b46b829ae7526bd2ee5102c3ba4b1a8b5f
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Mon Mar 16 21:34:06 2015 +0100

     cpufreq: Save user policy min/max instead of policy min/max during ho
    
    tplug
    
    When a CPU is hotplugged off, various fields of the policy are saved so
    that they can be restored when the CPU is hotplugged back in.
    
    The existing code saves the policy min/max field during hotplug remove
    and
    restores it into user policy min/max during hotplug add. This results in
    the loss of the user policy min/max values across a hotplug remove/add.
    
    Fix it by saving the user policy min/max instead of policy min/max
    during
    hotplug remove. During a hotplug add, after user policy min/max is
    restored, the policy min/max is already recomputed. So, there's no risk
    of
    going beyound any limits imposed by the CPUFREQ_ADJUST/INCOMPATIBLE
    notifiers.
    
    Change-Id: Ib8e09fa324c1f80f095c5754b5dcf2685e8e4a66
    Signed-off-by: Saravana Kannan <skannan@codeaurora.org>

commit dcca072692fbdc9dcae5eb9a77462c3200d9dce2
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Mon Mar 16 21:30:26 2015 +0100

     msm: hotplug: Block hotplug removal of CPU0
    
    Hotplug removal of CPU0 is not currently supported. Attempts to
    remove it may result in undefined behaviour. Explicitly block
    any such attempts before other hotplug notifiers get the chance
    to run by setting the notifier priority to INT_MAX.
    
    Change-Id: I9e1f5b4a0db554295a097220417f2bdf4c690f5f
    Signed-off-by: Matt Wagantall <mattw@codeaurora.org>

commit 8830d4cd8abc35f642cb01e0bf459b43f914b0c8
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Mon Mar 16 21:28:03 2015 +0100

     msm: mpdecision: update dcvs algo params at initialization
    
    By skipping the update of dcvs algo params when mpdecision first
    starts, there's a small window where dcvs may run with the incorrect
    parameters, depending on how many CPUs are online.
    
    Change-Id: I44094a9abb5e801b0869fb8b796a62eb4d0c49be
    Signed-off-by: Steve Muckle <smuckle@codeaurora.org>

commit bf9b581d94204291b294ab100adc4b59cade1d2c
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Mon Mar 16 21:26:55 2015 +0100

     ARM: cacheflush: add synchronization helpers for mixed cache state ac
    
    cesses
    
    Algorithms used by the MCPM layer rely on state variables which are
    accessed while the cache is either active or inactive, depending
    on the code path and the active state.
    
    This patch introduces generic cache maintenance helpers to provide the
    necessary cache synchronization for such state variables to always hit
    main memory in an ordered way.
    
    Signed-off-by: Nicolas Pitre <nico@linaro.org>
    Acked-by: Russell King <rmk+kernel@arm.linux.org.uk>
    Acked-by: Dave Martin <dave.martin@linaro.org>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit b91c6f0b766341f392407a79e16e9143aa9b4754
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Mon Mar 16 21:24:53 2015 +0100

     ARM: suspend: use hash of cpu_logical_map value to index into save array
    
    Currently we hash the MPIDR of the CPU being suspended to determine
    which
    entry in the sleep_save_sp array to use. In some situations, such as
    when
    we want to resume on another physical CPU, the MPIDR of another CPU
    should
    be used instead.
    
    So let's use the value of cpu_logical_map(smp_processor_id()) in place
    of the MPIDR in the suspend path.  This will result in the same index
    being used as with the previous code unless the caller has modified
    cpu_logical_map() beforehand with the MPIDR of the physical CPU the
    suspending logical CPU will resume on.
    
    Consequently, if doing a physical CPU migration, cpu_logical_map() must
    be updated appropriately somewhere between cpu_pm_enter() and
    cpu_suspend().
    
    The register allocation in __cpu_suspend is reworked in order to better
    accommodate the additional argument.
    
    Signed-off-by: Nicolas Pitre <nico@linaro.org>
    Acked-by: Lorenzo Pieralisi <lorenzo.pieralisi@arm.com>
    Reviewed-by: Dave Martin <Dave.Martin@arm.com>
    Signed-off-by: Murali Nalajala <mnalajal@codeaurora.org>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 131a15ac7442156801fac70c5d8f4f27b623a01d
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Mon Mar 16 21:20:21 2015 +0100

     ARM: kernel: implement stack pointer save array through MPIDR hashing
    
    Current implementation of cpu_{suspend}/cpu_{resume} relies on the MPIDR
    to index the array of pointers where the context is saved and restored.
    The current approach works as long as the MPIDR can be considered a
    linear index, so that the pointers array can simply be dereferenced by
    using the MPIDR[7:0] value.
    On ARM multi-cluster systems, where the MPIDR may not be a linear index,
    to properly dereference the stack pointer array, a mapping function
    should
    be applied to it so that it can be used for arrays look-ups.
    
    This patch adds code in the cpu_{suspend}/cpu_{resume} implementation
    that relies on shifting and ORing hashing method to map a MPIDR value to
    a
    set of buckets precomputed at boot to have a collision free mapping from
    MPIDR to context pointers.
    
    The hashing algorithm must be simple, fast, and implementable with few
    instructions since in the cpu_resume path the mapping is carried out
    with
    the MMU off and the I-cache off, hence code and data are fetched from
    DRAM
    with no-caching available. Simplicity is counterbalanced with a little
    increase of memory (allocated dynamically) for stack pointers buckets,
    that
    should be anyway fairly limited on most systems.
    
    Memory for context pointers is allocated in a early_initcall with
    size precomputed and stashed previously in kernel data structures.
    Memory for context pointers is allocated through kmalloc; this
    guarantees contiguous physical addresses for the allocated memory which
    is fundamental to the correct functioning of the resume mechanism that
    relies on the context pointer array to be a chunk of contiguous physical
    memory. Virtual to physical address conversion for the context pointer
    array base is carried out at boot to avoid fiddling with virt_to_phys
    conversions in the cpu_resume path which is quite fragile and should be
    optimized to execute as few instructions as possible.
    Virtual and physical context pointer base array addresses are stashed in
    a
    struct that is accessible from assembly using values generated through
    the
    asm-offsets.c mechanism.
    
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Russell King <linux@arm.linux.org.uk>
    Cc: Colin Cross <ccross@android.com>
    Cc: Santosh Shilimkar <santosh.shilimkar@ti.com>
    Cc: Daniel Lezcano <daniel.lezcano@linaro.org>
    Cc: Amit Kucheria <amit.kucheria@linaro.org>
    Signed-off-by: Lorenzo Pieralisi <lorenzo.pieralisi@arm.com>
    Reviewed-by: Dave Martin <Dave.Martin@arm.com>
    Reviewed-by: Nicolas Pitre <nico@linaro.org>
    Tested-by: Shawn Guo <shawn.guo@linaro.org>
    Tested-by: Kevin Hilman <khilman@linaro.org>
    Tested-by: Stephen Warren <swarren@wwwdotorg.org>
    Signed-off-by: Murali Nalajala <mnalajal@codeaurora.org>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit f01aae2c3bb78702489301f1e749f0d030bd4e0f
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Mon Mar 16 21:12:28 2015 +0100

     ARM: kernel: build MPIDR hash function data structure
    
    On ARM SMP systems, cores are identified by their MPIDR register.
    The MPIDR guidelines in the ARM ARM do not provide strict enforcement of
    MPIDR layout, only recommendations that, if followed, split the MPIDR
    on ARM 32 bit platforms in three affinity levels. In multi-cluster
    systems like big.LITTLE, if the affinity guidelines are followed, the
    MPIDR can not be considered an index anymore. This means that the
    association between logical CPU in the kernel and the HW CPU identifier
    becomes somewhat more complicated requiring methods like hashing to
    associate a given MPIDR to a CPU logical index, in order for the look-up
    to be carried out in an efficient and scalable way.
    
    This patch provides a function in the kernel that starting from the
    cpu_logical_map, implement collision-free hashing of MPIDR values by
    checking
    all significative bits of MPIDR affinity level bitfields. The hashing
    can then be carried out through bits shifting and ORing; the resulting
    hash algorithm is a collision-free though not minimal hash that can be
    executed with few assembly instructions. The mpidr is filtered through a
    mpidr mask that is built by checking all bits that toggle in the set of
    MPIDRs corresponding to possible CPUs. Bits that do not toggle do not
    carry
    information so they do not contribute to the resulting hash.
    
    Pseudo code:
    
    /* check all bits that toggle, so they are required */
    for (i = 1, mpidr_mask = 0; i < num_possible_cpus(); i++)
    	mpidr_mask |= (cpu_logical_map(i) ^ cpu_logical_map(0));
    
    /*
    * Build shifts to be applied to aff0, aff1, aff2 values to hash the
    mpidr
    * fls() returns the last bit set in a word, 0 if none
    * ffs() returns the first bit set in a word, 0 if none
    */
    fs0 = mpidr_mask[7:0] ? ffs(mpidr_mask[7:0]) - 1 : 0;
    fs1 = mpidr_mask[15:8] ? ffs(mpidr_mask[15:8]) - 1 : 0;
    fs2 = mpidr_mask[23:16] ? ffs(mpidr_mask[23:16]) - 1 : 0;
    ls0 = fls(mpidr_mask[7:0]);
    ls1 = fls(mpidr_mask[15:8]);
    ls2 = fls(mpidr_mask[23:16]);
    bits0 = ls0 - fs0;
    bits1 = ls1 - fs1;
    bits2 = ls2 - fs2;
    aff0_shift = fs0;
    aff1_shift = 8 + fs1 - bits0;
    aff2_shift = 16 + fs2 - (bits0 + bits1);
    u32 hash(u32 mpidr) {
    	u32 l0, l1, l2;
    	u32 mpidr_masked = mpidr & mpidr_mask;
    	l0 = mpidr_masked & 0xff;
    	l1 = mpidr_masked & 0xff00;
    	l2 = mpidr_masked & 0xff0000;
    	return (l0 >> aff0_shift | l1 >> aff1_shift | l2 >> aff2_shift);
    }
    
    The hashing algorithm relies on the inherent properties set in the ARM
    ARM
    recommendations for the MPIDR. Exotic configurations, where for instance
    the
    MPIDR values at a given affinity level have large holes, can end up
    requiring
    big hash tables since the compression of values that can be achieved
    through
    shifting is somewhat crippled when holes are present. Kernel warns if
    the number of buckets of the resulting hash table exceeds the number of
    possible CPUs by a factor of 4, which is a symptom of a very sparse HW
    MPIDR configuration.
    
    The hash algorithm is quite simple and can easily be implemented in
    assembly
    code, to be used in code paths where the kernel virtual address space is
    not set-up (ie cpu_resume) and instruction and data fetches are strongly
    ordered so code must be compact and must carry out few data accesses.
    
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Russell King <linux@arm.linux.org.uk>
    Cc: Colin Cross <ccross@android.com>
    Cc: Santosh Shilimkar <santosh.shilimkar@ti.com>
    Cc: Daniel Lezcano <daniel.lezcano@linaro.org>
    Cc: Amit Kucheria <amit.kucheria@linaro.org>
    Signed-off-by: Lorenzo Pieralisi <lorenzo.pieralisi@arm.com>
    Reviewed-by: Dave Martin <Dave.Martin@arm.com>
    Reviewed-by: Nicolas Pitre <nico@linaro.org>
    Tested-by: Shawn Guo <shawn.guo@linaro.org>
    Tested-by: Kevin Hilman <khilman@linaro.org>
    Tested-by: Stephen Warren <swarren@wwwdotorg.org>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 801467c9531c15cd30753b38e862413c25614661
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Mon Mar 16 21:08:44 2015 +0100

     ARM: kernel: smp_setup_processor_id() updates
    
    This patch applies some basic changes to the smp_setup_processor_id()
    ARM implementation to make the code that builds cpu_logical_map more
    uniform across the kernel.
    
    The function now prints the full extent of the boot CPU MPIDR[23:0] and
    initializes the cpu_logical_map for CPUs up to nr_cpu_ids.
    
    Signed-off-by: Lorenzo Pieralisi <lorenzo.pieralisi@arm.com>
    Acked-by: Nicolas Pitre <nico@linaro.org>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit c59c46e0196fa615b7c3e3aae29f30ed207c3ed7
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Mon Mar 16 21:07:17 2015 +0100

    arch/arm : compile with hardfloat + neon-vfpv4

commit b7748314b913eb1e7d8b87b9deeceed956287008
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Mon Mar 16 21:02:09 2015 +0100

     usb: gadget: mass_storage: added sysfs entry for cdrom to LUNs
    
    This patch adds a "cdrom" sysfs entry for each mass_storage LUN, just
    like "ro" sysfs entry. This allows switching between USB and CD-ROM
    emulation without reinserting the module or recompiling the kernel.
    
    Change-Id: I115e3c8b5cf6e473cb081ef84f7be451ecdc1f0f

commit 18fb1bf2f39e8615d0c4cc6babbc80497a5e7df5
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Mon Mar 16 20:59:05 2015 +0100

     USB: f_mass_storage: 2048 block size for cdrom devices
    
    Bios will recognize the cdrom device like most other cdrom
    devices. This allows people to boot ISOs from their phone.
    
    Original by Andreas Kemnade:
    http://www.spinics.net/lists/linux-usb/msg25178.html

commit 31f99682b9f8d384b3578bb7f3e5a92b0486d50e
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Mon Mar 16 20:53:29 2015 +0100

     ARM: topology: Suppress clock-frequency error
    
    * As per discussions over Android developer forums,
    this error can be safely ignored. There is no
    performance impact if this DT node is missing.
    * Actual fix would be adding respective DT node
    for krait but this is even missing on msm-3.10
    branch where ARM topology is being used.
    
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 50792c6b66614a8792b19f702b19540f1f705854
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Mon Mar 16 20:52:48 2015 +0100

     ARM: topology: Add efficiency coefficient for Krait
    
    * Instead using default value of CPU efficiency,
    use same as that of cortex-a15.
    
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit e394a526dbc1e22693942b8cc01173b948113b83
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Mon Mar 16 20:52:10 2015 +0100

     ARM: 8008/1: topology: Coding style fixes
    
    Use kcalloc() and ULONG_MAX rather than open coding them.
    
    Signed-off-by: Mark Brown <broonie@linaro.org>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 15f87f9ef11fab9e2024045942881de4f98c2726
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Mon Mar 16 20:51:42 2015 +0100

     ARM: 7920/1: topology: Staticise non-exported symbols
    
    These symbols are only referenced in this source file so can be made
    static, and the efficiency table is constant data so can be declared as
    such.  This avoids polluting the global namespace and fixes warnings
    from sparse.
    
    The function arch_scale_freq_power() is still not prototyped or static,
    this is a separate issue as this is overriding a weak symbol from the
    scheduler which neglects to provide a prototype.
    
    Signed-off-by: Mark Brown <broonie@linaro.org>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit d5656a4c2a0b594775ad2d14279a8169ee70cc30
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Mon Mar 16 20:51:02 2015 +0100

     ARM: topology: remove hwid/MPIDR dependency from cpu_capacity
    
    Currently the topology code computes cpu capacity and stores it in
    the list along with hwid(which is MPIDR) as it parses the CPU nodes
    in the device tree. This is required as it needs to be mapped to the
    logical CPU later.
    
    Since the CPU device nodes can be retrieved in the logical ordering
    using DT/OF helpers, its possible to store cpu_capacity also in logical
    ordering and avoid storing hwid for each entry.
    
    This patch removes hwid by making use of of_get_cpu_node.
    
    Cc: Russell King <linux@arm.linux.org.uk>
    Cc: Lorenzo Pieralisi <lorenzo.pieralisi@arm.com>
    Acked-by: Rob Herring <rob.herring@calxeda.com>
    Acked-by: Nicolas Pitre <nico@linaro.org>
    Signed-off-by: Sudeep KarkadaNagesha <sudeep.karkadanagesha@arm.com>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 659b8c95c7aa752a5499550fa13f56a69b23973f
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Mon Mar 16 20:50:32 2015 +0100

     arm: remove cast for kzalloc return value
    
    remove cast for kzalloc return value.
    
    Signed-off-by: Zhang Yanfei <zhangyanfei@cn.fujitsu.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Russell King <linux@arm.linux.org.uk>
    Cc: linux-arm-kernel@lists.infradead.org
    Signed-off-by: Jiri Kosina <jkosina@suse.cz>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 8afc9d79060d69feac83b16b775b3d70eb0d56cd
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Mon Mar 16 20:49:30 2015 +0100

     ARM: kernel: update topology to use new MPIDR macros
    
    This patch updates the topology initialization code to use the newly
    defined accessors to retrieve the MPIDR affinity levels.
    
    Signed-off-by: Lorenzo Pieralisi <lorenzo.pieralisi@arm.com>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Acked-by: Nicolas Pitre <nico@linaro.org>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 021e7d463f659bbf2e75db6a36f5a38abf1a37a6
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Mon Mar 16 20:48:44 2015 +0100

     ARM: kernel: enhance MPIDR macro definitions
    
    Kernel subsystems other than the topology layer need the MPIDR
    mask definitions to access the MPIDR without relying on hardcoded
    masks. This patch moves the MPIDR register masks definition to
    a header file and defines a macro to simplify access to MPIDR bit fields
    representing affinity levels.
    
    Signed-off-by: Lorenzo Pieralisi <lorenzo.pieralisi@arm.com>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Acked-by: Nicolas Pitre <nico@linaro.org>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 37b76b475942658f297b48d4b1609b3ddafe052d
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Mon Mar 16 20:47:26 2015 +0100

     ARM: 7482/1: topology: fix section mismatch warning for init_cpu_topo
    
    logy
    
    Get rid of this warning..
    arch/arm/kernel/built-in.o(.text+0xac78): Section mismatch in reference
    from the function init_cpu_topology() to the function
    .init.text:parse_dt_topology()
    The function init_cpu_topology() references
    the function __init parse_dt_topology().
    This is often because init_cpu_topology lacks a __init
    annotation or the annotation of parse_dt_topology is wrong.
    
    Signed-off-by: Venkatraman S <svenkatr@ti.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 3195d4c0df7b361a01fe4e445f0305a9c8a0a249
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Mon Mar 16 20:46:40 2015 +0100

     ARM: 7463/1: topology: Update cpu_power according to DT information
    
    Use cpu compatibility field and clock-frequency field of DT to
    estimate the capacity of each core of the system and to update
    the cpu_power field accordingly.
    This patch enables to put more running tasks on big cores than
    on LITTLE ones. But this patch doesn't ensure that long running
    tasks will run on big cores and short ones on LITTLE cores.
    
    Signed-off-by: Vincent Guittot <vincent.guittot@linaro.org>
    Reviewed-by: Namhyung Kim <namhyung@kernel.org>
    Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit d0a342ffd2d9e6dd05f0565462899cad4f4dfb3d
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Mon Mar 16 20:45:59 2015 +0100

     ARM: 7462/1: topology: factorize the update of sibling masks
    
    This factorization has also been proposed in another patch that has not
    been
    merged yet:
    http://lists.infradead.org/pipermail/linux-arm-kernel/2012-January/080873.html
    So, this patch could be dropped depending of the state of the other one.
    
    Signed-off-by: Lorenzo Pieralisi <lorenzo.pieralisi@arm.com>
    Signed-off-by: Vincent Guittot <vincent.guittot@linaro.org>
    Reviewed-by: Namhyung Kim <namhyung@kernel.org>
    Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 9df5222a4ebdfdf79cc1a0097da8aa6f029cd8fe
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Mon Mar 16 20:44:35 2015 +0100

     ARM: 7461/1: topology: Add arch_scale_freq_power function
    
    Add infrastructure to be able to modify the cpu_power of each core
    
    Signed-off-by: Vincent Guittot <vincent.guittot@linaro.org>
    Reviewed-by: Namhyung Kim <namhyung@kernel.org>
    Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 7f7ff5315bc13f9aea323208eda66ff6954a718c
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Mon Mar 16 20:40:13 2015 +0100

     ARM: sched_clock: Load cycle count after epoch stabilizes
    
    There is a small race between when the cycle count is read from
    the hardware and when the epoch stabilizes. Consider this
    scenario:
    
    CPU0                           CPU1
    ----                           ----
    cyc = read_sched_clock()
    cyc_to_sched_clock()
                                     update_sched_clock()
                                      ...
                                      cd.epoch_cyc = cyc;
    epoch_cyc = cd.epoch_cyc;
    ...
    epoch_ns + cyc_to_ns((cyc - epoch_cyc)
    
    The cyc on cpu0 was read before the epoch changed. But we
    calculate the nanoseconds based on the new epoch by subtracting
    the new epoch from the old cycle count. Since epoch is most likely
    larger than the old cycle count we calculate a large number that
    will be converted to nanoseconds and added to epoch_ns, causing
    time to jump forward too much.
    
    Fix this problem by reading the hardware after the epoch has
    stabilized.
    
    Change-Id: I995133b229b2c2fedd5091406d1dc366d8bfff7b
    Cc: Russell King <linux@arm.linux.org.uk>
    Signed-off-by: Stephen Boyd <sboyd@codeaurora.org>
    Signed-off-by: John Stultz <john.stultz@linaro.org>
    Git-commit: 336ae11
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    [sboyd: reworked for file movement kernel/time -> arm/kernel]
    Signed-off-by: Stephen Boyd <sboyd@codeaurora.org>
    
    Signed-off-by: franciscofranco <franciscofranco.1990@gmail.com>

commit 5c7cb4f58817f3c756185314746c43bd11744b42
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Mon Mar 16 20:39:15 2015 +0100

     CPU hotplug, debug: detect imbalance between get_online_cpus() and pu
    
    t_online_cpus()
    
    The synchronization between CPU hotplug readers and writers is achieved
    by means of refcounting, safeguarded by the cpu_hotplug.lock.
    
    get_online_cpus() increments the refcount, whereas put_online_cpus()
    decrements it.  If we ever hit an imbalance between the two, we end up
    compromising the guarantees of the hotplug synchronization i.e, for
    example, an extra call to put_online_cpus() can end up allowing a
    hotplug reader to execute concurrently with a hotplug writer.
    
    So, add a WARN_ON() in put_online_cpus() to detect such cases where the
    refcount can go negative, and also attempt to fix it up, so that we can
    continue to run.
    
    Change-Id: I144efeaa5899a2e8a3cddd21f010679cbaaa2459
    Signed-off-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Reviewed-by: Yasuaki Ishimatsu <isimatu.yasuaki@jp.fujitsu.com>
    Cc: Jiri Kosina <jkosina@suse.cz>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Git-commit: 075663d
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Osvaldo Banuelos <osvaldob@codeaurora.org>

commit d399887cc1e7eceec9c68fbd9ad359312a642c3e
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Mon Mar 16 20:38:14 2015 +0100

     cpuidle: remove cross-cpu IPI by new latency request.
    
    when drivers request new latency requirement, it's not necessary to
    immediately wake up another cpu by sending cross-cpu IPI, we can
    consider
    the new latency to be taken into effect after next wakeup from idle,
    this can save the unnecessary wakeup cost, and reduce the risk that
    drivers may request latency in irq disabled context.
    
    [<c08e0cc0>] (__irq_svc+0x40/0x70) from [<c00e801c>]
    (smp_call_function_single+0x16c/0x240)
    [<c00e801c>] (smp_call_function_single+0x16c/0x240) from [<c00e855c>]
    (smp_call_function+0x40/0x6c)
    [<c00e855c>] (smp_call_function+0x40/0x6c) from [<c0601c9c>]
    (cpuidle_latency_notify+0x18/0x20)
    [<c0601c9c>] (cpuidle_latency_notify+0x18/0x20) from [<c00b7c28>]
    (blocking_notifier_call_chain+0x74/0x94)
    [<c00b7c28>] (blocking_notifier_call_chain+0x74/0x94) from [<c00d563c>]
    (pm_qos_update_target+0xe0/0x128)
    [<c00d563c>] (pm_qos_update_target+0xe0/0x128) from [<c0620d3c>]
    (msmsdcc_enable+0xac/0x158)
    [<c0620d3c>] (msmsdcc_enable+0xac/0x158) from [<c06050e0>]
    (mmc_try_claim_host+0xb0/0xb8)
    [<c06050e0>] (mmc_try_claim_host+0xb0/0xb8) from [<c0605318>]
    (mmc_start_bkops.part.15+0x50/0x2f4)
    [<c0605318>] (mmc_start_bkops.part.15+0x50/0x2f4) from [<c00ab768>]
    (process_one_work+0x124/0x55c)
    [<c00ab768>] (process_one_work+0x124/0x55c) from [<c00abfc8>]
    (worker_thread+0x178/0x45c)
    [<c00abfc8>] (worker_thread+0x178/0x45c) from [<c00b0b24>]
    (kthread+0x84/0x90)
    [<c00b0b24>] (kthread+0x84/0x90) from [<c000fdd4>]
    (kernel_thread_exit+0x0/0x8)
    Disabling lock debugging due to kernel taint
    coresight-etb coresight-etb.0: ETB aborted
    Kernel panic - not syncing: softlockup: hung tasks
    
    Signed-off-by: Guojian Chen <a21757@motorola.com>
    Reviewed-on: http://gerrit.pcs.mot.com/532702
    SLT-Approved: Slta Waiver <sltawvr@motorola.com>
    Tested-by: Jira Key <jirakey@motorola.com>
    Reviewed-by: Klocwork kwcheck <klocwork-kwcheck@sourceforge.mot.com>
    Reviewed-by: Christopher Fries <qcf001@motorola.com>
    Reviewed-by: David Ding <dding@motorola.com>
    Submit-Approved: Jira Key <jirakey@motorola.com>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 061235ce25aa01585fcd54fb356ceca540c8302b
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Mon Mar 16 20:36:26 2015 +0100

     nohz: Reduce overhead under high-freq idling patterns
    
    One testbox of mine (Intel Nehalem, 16-way) uses MWAIT for its idle
    routine,
    which apparently can break out of its idle loop rather frequently, with
    high frequency.
    
    In that case NO_HZ_FULL=y kernels show high ksoftirqd overhead and
    constant
    context switching, because tick_nohz_stop_sched_tick() will, if
    delta_jiffies == 0, mis-identify this as a timer event - activating the
    TIMER_SOFTIRQ, which wakes up ksoftirqd.
    
    Fix this by treating delta_jiffies == 0 the same way we treat other
    short
    wakeups, delta_jiffies == 1.
    
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Chris Metcalf <cmetcalf@tilera.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Geoff Levand <geoff@infradead.org>
    Cc: Gilad Ben Yossef <gilad@benyossef.com>
    Cc: Hakan Akkan <hakanakkan@gmail.com>
    Cc: Kevin Hilman <khilman@linaro.org>
    Cc: Li Zhong <zhong@linux.vnet.ibm.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Paul Gortmaker <paul.gortmaker@windriver.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: franciscofranco <franciscofranco.1990@gmail.com>

commit fbf3684e5496a6a0bda6e754ec27b18a6abd95c0
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Mon Mar 16 20:35:23 2015 +0100

     ARM: smp: Wait just 1 second for other CPU to halt
    
    Currently, the busyloop waiting for a 2nd CPU to stop takes about 4
    seconds.  Adjust for the overhead of the loop by looping every 1ms
    instead of 1us.
    
    Signed-off-by: Chris Fries <C.Fries@motorola.com>
    Reviewed-on: http://gerrit.pcs.mot.com/537864
    SLT-Approved: Slta Waiver <sltawvr@motorola.com>
    Tested-by: Jira Key <jirakey@motorola.com>
    Reviewed-by: Check Patch <CHEKPACH@motorola.com>
    Reviewed-by: Klocwork kwcheck <klocwork-kwcheck@sourceforge.mot.com>
    Reviewed-by: Igor Kovalenko <cik009@motorola.com>
    Reviewed-by: Russell Knize <rknize2@motorola.com>
    Submit-Approved: Jira Key <jirakey@motorola.com>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>
    Signed-off-by: franciscofranco <franciscofranco.1990@gmail.com>

commit acbc23fcf320189d13c1f4976558e527888f2ddc
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Mon Mar 16 20:34:38 2015 +0100

    Revert: " cpufreq: updated darkness and nightmare to latest version, thanks to 
    
    alucard24"

commit c933a450b910b647e250f9fa895f125d37af22ac
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Mon Mar 16 19:36:23 2015 +0100

     timer: optimize apply_slack()
    
    __fls(mask) is equivalent to find_last_bit(&mask, BITS_PER_LONG), but
    cheaper.
    find_last_bit was showing up high on the list when I was profiling for
    stalls
    on icache misses on a system with very small cache size (MIPS).
    
    Signed-off-by: Felix Fietkau <nbd@openwrt.org>
    Signed-off-by: edoko <r_data@naver.com>

commit 554f9418d7f3a031780bf785e3b4afbe768b42ef
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Mon Mar 16 19:35:24 2015 +0100

     cpuidle: make a single register function for all
    
    The usual scheme to initialize a cpuidle driver on a SMP is:
    
    	cpuidle_register_driver(drv);
    	for_each_possible_cpu(cpu) {
    		device = &per_cpu(cpuidle_dev, cpu);
    		cpuidle_register_device(device);
    	}
    
    This code is duplicated in each cpuidle driver.
    
    On UP systems, it is done this way:
    
    	cpuidle_register_driver(drv);
    	device = &per_cpu(cpuidle_dev, cpu);
    	cpuidle_register_device(device);
    
    On UP, the macro 'for_each_cpu' does one iteration:
    
            for ((cpu) = 0; (cpu) < 1; (cpu)++, (void)mask)
    
    Hence, the initialization loop is the same for UP than SMP.
    
    Beside, we saw different bugs / mis-initialization / return code
    unchecked in
    the different drivers, the code is duplicated including bugs. After
    fixing all
    these ones, it appears the initialization pattern is the same for
    everyone.
    
    Please note, some drivers are doing dev->state_count = drv->state_count.
    This is
    not necessary because it is done by the cpuidle_enable_device function
    in the
    cpuidle framework. This is true, until you have the same states for all
    your
    devices. Otherwise, the 'low level' API should be used instead with the
    specific
    initialization for the driver.
    
    Let's add a wrapper function doing this initialization with a cpumask
    parameter
    for the coupled idle states and use it for all the drivers.
    
    That will save a lot of LOC, consolidate the code, and the modifications
    in the
    future could be done in a single place. Another benefit is the
    consolidation of
    the cpuidle_device variable which is now in the cpuidle framework and no
    longer
    spread accross the different arch specific drivers.
    
    Change-Id: I8f75188d5770871620f758c2973b74384ac570d5
    Signed-off-by: Daniel Lezcano <daniel.lezcano@linaro.org>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Signed-off-by: Karthik Parsha <kparsha@codeaurora.org>
    Signed-off-by: Mahesh Sivasubramanian <msivasub@codeaurora.org>

commit e6ffcb42d51ddd4f7483ddfaf7c219b5de360441
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Mon Mar 16 19:30:19 2015 +0100

    cpufreq: updated darkness and nightmare to latest version, thanks to alucard24

commit b6bfcf2332705ed2d5f07fc9da4f29007d055bd8
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Mon Mar 16 19:27:35 2015 +0100

     cpufreq: Fix policy stucking if user & kernel min/max don't overlap
    
    Every __cpufreq_set_policy starts with checking the new policy min/max
    has
    some overlap with the current policy min/max. This works out fine until
    we
    end up with the policy min/max being set to a range that doesn't overlap
    with the user policy min/max. Once we get into this situation, the check
    at
    the start of __cpufreq_set_policy fails and prevents us from getting out
    of
    this state.
    
    This only happens when one of the CPUFREQ_ADJUST/CPUFREQ_INCOMPATIBLE
    notifiers called inside __cpufreq_set_policy pick a min/max outside the
    range of user policy min/max.
    
    The real intent of the check at the start of __cpufreq_set_policy is to
    make sure userspace can't set user policy min > user policy max. Since
    __cpufreq_set_policy always gets called only with current user policy
    min/max except when the actual user space policy min/max is changed, we
    can
    fix the issue by simply checking the new policy min/max against current
    user policy min/max.
    
    Change-Id: Iaac805825e64d7985c41fb9052bd96baacdf3d6f
    Signed-off-by: Saravana Kannan <skannan@codeaurora.org>

commit f7a74ee6bf42b7ac2d0dd201a6ecfe5e888e8fae
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Mon Mar 16 19:25:56 2015 +0100

     ARM: add cpufreq transiton notifier to adjust loops_per_jiffy for smp
    
    If CONFIG_SMP, cpufreq skips loops_per_jiffy update, because different
    arch has different per-cpu loops_per_jiffy definition.
    
    Signed-off-by: Richard Zhao <richard.zhao@linaro.org>
    Acked-by: Russell King <rmk+kernel@arm.linux.org.uk>
    Acked-by: Santosh Shilimkar <santosh.shilimkar@ti.com>
    Signed-off-by: Shawn Guo <shawn.guo@linaro.org>
    Signed-off-by: Rafael J. Wysocki <rjw@sisk.pl>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit 4fa4e134bbefd915dd8dedd385c8c4670f4074bb
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Mon Mar 16 19:17:04 2015 +0100

     ARM: 7752/1: errata: LoUIS bit field in CLIDR register is incorrect
    
    On Cortex-A9 before version r1p0, the LoUIS bit field of the CLIDR
    register returns zero when it should return one. This leads to cache
    maintenance operations which rely on this value to not function as
    intended, causing data corruption.
    
    The workaround for this errata is to detect affected CPUs and correct
    the LoUIS value read.
    
    Acked-by: Will Deacon <will.deacon@arm.com>
    Acked-by: Nicolas Pitre <nico@linaro.org>
    Cc: stable@vger.kernel.org
    Signed-off-by: Jon Medhurst <tixy@linaro.org>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit e702b0e39b79005556293f8dc72fe3b238afc7b9
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Mon Mar 16 19:14:40 2015 +0100

     ARM: 7606/1: cache: flush to LoUU instead of LoUIS on uniprocessor CPUs
    
    flush_cache_louis flushes the D-side caches to the point of unification
    inner-shareable. On uniprocessor CPUs, this is defined as zero and
    therefore no flushing will take place. Rather than invent a new
    interface
    for UP systems, instead use our SMP_ON_UP patching code to read the
    LoUU from the CLIDR instead.
    
    Cc: <stable@vger.kernel.org>
    Cc: Lorenzo Pieralisi <Lorenzo.Pieralisi@arm.com>
    Tested-by: Guennadi Liakhovetski <g.liakhovetski@gmx.de>
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit a4b3e60a2b4ebcc850856aa2e40345143e1c1138
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Mon Mar 16 19:12:56 2015 +0100

     ARM: smp: flush L1 cache in cpu_die()
    
    Flush the L1 cache for the CPU which is going down in cpu_die() so
    that we don't end up with all platforms doing this.  This ensures
    that any cache lines we own are pushed out before the cache becomes
    inaccessible.
    
    We may end up subsequently creating some dirty cache lines - for
    example, with the complete() call, but this update must become
    visible to other CPUs before __cpu_die() can proceed.  Subsequent
    accesses from the platforms cpu_die() function should _not_ matter.
    
    Also place a mb() after the complete() call to ensure that this is
    visible to other CPUs.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit e14633442516ec08495e0241fb348ddd705723df
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Mon Mar 16 19:09:47 2015 +0100

     ARM: idmap: use flush_cache_louis() and flush TLBs only when necessary
    
    Flushing the cache is needed for the hardware to see the idmap table
    and therefore can be done at init time.  On ARMv7 it is not necessary to
    flush L2 so flush_cache_louis() is used here instead.
    
    There is no point flushing the cache in setup_mm_for_reboot() as the
    caller should, and already is, taking care of this.  If switching the
    memory map requires a cache flush, then cpu_switch_mm() already includes
    that operation.
    
    What is not done by cpu_switch_mm() on ASID capable CPUs is TLB flushing
    as the whole point of the ASID is to tag the TLBs and avoid flushing
    them
    on a context switch.  Since we don't have a clean ASID for the identity
    mapping, we need to flush the TLB explicitly in that case.  Otherwise
    this is already performed by cpu_switch_mm().
    
    Signed-off-by: Nicolas Pitre <nico@linaro.org>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit cc2c0e27028f051d2095f3303a078a6d5c8533e7
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Mon Mar 16 19:08:01 2015 +0100

    tspdrv: add vibration intensity control (thx cyanogenmod)

commit eb02a9421dd2f70c22426f7e41c69692fe029524
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Mon Mar 16 18:57:10 2015 +0100

    cpuidle: Fix NULL pointer dereference during hotplug

commit 00e11143eee0a8778c53f02afe85583372556fec
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Mon Mar 16 18:55:33 2015 +0100

    Revert: " scheduler: compute time-average nr_running per run-queue"
    
    Compute the time-average number of running tasks per run-queue for a
    trailing window of a fixed time period. The detla add/sub to the
    average value is weighted by the amount of time per nr_running value
    relative to the total measurement period.
    
    Change-Id: I076e24ff4ed65bed3b8dd8d2b279a503318071ff
    Signed-off-by: Diwakar Tundlam <dtundlam@nvidia.com>
    (cherry picked from commit 3a12d7499cee352e8a46eaf700259ba3c733f0e3)
    Reviewed-on: http://git-master/r/111635
    Reviewed-by: Automatic_Commit_Validation_User
    Reviewed-by: Sai Gurrappadi <sgurrappadi@nvidia.com>
    Tested-by: Sai Gurrappadi <sgurrappadi@nvidia.com>
    Reviewed-by: Peter Boonstoppel <pboonstoppel@nvidia.com>
    Reviewed-by: Yu-Huan Hsu <yhsu@nvidia.com>
    
    forward ported to Linux 3.4 for use on Mako
    
    Signed-off-by: faux123 <reioux@gmail.com>
    
    Conflicts:
    	kernel/sched/sched.h
    
    Conflicts:
    
    	kernel/sched/sched.h

commit 5cec511895efb4e49f408c4679ca688471982ef2
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Mon Mar 16 18:51:18 2015 +0100

     scheduler: compute time-average nr_running per run-queue
    
    Compute the time-average number of running tasks per run-queue for a
    trailing window of a fixed time period. The detla add/sub to the
    average value is weighted by the amount of time per nr_running value
    relative to the total measurement period.
    
    Change-Id: I076e24ff4ed65bed3b8dd8d2b279a503318071ff
    Signed-off-by: Diwakar Tundlam <dtundlam@nvidia.com>
    (cherry picked from commit 3a12d7499cee352e8a46eaf700259ba3c733f0e3)
    Reviewed-on: http://git-master/r/111635
    Reviewed-by: Automatic_Commit_Validation_User
    Reviewed-by: Sai Gurrappadi <sgurrappadi@nvidia.com>
    Tested-by: Sai Gurrappadi <sgurrappadi@nvidia.com>
    Reviewed-by: Peter Boonstoppel <pboonstoppel@nvidia.com>
    Reviewed-by: Yu-Huan Hsu <yhsu@nvidia.com>
    
    forward ported to Linux 3.4 for use on Mako
    
    Signed-off-by: faux123 <reioux@gmail.com>
    
    Conflicts:
    	kernel/sched/sched.h
    
    Conflicts:
    
    	kernel/sched/sched.h

commit 1b8cafcb54b9a25cd484ec3e15b3781794ecfd07
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Mon Mar 16 18:44:21 2015 +0100

    bcmdhd: reduce wakelocks

commit 11fdadd25fab817c3699e5c25b59db08d89ccc3b
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Mon Mar 16 18:42:03 2015 +0100

    ARM: boot compressed optimisations

commit feee37b957bc9109f5e9e0f5d67f57c977d3f918
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Mon Mar 16 18:37:25 2015 +0100

     HACK: ARM: disable sleeping while atomic warning in do_signal
    
    ARM disables interrupts in do_signal, which triggers a warning in
    try_to_freeze, see details at https://lkml.org/lkml/2011/8/23/221.
    To prevent the warnings, add try_to_freeze_nowarn and call it from
    do_signal.
    
    Change-Id: If7482de21c386adc705fa1ac4ecb8c7ece5bb356
    Signed-off-by: Colin Cross <ccross@android.com>

commit e020d1a6fa8f9667c5505479daab8219bc353597
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Mon Mar 16 18:34:51 2015 +0100

     ARM: mutex: use generic atomic_dec-based implementation for ARMv6+
    
    Commit a76d7bd96d65 ("ARM: 7467/1: mutex: use generic xchg-based
    implementation for ARMv6+") removed the barrier-less, ARM-specific
    mutex implementation in favour of the generic xchg-based code.
    
    Since then, a bug was uncovered in the xchg code when running on SMP
    platforms, due to interactions between the locking paths and the
    MUTEX_SPIN_ON_OWNER code. This was fixed in 0bce9c46bf3b ("mutex: place
    lock in contended state after fastpath_lock failure"), however, the
    atomic_dec-based mutex algorithm is now marginally more efficient for
    ARM (~0.5% improvement in hackbench scores on dual A15).
    
    This patch moves ARMv6+ platforms to the atomic_dec-based mutex code.
    
    Change-Id: I5cb600b15f891dc692356322b38cae1d692a084c
    Cc: Nicolas Pitre <nico@fluxnic.net>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

commit 512a35839c30fc7a2ab696483f56b291ce90e386
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Mon Mar 16 18:33:45 2015 +0100

    Async I/O latency to a ssd greatly increased, 34% gain (Creds: kleikamp)

commit 0629e82a0753bcf2f947181cdfb63c100da7dab1
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Mon Mar 16 18:32:38 2015 +0100

    kernel: workqueue various fixes

commit 6bf9e33802b16219b2a12416d000f24c4ab667b6
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Mon Mar 16 18:22:14 2015 +0100

    sched: enable arch power

commit 975869972640a2ee5eb2e66d24dcc140db810c9f
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Mon Mar 16 18:21:41 2015 +0100

     softirq: reduce latencies
    
    Date	Thu, 03 Jan 2013 23:49:40 -0800
    
    In various network workloads, __do_softirq() latencies can be up
    to 20 ms if HZ=1000, and 200 ms if HZ=100.
    
    This is because we iterate 10 times in the softirq dispatcher,
    and some actions can consume a lot of cycles.
    
    This patch changes the fallback to ksoftirqd condition to :
    
    - A time limit of 2 ms.
    - need_resched() being set on current task
    
    When one of this condition is met, we wakeup ksoftirqd for further
    softirq processing if we still have pending softirqs.
    
    Using need_resched() as the only condition can trigger RCU stalls,
    as we can keep BH disabled for too long.
    
    I ran several benchmarks and got no significant difference in
    throughput, but a very significant reduction of latencies (one order
    of magnitude) :
    
    In following bench, 200 antagonist "netperf -t TCP_RR" are started in
    background, using all available cpus.
    
    Then we start one "netperf -t TCP_RR", bound to the cpu handling the NIC
    IRQ (hard+soft)
    
    Before patch :
    
    RT_LATENCY,MIN_LATENCY,MAX_LATENCY,P50_LATENCY,P90_LATENCY,P99_LATENCY,MEAN_LATENCY,STDDEV_LATENCY
    MIGRATED TCP REQUEST/RESPONSE TEST from 0.0.0.0 (0.0.0.0) port 0 AF_INET
    to 7.7.7.84 () port 0 AF_INET : first burst 0 : cpu bind
    RT_LATENCY=550110.424
    MIN_LATENCY=146858
    MAX_LATENCY=997109
    P50_LATENCY=305000
    P90_LATENCY=550000
    P99_LATENCY=710000
    MEAN_LATENCY=376989.12
    STDDEV_LATENCY=184046.92
    After patch :
    
    RT_LATENCY,MIN_LATENCY,MAX_LATENCY,P50_LATENCY,P90_LATENCY,P99_LATENCY,MEAN_LATENCY,STDDEV_LATENCY
    MIGRATED TCP REQUEST/RESPONSE TEST from 0.0.0.0 (0.0.0.0) port 0 AF_INET
    to 7.7.7.84 () port 0 AF_INET : first burst 0 : cpu bind
    RT_LATENCY=40545.492
    MIN_LATENCY=9834
    MAX_LATENCY=78366
    P50_LATENCY=33583
    P90_LATENCY=59000
    P99_LATENCY=69000
    MEAN_LATENCY=38364.67
    STDDEV_LATENCY=12865.26
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Cc: David Miller <davem@davemloft.net>
    Cc: Tom Herbert <therbert@google.com>
    Cc: Ben Hutchings <bhutchings@solarflare.com>
    
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit 26d013164b9145c1ee247fdbbd7e8007cc789ad5
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Mon Mar 16 18:17:49 2015 +0100

    f2fs: various fixes

commit 7d927b6a64a6b49bd9ed2942ceddbe4fac9e16ca
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Mon Mar 16 18:14:39 2015 +0100

     hostfs: move HOSTFS_SUPER_MAGIC to <linux/magic.h> plus some various fixes
    
    Move HOSTFS_SUPER_MAGIC to <linux/magic.h> to be with it's magical
    friends from other file systems.
    
    Reported-by: Al Viro <viro@zeniv.linux.org.uk>
    Signed-off-by: James Hogan <james.hogan@imgtec.com>
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

commit 1700debb8a69f2a58381e3c14772fd18fb9afae7
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Mon Mar 16 12:47:20 2015 +0100

     CHROMIUM: zram: finish fixing 32-bit overflow
    
    Nasty bug.  PAGE_MASK is unsigned, so it extends to 64 bit with zeros.
    The first read in the zram block device is done by mkswap, which
    tries to read at the end of device.  zram_set_disksize is a side
    effect of the first read, so the disk size is now silently changed
    from 6GB to 2GB.  The read fails, but mkswap does not complain either:
    it finds the size of the device by trial and error and continues.
    Swapon detects the mismatch and fails.
    
    BUG=chromium:245703
    TEST=ran before and after change with various printks
    Signed-off-by: Luigi Semenzato <semenzato@chromium.org>
    
    Change-Id: I01750554964756c2759375df1f14d7bb8b859ccf
    Reviewed-on: https://gerrit.chromium.org/gerrit/65752
    Tested-by: Luigi Semenzato <semenzato@chromium.org>
    Reviewed-by: Mike Frysinger <vapier@chromium.org>
    Commit-Queue: Luigi Semenzato <semenzato@chromium.org>

commit 4e7842448c6b30442c6e618aebf2552d9238fbc1
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Mon Mar 16 12:46:54 2015 +0100

     CHROMIUM: zram: fix 32-bit overflow
    
    The 64-bit division operator is not available in 32-bit kernels,
    so have to use do_div.
    
    BUG=chromium:245703
    TEST=compiled
    
    Change-Id: I44ab763ea9e115cef6212f6524518cf7d9eac8c7
    Signed-off-by: Luigi Semenzato <semenzato@chromium.org>
    Reviewed-on: https://gerrit.chromium.org/gerrit/64695
    Reviewed-by: Doug Anderson <dianders@chromium.org>

commit 4a0a91b0307468cfc79834531a990bb58b6dc7ff
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Mon Mar 16 12:43:28 2015 +0100

     CHROMIUM: mm: Fix calculation of dirtyable memory
    
    The system uses global_dirtyable_memory() to calculate
    number of dirtyable pages/pages that can be allocated
    to the page cache.  A bug causes an underflow thus making
    the page count look like a big unsigned number.  This in turn
    confuses the dirty writeback throttling to aggressively write
    back pages as they become dirty (usually 1 page at a time).
    
    Fix is to ensure there is no underflow while doing the math.
    
    Signed-off-by: Sonny Rao <sonnyrao@chromium.org>
    Signed-off-by: Puneet Kumar <puneetster@chromium.org>
    
    BUG=chrome-os-partner:16011
    TEST=Manual; boot kernel, powerwash, login with testaccount and
    make sure no jank occurs on sync of applications
    
    Change-Id: I614e7c3156e014f0f28a4ef9bdd8cb8a2cd07b2a
    Reviewed-on: https://gerrit.chromium.org/gerrit/37612
    Reviewed-by: Kees Cook <keescook@chromium.org>
    Reviewed-by: Olof Johansson <olofj@chromium.org>
    Commit-Ready: Puneet Kumar <puneetster@chromium.org>
    Reviewed-by: Puneet Kumar <puneetster@chromium.org>
    Tested-by: Puneet Kumar <puneetster@chromium.org>

commit 12b13deab654a14dd7a6b6d214b2a5fb1029d33b
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Mon Mar 16 12:11:06 2015 +0100

     writeback: remove nr_pages_dirtied arg from balance_dirty_pages_ratel
    
    imited_nr()
    
    There is no reason to pass the nr_pages_dirtied argument, because
    nr_pages_dirtied value from the caller is unused in
    balance_dirty_pages_ratelimited_nr().
    
    Signed-off-by: Namjae Jeon <linkinjeon@gmail.com>
    Signed-off-by: Vivek Trivedi <vtrivedi018@gmail.com>
    Cc: Wu Fengguang <fengguang.wu@intel.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Francisco Franco <franciscofranco.1990@gmail.com>

commit 5c65f329c85c172c2c49f9252c24afeab7091c23
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Mon Mar 16 12:03:52 2015 +0100

     CPU hotplug, writeback: Don't call writeback_set_ratelimit() too ofte
    
    n during hotplug
    
    The CPU hotplug callback related to writeback calls writeback_set_ratelimit()
    during every state change in the hotplug sequence. This is unnecessary
    since num_online_cpus() changes only once during the entire hotplug operation.
    
    So invoke the function only once per hotplug, thereby avoiding the
    unnecessary repetition of those costly calculations.
    
    Signed-off-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Signed-off-by: Francisco Franco <franciscofranco.1990@gmail.com>

commit 214dabdeb2b3f7bc2b1fe1a5128f5deee8ff9ad5
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Mon Mar 16 12:00:32 2015 +0100

    video: msm: enable lcd_notify thanks to dorimax and alucard24
    also added missing yankactive.h file

commit 0033047948ceef8956cdab6aac93e1976135ca49
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Mon Mar 16 11:52:59 2015 +0100

    Various bugfixes, caused by my mistakes

commit c1574ed8b9cbe01e1feed72c7ac77275e5804a13
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun Mar 15 22:54:41 2015 +0100

    CPU: Add Voltage control

commit 14493a8f438c317b072da0109bdaa7fc8081859d
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun Mar 15 22:50:09 2015 +0100

    GPU: Add Voltage control
    Thanks to ktoonsez

commit 28a4df0f499c078e55e213d6e56acd89e68ff0fa
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun Mar 15 22:45:53 2015 +0100

    f2fs: add support for f2fs filesystem

commit f3b8f14ac63b7db7e774ebc277c06bf5f4999d90
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun Mar 15 22:31:00 2015 +0100

    Some misc tweaks and stuff

commit b3a1b153687bdacbf25790c4378a2dd2c9811397
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun Mar 15 22:25:35 2015 +0100

    sched: make gentle_fair_sleepers switchable via sysfs / Stweaks

commit f29e12ddf348eacf5c1cd0c8b45c7525d51acb9a
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun Mar 15 22:23:11 2015 +0100

    cpufreq: added missing stuff for some governors

commit 7e89b634f11ed4370f42f840d619afcc04a264b7
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun Mar 15 22:14:48 2015 +0100

    kernel:sched: LOAD_FREQ (4*HZ+61) avoids loadavg Moire

commit 8b60610a3ee6271ebfaf7fa792cc35e9a27eb21c
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun Mar 15 22:10:08 2015 +0100

     KGSL: Add a simple GPU governor for Adreno xxx GPU series (faux123)
    
    Conflicts:
    	arch/arm/configs/chronic_jf_defconfig
    	drivers/gpu/msm/Kconfig

commit 0c0372c6fccf12c6ba426860f0852044f196ede2
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun Mar 15 22:04:06 2015 +0100

     lib/lzo: huge LZO decompression speedup on ARM by using unaligned access
    
    Signed-off-by: Markus F.X.J. Oberhumer <markus@oberhumer.com>

commit 5d22821e619be1ae74732554bff88a6cc8e31a50
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun Mar 15 20:48:42 2015 +0100

     leds: add LED control (fading & brightness)
    
    (thx AndreiLux)

commit 1049d3708021e790d524c45a1e031770eae0bb62
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun Mar 15 20:35:29 2015 +0100

     jf kernel: brighter, fading LED
    
    Change-Id: Idac135503a37daeb09823c00c1dffb7c4caaea74

commit dedaff852dd07783bb1aaff8fc0a73c6cb3c6e01
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun Mar 15 20:32:30 2015 +0100

     ext4: speed up truncate/unlink by not using bforget() unless needed
    
    Do not iterate over data blocks scanning for bh's to forget as they're
    never exist. This improves time taken by unlink / truncate syscall.
    Tested by continuously truncating file that is being written by dd.
    Another test is rm -rf of linux tree while tar unpacks it. With
    ordered data mode condition unlikely(!tbh) was always met in
    ext4_free_blocks. With journal data mode tbh was found only few times,
    so optimisation is also possible.
    
    Unlinking fallocated 60G file after doing sync && echo 3 >
    /proc/sys/vm/drop_caches && time rm --help
    
    X86 before (linux 3.6-rc4):
    real    0m2.710s
    user    0m0.000s
    sys     0m1.530s
    
    X86 after:
    real    0m0.644s
    user    0m0.003s
    sys     0m0.060s
    
    MIPS before (linux 2.6.37):
    real    0m 4.93s
    user    0m 0.00s
    sys     0m 4.61s
    
    MIPS after:
    real    0m 0.16s
    user    0m 0.00s
    sys     0m 0.06s
    
    Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>
    Signed-off-by: Andrey Sidorov <qrxd43@motorola.com>
    Signed-off-by: franciscofranco <franciscofranco.1990@gmail.com>
    
    Change-Id: Ie78a945cb82b3892eaf88701f2dc3b7726104fb5

commit b18f42bb79ff3075866a6665fb3761cd9637c90e
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun Mar 15 19:45:21 2015 +0100

    drivers: battery: fixed fastchg values thanks to googy and ktoonsez

commit 1096d12807e80a871b0ddbe9916975a1bb43edc1
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun Mar 15 19:15:23 2015 +0100

     fastcharge: initial GT9505 adaptation
    
    Signed-off-by: Paul Reioux <reioux@gmail.com>
    
    Conflicts:
    	arch/arm/mach-msm/Kconfig
    	arch/arm/mach-msm/Makefile

commit 1f75aaad155886c3912588f69aaca7147e665c67
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun Mar 15 18:50:47 2015 +0100

     fs/sync: Make sync() satisfy many requests with one invocation
    
    Dave Jones reported RCU stalls, overly long hrtimer interrupts, and
    amazingly long NMI handlers from a trinity-induced workload involving
    lots of concurrent sync() calls (https://lkml.org/lkml/2013/7/23/369).
    There are any number of things that one might do to make sync() behave
    better under high levels of contention, but it is also the case that
    multiple concurrent sync() system calls can be satisfied by a single
    sys_sync() invocation.
    
    Given that this situation is reminiscent of rcu_barrier(), this commit
    applies the rcu_barrier() approach to sys_sync().  This approach uses
    a global mutex and a sequence counter.  The mutex is held across the
    sync() operation, which eliminates contention between concurrent sync()
    operations.  The counter is incremented at the beginning and end of
    each sync() operation, so that it is odd while a sync() operation is in
    progress and even otherwise, just like sequence locks.
    
    The code that used to be in sys_sync() is now in do_sync(), and sys_sync()
    now handles the concurrency.  The sys_sync() function first takes a
    snapshot of the counter, then acquires the mutex, and then takes another
    snapshot of the counter.  If the values of the two snapshots indicate that
    a full do_sync() executed during the mutex acquisition, the sys_sync()
    function releases the mutex and returns ("Our work is done!").  Otherwise,
    sys_sync() increments the counter, invokes do_sync(), and increments
    the counter again.
    
    This approach allows a single call to do_sync() to satisfy an arbitrarily
    large number of sync() system calls, which should eliminate issues due
    to large numbers of concurrent invocations of the sync() system call.
    
    Changes since v1 (https://lkml.org/lkml/2013/7/24/683):
    
    o	Add a pair of memory barriers to keep the increments from
    	bleeding into the do_sync() code.  (The failure probability
    	is insanely low, but when you have several hundred million
    	devices running Linux, you can expect several hundred instances
    	of one-in-a-million failures.)
    
    o	Actually CC some people who have experience in this area.
    
    Reported-by: Dave Jones <davej@redhat.com>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Alexander Viro <viro@zeniv.linux.org.uk>
    Cc: Christoph Hellwig <hch@lst.de>
    Cc: Jan Kara <jack@suse.cz>
    Cc: Curt Wohlgemuth <curtw@google.com>
    Cc: Jens Axboe <jaxboe@fusionio.com>
    Cc: linux-fsdevel@vger.kernel.org
    
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit fc3f881c02b58f81472dde15dd86a4ec0cbb23e4
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun Mar 15 18:38:13 2015 +0100

     arch/arm/Kconfig: enable unaligned capability for ARM
    
    Signed-off-by: faux123 <reioux@gmail.com>

commit abee30001fda5e22c21571cf3ce1463a33b88bd9
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun Mar 15 18:37:12 2015 +0100

     ARM: 7583/1: decompressor: Enable unaligned memory access for v6 and 
    
    above
    
    Modern GCC can generate code which makes use of the CPU's native
    unaligned memory access capabilities.  This is useful for the C
    decompressor implementations used for unpacking compressed kernels.
    
    This patch disables alignment faults and enables the v6 unaligned
    access model on CPUs which support these features (i.e., v6 and
    later), allowing full unaligned access support for C code in the
    decompressor.
    
    The decompressor C code must not be built to assume that unaligned
    access works if support for v5 or older platforms is included in
    the kernel.
    
    For correct code generation, C decompressor code must always use
    the get_unaligned and put_unaligned accessors when dealing with
    unaligned pointers, regardless of this patch.
    
    Signed-off-by: Dave Martin <dave.martin@linaro.org>
    Acked-by: Nicolas Pitre <nico@linaro.org>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

commit d41b17a104cad694074d1484e4a83f09209da52f
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun Mar 15 18:34:50 2015 +0100

     Optimized ARM RWSEM algorithm
    
    RWSEM implementation for ARM using atomic functions.
    Heavily based on arch/sh/include/asm/rwsem.h
    
    Signed-off-by: Ashwin Chaugule <ashwinc@codeaurora.org>

commit 0bb0615bf2b78b58a4ee3a2c134bf1ac691a65c7
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun Mar 15 18:32:37 2015 +0100

     lib/string: use glibc version
    
    the performance of memcpy and memmove of the general version is very
    inefficient, this patch improved them.
    
    Signed-off-by: Miao Xie <miaox*******>
    Signed-off-by: faux123 <reioux@gmail.com>

commit 8df395480e92019f34e58f11d73ae95e648b0bb9
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun Mar 15 18:29:45 2015 +0100

     lib/memcopy: use glibc version
    
    the kernel's memcpy and memmove is very inefficient. But the glibc version is
    quite fast, in some cases it is 10 times faster than the kernel version. So I
    introduce some memory copy macros and functions of the glibc to improve the
    kernel version's performance.
    
    The strategy of the memory functions is:
    1. Copy bytes until the destination pointer is aligned.
    2. Copy words in unrolled loops.  If the source and destination are not
       aligned in the same way, use word memory operations, but shift and merge
       two read words before writing.
    3. Copy the few remaining bytes.
    
    Signed-off-by: Miao Xie <miaox*******>
    Signed-off-by: faux123 <reioux@gmail.com>

commit 9014bb0b1da1950599fe8f2a31be88f10832914e
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun Mar 15 15:10:47 2015 +0100

     ARM: mm: implement LoUIS API for cache maintenance ops
    
    ARM v7 architecture introduced the concept of cache levels and related
    control registers. New processors like A7 and A15 embed an L2 unified
    cache
    controller that becomes part of the cache level hierarchy. Some
    operations in
    the kernel like cpu_suspend and __cpu_disable do not require a flush of
    the
    entire cache hierarchy to DRAM but just the cache levels belonging to
    the
    Level of Unification Inner Shareable (LoUIS), which in most of ARM v7
    systems
    correspond to L1.
    
    The current cache flushing API used in cpu_suspend and __cpu_disable,
    flush_cache_all(), ends up flushing the whole cache hierarchy since for
    v7 it cleans and invalidates all cache levels up to Level of Coherency
    (LoC) which cripples system performance when used in hot paths like
    hotplug
    and cpuidle.
    
    Therefore a new kernel cache maintenance API must be added to cope with
    latest ARM system requirements.
    
    This patch adds flush_cache_louis() to the ARM kernel cache maintenance
    API.
    
    This function cleans and invalidates all data cache levels up to the
    Level of Unification Inner Shareable (LoUIS) and invalidates the
    instruction
    cache for processors that support it (> v7).
    
    This patch also creates an alias of the cache LoUIS function to
    flush_kern_all
    for all processor versions prior to v7, so that the current cache
    flushing
    behaviour is unchanged for those processors.
    
    v7 cache maintenance code implements a cache LoUIS function that cleans
    and
    invalidates the D-cache up to LoUIS and invalidates the I-cache,
    according
    to the new API.
    
    Reviewed-by: Santosh Shilimkar <santosh.shilimkar@xxxxxx>
    Signed-off-by: Lorenzo Pieralisi <lorenzo.pieralisi@xxxxxxx>
    
    Signed-off-by: Paul Reioux <reioux@gmail.com>
    
    Conflicts:
    	arch/arm/mm/cache-v7.S

commit c00932a9634be53d30699d710d6db269ba1fd2fb
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun Mar 15 14:33:24 2015 +0100

     ARM: Add support for 64 bit register reads/writes
    
    Add macros to read and write to 64 bit registers.
    
    Signed-off-by: Olav Haugan <ohaugan@codeaurora.org>
    
    Conflicts:
    	arch/arm/include/asm/io.h
    
    Change-Id: Ied2b38e6a3803377cd271fe7b13f6cf0fc586e70

commit a0d3a3a65a881b118620edca33c335d0d5adb08e
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun Mar 15 14:28:03 2015 +0100

     jiffies conversions: Use compile time constants when possible
    
    Date	Fri, 04 Jan 2013 13:15:43 -0800
    
    Do the multiplications and divisions at compile time
    instead of runtime when the converted value is a constant.
    
    Make the calculation functions static __always_inline to jiffies.h.
    
    Add #defines with __builtin_constant_p to test and use the
    static inline or the runtime functions as appropriate.
    
    Prefix the old exported symbols/functions with __
    
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit 6d477e4d5b20f8b263cfb2cab7b5c1485d9b7a29
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun Mar 15 14:22:47 2015 +0100

     ARM: mm: update __v7_setup() to the new LoUIS cache maintenance API
    
    The ARMv7 processor setup function __v7_setup() cleans and invalidates the
    CPU cache before enabling MMU to start the CPU with a clean CPU local cache.
    
    But on ARMv7 architectures like Cortex-[A15/A8], this code will end
    up flushing the L2 caches(up to level of Coherency) which is undesirable
    and expensive. The setup functions are used in the CPU hotplug scenario too
    and hence flushing all cache levels should be avoided.
    
    This patch replaces the cache flushing call with the newly introduced
    v7 dcache LoUIS API where only cache levels up to LoUIS are cleaned and
    invalidated when a processors executes __v7_setup which is the expected
    behavior.
    
    For processors like A9 and A5 where the L2 cache is an outer one the
    behavior should be unchanged.
    
    Signed-off-by: Santosh Shilimkar <santosh.shilimkar@xxxxxx>
    Signed-off-by: Lorenzo Pieralisi <lorenzo.pieralisi@xxxxxxx>
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit 931ec4914e6be3e97f224f300db1e1f7cdb9bed9
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun Mar 15 14:21:58 2015 +0100

     ARM: kernel: update __cpu_disable to use cache LoUIS maintenance API
    
    When a CPU is hotplugged out caches that reside in its power domain
    lose their contents and so must be cleaned to the next memory level.
    
    Currently, __cpu_disable calls flush_cache_all() that for new generation
    processor like A15/A7 ends up cleaning and invalidating all cache levels
    up to Level of Coherency, which includes the unified L2.
    
    This ends up being a waste of cycles since the L2 cache contents are not
    lost on power down.
    
    This patch updates __cpu_disable to use the new LoUIS API cache operations.
    
    Reviewed-by: Santosh Shilimkar <santosh.shilimkar@xxxxxx>
    Signed-off-by: Lorenzo Pieralisi <lorenzo.pieralisi@xxxxxxx>
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit 6a07eb742fae49371ca7fac9ef702677328deddd
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun Mar 15 14:20:33 2015 +0100

     ARM: kernel: update cpu_suspend code to use cache LoUIS operations
    
    In processors like A15/A7 L2 cache is unified and integrated within the
    processor cache hierarchy, so that it is not considered an outer cache
    anymore. For processors like A15/A7 flush_cache_all() ends up cleaning
    all cache levels up to Level of Coherency (LoC) that includes
    the L2 unified cache.
    
    When a single CPU is suspended (CPU idle) a complete L2 clean is not
    required, so generic cpu_suspend code must clean the data cache using the
    newly introduced cache LoUIS function.
    
    The context and stack pointer (context pointer) are cleaned to main memory
    using cache area functions that operate on MVA and guarantee that the data
    is written back to main memory (perform cache cleaning up to the Point of
    Coherency - PoC) so that the processor can fetch the context when the MMU
    is off in the cpu_resume code path.
    
    outer_cache management remains unchanged.
    
    Reviewed-by: Santosh Shilimkar <santosh.shilimkar@xxxxxx>
    Signed-off-by: Lorenzo Pieralisi <lorenzo.pieralisi@xxxxxxx>
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit 7b589f22e0927950d14bebae6d7f4ba03c996e64
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun Mar 15 14:18:31 2015 +0100

     ARM: mm: rename jump labels in v7_flush_dcache_all function
    
    This patch renames jump labels in v7_flush_dcache_all in order to define
    a specific flush cache levels entry point.
    
    TODO: factor out the level flushing loop if considered worthwhile and
          define the input registers requirements.
    
    Signed-off-by: Lorenzo Pieralisi <lorenzo.pieralisi@xxxxxxx>
    Signed-off-by: Paul Reioux <reioux@gmail.com>
    
    Conflicts:
    	arch/arm/mm/cache-v7.S

commit b9ac93f72f5b5f57924ee39a831704a38aca0635
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun Mar 15 14:07:53 2015 +0100

     Asynchronous Fsync: initial extraction of Async Fsync from HTC
    
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit 3f02e94500ab28a97c7b1b13978012b09edbf1c3
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun Mar 15 13:58:27 2015 +0100

     arm: vfpmodule: Fix warning procfs vfp_bounce reporting failed
    
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit d0ddbcd97d25e5e700cd2e1f1e1133731e42bb39
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun Mar 15 13:56:21 2015 +0100

     ARM: only allow kernel mode neon with AEABI
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

commit 72c2bee8e1beb8dfa2a768afcfb1183810fb42a0
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun Mar 15 13:55:24 2015 +0100

     ARM: crypto: add NEON accelerated XOR implementation
    
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Acked-by: Nicolas Pitre <nico@linaro.org>
    
    ARM: 7835/2: fix modular build of xor_blocks() with NEON enabled
    
    Reported-by: Josh Boyer <jwboyer@fedoraproject.org>
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

commit 4597ee26372edf4d851930ad266b5003f22f4f90
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun Mar 15 13:50:26 2015 +0100

     ARM: add support for kernel mode NEON
    
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit 4c893dcdfe3a4b346244315427296662df8015ad
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun Mar 15 13:45:56 2015 +0100

     ARM: be strict about FP exceptions in kernel mode
    
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel at linaro.org>
    Acked-by: Nicolas Pitre <nico at linaro.org>
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit c347364a95ce0fe0f758075c4bcc07cb4461ceb7
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun Mar 15 13:43:21 2015 +0100

     ARM: move VFP init to an earlier boot stage
    
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit b15be6308b34abde69f76c0354fd53c98e3d56cc
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun Mar 15 13:37:33 2015 +0100

     sched: reinitialize rq->next_balance when a CPU is hot-added
    
    Signed-off-by: Paul Walmsley <pwalmsley@nvidia.com>
    Cc: Peter Boonstoppel <pboonstoppel@nvidia.com>
    Reviewed-on: http://git-master/r/208927
    Reviewed-by: Automatic_Commit_Validation_User
    Reviewed-by: Peter Zu <pzu@nvidia.com>
    Reviewed-by: Peter Boonstoppel <pboonstoppel@nvidia.com>
    GVS: Gerrit_Virtual_Submit
    Tested-by: Peter Zu <pzu@nvidia.com>
    Reviewed-by: Yu-Huan Hsu <yhsu@nvidia.com>

commit 6ed590685cab47e85dc92283e925132e30a48217
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun Mar 15 02:37:05 2015 +0100

     SELinux: include definition of new capabilities
    
    Signed-off-by: Eric Paris <eparis@redhat.com>
    Signed-off-by: James Morris <james.l.morris@oracle.com>
    [sds: rename epollwakeup to block_suspend to match upstream merge]
    Signed-off-by: Stephen Smalley <sds@tycho.nsa.gov>

commit 52f514822a4a7452384493519684cc9c0ceb8b2e
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun Mar 15 02:32:26 2015 +0100

    Changes for linaro compilation

commit 70b9d964eb645b257e6f5e2eb0e616700cc5087b
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun Mar 15 02:25:11 2015 +0100

     net: flow: Prevent bringing up new CPUs during per-CPU initialization
    
    Signed-off-by: Matt Wagantall <mattw@codeaurora.org>

commit fe43286187d08a4a302711a6ef049c1924c160cf
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun Mar 15 02:23:51 2015 +0100

     sync: don't block the flusher thread waiting on IO
    
    Signed-off-by: Dave Chinner <dchinner@redhat.com>
    Reviewed-by: Jan Kara <jack@suse.cz>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit 8a40114a40c4b0ed87200a70e49e107a1c08f398
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun Mar 15 02:16:59 2015 +0100

     writeback: fix writeback cache thrashing
    
    From: Namjae Jeon <namjae.jeon@samsung.com>

commit 14a3c871bb599829ca485f01a0e894303d250822
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun Mar 15 02:16:18 2015 +0100

     fs-writeback: Fix occasional slow sync
    
    Signed-off-by: Jan Kara <jack@suse.cz>

commit 3c3c0122b7cb8e50806ca4c46fe9dbb8499ca05d
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun Mar 15 02:14:21 2015 +0100

    Added missing files

commit 29dd9aa89f25d6db5d955f7b25e021aec2934579
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun Mar 15 02:12:15 2015 +0100

    Replace the offending argument earlier in the boot process

commit 5c7d688e3cba27f4228512702b1005ddfeb50914
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun Mar 15 02:09:31 2015 +0100

    drivers:staging enable android logger interface

commit ab7ce84abb0bd06c224b6aa20f5a6d64e33efb78
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun Mar 15 02:05:12 2015 +0100

    Remove unused dirty versions

commit 9729e811b2ab24d8ea1b40a349f6ded3e62cd8ff
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun Mar 8 19:24:05 2015 +0100

    block: added BFQ, FIFO, FIOPS, SIOPLUS, ZEN, VR, SIO

commit 1299ef4543adab9271438b0c2831631a1cd34249
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun Mar 8 18:58:47 2015 +0100

    Added Faux sound control version 3.6 (thanks to googy_anas)

commit e8c86e0f49c027e78a107dcddec8a2e91a0cd20d
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Sun Mar 8 18:29:53 2015 +0100

    cpufreq: added all previous governors back

commit 3e970bcc219123fc4d659d44ed4f701e41302a6f
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Fri Mar 6 19:07:42 2015 +0100

    Fix compiling errors

commit 149abb79680fbcd159f58fc55fc70eecfc5d2825
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Fri Mar 6 12:10:51 2015 +0100

    Linux 3.4.106

commit 1fe090470bdb10948263c1e4fa12f219ac778185
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Fri Mar 6 12:06:26 2015 +0100

    Linux 3.4.105

commit 0beab8cbaf390ff7fe1e45bd117b85f487759d50
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Fri Mar 6 11:53:46 2015 +0100

    Linux 3.4.104

commit bf2ef79d3adced9284f81c4bcc1931682fbe0558
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Fri Mar 6 11:49:16 2015 +0100

    Linux 3.4.103

commit 58c92d7b8b875bfc87fa1ce9ef9bd93cb1a868c3
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Fri Mar 6 11:48:50 2015 +0100

    Linux 3.4.102

commit fe360f4c0d2cd629a1a6fd580bbabe8557c29e75
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Fri Mar 6 11:47:55 2015 +0100

    Linux 3.4.101

commit c5ec7819b02eb110839f23946fb0ef0d94c2bcb1
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Fri Mar 6 11:47:34 2015 +0100

    Linux 3.4.100

commit be7875f9f5e3c2e7748f7de9f5236de136be1b3f
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Fri Mar 6 11:47:19 2015 +0100

    Linux 3.4.99

commit 162569804e5265fab3b7c12ee62e1d164c00ecf9
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Fri Mar 6 11:46:58 2015 +0100

    Linux 3.4.98

commit a04308d239cb754c94ccdc4e4660b6ec4f6e7eca
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Fri Mar 6 11:44:58 2015 +0100

    Linux 3.4.97

commit 8124bf890dd6b95b48f375a3e9b6afd3ea4285d5
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Fri Mar 6 11:44:39 2015 +0100

    Linux 3.4.96

commit 57ee41804c9bba8cf9e5f5fdf6869c9b08519a8e
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Fri Mar 6 11:35:25 2015 +0100

    Linux 3.4.95

commit ed906a4ca563d24b9d7d956b733674a25b121982
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Fri Mar 6 00:31:57 2015 +0100

    Linux 3.4.94

commit a85506f076c46b17caf49e0b8c724d08f8fa4717
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Fri Mar 6 00:29:14 2015 +0100

    Linux 3.4.93

commit c62c5b4b28fc60fa6f695a6cd44fb9093f798ec8
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Fri Mar 6 00:13:08 2015 +0100

    Linux 3.4.92

commit a231079f7c8e58987a70a9e37210538cb0de65e7
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Fri Mar 6 00:06:57 2015 +0100

    Linux 3.4.91

commit 0d3f98441fea99c1769d564a19825739aa657f1c
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Fri Mar 6 00:06:39 2015 +0100

    Linux 3.4.90

commit 432770d35094f0393eae061364a622b8aeac184b
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Fri Mar 6 00:06:16 2015 +0100

    Linux 3.4.89

commit 7843e5c416eba863fa6c9ddd02f43b14edeff353
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Fri Mar 6 00:03:13 2015 +0100

    Linux 3.4.88

commit 554adb97012f7e37c4b0887d53184ce45ab0ebea
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Fri Mar 6 00:01:27 2015 +0100

    Linux 3.4.87

commit edd82183201927df7d2ca8dec98e29da5aa1d7e0
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Mar 5 23:53:11 2015 +0100

    Linux 3.4.86

commit f0fadd5f0f56e9be6375990b02d684a906ac4c34
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Mar 5 23:52:56 2015 +0100

    Linux 3.4.85

commit 9de3413ce0c95b92e6119efd4dbc27aa2062173c
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Mar 5 23:52:43 2015 +0100

    Linux 3.4.84

commit 95aa9c441cd6249250eb9732b1a365ac0662a0f0
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Mar 5 23:52:27 2015 +0100

    Linux 3.4.83

commit 6e341c63716a9ccc417698027f2816f7012ca85e
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Mar 5 23:48:19 2015 +0100

    Linux 3.4.82

commit 5bf76e4a44a3c1d8dd2c9034d08b17d38289fa39
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Mar 5 23:45:31 2015 +0100

    Linux 3.4.81

commit f6adf903a79f14f0664ca49198219f0aa463802a
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Mar 5 23:44:22 2015 +0100

    Linux 3.4.80

commit 24821e8ea3886256466a6e0b908bb50924461764
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Mar 5 23:41:12 2015 +0100

    Linux 3.4.79

commit 1740eb34bbf11fd6f514d85dfd3075809ac8a7ba
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Mar 5 23:40:52 2015 +0100

    Linux 3.4.78

commit 20caf341c5035e81e7f2440adef9821721ee180e
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Mar 5 23:40:30 2015 +0100

    Linux 3.4.77

commit 21a57f7a2c5f5e84b6959b0e095eb679a57781b1
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Mar 5 23:40:12 2015 +0100

    Linux 3.4.76

commit 7ccdc3f6e1e4672e5245f183031f4b476704b4b8
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Mar 5 23:38:15 2015 +0100

    Linux 3.4.75

commit c15cde1bc9a9493d32893b089e9f8cf0cd6f79f6
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Mar 5 23:37:58 2015 +0100

    Linux 3.4.74

commit b0b570d9ab81af82e8f9c6812d94c3b8546ca8a7
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Mar 5 23:35:33 2015 +0100

    Linux 3.4.73

commit 2b1696d42b895b296fdf4be937a324c1350088fa
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Mar 5 23:22:09 2015 +0100

    Linux 3.4.72

commit 63840502d6a47c38eb8a9a3e6e54e975a106faf4
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Mar 5 23:20:35 2015 +0100

    Small fixes

commit dcd974be53a32ccf33b89a36760e6b84affa120c
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Mar 5 22:57:41 2015 +0100

    Linux 3.4.71

commit 97f93f6bede4ce81953d489a959ab20a399ad9e0
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Mar 5 22:54:09 2015 +0100

    Linux 3.4.70

commit 6d8c5f77aa08744c6e4a0ad77d0c85e35e3e9704
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Mar 5 22:53:47 2015 +0100

    Linux 3.4.69

commit f62a24320c8dc1d2ef3b2e748722a8867d87682b
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Mar 5 22:53:31 2015 +0100

    Linux 3.4.68

commit 7d6f0c29ecc1eb83bd939ee95787fcd20760d1c5
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Mar 5 22:53:12 2015 +0100

    Linux 3.4.67

commit e2111bf373ae3c1fea62700d48bb4369f2786e87
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Mar 5 22:52:55 2015 +0100

    Linux 3.4.66

commit 2bbdf6c91ca9928c26b81340da706ee668f8247f
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Mar 5 22:50:51 2015 +0100

    Linux 3.4.65

commit 8ef7b8bf51d2d74c856e2581d3a53b7f067f89bd
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Mar 5 22:50:29 2015 +0100

    Linux 3.4.64

commit 0bfb310a01ca70ca70034269be9600329e9dcb19
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Mar 5 22:48:14 2015 +0100

    Linux 3.4.63

commit ad67cbdafd6549b088f7e153615970cb45600ee7
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Mar 5 22:44:55 2015 +0100

    Linux 3.4.62

commit f9faf9c9c0cdb741a0415363bdc541a2d30ad363
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Mar 5 22:44:22 2015 +0100

    Linux 3.4.61

commit 526632dfbaa1f6a134634b11cc13cfe215c3624b
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Mar 5 22:44:04 2015 +0100

    Linux 3.4.60

commit bdfff1e6f11136278e8cef83bdaca04c6da1dc83
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Mar 5 22:43:49 2015 +0100

    Linux 3.4.59

commit bf585fb1cfec842f922da322b6b1e26595a873cb
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Mar 5 22:41:12 2015 +0100

    Linux 3.4.58

commit b0c918d3259a7569b7b844317f9bb57d5de77024
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Mar 5 22:40:59 2015 +0100

    Linux 3.4.57

commit fd8c6c8322c2b87880a8bf662660380cb55909c5
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Mar 5 22:40:42 2015 +0100

    Linux 3.4.56

commit c69626d298cd6a38e51e16c2d749e78654fd55e1
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Mar 5 22:33:38 2015 +0100

    Linux 3.4.55

commit 56a21b53922ce402e3d1d46c94cc4857c289614a
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Mar 5 22:31:42 2015 +0100

    Linux 3.4.54

commit 03117c50110d9b0f9113f61d0e18763f1e26c620
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Mar 5 22:31:12 2015 +0100

    Some compiling errors and their fixes

commit eb29d6e73008b8804ba6d9597ddd18707393c9d4
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Mar 5 21:44:08 2015 +0100

    Linux 3.4.53

commit 0107de4d5ede12d5b2ac7b0115aa4cbda717f1de
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Mar 5 21:42:35 2015 +0100

    Linux 3.4.52

commit 0b128a0840eaf1447d3a1b05fb2a88ee364b73e9
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Mar 5 21:42:18 2015 +0100

    Linux 3.4.51

commit 460ddbe4c82b8229449a012e78313e084022d33c
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Mar 5 21:41:48 2015 +0100

    Linux 3.4.50

commit 9b1ea7d1ada56a4d95ac80e513baa09594328b4b
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Mar 5 21:38:22 2015 +0100

    Linux 3.4.49

commit a4bf9f7c5a6a401917da261ef03c31fc94a93c5a
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Mar 5 21:38:00 2015 +0100

    Linux 3.4.48

commit 4f3066dea7509b2b8b9ba2d0009029ff2614ef14
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Mar 5 21:37:34 2015 +0100

    Linux 3.4.47

commit e729b3aa56fb2479c5b4cd516dd3fae1b160bf13
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Mar 5 21:37:16 2015 +0100

    Linux 3.4.46

commit 4b93c9569da136d9b08ffd9c414dedd81bb7c64a
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Mar 5 21:35:52 2015 +0100

    Linux 3.4.45

commit 371193df8e5d8f8e937e146829f79f40ba43a1c2
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Mar 5 21:34:57 2015 +0100

    Linux 3.4.44

commit b687f3323560ea89f5583daf0a72d618e7a2cf07
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Mar 5 21:28:28 2015 +0100

    Linux 3.4.43

commit 4a57bcf01c79cb67d5d3dea828b86ab5f7ad303b
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Mar 5 21:25:38 2015 +0100

    Linux 3.4.42

commit 7d38537138271f1c8ba824aeffc7e28d2feda016
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Mar 5 21:21:42 2015 +0100

    Linux 3.4.41

commit a900df9aaeb014be7c9ad09c1f7f5b1f3294a3de
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Mar 5 21:20:49 2015 +0100

    Fixed some compilation errors

commit 130d170861c1a94a38f3b1ca58963b31a27bf0f8
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Mar 5 19:11:16 2015 +0100

    Linux 3.4.40

commit 1adb958cf4536952d77f503b14f85fb2c4cf6535
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Mar 5 19:10:51 2015 +0100

    Linux 3.4.39

commit be5381718171cf306c7f745baaa53ee04446f53f
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Mar 5 19:03:24 2015 +0100

    Linux 3.4.38

commit 2982b641928091d9b13a480c89c83392e81bd8db
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Mar 5 19:01:55 2015 +0100

    Linux 3.4.37

commit 9de7acc2cfaa23649e74ae808ab8d5a5e7159982
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Mar 5 18:55:31 2015 +0100

    Linux 3.4.36

commit 393358e179233fdc57b70a94fc675f215f8e6b45
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Mar 5 18:55:03 2015 +0100

    Linux 3.4.35

commit 9b4ce8346b3de7ba4959a1a81c80ceb1ef516418
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Mar 5 18:51:57 2015 +0100

    Linux 3.4.34

commit 0e84fcc4261d04e7296d166e9b0e643d916c476d
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Mar 5 18:49:51 2015 +0100

    Linux 3.4.33

commit 1ae1ffa9f218ab4830c1e5931eeb687fbbf8478f
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Mar 5 18:49:33 2015 +0100

    Linux 3.4.32

commit 1dff302b0b08594df5dd4a7d9bc3615d17e077e9
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Mar 5 18:49:14 2015 +0100

    Linux 3.4.31

commit a033c433d7df2cab39aba4fa1939030ba7b5c5d3
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Mar 5 15:01:18 2015 +0100

    Linux 3.4.30

commit d5d2bcfe132ebac7320355edb2e639fcfd9e82d9
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Mar 5 15:00:43 2015 +0100

    Linux 3.4.29

commit aa91c255230da7e37efae5f6fac5c26dd70509f8
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Mar 5 14:40:52 2015 +0100

    Linux 3.4.28

commit 7a405061a758c482c1b5ce33e6f02dd793668e6e
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Mar 5 14:37:21 2015 +0100

    Linux 3.4.27

commit 09c40763475caf792ba06f62c847f6b45cf7b37f
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Mar 5 14:36:50 2015 +0100

    Linux 3.4.26

commit 8733ce2304de9ac304c01ae050636f99cacf6290
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Mar 5 14:29:38 2015 +0100

    Linux 3.4.25

commit f108c5cf91ca402ca63ed06c4094d6b5f7ae45ad
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Mar 5 14:22:00 2015 +0100

    Linux 3.4.24

commit 1b82554fa48cffeeac0e8456eb8fc8b7fa8aa874
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Mar 5 14:21:36 2015 +0100

    Linux 3.4.23

commit 9e61e31c28a4e739af72c79b00aad1ae1791a806
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Mar 5 14:18:55 2015 +0100

    Linux 3.4.22

commit c967564a8eda4a021b0b04b14226cd999c8c236a
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Mar 5 14:17:30 2015 +0100

    Linux 3.4.21

commit 590d38b14c97c5951c7052b2026c6812329d439d
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Mar 5 14:10:03 2015 +0100

    Linux 3.4.20

commit b57227fcb874f17e0363a1f3fb57b53d84f313f3
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Mar 5 14:07:27 2015 +0100

    Linux 3.4.19

commit 5cc9f8e5aee2ff1c9150fdd9414353044793d152
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Mar 5 14:05:59 2015 +0100

    Linux 3.4.18

commit 43cb3f636f8f9b8efe3d680aa40552ff9c76b0e6
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Mar 5 14:02:39 2015 +0100

    Linux 3.4.17

commit 0a5d08639b708a487b30e4a09af876cc14136d56
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Mar 5 13:59:44 2015 +0100

    Linux 3.4.16

commit 68c04fda90cb4c0b9e4d66a8c2bf9346c1fde661
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Mar 5 13:56:55 2015 +0100

    Linux 3.4.15

commit 4216cdc9fa72cf1a3a4450c41329fb680c00108d
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Mar 5 13:51:47 2015 +0100

    Linux 3.4.14

commit ff8f2aa98b751bcee7712a07b0e231d198085942
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Mar 5 13:48:05 2015 +0100

    Linux 3.4.13

commit 9a87d341a6000e55e223a8e0ba97a26775da90c0
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Mar 5 13:45:29 2015 +0100

    Linux 3.4.12

commit 2d11605085eb530e867dc1fa88ea5808f9d0d0a9
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Mar 5 12:15:50 2015 +0100

    Linux 3.4.11

commit d65045ac2639b4008ea256d8d8cfdca5137b7fad
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Mar 5 11:59:49 2015 +0100

    Linux 3.4.10

commit ec32dde45d05704292d3b3696ca489c94439e5b6
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Mar 5 11:53:57 2015 +0100

    Linux 3.4.9

commit 2bb1fa88b8e61f5c67e9919f7190f7f4841ee1a0
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Mar 5 11:44:42 2015 +0100

    Linux 3.4.8

commit 16382927aec0cb1a61343fe7f5734641ce66abeb
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Mar 5 11:38:39 2015 +0100

    Linux 3.4.7

commit 0dbdf1bd840a5a0f7ad74d42321bec671160848b
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Mar 5 11:38:08 2015 +0100

    Linux 3.4.6

commit 5b021d808626664a13c6c6cb638d1064612af4b4
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Mar 5 11:37:10 2015 +0100

    Linux 3.4.5

commit 117eae91f6c2ab5e5a34f37048cff1b6f6db637e
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Mar 5 00:31:46 2015 +0100

    Linux 3.4.4

commit 39185c28fafa56f55ecec4fe0acc020f55131cbb
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Mar 5 00:30:31 2015 +0100

    Linux 3.4.3

commit be26b08504528bac254e5794f98f409ac6d06460
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Mar 5 00:28:32 2015 +0100

    Linux 3.4.2

commit aba0739be6ef7cbe13f953dc999fc559a3803bac
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Mar 5 00:24:54 2015 +0100

    Linux 3.4.1

commit 83a607f1eb7aef8bc5cf23f3f68f3f73ff0cb372
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Thu Mar 5 00:04:59 2015 +0100

    Initial changes to set it up with my system

commit 94625c6e9624bf6adf0ccc691c3614535ba0a327
Merge: 0dd931d b796073
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Mar 4 23:31:11 2015 +0100

    Merge branch 'master' of https://github.com/Tkkg1994/Hulk-Kernel-V2

commit b796073c4e1309dae8be1600640e729b310ab975
Author: Luca Grifo <luca.grifo@outlook.com>
Date:   Wed Mar 4 23:30:34 2015 +0100

    Initial commit

commit 0dd931df97699435571fc51dd1299295f8c14a07
Author: Tkkg1994 <luca.grifo@outlook.com>
Date:   Wed Mar 4 23:29:07 2015 +0100

    Initial commit I9505XXUHOA7
